doi,url,title,abstract,published
10.1145/3545947.3569630,http://arxiv.org/pdf/2212.05113v1.pdf,"Automatically Generating CS Learning Materials with Large Language
  Models","Recent breakthroughs in Large Language Models (LLMs), such as GPT-3 and
Codex, now enable software developers to generate code based on a natural
language prompt. Within computer science education, researchers are exploring
the potential for LLMs to generate code explanations and programming
assignments using carefully crafted prompts. These advances may enable students
to interact with code in new ways while helping instructors scale their
learning materials. However, LLMs also introduce new implications for academic
integrity, curriculum design, and software engineering careers. This workshop
will demonstrate the capabilities of LLMs to help attendees evaluate whether
and how LLMs might be integrated into their pedagogy and research. We will also
engage attendees in brainstorming to consider how LLMs will impact our field.",2022-12-09T20:37:44Z
10.1007/978-3-031-64302-6_19,http://arxiv.org/pdf/2310.05292v4.pdf,"How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent
  for Debugging","Large Language Models (LLMs) now excel at generative skills and can create
content at impeccable speeds. However, they are imperfect and still make
various mistakes. In a Computer Science education context, as these models are
widely recognized as ""AI pair programmers,"" it becomes increasingly important
to train students on evaluating and debugging the LLM-generated code. In this
work, we introduce HypoCompass, a novel system to facilitate deliberate
practice on debugging, where human novices play the role of Teaching Assistants
and help LLM-powered teachable agents debug code. We enable effective task
delegation between students and LLMs in this learning-by-teaching environment:
students focus on hypothesizing the cause of code errors, while adjacent skills
like code completion are offloaded to LLM-agents. Our evaluations demonstrate
that HypoCompass generates high-quality training materials (e.g., bugs and
fixes), outperforming human counterparts fourfold in efficiency, and
significantly improves student performance on debugging by 12% in the
pre-to-post test.",2023-10-08T21:39:47Z
,http://arxiv.org/pdf/2405.18062v2.pdf,Towards Integrating Emerging AI Applications in SE Education,"Artificial Intelligence (AI) approaches have been incorporated into modern
learning environments and software engineering (SE) courses and curricula for
several years. However, with the significant rise in popularity of large
language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in
particular in the last year, educators are faced with rapidly changing
classroom environments and disrupted teaching principles. Examples range from
programming assignment solutions that are fully generated via ChatGPT, to
various forms of cheating during exams. However, despite these negative aspects
and emerging challenges, AI tools in general, and LLM applications in
particular, can also provide significant opportunities in a wide variety of SE
courses, supporting both students and educators in meaningful ways. In this
early research paper, we present preliminary results of a systematic analysis
of current trends in the area of AI, and how they can be integrated into
university-level SE curricula, guidelines, and approaches to support both
instructors and learners. We collected both teaching and research papers and
analyzed their potential usage in SE education, using the ACM Computer Science
Curriculum Guidelines CS2023. As an initial outcome, we discuss a series of
opportunities for AI applications and further research areas.",2024-05-28T11:21:45Z
,http://arxiv.org/pdf/2401.12453v1.pdf,"""The teachers are confused as well"": A Multiple-Stakeholder Ethics
  Discussion on Large Language Models in Computing Education","Large Language Models (LLMs) are advancing quickly and impacting people's
lives for better or worse. In higher education, concerns have emerged such as
students' misuse of LLMs and degraded education outcomes. To unpack the ethical
concerns of LLMs for higher education, we conducted a case study consisting of
stakeholder interviews (n=20) in higher education computer science. We found
that students use several distinct mental models to interact with LLMs - LLMs
serve as a tool for (a) writing, (b) coding, and (c) information retrieval,
which differ somewhat in ethical considerations. Students and teachers brought
up ethical issues that directly impact them, such as inaccurate LLM responses,
hallucinations, biases, privacy leakage, and academic integrity issues.
Participants emphasized the necessity of guidance and rules for the use of LLMs
in higher education, including teaching digital literacy, rethinking education,
and having cautious and contextual policies. We reflect on the ethical
challenges and propose solutions.",2024-01-23T02:43:00Z
,http://arxiv.org/pdf/2405.20183v1.pdf,"A Survey Study on the State of the Art of Programming Exercise
  Generation using Large Language Models","This paper analyzes Large Language Models (LLMs) with regard to their
programming exercise generation capabilities. Through a survey study, we
defined the state of the art, extracted their strengths and weaknesses and
finally proposed an evaluation matrix, helping researchers and educators to
decide which LLM is the best fitting for the programming exercise generation
use case. We also found that multiple LLMs are capable of producing useful
programming exercises. Nevertheless, there exist challenges like the ease with
which LLMs might solve exercises generated by LLMs. This paper contributes to
the ongoing discourse on the integration of LLMs in education.",2024-05-30T15:49:34Z
,http://arxiv.org/pdf/2403.18679v2.pdf,"An Exploratory Study on Upper-Level Computing Students' Use of Large
  Language Models as Tools in a Semester-Long Project","Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are
influencing software engineering practice. Software engineering educators must
teach future software engineers how to use such tools well. As of yet, there
have been few studies that report on the use of LLMs in the classroom. It is,
therefore, important to evaluate students' perception of LLMs and possible ways
of adapting the computing curriculum to these shifting paradigms.
  Purpose: The purpose of this study is to explore computing students'
experiences and approaches to using LLMs during a semester-long software
engineering project.
  Design/Method: We collected data from a senior-level software engineering
course at Purdue University. This course uses a project-based learning (PBL)
design. The students used LLMs such as ChatGPT and Copilot in their projects. A
sample of these student teams were interviewed to understand (1) how they used
LLMs in their projects; and (2) whether and how their perspectives on LLMs
changed over the course of the semester. We analyzed the data to identify
themes related to students' usage patterns and learning outcomes.
  Results/Discussion: When computing students utilize LLMs within a project,
their use cases cover both technical and professional applications. In
addition, these students perceive LLMs to be efficient tools in obtaining
information and completion of tasks. However, there were concerns about the
responsible use of LLMs without being detrimental to their own learning
outcomes. Based on our findings, we recommend future research to investigate
the usage of LLM's in lower-level computer engineering courses to understand
whether and how LLMs can be integrated as a learning aid without hurting the
learning outcomes.",2024-03-27T15:21:58Z
,http://arxiv.org/pdf/2309.14534v3.pdf,"Teach AI How to Code: Using Large Language Models as Teachable Agents
  for Programming Education","This work investigates large language models (LLMs) as teachable agents for
learning by teaching (LBT). LBT with teachable agents helps learners identify
knowledge gaps and discover new knowledge. However, teachable agents require
expensive programming of subject-specific knowledge. While LLMs as teachable
agents can reduce the cost, LLMs' expansive knowledge as tutees discourages
learners from teaching. We propose a prompting pipeline that restrains LLMs'
knowledge and makes them initiate ""why"" and ""how"" questions for effective
knowledge-building. We combined these techniques into TeachYou, an LBT
environment for algorithm learning, and AlgoBo, an LLM-based tutee chatbot that
can simulate misconceptions and unawareness prescribed in its knowledge state.
Our technical evaluation confirmed that our prompting pipeline can effectively
configure AlgoBo's problem-solving performance. Through a between-subject study
with 40 algorithm novices, we also observed that AlgoBo's questions led to
knowledge-dense conversations (effect size=0.71). Lastly, we discuss design
implications, cost-efficiency, and personalization of LLM-based teachable
agents.",2023-09-25T21:20:04Z
,http://arxiv.org/pdf/2401.16186v1.pdf,"An Empirical Study on Usage and Perceptions of LLMs in a Software
  Engineering Project","Large Language Models (LLMs) represent a leap in artificial intelligence,
excelling in tasks using human language(s). Although the main focus of
general-purpose LLMs is not code generation, they have shown promising results
in the domain. However, the usefulness of LLMs in an academic software
engineering project has not been fully explored yet. In this study, we explore
the usefulness of LLMs for 214 students working in teams consisting of up to
six members. Notably, in the academic course through which this study is
conducted, students were encouraged to integrate LLMs into their development
tool-chain, in contrast to most other academic courses that explicitly prohibit
the use of LLMs.
  In this paper, we analyze the AI-generated code, prompts used for code
generation, and the human intervention levels to integrate the code into the
code base. We also conduct a perception study to gain insights into the
perceived usefulness, influencing factors, and future outlook of LLM from a
computer science student's perspective. Our findings suggest that LLMs can play
a crucial role in the early stages of software development, especially in
generating foundational code structures, and helping with syntax and error
debugging. These insights provide us with a framework on how to effectively
utilize LLMs as a tool to enhance the productivity of software engineering
students, and highlight the necessity of shifting the educational focus toward
preparing students for successful human-AI collaboration.",2024-01-29T14:32:32Z
,http://arxiv.org/pdf/2406.13972v1.pdf,"CREF: An LLM-based Conversational Software Repair Framework for
  Programming Tutors","Program repair techniques offer cost-saving benefits for debugging within
software development and programming education scenarios. With the proven
effectiveness of Large Language Models (LLMs) in code-related tasks,
researchers have explored their potential for program repair. However, it is
crucial to recognize that existing repair benchmarks may have influenced LLM
training data, potentially causing data leakage. To evaluate LLMs' realistic
repair capabilities, (1) we introduce an extensive, non-crawled benchmark,
referred to as TutorCode, comprising 1,239 C++ defect codes and associated
information such as tutor guidance, solution description, failing test cases,
and the corrected code. Our work assesses the repair performance of 12 LLMs on
TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision
(RPSR). (2) We then provide a comprehensive investigation into which types of
extra information can help LLMs improve their performance in repairing defects.
Among these types, tutor guidance was found to be the most effective
information in enhancing LLM repair capabilities. To fully harness LLMs'
conversational capabilities and the benefits of augmented information, (3) we
introduce a novel conversational semi-automatic repair framework CREF assisting
human tutor. It demonstrates a remarkable AVG-5 improvement of 17.2%-24.6%
compared to the baseline, achieving an impressive AVG-5 of 76.6% when utilizing
GPT-4. These results highlight the potential for enhancing LLMs' repair
capabilities through interactions with tutors and historical conversations
involving incorrect responses. The successful application of CREF in a
real-world educational setting demonstrates its effectiveness in reducing
tutors' workload and improving students' learning experience, while also
showcasing its promise for facilitating other software engineering tasks, such
as code review.",2024-06-20T03:36:34Z
,http://arxiv.org/pdf/2307.02792v2.pdf,What Should Data Science Education Do with Large Language Models?,"The rapid advances of large language models (LLMs), such as ChatGPT, are
revolutionizing data science and statistics. These state-of-the-art tools can
streamline complex processes. As a result, it reshapes the role of data
scientists. We argue that LLMs are transforming the responsibilities of data
scientists, shifting their focus from hands-on coding, data-wrangling and
conducting standard analyses to assessing and managing analyses performed by
these automated AIs. This evolution of roles is reminiscent of the transition
from a software engineer to a product manager. We illustrate this transition
with concrete data science case studies using LLMs in this paper. These
developments necessitate a meaningful evolution in data science education.
Pedagogy must now place greater emphasis on cultivating diverse skillsets among
students, such as LLM-informed creativity, critical thinking, AI-guided
programming. LLMs can also play a significant role in the classroom as
interactive teaching and learning tools, contributing to personalized
education. This paper discusses the opportunities, resources and open
challenges for each of these directions. As with any transformative technology,
integrating LLMs into education calls for careful consideration. While LLMs can
perform repetitive tasks efficiently, it's crucial to remember that their role
is to supplement human intelligence and creativity, not to replace it.
Therefore, the new era of data science education should balance the benefits of
LLMs while fostering complementary human expertise and innovations. In
conclusion, the rise of LLMs heralds a transformative period for data science
and its education. This paper seeks to shed light on the emerging trends,
potential opportunities, and challenges accompanying this paradigm shift,
hoping to spark further discourse and investigation into this exciting,
uncharted territory.",2023-07-06T06:07:29Z
,http://arxiv.org/pdf/2401.10759v1.pdf,"Interactions with Prompt Problems: A New Way to Teach Programming with
  Large Language Models","Large Language Models (LLMs) have upended decades of pedagogy in computing
education. Students previously learned to code through \textit{writing} many
small problems with less emphasis on code reading and comprehension. Recent
research has shown that free code generation tools powered by LLMs can solve
introductory programming problems presented in natural language with ease. In
this paper, we propose a new way to teach programming with Prompt Problems.
Students receive a problem visually, indicating how input should be transformed
to output, and must translate that to a prompt for an LLM to decipher. The
problem is considered correct when the code that is generated by the student
prompt can pass all test cases. In this paper we present the design of this
tool, discuss student interactions with it as they learn, and provide insights
into this new class of programming problems as well as the design tools that
integrate LLMs.",2024-01-19T15:32:46Z
,http://arxiv.org/pdf/2402.07913v2.pdf,"QACP: An Annotated Question Answering Dataset for Assisting Chinese
  Python Programming Learners","In online learning platforms, particularly in rapidly growing computer
programming courses, addressing the thousands of students' learning queries
requires considerable human cost. The creation of intelligent assistant large
language models (LLMs) tailored for programming education necessitates distinct
data support. However, in real application scenarios, the data resources for
training such LLMs are relatively scarce. Therefore, to address the data
scarcity in intelligent educational systems for programming, this paper
proposes a new Chinese question-and-answer dataset for Python learners. To
ensure the authenticity and reliability of the sources of the questions, we
collected questions from actual student questions and categorized them
according to various dimensions such as the type of questions and the type of
learners. This annotation principle is designed to enhance the effectiveness
and quality of online programming education, providing a solid data foundation
for developing the programming teaching assists (TA). Furthermore, we conducted
comprehensive evaluations of various LLMs proficient in processing and
generating Chinese content, highlighting the potential limitations of general
LLMs as intelligent teaching assistants in computer programming courses.",2024-01-30T13:11:23Z
,http://arxiv.org/pdf/2406.15379v1.pdf,CS1-LLM: Integrating LLMs into CS1 Instruction,"The recent, widespread availability of Large Language Models (LLMs) like
ChatGPT and GitHub Copilot may impact introductory programming courses (CS1)
both in terms of what should be taught and how to teach it. Indeed, recent
research has shown that LLMs are capable of solving the majority of the
assignments and exams we previously used in CS1. In addition, professional
software engineers are often using these tools, raising the question of whether
we should be training our students in their use as well. This experience report
describes a CS1 course at a large research-intensive university that fully
embraces the use of LLMs from the beginning of the course. To incorporate the
LLMs, the course was intentionally altered to reduce emphasis on syntax and
writing code from scratch. Instead, the course now emphasizes skills needed to
successfully produce software with an LLM. This includes explaining code,
testing code, and decomposing large problems into small functions that are
solvable by an LLM. In addition to frequent, formative assessments of these
skills, students were given three large, open-ended projects in three separate
domains (data science, image processing, and game design) that allowed them to
showcase their creativity in topics of their choosing. In an end-of-term
survey, students reported that they appreciated learning with the assistance of
the LLM and that they interacted with the LLM in a variety of ways when writing
code. We provide lessons learned for instructors who may wish to incorporate
LLMs into their course.",2024-04-17T14:44:28Z
,http://arxiv.org/pdf/2310.13712v2.pdf,"Impact of Guidance and Interaction Strategies for LLM Use on Learner
  Performance and Perception","Personalized chatbot-based teaching assistants can be crucial in addressing
increasing classroom sizes, especially where direct teacher presence is
limited. Large language models (LLMs) offer a promising avenue, with increasing
research exploring their educational utility. However, the challenge lies not
only in establishing the efficacy of LLMs but also in discerning the nuances of
interaction between learners and these models, which impact learners'
engagement and results. We conducted a formative study in an undergraduate
computer science classroom (N=145) and a controlled experiment on Prolific
(N=356) to explore the impact of four pedagogically informed guidance
strategies on the learners' performance, confidence and trust in LLMs. Direct
LLM answers marginally improved performance, while refining student solutions
fostered trust. Structured guidance reduced random queries as well as instances
of students copy-pasting assignment questions to the LLM. Our work highlights
the role that teachers can play in shaping LLM-supported learning environments.",2023-10-13T01:21:52Z
,http://arxiv.org/pdf/2403.11984v1.pdf,"Using Generative Text Models to Create Qualitative Codebooks for Student
  Evaluations of Teaching","Feedback is a critical aspect of improvement. Unfortunately, when there is a
lot of feedback from multiple sources, it can be difficult to distill the
information into actionable insights. Consider student evaluations of teaching
(SETs), which are important sources of feedback for educators. They can give
instructors insights into what worked during a semester. A collection of SETs
can also be useful to administrators as signals for courses or entire programs.
However, on a large scale as in high-enrollment courses or administrative
records over several years, the volume of SETs can render them difficult to
analyze. In this paper, we discuss a novel method for analyzing SETs using
natural language processing (NLP) and large language models (LLMs). We
demonstrate the method by applying it to a corpus of 5,000 SETs from a large
public university. We show that the method can be used to extract, embed,
cluster, and summarize the SETs to identify the themes they express. More
generally, this work illustrates how to use the combination of NLP techniques
and LLMs to generate a codebook for SETs. We conclude by discussing the
implications of this method for analyzing SETs and other types of student
writing in teaching and research settings.",2024-03-18T17:21:35Z
,http://arxiv.org/pdf/2402.07081v1.pdf,"Using Large Language Models for Student-Code Guided Test Case Generation
  in Computer Science Education","In computer science education, test cases are an integral part of programming
assignments since they can be used as assessment items to test students'
programming knowledge and provide personalized feedback on student-written
code. The goal of our work is to propose a fully automated approach for test
case generation that can accurately measure student knowledge, which is
important for two reasons. First, manually constructing test cases requires
expert knowledge and is a labor-intensive process. Second, developing test
cases for students, especially those who are novice programmers, is
significantly different from those oriented toward professional-level software
developers. Therefore, we need an automated process for test case generation to
assess student knowledge and provide feedback. In this work, we propose a large
language model-based approach to automatically generate test cases and show
that they are good measures of student knowledge, using a publicly available
dataset that contains student-written Java code. We also discuss future
research directions centered on using test cases to help students.",2024-02-11T01:37:48Z
10.1145/3587102.3588785,http://arxiv.org/pdf/2304.03938v1.pdf,"Comparing Code Explanations Created by Students and Large Language
  Models","Reasoning about code and explaining its purpose are fundamental skills for
computer scientists. There has been extensive research in the field of
computing education on the relationship between a student's ability to explain
code and other skills such as writing and tracing code. In particular, the
ability to describe at a high-level of abstraction how code will behave over
all possible inputs correlates strongly with code writing skills. However,
developing the expertise to comprehend and explain code accurately and
succinctly is a challenge for many students. Existing pedagogical approaches
that scaffold the ability to explain code, such as producing exemplar code
explanations on demand, do not currently scale well to large classrooms. The
recent emergence of powerful large language models (LLMs) may offer a solution.
In this paper, we explore the potential of LLMs in generating explanations that
can serve as examples to scaffold students' ability to understand and explain
code. To evaluate LLM-created explanations, we compare them with explanations
created by students in a large course ($n \approx 1000$) with respect to
accuracy, understandability and length. We find that LLM-created explanations,
which can be produced automatically on demand, are rated as being significantly
easier to understand and more accurate summaries of code than student-created
explanations. We discuss the significance of this finding, and suggest how such
models can be incorporated into introductory programming education.",2023-04-08T06:52:54Z
,http://arxiv.org/pdf/2308.08572v1.pdf,"Large Language Models in Introductory Programming Education: ChatGPT's
  Performance and Implications for Assessments","This paper investigates the performance of the Large Language Models (LLMs)
ChatGPT-3.5 and GPT-4 in solving introductory programming tasks. Based on the
performance, implications for didactic scenarios and assessment formats
utilizing LLMs are derived. For the analysis, 72 Python tasks for novice
programmers were selected from the free site CodingBat. Full task descriptions
were used as input to the LLMs, while the generated replies were evaluated
using CodingBat's unit tests. In addition, the general availability of textual
explanations and program code was analyzed. The results show high scores of
94.4 to 95.8% correct responses and reliable availability of textual
explanations and program code, which opens new ways to incorporate LLMs into
programming education and assessment.",2023-08-15T19:48:31Z
,http://arxiv.org/pdf/2312.07343v2.pdf,"Can ChatGPT Play the Role of a Teaching Assistant in an Introductory
  Programming Course?","The emergence of Large language models (LLMs) is expected to have a major
impact on education. This paper explores the potential of using ChatGPT, an
LLM, as a virtual Teaching Assistant (TA) in an Introductory Programming
Course. We evaluate ChatGPT's capabilities by comparing its performance with
that of human TAs in some of the important TA functions. The TA functions which
we focus on include (1) grading student code submissions, and (2) providing
feedback to undergraduate students in an introductory programming course.
Firstly, we assess ChatGPT's proficiency in grading student code submissions
using a given grading rubric and compare its performance with the grades
assigned by human TAs. Secondly, we analyze the quality and relevance of the
feedback provided by ChatGPT. This evaluation considers how well ChatGPT
addresses mistakes and offers suggestions for improvement in student solutions
from both code correctness and code quality perspectives. We conclude with a
discussion on the implications of integrating ChatGPT into computing education
for automated grading, personalized learning experiences, and instructional
support.",2023-12-12T15:06:44Z
10.1145/3636243.3636249,http://arxiv.org/pdf/2310.16984v1.pdf,"Patterns of Student Help-Seeking When Using a Large Language
  Model-Powered Programming Assistant","Providing personalized assistance at scale is a long-standing challenge for
computing educators, but a new generation of tools powered by large language
models (LLMs) offers immense promise. Such tools can, in theory, provide
on-demand help in large class settings and be configured with appropriate
guardrails to prevent misuse and mitigate common concerns around learner
over-reliance. However, the deployment of LLM-powered tools in authentic
classroom settings is still rare, and very little is currently known about how
students will use them in practice and what type of help they will seek. To
address this, we examine students' use of an innovative LLM-powered tool that
provides on-demand programming assistance without revealing solutions directly.
We deployed the tool for 12 weeks in an introductory computer and data science
course ($n = 52$), collecting more than 2,500 queries submitted by students
throughout the term. We manually categorized all student queries based on the
type of assistance sought, and we automatically analyzed several additional
query characteristics. We found that most queries requested immediate help with
programming assignments, whereas fewer requests asked for help on related
concepts or for deepening conceptual understanding. Furthermore, students often
provided minimal information to the tool, suggesting this is an area in which
targeted instruction would be beneficial. We also found that students who
achieved more success in the course tended to have used the tool more
frequently overall. Lessons from this research can be leveraged by programming
educators and institutions who plan to augment their teaching with emerging
LLM-powered tools.",2023-10-25T20:36:05Z
10.1145/3568813.3600139,http://arxiv.org/pdf/2306.05715v1.pdf,"Exploring the Responses of Large Language Models to Beginner
  Programmers' Help Requests","Background and Context: Over the past year, large language models (LLMs) have
taken the world by storm. In computing education, like in other walks of life,
many opportunities and threats have emerged as a consequence.
  Objectives: In this article, we explore such opportunities and threats in a
specific area: responding to student programmers' help requests. More
specifically, we assess how good LLMs are at identifying issues in problematic
code that students request help on.
  Method: We collected a sample of help requests and code from an online
programming course. We then prompted two different LLMs (OpenAI Codex and
GPT-3.5) to identify and explain the issues in the students' code and assessed
the LLM-generated answers both quantitatively and qualitatively.
  Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently
find at least one actual issue in each student program (GPT-3.5 in 90% of the
cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57%
of the time). False positives are common (40% chance for GPT-3.5). The advice
that the LLMs provide on the issues is often sensible. The LLMs perform better
on issues involving program logic rather than on output formatting. Model
solutions are frequently provided even when the LLM is prompted not to. LLM
responses to prompts in a non-English language are only slightly worse than
responses to English prompts.
  Implications: Our results continue to highlight the utility of LLMs in
programming education. At the same time, the results highlight the
unreliability of LLMs: LLMs make some of the same mistakes that students do,
perhaps especially when formatting output as required by automated assessment
systems. Our study informs teachers interested in using LLMs as well as future
efforts to customize LLMs for the needs of programming education.",2023-06-09T07:19:43Z
,http://arxiv.org/pdf/2308.08102v1.pdf,"ChatLogo: A Large Language Model-Driven Hybrid Natural-Programming
  Language Interface for Agent-based Modeling and Programming","Building on Papert (1980)'s idea of children talking to computers, we propose
ChatLogo, a hybrid natural-programming language interface for agent-based
modeling and programming. We build upon previous efforts to scaffold ABM & P
learning and recent development in leveraging large language models (LLMs) to
support the learning of computational programming. ChatLogo aims to support
conversations with computers in a mix of natural and programming languages,
provide a more user-friendly interface for novice learners, and keep the
technical system from over-reliance on any single LLM. We introduced the main
elements of our design: an intelligent command center, and a conversational
interface to support creative expression. We discussed the presentation format
and future work. Responding to the challenges of supporting open-ended
constructionist learning of ABM & P and leveraging LLMs for educational
purposes, we contribute to the field by proposing the first constructionist
LLM-driven interface to support computational and complex systems thinking.",2023-08-16T02:21:52Z
,http://arxiv.org/pdf/2309.00029v1.pdf,"Exploring the Potential of Large Language Models to Generate Formative
  Programming Feedback","Ever since the emergence of large language models (LLMs) and related
applications, such as ChatGPT, its performance and error analysis for
programming tasks have been subject to research. In this work-in-progress
paper, we explore the potential of such LLMs for computing educators and
learners, as we analyze the feedback it generates to a given input containing
program code. In particular, we aim at (1) exploring how an LLM like ChatGPT
responds to students seeking help with their introductory programming tasks,
and (2) identifying feedback types in its responses. To achieve these goals, we
used students' programming sequences from a dataset gathered within a CS1
course as input for ChatGPT along with questions required to elicit feedback
and correct solutions. The results show that ChatGPT performs reasonably well
for some of the introductory programming tasks and student errors, which means
that students can potentially benefit. However, educators should provide
guidance on how to use the provided feedback, as it can contain misleading
information for novices.",2023-08-31T15:22:11Z
10.1145/3585059.3611431,http://arxiv.org/pdf/2307.16650v1.pdf,"ChatGPT for Teaching and Learning: An Experience from Data Science
  Education","ChatGPT, an implementation and application of large language models, has
gained significant popularity since its initial release. Researchers have been
exploring ways to harness the practical benefits of ChatGPT in real-world
scenarios. Educational researchers have investigated its potential in various
subjects, e.g., programming, mathematics, finance, clinical decision support,
etc. However, there has been limited attention given to its application in data
science education. This paper aims to bridge that gap by utilizing ChatGPT in a
data science course, gathering perspectives from students, and presenting our
experiences and feedback on using ChatGPT for teaching and learning in data
science education. The findings not only distinguish data science education
from other disciplines but also uncover new opportunities and challenges
associated with incorporating ChatGPT into the data science curriculum.",2023-07-31T13:31:19Z
10.1145/3657604.3662036,http://arxiv.org/pdf/2404.13414v3.pdf,"Evaluating the Effectiveness of LLMs in Introductory Computer Science
  Education: A Semester-Long Field Study","The integration of AI assistants, especially through the development of Large
Language Models (LLMs), into computer science education has sparked significant
debate. An emerging body of work has looked into using LLMs in education, but
few have examined the impacts of LLMs on students in entry-level programming
courses, particularly in real-world contexts and over extended periods. To
address this research gap, we conducted a semester-long, between-subjects study
with 50 students using CodeTutor, an LLM-powered assistant developed by our
research team. Our study results show that students who used CodeTutor (the
experimental group) achieved statistically significant improvements in their
final scores compared to peers who did not use the tool (the control group).
Within the experimental group, those without prior experience with LLM-powered
tools demonstrated significantly greater performance gain than their
counterparts. We also found that students expressed positive feedback regarding
CodeTutor's capability, though they also had concerns about CodeTutor's limited
role in developing critical thinking skills. Over the semester, students'
agreement with CodeTutor's suggestions decreased, with a growing preference for
support from traditional human teaching assistants. Our analysis further
reveals that the quality of user prompts was significantly correlated with
CodeTutor's response effectiveness. Building upon our results, we discuss the
implications of our findings for integrating Generative AI literacy into
curricula to foster critical thinking skills and turn to examining the temporal
dynamics of user engagement with LLM-powered tools. We further discuss the
discrepancy between the anticipated functions of tools and students' actual
capabilities, which sheds light on the need for tailored strategies to improve
educational outcomes.",2024-04-20T15:58:22Z
10.1145/3649217.3653574,http://arxiv.org/pdf/2405.14178v1.pdf,"Desirable Characteristics for AI Teaching Assistants in Programming
  Education","Providing timely and personalized feedback to large numbers of students is a
long-standing challenge in programming courses. Relying on human teaching
assistants (TAs) has been extensively studied, revealing a number of potential
shortcomings. These include inequitable access for students with low confidence
when needing support, as well as situations where TAs provide direct solutions
without helping students to develop their own problem-solving skills. With the
advent of powerful large language models (LLMs), digital teaching assistants
configured for programming contexts have emerged as an appealing and scalable
way to provide instant, equitable, round-the-clock support. Although digital
TAs can provide a variety of help for programming tasks, from high-level
problem solving advice to direct solution generation, the effectiveness of such
tools depends on their ability to promote meaningful learning experiences. If
students find the guardrails implemented in digital TAs too constraining, or if
other expectations are not met, they may seek assistance in ways that do not
help them learn. Thus, it is essential to identify the features that students
believe make digital teaching assistants valuable. We deployed an LLM-powered
digital assistant in an introductory programming course and collected student
feedback ($n=813$) on the characteristics of the tool they perceived to be most
important. Our results highlight that students value such tools for their
ability to provide instant, engaging support, particularly during peak times
such as before assessment deadlines. They also expressed a strong preference
for features that enable them to retain autonomy in their learning journey,
such as scaffolding that helps to guide them through problem-solving steps
rather than simply being shown direct solutions.",2024-05-23T05:03:49Z
,http://arxiv.org/pdf/2312.15223v1.pdf,A Survey on Large Language Models for Software Engineering,"Software Engineering (SE) is the systematic design, development, and
maintenance of software applications, underpinning the digital infrastructure
of our modern mainworld. Very recently, the SE community has seen a rapidly
increasing number of techniques employing Large Language Models (LLMs) to
automate a broad range of SE tasks. Nevertheless, existing information of the
applications, effects, and possible limitations of LLMs within SE is still not
well-studied.
  In this paper, we provide a systematic survey to summarize the current
state-of-the-art research in the LLM-based SE community. We summarize 30
representative LLMs of Source Code across three model architectures, 15
pre-training objectives across four categories, and 16 downstream tasks across
five categories. We then present a detailed summarization of the recent SE
studies for which LLMs are commonly utilized, including 155 studies for 43
specific code-related tasks across four crucial phases within the SE workflow.
Besides, we summarize existing attempts to empirically evaluate LLMs in SE,
such as benchmarks, empirical studies, and exploration of SE education. We also
discuss several critical aspects of optimization and applications of LLMs in
SE, such as security attacks, model tuning, and model compression. Finally, we
highlight several challenges and potential opportunities on applying LLMs for
future SE studies, such as exploring domain LLMs and constructing clean
evaluation datasets. Overall, our work can help researchers gain a
comprehensive understanding about the achievements of the existing LLM-based SE
studies and promote the practical application of these techniques. Our
artifacts are publicly available and will continuously updated at the living
repository: \url{https://github.com/iSEngLab/AwesomeLLM4SE}.",2023-12-23T11:09:40Z
,http://arxiv.org/pdf/2404.04603v1.pdf,Analyzing LLM Usage in an Advanced Computing Class in India,"This paper investigates the usage patterns of undergraduate and graduate
students when engaging with large language models (LLMs) to tackle programming
assignments in the context of advanced computing courses. Existing work
predominantly focuses on the influence of LLMs in introductory programming
contexts. Additionally, there is a scarcity of studies analyzing actual
conversations between students and LLMs. Our study provides a comprehensive
quantitative and qualitative analysis of raw interactions between students and
LLMs within an advanced computing course (Distributed Systems) at an Indian
University. We further complement this by conducting student interviews to gain
deeper insights into their usage patterns. Our study shows that students make
use of large language models (LLMs) in various ways: generating code or
debugging code by identifying and fixing errors. They also copy and paste
assignment descriptions into LLM interfaces for specific solutions, ask
conceptual questions about complex programming ideas or theoretical concepts,
and generate test cases to check code functionality and robustness. Our
analysis includes over 4,000 prompts from 411 students and conducting
interviews with 10 students. Our analysis shows that LLMs excel at generating
boilerplate code and assisting in debugging, while students handle the
integration of components and system troubleshooting. This aligns with the
learning objectives of advanced computing courses, which are oriented towards
teaching students how to build systems and troubleshoot, with less emphasis on
generating code from scratch. Therefore, LLM tools can be leveraged to increase
student productivity, as shown by the data we collected. This study contributes
to the ongoing discussion on LLM use in education, advocating for their
usefulness in advanced computing courses to complement higher-level learning
and productivity.",2024-04-06T12:06:56Z
,http://arxiv.org/pdf/2401.05399v1.pdf,Automated Assessment of Students' Code Comprehension using LLMs,"Assessing student's answers and in particular natural language answers is a
crucial challenge in the field of education. Advances in machine learning,
including transformer-based models such as Large Language Models(LLMs), have
led to significant progress in various natural language tasks. Nevertheless,
amidst the growing trend of evaluating LLMs across diverse tasks, evaluating
LLMs in the realm of automated answer assesment has not received much
attention. To address this gap, we explore the potential of using LLMs for
automated assessment of student's short and open-ended answer. Particularly, we
use LLMs to compare students' explanations with expert explanations in the
context of line-by-line explanations of computer programs.
  For comparison purposes, we assess both Large Language Models (LLMs) and
encoder-based Semantic Textual Similarity (STS) models in the context of
assessing the correctness of students' explanation of computer code. Our
findings indicate that LLMs, when prompted in few-shot and chain-of-thought
setting perform comparable to fine-tuned encoder-based models in evaluating
students' short answers in programming domain.",2023-12-19T20:39:12Z
10.1145/3626252.3630773,http://arxiv.org/pdf/2403.14986v1.pdf,"AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style
  Feedback in a Global Course","Teaching students how to write code that is elegant, reusable, and
comprehensible is a fundamental part of CS1 education. However, providing this
""style feedback"" in a timely manner has proven difficult to scale. In this
paper, we present our experience deploying a novel, real-time style feedback
tool in Code in Place, a large-scale online CS1 course. Our tool is based on
the latest breakthroughs in large-language models (LLMs) and was carefully
designed to be safe and helpful for students. We used our Real-Time Style
Feedback tool (RTSF) in a class with over 8,000 diverse students from across
the globe and ran a randomized control trial to understand its benefits. We
show that students who received style feedback in real-time were five times
more likely to view and engage with their feedback compared to students who
received delayed feedback. Moreover, those who viewed feedback were more likely
to make significant style-related edits to their code, with over 79% of these
edits directly incorporating their feedback. We also discuss the practicality
and dangers of LLM-based tools for feedback, investigating the quality of the
feedback generated, LLM limitations, and techniques for consistency,
standardization, and safeguarding against demographic bias, all of which are
crucial for a tool utilized by students.",2024-03-22T06:45:39Z
,http://arxiv.org/pdf/2405.05347v1.pdf,Benchmarking Educational Program Repair,"The emergence of large language models (LLMs) has sparked enormous interest
due to their potential application across a range of educational tasks. For
example, recent work in programming education has used LLMs to generate
learning resources, improve error messages, and provide feedback on code.
However, one factor that limits progress within the field is that much of the
research uses bespoke datasets and different evaluation metrics, making direct
comparisons between results unreliable. Thus, there is a pressing need for
standardization and benchmarks that facilitate the equitable comparison of
competing approaches. One task where LLMs show great promise is program repair,
which can be used to provide debugging support and next-step hints to students.
In this article, we propose a novel educational program repair benchmark. We
curate two high-quality publicly available programming datasets, present a
unified evaluation procedure introducing a novel evaluation metric rouge@k for
approximating the quality of repairs, and evaluate a set of five recent models
to establish baseline performance.",2024-05-08T18:23:59Z
10.1145/3639474.3640052,http://arxiv.org/pdf/2403.06254v1.pdf,"LLMs Still Can't Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4
  and Bard's Capacity to Handle Object-Oriented Programming Assignments","Large Language Models (LLMs) have emerged as promising tools to assist
students while solving programming assignments. However, object-oriented
programming (OOP), with its inherent complexity involving the identification of
entities, relationships, and responsibilities, is not yet mastered by these
tools. Contrary to introductory programming exercises, there exists a research
gap with regard to the behavior of LLMs in OOP contexts. In this study, we
experimented with three prominent LLMs - GPT-3.5, GPT-4, and Bard - to solve
real-world OOP exercises used in educational settings, subsequently validating
their solutions using an Automatic Assessment Tool (AAT). The findings revealed
that while the models frequently achieved mostly working solutions to the
exercises, they often overlooked the best practices of OOP. GPT-4 stood out as
the most proficient, followed by GPT-3.5, with Bard trailing last. We advocate
for a renewed emphasis on code quality when employing these models and explore
the potential of pairing LLMs with AATs in pedagogical settings. In conclusion,
while GPT-4 showcases promise, the deployment of these models in OOP education
still mandates supervision.",2024-03-10T16:40:05Z
10.1145/3632620.3671098,http://arxiv.org/pdf/2406.06451v1.pdf,"Insights from Social Shaping Theory: The Appropriation of Large Language
  Models in an Undergraduate Programming Course","The capability of large language models (LLMs) to generate, debug, and
explain code has sparked the interest of researchers and educators in
undergraduate programming, with many anticipating their transformative
potential in programming education. However, decisions about why and how to use
LLMs in programming education may involve more than just the assessment of an
LLM's technical capabilities. Using the social shaping of technology theory as
a guiding framework, our study explores how students' social perceptions
influence their own LLM usage. We then examine the correlation of self-reported
LLM usage with students' self-efficacy and midterm performances in an
undergraduate programming course. Triangulating data from an anonymous
end-of-course student survey (n = 158), a mid-course self-efficacy survey
(n=158), student interviews (n = 10), self-reported LLM usage on homework, and
midterm performances, we discovered that students' use of LLMs was associated
with their expectations for their future careers and their perceptions of peer
usage. Additionally, early self-reported LLM usage in our context correlated
with lower self-efficacy and lower midterm scores, while students' perceived
over-reliance on LLMs, rather than their usage itself, correlated with
decreased self-efficacy later in the course.",2024-06-10T16:40:14Z
,http://arxiv.org/pdf/2306.04556v1.pdf,"StudentEval: A Benchmark of Student-Written Prompts for Large Language
  Models of Code","Code LLMs are being rapidly deployed and there is evidence that they can make
professional programmers more productive. Current benchmarks for code
generation measure whether models generate correct programs given an expert
prompt. In this paper, we present a new benchmark containing multiple prompts
per problem, written by a specific population of non-expert prompters:
beginning programmers. StudentEval contains 1,749 prompts for 48 problems,
written by 80 students who have only completed one semester of Python
programming. Our students wrote these prompts while working interactively with
a Code LLM, and we observed very mixed success rates. We use StudentEval to
evaluate 5 Code LLMs and find that StudentEval is a better discriminator of
model performance than existing benchmarks. We analyze the prompts and find
significant variation in students' prompting techniques. We also find that
nondeterministic LLM sampling could mislead students into thinking that their
prompts are more (or less) effective than they actually are, which has
implications for how to teach with Code LLMs.",2023-06-07T16:03:55Z
,http://arxiv.org/pdf/2404.02540v2.pdf,CSEPrompts: A Benchmark of Introductory Computer Science Prompts,"Recent advances in AI, machine learning, and NLP have led to the development
of a new generation of Large Language Models (LLMs) that are trained on massive
amounts of data and often have trillions of parameters. Commercial applications
(e.g., ChatGPT) have made this technology available to the general public, thus
making it possible to use LLMs to produce high-quality texts for academic and
professional purposes. Schools and universities are aware of the increasing use
of AI-generated content by students and they have been researching the impact
of this new technology and its potential misuse. Educational programs in
Computer Science (CS) and related fields are particularly affected because LLMs
are also capable of generating programming code in various programming
languages. To help understand the potential impact of publicly available LLMs
in CS education, we introduce CSEPrompts, a framework with hundreds of
programming exercise prompts and multiple-choice questions retrieved from
introductory CS and programming courses. We also provide experimental results
on CSEPrompts to evaluate the performance of several LLMs with respect to
generating Python code and answering basic computer science and programming
questions.",2024-04-03T07:55:57Z
,http://arxiv.org/pdf/2404.18496v1.pdf,AI-powered Code Review with LLMs: Early Results,"In this paper, we present a novel approach to improving software quality and
efficiency through a Large Language Model (LLM)-based model designed to review
code and identify potential issues. Our proposed LLM-based AI agent model is
trained on large code repositories. This training includes code reviews, bug
reports, and documentation of best practices. It aims to detect code smells,
identify potential bugs, provide suggestions for improvement, and optimize the
code. Unlike traditional static code analysis tools, our LLM-based AI agent has
the ability to predict future potential risks in the code. This supports a dual
goal of improving code quality and enhancing developer education by encouraging
a deeper understanding of best practices and efficient coding techniques.
Furthermore, we explore the model's effectiveness in suggesting improvements
that significantly reduce post-release bugs and enhance code review processes,
as evidenced by an analysis of developer sentiment toward LLM feedback. For
future work, we aim to assess the accuracy and efficiency of LLM-generated
documentation updates in comparison to manual methods. This will involve an
empirical study focusing on manually conducted code reviews to identify code
smells and bugs, alongside an evaluation of best practice documentation,
augmented by insights from developer discussions and code reviews. Our goal is
to not only refine the accuracy of our LLM-based tool but also to underscore
its potential in streamlining the software development lifecycle through
proactive code improvement and education.",2024-04-29T08:27:50Z
,http://arxiv.org/pdf/2406.04817v1.pdf,"Experiences from Integrating Large Language Model Chatbots into the
  Classroom","In the present study, we provided students an unfiltered access to a
state-of-the-art large language model (LLM) chatbot. The chatbot was
intentionally designed to mimic proprietary commercial chatbots such as ChatGPT
where the chatbot has not been tailored for the educational context; the
underlying engine was OpenAI GPT-4. The chatbot was integrated into online
learning materials of three courses. One of the courses focused on software
engineering with LLMs, while the two other courses were not directly related to
LLMs. Our results suggest that only a minority of students engage with the
chatbot in the courses that do not relate to LLMs. At the same time,
unsurprisingly, nearly all students in the LLM-focused course leveraged the
chatbot. In all courses, the majority of the LLM usage came from a few
superusers, whereas the majority of the students did not heavily use the
chatbot even though it was readily available and effectively provided a free
access to the OpenAI GPT-4 model. We also observe that in addition to students
using the chatbot for course-specific purposes, many use the chatbot for their
own purposes. These results suggest that the worst fears of educators -- all
students overrelying on LLMs -- did not materialize even when the chatbot
access was unfiltered. We finally discuss potential reasons for the low usage,
suggesting the need for more tailored and scaffolded LLM experiences targeted
for specific types of student use cases.",2024-06-07T10:37:14Z
,http://arxiv.org/pdf/2405.19132v1.pdf,"Analyzing Chat Protocols of Novice Programmers Solving Introductory
  Programming Tasks with ChatGPT","Large Language Models (LLMs) have taken the world by storm, and students are
assumed to use related tools at a great scale. In this research paper we aim to
gain an understanding of how introductory programming students chat with LLMs
and related tools, e.g., ChatGPT-3.5. To address this goal, computing students
at a large German university were motivated to solve programming exercises with
the assistance of ChatGPT as part of their weekly introductory course
exercises. Then students (n=213) submitted their chat protocols (with 2335
prompts in sum) as data basis for this analysis. The data was analyzed w.r.t.
the prompts, frequencies, the chats' progress, contents, and other use pattern,
which revealed a great variety of interactions, both potentially supportive and
concerning. Learning about students' interactions with ChatGPT will help inform
and align teaching practices and instructions for future introductory
programming courses in higher education.",2024-05-29T14:38:32Z
,http://arxiv.org/pdf/2307.16364v1.pdf,"Promptly: Using Prompt Problems to Teach Learners How to Effectively
  Utilize AI Code Generators","With their remarkable ability to generate code, large language models (LLMs)
are a transformative technology for computing education practice. They have
created an urgent need for educators to rethink pedagogical approaches and
teaching strategies for newly emerging skill sets. Traditional approaches to
learning programming have focused on frequent and repeated practice at writing
code. The ease with which code can now be generated has resulted in a shift in
focus towards reading, understanding and evaluating LLM-generated code. In
parallel with this shift, a new essential skill is emerging -- the ability to
construct good prompts for code-generating models. This paper introduces a
novel pedagogical concept known as a `Prompt Problem', designed to help
students learn how to craft effective prompts for LLMs. A Prompt Problem
challenges a student to create a natural language prompt that leads an LLM to
produce the correct code for a specific problem. To support the delivery of
Prompt Problems at scale, in this paper we also present a novel tool called
Promptly which hosts a repository of Prompt Problems and automates the
evaluation of prompt-generated code. We report empirical findings from a field
study in which Promptly was deployed in a first-year Python programming course
(n=54). We explore student interactions with the tool and their perceptions of
the Prompt Problem concept. We found that Promptly was largely well-received by
students for its ability to engage their computational thinking skills and
expose them to new programming constructs. We also discuss avenues for future
work, including variations on the design of Prompt Problems and the need to
study their integration into the curriculum and teaching practice.",2023-07-31T01:46:42Z
,http://arxiv.org/pdf/2404.14662v1.pdf,NExT: Teaching Large Language Models to Reason about Code Execution,"A fundamental skill among human developers is the ability to understand and
reason about program execution. As an example, a programmer can mentally
simulate code execution in natural language to debug and repair code (aka.
rubber duck debugging). However, large language models (LLMs) of code are
typically trained on the surface textual form of programs, thus may lack a
semantic understanding of how programs execute at run-time. To address this
issue, we propose NExT, a method to teach LLMs to inspect the execution traces
of programs (variable states of executed lines) and reason about their run-time
behavior through chain-of-thought (CoT) rationales. Specifically, NExT uses
self-training to bootstrap a synthetic training set of execution-aware
rationales that lead to correct task solutions (e.g., fixed programs) without
laborious manual annotation. Experiments on program repair tasks based on MBPP
and HumanEval demonstrate that NExT improves the fix rate of a PaLM 2 model, by
26.1% and 14.3% absolute, respectively, with significantly improved rationale
quality as verified by automated metrics and human raters. Our model can also
generalize to scenarios where program traces are absent at test-time.",2024-04-23T01:46:32Z
,http://arxiv.org/pdf/2405.16133v2.pdf,"Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via
  Code Rewriting","Large Language Models (LLMs) have exhibited remarkable proficiency in
generating code. However, the misuse of LLM-generated (Synthetic) code has
prompted concerns within both educational and industrial domains, highlighting
the imperative need for the development of synthetic code detectors. Existing
methods for detecting LLM-generated content are primarily tailored for general
text and often struggle with code content due to the distinct grammatical
structure of programming languages and massive ""low-entropy"" tokens. Building
upon this, our work proposes a novel zero-shot synthetic code detector based on
the similarity between the code and its rewritten variants. Our method relies
on the intuition that the differences between the LLM-rewritten and original
codes tend to be smaller when the original code is synthetic. We utilize
self-supervised contrastive learning to train a code similarity model and
assess our approach on two synthetic code detection benchmarks. Our results
demonstrate a notable enhancement over existing synthetic content detectors
designed for general texts, with an improvement of 20.5% in the APPS benchmark
and 29.1% in the MBPP benchmark.",2024-05-25T08:57:28Z
,http://arxiv.org/pdf/2303.17780v3.pdf,AceCoder: Utilizing Existing Code to Enhance Code Generation,"Large Language Models (LLMs) have shown great success in code generation.
LLMs take as the input a prompt and output the code. A key question is how to
make prompts (i.e., Prompting Techniques). Existing prompting techniques are
designed for natural language generation and have low accuracy in code
generation.
  In this paper, we propose a new prompting technique named AceCoder. Our
motivation is that code generation meets two unique challenges (i.e.,
requirement understanding and code implementation). AceCoder contains two novel
mechanisms (i.e., guided code generation and example retrieval) to solve these
challenges. (1) Guided code generation asks LLMs first to analyze requirements
and output an intermediate preliminary (e.g., test cases). The preliminary is
used to clarify requirements and tell LLMs ""what to write"". (2) Example
retrieval selects similar programs as examples in prompts, which provide lots
of relevant content (e.g., algorithms, APIs) and teach LLMs ""how to write"". We
apply AceCoder to three LLMs (e.g., Codex) and evaluate it on three public
benchmarks using the Pass@k. Results show that AceCoder can significantly
improve the performance of LLMs on code generation. (1) In terms of Pass@1,
AceCoder outperforms the state-of-the-art baseline by up to 56.4% in MBPP,
70.7% in MBJP, and 88.4% in MBJSP. (2) AceCoder is effective in LLMs with
different sizes (i.e., 6B to 13B) and different languages (i.e., Python, Java,
and JavaScript). (3) Human evaluation shows human developers prefer programs
from AceCoder.",2023-03-31T02:57:15Z
,http://arxiv.org/pdf/2402.01687v2.pdf,"""Which LLM should I use?"": Evaluating LLMs for tasks performed by
  Undergraduate Computer Science Students","This study evaluates the effectiveness of various large language models
(LLMs) in performing tasks common among undergraduate computer science
students. Although a number of research studies in the computing education
community have explored the possibility of using LLMs for a variety of tasks,
there is a lack of comprehensive research comparing different LLMs and
evaluating which LLMs are most effective for different tasks. Our research
systematically assesses some of the publicly available LLMs such as Google
Bard, ChatGPT(3.5), GitHub Copilot Chat, and Microsoft Copilot across diverse
tasks commonly encountered by undergraduate computer science students in India.
These tasks include code explanation and documentation, solving class
assignments, technical interview preparation, learning new concepts and
frameworks, and email writing. Evaluation for these tasks was carried out by
pre-final year and final year undergraduate computer science students and
provides insights into the models' strengths and limitations. This study aims
to guide students as well as instructors in selecting suitable LLMs for any
specific task and offers valuable insights on how LLMs can be used
constructively by students and instructors.",2024-01-22T15:11:36Z
,http://arxiv.org/pdf/2404.07235v1.pdf,Explaining EDA synthesis errors with LLMs,"Training new engineers in digital design is a challenge, particularly when it
comes to teaching the complex electronic design automation (EDA) tooling used
in this domain. Learners will typically deploy designs in the Verilog and VHDL
hardware description languages to Field Programmable Gate Arrays (FPGAs) from
Altera (Intel) and Xilinx (AMD) via proprietary closed-source toolchains
(Quartus Prime and Vivado, respectively). These tools are complex and difficult
to use -- yet, as they are the tools used in industry, they are an essential
first step in this space. In this work, we examine how recent advances in
artificial intelligence may be leveraged to address aspects of this challenge.
Specifically, we investigate if Large Language Models (LLMs), which have
demonstrated text comprehension and question-answering capabilities, can be
used to generate novice-friendly explanations of compile-time synthesis error
messages from Quartus Prime and Vivado. To perform this study we generate 936
error message explanations using three OpenAI LLMs over 21 different buggy code
samples. These are then graded for relevance and correctness, and we find that
in approximately 71% of cases the LLMs give correct & complete explanations
suitable for novice learners.",2024-04-07T07:12:16Z
,http://arxiv.org/pdf/2404.17739v2.pdf,How LLMs Aid in UML Modeling: An Exploratory Study with Novice Analysts,"Since the emergence of GPT-3, Large Language Models (LLMs) have caught the
eyes of researchers, practitioners, and educators in the field of software
engineering. However, there has been relatively little investigation regarding
the performance of LLMs in assisting with requirements analysis and UML
modeling. This paper explores how LLMs can assist novice analysts in creating
three types of typical UML models: use case models, class diagrams, and
sequence diagrams. For this purpose, we designed the modeling tasks of these
three UML models for 45 undergraduate students who participated in a
requirements modeling course, with the help of LLMs. By analyzing their project
reports, we found that LLMs can assist undergraduate students as novice
analysts in UML modeling tasks, but LLMs also have shortcomings and limitations
that should be considered when using them.",2024-04-27T00:38:20Z
,http://arxiv.org/pdf/2403.09740v1.pdf,Teaching Machines to Code: Smart Contract Translation with LLMs,"The advent of large language models (LLMs) has marked a significant milestone
in the realm of artificial intelligence, with their capabilities often matching
or surpassing human expertise in various domains. Among these achievements,
their adeptness in translation tasks stands out, closely mimicking the
intricate and preliminary processes undertaken by human translators to ensure
the fidelity and quality of the translated content. Despite the advancements in
utilizing LLMs for translating programming code across different languages, the
domain of smart contract translation, particularly into languages not
previously encountered by the LLM, remains largely unexplored. In our research,
we present a pioneering approach, SolMover, which harnesses the synergy of two
distinct LLMs within a unified framework. This framework is designed to grasp
coding principles and apply this understanding to the translation of code into
an unfamiliar language. Our study delves into the capacity of LLMs to mimic
human learning processes, offering an in-depth evaluation of our methodology
for converting smart contracts written in Solidity to Move, a language with
limited resources. The framework employs one LLM to decipher coding conventions
for the new language, creating a blueprint for the second LLM, which, lacking
planning abilities, possesses coding expertise. The empirical evidence from our
experiments suggests that SolMover substantially enhances performance compared
to gpt-3.5-turbo-1106, and achieves superior results over competitors such as
Palm2 and Mixtral-8x7B-Instruct. Additionally, our analysis highlights the
efficacy of our bug mitigation strategy in elevating code quality across all
models, even outside the SolMover framework.",2024-03-13T18:55:20Z
,http://arxiv.org/pdf/2310.15317v1.pdf,"Exploring the Potential of Large Language Models in Generating
  Code-Tracing Questions for Introductory Programming Courses","In this paper, we explore the application of large language models (LLMs) for
generating code-tracing questions in introductory programming courses. We
designed targeted prompts for GPT4, guiding it to generate code-tracing
questions based on code snippets and descriptions. We established a set of
human evaluation metrics to assess the quality of questions produced by the
model compared to those created by human experts. Our analysis provides
insights into the capabilities and potential of LLMs in generating diverse
code-tracing questions. Additionally, we present a unique dataset of human and
LLM-generated tracing questions, serving as a valuable resource for both the
education and NLP research communities. This work contributes to the ongoing
dialogue on the potential uses of LLMs in educational settings.",2023-10-23T19:35:01Z
,http://arxiv.org/pdf/2403.16159v2.pdf,"Designing Child-Centric AI Learning Environments: Insights from
  LLM-Enhanced Creative Project-Based Learning","Project-based learning (PBL) is an instructional method that is very helpful
in nurturing students' creativity, but it requires significant time and energy
from both students and teachers. Large language models (LLMs) have been proven
to assist in creative tasks, yet much controversy exists regarding their role
in fostering creativity. This paper explores the potential of LLMs in PBL
settings, with a special focus on fostering creativity. We began with an
exploratory study involving 12 middle school students and identified five
design considerations for LLM applications in PBL. Building on this, we
developed an LLM-empowered, 48-hour PBL program and conducted an instructional
experiment with 31 middle school students. Our results indicated that LLMs can
enhance every stage of PBL. Additionally, we also discovered ambivalent
perspectives among students and mentors toward LLM usage. Furthermore, we
explored the challenge and design implications of integrating LLMs into PBL and
reflected on the program. By bridging AI advancements into educational
practice, our work aims to inspire further discourse and investigation into
harnessing AI's potential in child-centric educational settings.",2024-03-24T13:54:05Z
,http://arxiv.org/pdf/2401.02985v1.pdf,"Evaluating Large Language Models on the GMAT: Implications for the
  Future of Business Education","The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.",2024-01-02T03:54:50Z
,http://arxiv.org/pdf/2304.05128v2.pdf,Teaching Large Language Models to Self-Debug,"Large language models (LLMs) have achieved impressive performance on code
generation. However, for complex programming tasks, generating the correct
solution in one go becomes challenging, thus some prior works have designed
program repair approaches to improve code generation performance. In this work,
we propose Self-Debugging, which teaches a large language model to debug its
predicted program via few-shot demonstrations. In particular, we demonstrate
that Self-Debugging can teach the large language model to perform rubber duck
debugging; i.e., without any human feedback on the code correctness or error
messages, the model is able to identify its mistakes by investigating the
execution results and explaining the generated code in natural language.
Self-Debugging achieves the state-of-the-art performance on several code
generation benchmarks, including the Spider dataset for text-to-SQL generation,
TransCoder for C++-to-Python translation, and MBPP for text-to-Python
generation. On the Spider benchmark where there are no unit tests to verify the
correctness of predictions, Self-Debugging with code explanation consistently
improves the baseline by 2-3%, and improves the prediction accuracy on problems
of the hardest level by 9%. On TransCoder and MBPP where unit tests are
available, Self-Debugging improves the baseline accuracy by up to 12%.
Meanwhile, by leveraging feedback messages and reusing failed predictions,
Self-Debugging notably improves sample efficiency, and can match or outperform
baseline models that generate more than 10x candidate programs.",2023-04-11T10:43:43Z
,http://arxiv.org/pdf/2310.10690v3.pdf,"Large Language Models for In-Context Student Modeling: Synthesizing
  Student's Behavior in Visual Programming","Student modeling is central to many educational technologies as it enables
predicting future learning outcomes and designing targeted instructional
strategies. However, open-ended learning domains pose challenges for accurately
modeling students due to the diverse behaviors and a large space of possible
misconceptions. To approach these challenges, we explore the application of
large language models (LLMs) for in-context student modeling in open-ended
learning domains. More concretely, given a particular student's attempt on a
reference task as observation, the objective is to synthesize the student's
attempt on a target task. We introduce a novel framework, LLM for Student
Synthesis (LLM-SS), that leverages LLMs for synthesizing a student's behavior.
Our framework can be combined with different LLMs; moreover, we fine-tune LLMs
to boost their student modeling capabilities. We instantiate several methods
based on LLM-SS framework and evaluate them using an existing benchmark,
StudentSyn, for student attempt synthesis in a visual programming domain.
Experimental results show that our methods perform significantly better than
the baseline method NeurSS provided in the StudentSyn benchmark. Furthermore,
our method using a fine-tuned version of the GPT-3.5 model is significantly
better than using the base GPT-3.5 model and gets close to human tutors'
performance.",2023-10-15T12:56:13Z
,http://arxiv.org/pdf/2307.07411v1.pdf,"Detecting LLM-Generated Text in Computing Education: A Comparative Study
  for ChatGPT Cases","Due to the recent improvements and wide availability of Large Language Models
(LLMs), they have posed a serious threat to academic integrity in education.
Modern LLM-generated text detectors attempt to combat the problem by offering
educators with services to assess whether some text is LLM-generated. In this
work, we have collected 124 submissions from computer science students before
the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this
data to evaluate eight publicly-available LLM-generated text detectors through
the measures of accuracy, false positives, and resilience. The purpose of this
work is to inform the community of what LLM-generated text detectors work and
which do not, but also to provide insights for educators to better maintain
academic integrity in their courses. Our results find that CopyLeaks is the
most accurate LLM-generated text detector, GPTKit is the best LLM-generated
text detector to reduce false positives, and GLTR is the most resilient
LLM-generated text detector. We also express concerns over 52 false positives
(of 114 human written submissions) generated by GPTZero. Finally, we note that
all LLM-generated text detectors are less accurate with code, other languages
(aside from English), and after the use of paraphrasing tools (like QuillBot).
Modern detectors are still in need of improvements so that they can offer a
full-proof solution to help maintain academic integrity. Further, their
usability can be improved by facilitating a smooth API integration, providing
clear documentation of their features and the understandability of their
model(s), and supporting more commonly used languages.",2023-07-10T12:18:34Z
,http://arxiv.org/pdf/2312.10055v1.pdf,"Next-Step Hint Generation for Introductory Programming Using Large
  Language Models","Large Language Models possess skills such as answering questions, writing
essays or solving programming exercises. Since these models are easily
accessible, researchers have investigated their capabilities and risks for
programming education. This work explores how LLMs can contribute to
programming education by supporting students with automated next-step hints. We
investigate prompt practices that lead to effective next-step hints and use
these insights to build our StAP-tutor. We evaluate this tutor by conducting an
experiment with students, and performing expert assessments. Our findings show
that most LLM-generated feedback messages describe one specific next step and
are personalised to the student's code and approach. However, the hints may
contain misleading information and lack sufficient detail when students
approach the end of the assignment. This work demonstrates the potential for
LLM-generated feedback, but further research is required to explore its
practical implementation.",2023-12-03T17:51:07Z
10.1145/3639474.3640061,http://arxiv.org/pdf/2404.02548v2.pdf,AI-Tutoring in Software Engineering Education,"With the rapid advancement of artificial intelligence (AI) in various
domains, the education sector is set for transformation. The potential of
AI-driven tools in enhancing the learning experience, especially in
programming, is immense. However, the scientific evaluation of Large Language
Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an
AI-Tutor remains largely unexplored. Therefore, there is a need to understand
how students interact with such AI-Tutors and to analyze their experiences. In
this paper, we conducted an exploratory case study by integrating the
GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a
combination of empirical data collection and an exploratory survey, we
identified different user types based on their interaction patterns with the
AI-Tutor. Additionally, the findings highlight advantages, such as timely
feedback and scalability. However, challenges like generic responses and
students' concerns about a learning progress inhibition when using the AI-Tutor
were also evident. This research adds to the discourse on AI's role in
education.",2024-04-03T08:15:08Z
,http://arxiv.org/pdf/2401.00761v1.pdf,The Earth is Flat? Unveiling Factual Errors in Large Language Models,"Large Language Models (LLMs) like ChatGPT are foundational in various
applications due to their extensive knowledge from pre-training and
fine-tuning. Despite this, they are prone to generating factual and commonsense
errors, raising concerns in critical areas like healthcare, journalism, and
education to mislead users. Current methods for evaluating LLMs' veracity are
limited by test data leakage or the need for extensive human labor, hindering
efficient and accurate error detection. To tackle this problem, we introduce a
novel, automatic testing framework, FactChecker, aimed at uncovering factual
inaccuracies in LLMs. This framework involves three main steps: First, it
constructs a factual knowledge graph by retrieving fact triplets from a
large-scale knowledge database. Then, leveraging the knowledge graph,
FactChecker employs a rule-based approach to generates three types of questions
(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and
multi-hop relations, along with correct answers. Lastly, it assesses the LLMs'
responses for accuracy using tailored matching strategies for each question
type. Our extensive tests on six prominent LLMs, including text-davinci-002,
text-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal
that FactChecker can trigger factual errors in up to 45\% of questions in these
models. Moreover, we demonstrate that FactChecker's test cases can improve
LLMs' factual accuracy through in-context learning and fine-tuning (e.g.,
llama-2-13b-chat's accuracy increase from 35.3\% to 68.5\%). We are making all
code, data, and results available for future research endeavors.",2024-01-01T14:02:27Z
,http://arxiv.org/pdf/2405.16533v1.pdf,Chain of Tools: Large Language Model is an Automatic Multi-tool Learner,"Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to extend their utility, empowering them to solve practical
tasks. Existing work typically empowers LLMs as tool users with a manually
designed workflow, where the LLM plans a series of tools in a step-by-step
manner, and sequentially executes each tool to obtain intermediate results
until deriving the final answer. However, they suffer from two challenges in
realistic scenarios: (1) The handcrafted control flow is often ad-hoc and
constraints the LLM to local planning; (2) The LLM is instructed to use only
manually demonstrated tools or well-trained Python functions, which limits its
generalization to new tools. In this work, we first propose Automatic Tool
Chain (ATC), a framework that enables the LLM to act as a multi-tool user,
which directly utilizes a chain of tools through programming. To scale up the
scope of the tools, we next propose a black-box probing method. This further
empowers the LLM as a tool learner that can actively discover and document tool
usages, teaching themselves to properly master new tools. For a comprehensive
evaluation, we build a challenging benchmark named ToolFlow, which diverges
from previous benchmarks by its long-term planning scenarios and complex
toolset. Experiments on both existing datasets and ToolFlow illustrate the
superiority of our framework. Analysis on different settings also validates the
effectiveness and the utility of our black-box probing algorithm.",2024-05-26T11:40:58Z
,http://arxiv.org/pdf/2401.03676v1.pdf,"Assessing AI Detectors in Identifying AI-Generated Code: Implications
  for Education","Educators are increasingly concerned about the usage of Large Language Models
(LLMs) such as ChatGPT in programming education, particularly regarding the
potential exploitation of imperfections in Artificial Intelligence Generated
Content (AIGC) Detectors for academic misconduct. In this paper, we present an
empirical study where the LLM is examined for its attempts to bypass detection
by AIGC Detectors. This is achieved by generating code in response to a given
question using different variants. We collected a dataset comprising 5,069
samples, with each sample consisting of a textual description of a coding
problem and its corresponding human-written Python solution codes. These
samples were obtained from various sources, including 80 from Quescol, 3,264
from Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of
code problem variant prompts, which were used to instruct ChatGPT to generate
the outputs. Subsequently, we assessed the performance of five AIGC detectors.
Our results demonstrate that existing AIGC Detectors perform poorly in
distinguishing between human-written code and AI-generated code.",2024-01-08T05:53:52Z
,http://arxiv.org/pdf/2404.13066v2.pdf,"Leveraging Large Language Model as Simulated Patients for Clinical
  Education","Simulated Patients (SPs) play a crucial role in clinical medical education by
providing realistic scenarios for student practice. However, the high cost of
training and hiring qualified SPs, along with the heavy workload and potential
risks they face in consistently portraying actual patients, limit students'
access to this type of clinical training. Consequently, the integration of
computer program-based simulated patients has emerged as a valuable educational
tool in recent years. With the rapid development of Large Language Models
(LLMs), their exceptional capabilities in conversational artificial
intelligence and role-playing have been demonstrated, making them a feasible
option for implementing Virtual Simulated Patient (VSP). In this paper, we
present an integrated model-agnostic framework called CureFun that harnesses
the potential of LLMs in clinical medical education. This framework facilitates
natural conversations between students and simulated patients, evaluates their
dialogue, and provides suggestions to enhance students' clinical inquiry
skills. Through comprehensive evaluations, our approach demonstrates more
authentic and professional SP-scenario dialogue flows compared to other
LLM-based chatbots, thus proving its proficiency in simulating patients.
Additionally, leveraging CureFun's evaluation ability, we assess several
medical LLMs and discuss the possibilities and limitations of using LLMs as
virtual doctors from the perspective of their diagnostic abilities.",2024-04-13T06:36:32Z
,http://arxiv.org/pdf/2302.04662v2.pdf,"Generating High-Precision Feedback for Programming Syntax Errors using
  Large Language Models","Large language models (LLMs), such as Codex, hold great promise in enhancing
programming education by automatically generating feedback for students. We
investigate using LLMs to generate feedback for fixing syntax errors in Python
programs, a key scenario in introductory programming. More concretely, given a
student's buggy program, our goal is to generate feedback comprising a fixed
program along with a natural language explanation describing the errors/fixes,
inspired by how a human tutor would give feedback. While using LLMs is
promising, the critical challenge is to ensure high precision in the generated
feedback, which is imperative before deploying such technology in classrooms.
The main research question we study is: Can we develop LLMs-based feedback
generation techniques with a tunable precision parameter, giving educators
quality control over the feedback that students receive? To this end, we
introduce PyFiXV, our technique to generate high-precision feedback powered by
Codex. The key idea behind PyFiXV is to use a novel run-time validation
mechanism to decide whether the generated feedback is suitable for sharing with
the student; notably, this validation mechanism also provides a precision knob
to educators. We perform an extensive evaluation using two real-world datasets
of Python programs with syntax errors and show the efficacy of PyFiXV in
generating high-precision feedback.",2023-01-24T13:00:25Z
,http://arxiv.org/pdf/2310.06225v2.pdf,"GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using
  Large Language Models","Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs' performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4's ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models' capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.",2023-10-10T00:39:04Z
,http://arxiv.org/pdf/2401.05856v1.pdf,"Seven Failure Points When Engineering a Retrieval Augmented Generation
  System","Software engineers are increasingly adding semantic search capabilities to
applications using a strategy known as Retrieval Augmented Generation (RAG). A
RAG system involves finding documents that semantically match a query and then
passing the documents to a large language model (LLM) such as ChatGPT to
extract the right answer using an LLM. RAG systems aim to: a) reduce the
problem of hallucinated responses from LLMs, b) link sources/references to
generated responses, and c) remove the need for annotating documents with
meta-data. However, RAG systems suffer from limitations inherent to information
retrieval systems and from reliance on LLMs. In this paper, we present an
experience report on the failure points of RAG systems from three case studies
from separate domains: research, education, and biomedical. We share the
lessons learned and present 7 failure points to consider when designing a RAG
system. The two key takeaways arising from our work are: 1) validation of a RAG
system is only feasible during operation, and 2) the robustness of a RAG system
evolves rather than designed in at the start. We conclude with a list of
potential research directions on RAG systems for the software engineering
community.",2024-01-11T12:04:11Z
,http://arxiv.org/pdf/2404.01754v1.pdf,"Peer-aided Repairer: Empowering Large Language Models to Repair Advanced
  Student Assignments","Automated generation of feedback on programming assignments holds significant
benefits for programming education, especially when it comes to advanced
assignments. Automated Program Repair techniques, especially Large Language
Model based approaches, have gained notable recognition for their potential to
fix introductory assignments. However, the programs used for evaluation are
relatively simple. It remains unclear how existing approaches perform in
repairing programs from higher-level programming courses. To address these
limitations, we curate a new advanced student assignment dataset named
Defects4DS from a higher-level programming course. Subsequently, we identify
the challenges related to fixing bugs in advanced assignments. Based on the
analysis, we develop a framework called PaR that is powered by the LLM. PaR
works in three phases: Peer Solution Selection, Multi-Source Prompt Generation,
and Program Repair. Peer Solution Selection identifies the closely related peer
programs based on lexical, semantic, and syntactic criteria. Then Multi-Source
Prompt Generation adeptly combines multiple sources of information to create a
comprehensive and informative prompt for the last Program Repair stage. The
evaluation on Defects4DS and another well-investigated ITSP dataset reveals
that PaR achieves a new state-of-the-art performance, demonstrating impressive
improvements of 19.94% and 15.2% in repair rate compared to prior
state-of-the-art LLM- and symbolic-based approaches, respectively",2024-04-02T09:12:21Z
,http://arxiv.org/pdf/2401.08664v3.pdf,"Adapting Large Language Models for Education: Foundational Capabilities,
  Potentials, and Challenges","Online education platforms, leveraging the internet to distribute education
resources, seek to provide convenient education but often fall short in
real-time communication with students. They often struggle to address the
diverse obstacles students encounter throughout their learning journey. Solving
the problems encountered by students poses a significant challenge for
traditional deep learning models, as it requires not only a broad spectrum of
subject knowledge but also the ability to understand what constitutes a
student's individual difficulties. It's challenging for traditional machine
learning models, as they lack the capacity to comprehend students' personalized
needs. Recently, the emergence of large language models (LLMs) offers the
possibility for resolving this issue by comprehending individual requests.
Although LLMs have been successful in various fields, creating an LLM-based
education system is still challenging for the wide range of educational skills
required. This paper reviews the recently emerged LLM research related to
educational capabilities, including mathematics, writing, programming,
reasoning, and knowledge-based question answering, with the aim to explore
their potential in constructing the next-generation intelligent education
system. Specifically, for each capability, we focus on investigating two
aspects. Firstly, we examine the current state of LLMs regarding this
capability: how advanced they have become, whether they surpass human
abilities, and what deficiencies might exist. Secondly, we evaluate whether the
development methods for LLMs in this area are generalizable, that is, whether
these methods can be applied to construct a comprehensive educational
supermodel with strengths across various capabilities, rather than being
effective in only a singular aspect.",2023-12-27T14:37:32Z
,http://arxiv.org/pdf/2304.14342v1.pdf,"Thinking beyond chatbots' threat to education: Visualizations to
  elucidate the writing and coding process","The landscape of educational practices for teaching and learning languages
has been predominantly centered around outcome-driven approaches. The recent
accessibility of large language models has thoroughly disrupted these
approaches. As we transform our language teaching and learning practices to
account for this disruption, it is important to note that language learning
plays a pivotal role in developing human intelligence. Writing and computer
programming are two essential skills integral to our education systems. What
and how we write shapes our thinking and sets us on the path of self-directed
learning. While most educators understand that `process' and `product' are both
important and inseparable, in most educational settings, providing constructive
feedback on a learner's formative process is challenging. For instance, it is
straightforward in computer programming to assess whether a learner-submitted
code runs. However, evaluating the learner's creative process and providing
meaningful feedback on the process can be challenging. To address this
long-standing issue in education (and learning), this work presents a new set
of visualization tools to summarize the inherent and taught capabilities of a
learner's writing or programming process. These interactive Process
Visualizations (PVs) provide insightful, empowering, and personalized
process-oriented feedback to the learners. The toolbox is ready to be tested by
educators and learners and is publicly available at www.processfeedback.org.
Focusing on providing feedback on a learner's process--from self, peers, and
educators--will facilitate learners' ability to acquire higher-order skills
such as self-directed learning and metacognition.",2023-04-25T22:11:29Z
,http://arxiv.org/pdf/2403.15600v1.pdf,"Just another copy and paste? Comparing the security vulnerabilities of
  ChatGPT generated code and StackOverflow answers","Sonatype's 2023 report found that 97% of developers and security leads
integrate generative Artificial Intelligence (AI), particularly Large Language
Models (LLMs), into their development process. Concerns about the security
implications of this trend have been raised. Developers are now weighing the
benefits and risks of LLMs against other relied-upon information sources, such
as StackOverflow (SO), requiring empirical data to inform their choice. In this
work, our goal is to raise software developers awareness of the security
implications when selecting code snippets by empirically comparing the
vulnerabilities of ChatGPT and StackOverflow. To achieve this, we used an
existing Java dataset from SO with security-related questions and answers.
Then, we asked ChatGPT the same SO questions, gathering the generated code for
comparison. After curating the dataset, we analyzed the number and types of
Common Weakness Enumeration (CWE) vulnerabilities of 108 snippets from each
platform using CodeQL. ChatGPT-generated code contained 248 vulnerabilities
compared to the 302 vulnerabilities found in SO snippets, producing 20% fewer
vulnerabilities with a statistically significant difference. Additionally,
ChatGPT generated 19 types of CWE, fewer than the 22 found in SO. Our findings
suggest developers are under-educated on insecure code propagation from both
platforms, as we found 274 unique vulnerabilities and 25 types of CWE. Any code
copied and pasted, created by AI or humans, cannot be trusted blindly,
requiring good software engineering practices to reduce risk. Future work can
help minimize insecure code propagation from any platform.",2024-03-22T20:06:41Z
,http://arxiv.org/pdf/2405.05253v1.pdf,"Open Source Language Models Can Provide Feedback: Evaluating LLMs'
  Ability to Help Students Using GPT-4-As-A-Judge","Large language models (LLMs) have shown great potential for the automatic
generation of feedback in a wide range of computing contexts. However, concerns
have been voiced around the privacy and ethical implications of sending student
work to proprietary models. This has sparked considerable interest in the use
of open source LLMs in education, but the quality of the feedback that such
open models can produce remains understudied. This is a concern as providing
flawed or misleading generated feedback could be detrimental to student
learning. Inspired by recent work that has utilised very powerful LLMs, such as
GPT-4, to evaluate the outputs produced by less powerful models, we conduct an
automated analysis of the quality of the feedback produced by several open
source models using a dataset from an introductory programming course. First,
we investigate the viability of employing GPT-4 as an automated evaluator by
comparing its evaluations with those of a human expert. We observe that GPT-4
demonstrates a bias toward positively rating feedback while exhibiting moderate
agreement with human raters, showcasing its potential as a feedback evaluator.
Second, we explore the quality of feedback generated by several leading
open-source LLMs by using GPT-4 to evaluate the feedback. We find that some
models offer competitive performance with popular proprietary LLMs, such as
ChatGPT, indicating opportunities for their responsible use in educational
settings.",2024-05-08T17:57:39Z
10.1145/3613904.3642377,http://arxiv.org/pdf/2401.17163v2.pdf,"Learning Agent-based Modeling with LLM Companions: Experiences of
  Novices and Experts Using ChatGPT & NetLogo Chat","Large Language Models (LLMs) have the potential to fundamentally change the
way people engage in computer programming. Agent-based modeling (ABM) has
become ubiquitous in natural and social sciences and education, yet no prior
studies have explored the potential of LLMs to assist it. We designed NetLogo
Chat to support the learning and practice of NetLogo, a programming language
for ABM. To understand how users perceive, use, and need LLM-based interfaces,
we interviewed 30 participants from global academia, industry, and graduate
schools. Experts reported more perceived benefits than novices and were more
inclined to adopt LLMs in their workflow. We found significant differences
between experts and novices in their perceptions, behaviors, and needs for
human-AI collaboration. We surfaced a knowledge gap between experts and novices
as a possible reason for the benefit gap. We identified guidance,
personalization, and integration as major needs for LLM-based interfaces to
support the programming of ABM.",2024-01-30T16:49:50Z
,http://arxiv.org/pdf/2308.10454v1.pdf,"Elucidating STEM Concepts through Generative AI: A Multi-modal
  Exploration of Analogical Reasoning","This study explores the integration of generative artificial intelligence
(AI), specifically large language models, with multi-modal analogical reasoning
as an innovative approach to enhance science, technology, engineering, and
mathematics (STEM) education. We have developed a novel system that utilizes
the capacities of generative AI to transform intricate principles in
mathematics, physics, and programming into comprehensible metaphors. To further
augment the educational experience, these metaphors are subsequently converted
into visual form. Our study aims to enhance the learners' understanding of STEM
concepts and their learning engagement by using the visual metaphors. We
examine the efficacy of our system via a randomized A/B/C test, assessing
learning gains and motivation shifts among the learners. Our study demonstrates
the potential of applying large language models to educational practice on STEM
subjects. The results will shed light on the design of educational system in
terms of harnessing AI's potential to empower educational stakeholders.",2023-08-21T04:00:56Z
,http://arxiv.org/pdf/2312.01109v1.pdf,"Kattis vs. ChatGPT: Assessment and Evaluation of Programming Tasks in
  the Age of Artificial Intelligence","AI-powered education technologies can support students and teachers in
computer science education. However, with the recent developments in generative
AI, and especially the increasingly emerging popularity of ChatGPT, the
effectiveness of using large language models for solving programming tasks has
been underexplored. The present study examines ChatGPT's ability to generate
code solutions at different difficulty levels for introductory programming
courses. We conducted an experiment where ChatGPT was tested on 127 randomly
selected programming problems provided by Kattis, an automatic software grading
tool for computer science programs, often used in higher education. The results
showed that ChatGPT independently could solve 19 out of 127 programming tasks
generated and assessed by Kattis. Further, ChatGPT was found to be able to
generate accurate code solutions for simple problems but encountered
difficulties with more complex programming tasks. The results contribute to the
ongoing debate on the utility of AI-powered tools in programming education.",2023-12-02T11:09:17Z
,http://arxiv.org/pdf/2404.10779v1.pdf,Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations,"There is a compelling necessity from enterprises for fine tuning LLMs (Large
Language Models) o get them trained on proprietary domain knowledge. The
challenge is to imbibe the LLMs with domain specific knowledge using the most
optimial resource and cost and in the best possible time. Many enterprises rely
on RAG (Retrieval Augmented Generation) which does not need LLMs to be
ine-tuned but they are limited by the quality of vector databases and their
retrieval capabilities rather than the intrinsic capabilities of the LLMs
themselves. In our current work we focus on fine tuning LLaMA, an open source
LLM using proprietary documents and code from an enterprise repository and use
the fine tuned models to evaluate the quality of responses. As part of this
work, we aim to guide beginners on how to start with fine tuning an LLM for
documentation and code by making educated guesses on size of GPU required and
options that are available for formatting the data. We also propose pre
processing recipes for both documentation and code to prepare dataset in
different formats. The proposed methods of data preparation for document
datasets are forming paragraph chunks, forming question and answer pairs and
forming keyword and paragraph chunk pairs. For code dataset we propose forming
summary and function pairs. Further, we qualitatively evaluate the results of
the models for domain specific queries. Finally, we also propose practical
guidelines and recommendations for fine tuning LLMs.",2024-03-23T13:25:01Z
,http://arxiv.org/pdf/2302.12834v1.pdf,"Leveraging Large Language Model and Story-Based Gamification in
  Intelligent Tutoring System to Scaffold Introductory Programming Courses: A
  Design-Based Research Study","Programming skills are rapidly becoming essential for many educational paths
and career opportunities. Yet, for many international students, the traditional
approach to teaching introductory programming courses can be a significant
challenge due to the complexities of the language, the lack of prior
programming knowledge, and the language and cultural barriers. This study
explores how large language models and gamification can scaffold coding
learning and increase Chinese students sense of belonging in introductory
programming courses. In this project, a gamification intelligent tutoring
system was developed to adapt to Chinese international students learning needs
and provides scaffolding to support their success in introductory computer
programming courses.",2023-02-25T04:07:03Z
,http://arxiv.org/pdf/2308.06921v1.pdf,"CodeHelp: Using Large Language Models with Guardrails for Scalable
  Support in Programming Classes","Computing educators face significant challenges in providing timely support
to students, especially in large class settings. Large language models (LLMs)
have emerged recently and show great promise for providing on-demand help at a
large scale, but there are concerns that students may over-rely on the outputs
produced by these models. In this paper, we introduce CodeHelp, a novel
LLM-powered tool designed with guardrails to provide on-demand assistance to
programming students without directly revealing solutions. We detail the design
of the tool, which incorporates a number of useful features for instructors,
and elaborate on the pipeline of prompting strategies we use to ensure
generated outputs are suitable for students. To evaluate CodeHelp, we deployed
it in a first-year computer and data science course with 52 students and
collected student interactions over a 12-week period. We examine students'
usage patterns and perceptions of the tool, and we report reflections from the
course instructor and a series of recommendations for classroom use. Our
findings suggest that CodeHelp is well-received by students who especially
value its availability and help with resolving errors, and that for instructors
it is easy to deploy and complements, rather than replaces, the support that
they provide to students.",2023-08-14T03:52:24Z
,http://arxiv.org/pdf/2310.20105v1.pdf,"Efficient Classification of Student Help Requests in Programming Courses
  Using Large Language Models","The accurate classification of student help requests with respect to the type
of help being sought can enable the tailoring of effective responses.
Automatically classifying such requests is non-trivial, but large language
models (LLMs) appear to offer an accessible, cost-effective solution. This
study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying
help requests from students in an introductory programming class. In zero-shot
trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories,
while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests
related to debugging. Fine-tuning the GPT-3.5 model improved its performance to
such an extent that it approximated the accuracy and consistency across
categories observed between two human raters. Overall, this study demonstrates
the feasibility of using LLMs to enhance educational systems through the
automated classification of student needs.",2023-10-31T00:56:33Z
,http://arxiv.org/pdf/2404.15639v1.pdf,"CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models
  of Code","As Large Language Models (LLMs) are increasingly used to automate code
generation, it is often desired to know if the code is AI-generated and by
which model, especially for purposes like protecting intellectual property (IP)
in industry and preventing academic misconduct in education. Incorporating
watermarks into machine-generated content is one way to provide code
provenance, but existing solutions are restricted to a single bit or lack
flexibility. We present CodeIP, a new watermarking technique for LLM-based code
generation. CodeIP enables the insertion of multi-bit information while
preserving the semantics of the generated code, improving the strength and
diversity of the inerseted watermark. This is achieved by training a type
predictor to predict the subsequent grammar type of the next token to enhance
the syntactical and semantic correctness of the generated code. Experiments on
a real-world dataset across five programming languages showcase the
effectiveness of CodeIP.",2024-04-24T04:25:04Z
10.1145/3568813.3600142,http://arxiv.org/pdf/2306.10073v1.pdf,"Thrilled by Your Progress! Large Language Models (GPT-4) No Longer
  Struggle to Pass Assessments in Higher Education Programming Courses","This paper studies recent developments in large language models' (LLM)
abilities to pass assessments in introductory and intermediate Python
programming courses at the postsecondary level. The emergence of ChatGPT
resulted in heated debates of its potential uses (e.g., exercise generation,
code explanation) as well as misuses in programming classes (e.g., cheating).
Recent studies show that while the technology performs surprisingly well on
diverse sets of assessment instruments employed in typical programming classes
the performance is usually not sufficient to pass the courses. The release of
GPT-4 largely emphasized notable improvements in the capabilities related to
handling assessments originally designed for human test-takers. This study is
the necessary analysis in the context of this ongoing transition towards mature
generative AI systems. Specifically, we report the performance of GPT-4,
comparing it to the previous generations of GPT models, on three Python courses
with assessments ranging from simple multiple-choice questions (no code
involved) to complex programming projects with code bases distributed into
multiple files (599 exercises overall). Additionally, we analyze the
assessments that were not handled well by GPT-4 to understand the current
limitations of the model, as well as its capabilities to leverage feedback
provided by an auto-grader. We found that the GPT models evolved from
completely failing the typical programming class' assessments (the original
GPT-3) to confidently passing the courses with no human involvement (GPT-4).
While we identified certain limitations in GPT-4's handling of MCQs and coding
exercises, the rate of improvement across the recent generations of GPT models
strongly suggests their potential to handle almost any type of assessment
widely used in higher education programming courses. These findings could be
leveraged by educators and institutions to adapt the design of programming
assessments as well as to fuel the necessary discussions into how programming
classes should be updated to reflect the recent technological developments.
This study provides evidence that programming instructors need to prepare for a
world in which there is an easy-to-use widely accessible technology that can be
utilized by learners to collect passing scores, with no effort whatsoever, on
what today counts as viable programming knowledge and skills assessments.",2023-06-15T22:12:34Z
,http://arxiv.org/pdf/2307.16696v2.pdf,"Large Language Models for Education: Grading Open-Ended Questions Using
  ChatGPT","As a way of addressing increasingly sophisticated problems, software
professionals face the constant challenge of seeking improvement. However, for
these individuals to enhance their skills, their process of studying and
training must involve feedback that is both immediate and accurate. In the
context of software companies, where the scale of professionals undergoing
training is large, but the number of qualified professionals available for
providing corrections is small, delivering effective feedback becomes even more
challenging. To circumvent this challenge, this work presents an exploration of
using Large Language Models (LLMs) to support the correction process of
open-ended questions in technical training. In this study, we utilized ChatGPT
to correct open-ended questions answered by 42 industry professionals on two
topics. Evaluating the corrections and feedback provided by ChatGPT, we
observed that it is capable of identifying semantic details in responses that
other metrics cannot observe. Furthermore, we noticed that, in general, subject
matter experts tended to agree with the corrections and feedback given by
ChatGPT.",2023-07-31T14:12:06Z
,http://arxiv.org/pdf/2401.03374v2.pdf,"LLM-Powered Code Vulnerability Repair with Reinforcement Learning and
  Semantic Reward","In software development, the predominant emphasis on functionality often
supersedes security concerns, a trend gaining momentum with AI-driven
automation tools like GitHub Copilot. These tools significantly improve
developers' efficiency in functional code development. Nevertheless, it remains
a notable concern that such tools are also responsible for creating insecure
code, predominantly because of pre-training on publicly available repositories
with vulnerable code. Moreover, developers are called the ""weakest link in the
chain"" since they have very minimal knowledge of code security. Although
existing solutions provide a reasonable solution to vulnerable code, they must
adequately describe and educate the developers on code security to ensure that
the security issues are not repeated. Therefore we introduce a multipurpose
code vulnerability analysis system \texttt{SecRepair}, powered by a large
language model, CodeGen2 assisting the developer in identifying and generating
fixed code along with a complete description of the vulnerability with a code
comment. Our innovative methodology uses a reinforcement learning paradigm to
generate code comments augmented by a semantic reward mechanism. Inspired by
how humans fix code issues, we propose an instruction-based dataset suitable
for vulnerability analysis with LLMs. We further identify zero-day and N-day
vulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findings
underscore that incorporating reinforcement learning coupled with semantic
reward augments our model's performance, thereby fortifying its capacity to
address code vulnerabilities with improved efficacy.",2024-01-07T02:46:39Z
10.1145/3636243.3636257,http://arxiv.org/pdf/2311.09651v2.pdf,"""It's not like Jarvis, but it's pretty close!"" -- Examining ChatGPT's
  Usage among Undergraduate Students in Computer Science","Large language models (LLMs) such as ChatGPT and Google Bard have garnered
significant attention in the academic community. Previous research has
evaluated these LLMs for various applications such as generating programming
exercises and solutions. However, these evaluations have predominantly been
conducted by instructors and researchers, not considering the actual usage of
LLMs by students. This study adopts a student-first approach to comprehensively
understand how undergraduate computer science students utilize ChatGPT, a
popular LLM, released by OpenAI. We employ a combination of student surveys and
interviews to obtain valuable insights into the benefits, challenges, and
suggested improvements related to ChatGPT. Our findings suggest that a majority
of students (over 57%) have a convincingly positive outlook towards adopting
ChatGPT as an aid in coursework-related tasks. However, our research also
highlights various challenges that must be resolved for long-term acceptance of
ChatGPT amongst students. The findings from this investigation have broader
implications and may be applicable to other LLMs and their role in computing
education.",2023-11-16T08:10:18Z
,http://arxiv.org/pdf/2310.14735v4.pdf,"Unleashing the potential of prompt engineering in Large Language Models:
  a comprehensive review","This paper delves into the pivotal role of prompt engineering in unleashing
the capabilities of Large Language Models (LLMs). Prompt engineering is the
process of structuring input text for LLMs and is a technique integral to
optimizing the efficacy of LLMs. This survey elucidates foundational principles
of prompt engineering, such as role-prompting, one-shot, and few-shot
prompting, as well as more advanced methodologies such as the chain-of-thought
and tree-of-thoughts prompting. The paper sheds light on how external
assistance in the form of plugins can assist in this task, and reduce machine
hallucination by retrieving external knowledge. We subsequently delineate
prospective directions in prompt engineering research, emphasizing the need for
a deeper understanding of structures and the role of agents in Artificial
Intelligence-Generated Content (AIGC) tools. We discuss how to assess the
efficacy of prompt methods from different perspectives and using different
methods. Finally, we gather information about the application of prompt
engineering in such fields as education and programming, showing its
transformative potential. This comprehensive survey aims to serve as a friendly
guide for anyone venturing through the big world of LLMs and prompt
engineering.",2023-10-23T09:15:18Z
,http://arxiv.org/pdf/2403.14668v1.pdf,"Predicting Learning Performance with Large Language Models: A Study in
  Adult Literacy","Intelligent Tutoring Systems (ITSs) have significantly enhanced adult
literacy training, a key factor for societal participation, employment
opportunities, and lifelong learning. Our study investigates the application of
advanced AI models, including Large Language Models (LLMs) like GPT-4, for
predicting learning performance in adult literacy programs in ITSs. This
research is motivated by the potential of LLMs to predict learning performance
based on its inherent reasoning and computational capabilities. By using
reading comprehension datasets from the ITS, AutoTutor, we evaluate the
predictive capabilities of GPT-4 versus traditional machine learning methods in
predicting learning performance through five-fold cross-validation techniques.
Our findings show that the GPT-4 presents the competitive predictive abilities
with traditional machine learning methods such as Bayesian Knowledge Tracing,
Performance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor
factorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained
on local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected
XGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior
performance compared to local machine execution. Moreover, our investigation
into hyper-parameter tuning by GPT-4 versus grid-search suggests comparable
performance, albeit with less stability in the automated approach, using
XGBoost as the case study. Our study contributes to the field by highlighting
the potential of integrating LLMs with traditional machine learning models to
enhance predictive accuracy and personalize adult literacy education, setting a
foundation for future research in applying LLMs within ITSs.",2024-03-04T08:14:07Z
,http://arxiv.org/pdf/2310.15127v2.pdf,"Open-Ended Instructable Embodied Agents with Memory-Augmented Large
  Language Models","Pre-trained and frozen large language models (LLMs) can effectively map
simple scene rearrangement instructions to programs over a robot's visuomotor
functions through appropriate few-shot example prompting. To parse open-domain
natural language and adapt to a user's idiosyncratic procedures, not known
during prompt engineering time, fixed prompts fall short. In this paper, we
introduce HELPER, an embodied agent equipped with an external memory of
language-program pairs that parses free-form human-robot dialogue into action
programs through retrieval-augmented LLM prompting: relevant memories are
retrieved based on the current dialogue, instruction, correction, or VLM
description, and used as in-context prompt examples for LLM querying. The
memory is expanded during deployment to include pairs of user's language and
action plans, to assist future inferences and personalize them to the user's
language and routines. HELPER sets a new state-of-the-art in the TEACh
benchmark in both Execution from Dialog History (EDH) and Trajectory from
Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for
TfD. Our models, code, and video results can be found in our project's website:
https://helper-agent-llm.github.io.",2023-10-23T17:31:55Z
,http://arxiv.org/pdf/2311.16017v1.pdf,"Decoding Logic Errors: A Comparative Study on Bug Detection by Students
  and Large Language Models","Identifying and resolving logic errors can be one of the most frustrating
challenges for novices programmers. Unlike syntax errors, for which a compiler
or interpreter can issue a message, logic errors can be subtle. In certain
conditions, buggy code may even exhibit correct behavior -- in other cases, the
issue might be about how a problem statement has been interpreted. Such errors
can be hard to spot when reading the code, and they can also at times be missed
by automated tests. There is great educational potential in automatically
detecting logic errors, especially when paired with suitable feedback for
novices. Large language models (LLMs) have recently demonstrated surprising
performance for a range of computing tasks, including generating and explaining
code. These capabilities are closely linked to code syntax, which aligns with
the next token prediction behavior of LLMs. On the other hand, logic errors
relate to the runtime performance of code and thus may not be as well suited to
analysis by LLMs. To explore this, we investigate the performance of two
popular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly
explanation of logic errors. We compare LLM performance with a large cohort of
introductory computing students $(n=964)$ solving the same error detection
task. Through a mixed-methods analysis of student and model responses, we
observe significant improvement in logic error identification between the
previous and current generation of LLMs, and find that both LLM generations
significantly outperform students. We outline how such models could be
integrated into computing education tools, and discuss their potential for
supporting students when learning programming.",2023-11-27T17:28:33Z
,http://arxiv.org/pdf/2405.00750v1.pdf,"From Keyboard to Chatbot: An AI-powered Integration Platform with
  Large-Language Models for Teaching Computational Thinking for Young Children","Teaching programming in early childhood (4-9) to enhance computational
thinking has gained popularity in the recent movement of computer science for
all. However, current practices ignore some fundamental issues resulting from
young children's developmental readiness, such as the sustained capability to
keyboarding, the decomposition of complex tasks to small tasks, the need for
intuitive mapping from abstract programming to tangible outcomes, and the
limited amount of screen time exposure. To address these issues in this paper,
we present a novel methodology with an AI-powered integration platform to
effectively teach computational thinking for young children. The system
features a hybrid pedagogy that supports both the top-down and bottom-up
approach for teaching computational thinking. Young children can describe their
desired task in natural language, while the system can respond with an
easy-to-understand program consisting of the right level of decomposed
sub-tasks. A tangible robot can immediately execute the decomposed program and
demonstrate the program's outcomes to young children. The system is equipped
with an intelligent chatbot that can interact with young children through
natural languages, and children can speak to the chatbot to complete all the
needed programming tasks, while the chatbot orchestrates the execution of the
program onto the robot. This would completely eliminates the need of keyboards
for young children to program. By developing such a system, we aim to make the
concept of computational thinking more accessible to young children, fostering
a natural understanding of programming concepts without the need of explicit
programming skills. Through the interactive experience provided by the robotic
agent, our system seeks to engage children in an effective manner, contributing
to the field of educational technology for early childhood computer science
education.",2024-05-01T04:29:21Z
,http://arxiv.org/pdf/2401.07518v3.pdf,"Survey of Natural Language Processing for Education: Taxonomy,
  Systematic Review, and Future Trends","Natural Language Processing (NLP) aims to analyze text or speech via
techniques in the computer science field. It serves the applications in domains
of healthcare, commerce, education and so on. Particularly, NLP has been widely
applied to the education domain and its applications have enormous potential to
help teaching and learning. In this survey, we review recent advances in NLP
with the focus on solving problems relevant to the education domain. In detail,
we begin with introducing the related background and the real-world scenarios
in education where NLP techniques could contribute. Then, we present a taxonomy
of NLP in the education domain and highlight typical NLP applications including
question answering, question construction, automated assessment, and error
correction. Next, we illustrate the task definition, challenges, and
corresponding cutting-edge techniques based on the above taxonomy. In
particular, LLM-involved methods are included for discussion due to the wide
usage of LLMs in diverse NLP applications. After that, we showcase some
off-the-shelf demonstrations in this domain. At last, we conclude with six
promising directions for future research, including more datasets in education
domain, controllable usage of LLMs, intervention of difficulty-level control,
interpretable educational NLP, methods with adaptive learning, and integrated
systems for education. We organize all relevant datasets and papers in the
open-available Github Link for better
review~\url{https://github.com/LiXinyuan1015/NLP-for-Education}.",2024-01-15T07:48:42Z
10.1145/3636243.3636256,http://arxiv.org/pdf/2312.03173v1.pdf,"A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in
  Programming Education","There is a constant need for educators to develop and maintain effective
up-to-date assessments. While there is a growing body of research in computing
education on utilizing large language models (LLMs) in generation and
engagement with coding exercises, the use of LLMs for generating programming
MCQs has not been extensively explored. We analyzed the capability of GPT-4 to
produce multiple-choice questions (MCQs) aligned with specific learning
objectives (LOs) from Python programming classes in higher education.
Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs
from high-level course context and module-level LOs. We evaluated 651
LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python
courses. We found that GPT-4 was capable of producing MCQs with clear language,
a single correct choice, and high-quality distractors. We also observed that
the generated MCQs appeared to be well-aligned with the LOs. Our findings can
be leveraged by educators wishing to take advantage of the state-of-the-art
generative models to support MCQ authoring efforts.",2023-12-05T22:29:43Z
,http://arxiv.org/pdf/2308.10410v4.pdf,"Large Language Models on Wikipedia-Style Survey Generation: an
  Evaluation in NLP Concepts","Educational materials such as survey articles in specialized fields like
computer science traditionally require tremendous expert inputs and are
therefore expensive to create and update. Recently, Large Language Models
(LLMs) have achieved significant success across various general tasks. However,
their effectiveness and limitations in the education domain are yet to be fully
explored. In this work, we examine the proficiency of LLMs in generating
succinct survey articles specific to the niche field of NLP in computer
science, focusing on a curated list of 99 topics. Automated benchmarks reveal
that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by
margins ranging from 2% to 20% in comparison to the established ground truth.
We compare both human and GPT-based evaluation scores and provide in-depth
analysis. While our findings suggest that GPT-created surveys are more
contemporary and accessible than human-authored ones, certain limitations were
observed. Notably, GPT-4, despite often delivering outstanding content,
occasionally exhibited lapses like missing details or factual errors. At last,
we compared the rating behavior between humans and GPT-4 and found systematic
bias in using GPT evaluation.",2023-08-21T01:32:45Z
,http://arxiv.org/pdf/2312.11567v1.pdf,"Students' Perceptions and Preferences of Generative Artificial
  Intelligence Feedback for Programming","The rapid evolution of artificial intelligence (AI), specifically large
language models (LLMs), has opened opportunities for various educational
applications. This paper explored the feasibility of utilizing ChatGPT, one of
the most popular LLMs, for automating feedback for Java programming assignments
in an introductory computer science (CS1) class. Specifically, this study
focused on three questions: 1) To what extent do students view LLM-generated
feedback as formative? 2) How do students see the comparative affordances of
feedback prompts that include their code, vs. those that exclude it? 3) What
enhancements do students suggest for improving AI-generated feedback? To
address these questions, we generated automated feedback using the ChatGPT API
for four lab assignments in the CS1 class. The survey results revealed that
students perceived the feedback as aligning well with formative feedback
guidelines established by Shute. Additionally, students showed a clear
preference for feedback generated by including the students' code as part of
the LLM prompt, and our thematic study indicated that the preference was mainly
attributed to the specificity, clarity, and corrective nature of the feedback.
Moreover, this study found that students generally expected specific and
corrective feedback with sufficient code examples, but had diverged opinions on
the tone of the feedback. This study demonstrated that ChatGPT could generate
Java programming assignment feedback that students perceived as formative. It
also offered insights into the specific improvements that would make the
ChatGPT-generated feedback useful for students.",2023-12-17T22:26:53Z
,http://arxiv.org/pdf/2405.00302v3.pdf,"Generating Feedback-Ladders for Logical Errors in Programming using
  Large Language Models","In feedback generation for logical errors in programming assignments, large
language model (LLM)-based methods have shown great promise. These methods ask
the LLM to generate feedback given the problem statement and a student's
(buggy) submission. There are several issues with these types of methods.
First, the generated feedback messages are often too direct in revealing the
error in the submission and thus diminish valuable opportunities for the
student to learn. Second, they do not consider the student's learning context,
i.e., their previous submissions, current knowledge, etc. Third, they are not
layered since existing methods use a single, shared prompt for all student
submissions. In this paper, we explore using LLMs to generate a
""feedback-ladder"", i.e., multiple levels of feedback for the same
problem-submission pair. We evaluate the quality of the generated
feedback-ladder via a user study with students, educators, and researchers. We
have observed diminishing effectiveness for higher-level feedback and
higher-scoring submissions overall in the study. In practice, our method
enables teachers to select an appropriate level of feedback to show to a
student based on their personal learning context, or in a progressive manner to
go more detailed if a higher-level feedback fails to correct the student's
error.",2024-05-01T03:52:39Z
10.1145/3639474.3640058,http://arxiv.org/pdf/2404.11734v1.pdf,"Let's Ask AI About Their Programs: Exploring ChatGPT's Answers To
  Program Comprehension Questions","Recent research has explored the creation of questions from code submitted by
students. These Questions about Learners' Code (QLCs) are created through
program analysis, exploring execution paths, and then creating code
comprehension questions from these paths and the broader code structure.
Responding to the questions requires reading and tracing the code, which is
known to support students' learning. At the same time, computing education
researchers have witnessed the emergence of Large Language Models (LLMs) that
have taken the community by storm. Researchers have demonstrated the
applicability of these models especially in the introductory programming
context, outlining their performance in solving introductory programming
problems and their utility in creating new learning resources. In this work, we
explore the capability of the state-of-the-art LLMs (GPT-3.5 and GPT-4) in
answering QLCs that are generated from code that the LLMs have created. Our
results show that although the state-of-the-art LLMs can create programs and
trace program execution when prompted, they easily succumb to similar errors
that have previously been recorded for novice programmers. These results
demonstrate the fallibility of these models and perhaps dampen the expectations
fueled by the recent LLM hype. At the same time, we also highlight future
research possibilities such as using LLMs to mimic students as their behavior
can indeed be similar for some specific tasks.",2024-04-17T20:37:00Z
,http://arxiv.org/pdf/2403.15274v2.pdf,Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review,"The year 2023 marked a significant surge in the exploration of applying large
language model (LLM) chatbots, notably ChatGPT, across various disciplines. We
surveyed the applications of ChatGPT in bioinformatics and biomedical
informatics throughout the year, covering omics, genetics, biomedical text
mining, drug discovery, biomedical image understanding, bioinformatics
programming, and bioinformatics education. Our survey delineates the current
strengths and limitations of this chatbot in bioinformatics and offers insights
into potential avenues for future developments.",2024-03-22T15:16:23Z
,http://arxiv.org/pdf/2404.19065v1.pdf,"HELPER-X: A Unified Instructable Embodied Agent to Tackle Four
  Interactive Vision-Language Domains with Memory-Augmented Language Models","Recent research on instructable agents has used memory-augmented Large
Language Models (LLMs) as task planners, a technique that retrieves
language-program examples relevant to the input instruction and uses them as
in-context examples in the LLM prompt to improve the performance of the LLM in
inferring the correct action and task plans. In this technical report, we
extend the capabilities of HELPER, by expanding its memory with a wider array
of examples and prompts, and by integrating additional APIs for asking
questions. This simple expansion of HELPER into a shared memory enables the
agent to work across the domains of executing plans from dialogue, natural
language instruction following, active question asking, and commonsense room
reorganization. We evaluate the agent on four diverse interactive
visual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the
Tidy Task. HELPER-X achieves few-shot, state-of-the-art performance across
these benchmarks using a single agent, without requiring in-domain training,
and remains competitive with agents that have undergone in-domain training.",2024-04-29T19:12:42Z
10.1145/3626252.3630887,http://arxiv.org/pdf/2403.14971v1.pdf,Learners Teaching Novices: An Uplifting Alternative Assessment,"We propose and carry-out a novel method of formative assessment called
Assessment via Teaching (AVT), in which learners demonstrate their
understanding of CS1 topics by tutoring more novice students. AVT has powerful
benefits over traditional forms of assessment: it is centered around service to
others and is highly rewarding for the learners who teach. Moreover, teaching
greatly improves the learners' own understanding of the material and has a huge
positive impact on novices, who receive free 1:1 tutoring. Lastly, this form of
assessment is naturally difficult to cheat -- a critical property for
assessments in the era of large-language models.
  We use AVT in a randomised control trial with learners in a CS1 course at an
R1 university. The learners provide tutoring sessions to more novice students
taking a lagged online version of the same course. We show that learners who do
an AVT session before the course exam performed 20 to 30 percentage points
better than the class average on several questions. Moreover, compared to
students who did a practice exam, the AVT learners enjoyed their experience
more and were twice as likely to study for their teaching session. We believe
AVT is a scalable and uplifting method for formative assessment that could one
day replace traditional exams.",2024-03-22T06:01:00Z
,http://arxiv.org/pdf/2301.12867v4.pdf,"Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and
  Toxicity","Recent breakthroughs in natural language processing (NLP) have permitted the
synthesis and comprehension of coherent text in an open-ended way, therefore
translating the theoretical algorithms into practical applications. The large
language models (LLMs) have significantly impacted businesses such as report
summarization software and copywriters. Observations indicate, however, that
LLMs may exhibit social prejudice and toxicity, posing ethical and societal
dangers of consequences resulting from irresponsibility. Large-scale benchmarks
for accountable LLMs should consequently be developed. Although several
empirical investigations reveal the existence of a few ethical difficulties in
advanced LLMs, there is little systematic examination and user study of the
risks and harmful behaviors of current LLM usage. To further educate future
efforts on constructing ethical LLMs responsibly, we perform a qualitative
research method called ``red teaming'' on OpenAI's ChatGPT\footnote{In this
paper, ChatGPT refers to the version released on Dec 15th.} to better
understand the practical features of ethical dangers in recent LLMs. We analyze
ChatGPT comprehensively from four perspectives: 1) \textit{Bias} 2)
\textit{Reliability} 3) \textit{Robustness} 4) \textit{Toxicity}. In accordance
with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample
datasets. We find that a significant number of ethical risks cannot be
addressed by existing benchmarks, and hence illustrate them via additional case
studies. In addition, we examine the implications of our findings on AI ethics
and harmal behaviors of ChatGPT, as well as future problems and practical
design considerations for responsible LLMs. We believe that our findings may
give light on future efforts to determine and mitigate the ethical hazards
posed by machines in LLM applications.",2023-01-30T13:20:48Z
,http://arxiv.org/pdf/2401.13849v1.pdf,"TPD: Enhancing Student Language Model Reasoning via Principle Discovery
  and Guidance","Large Language Models (LLMs) have recently showcased remarkable reasoning
abilities. However, larger models often surpass their smaller counterparts in
reasoning tasks, posing the challenge of effectively transferring these
capabilities from larger models. Existing approaches heavily rely on extensive
fine-tuning data or continuous interactions with a superior teacher LLM during
inference. We introduce a principle-based teacher-student framework called
``Teaching via Principle Discovery'' (TPD) to address these limitations.
Inspired by human learning mechanisms, TPD mimics the interaction between a
teacher and a student using a principle-based approach. The teacher LLM
generates problem-solving instructions and corrective principles based on the
student LLM's errors. These principles guide the refinement of instructions and
the selection of instructive examples from a validation set. This enables the
student model to learn from both the teacher's guidance and its own mistakes.
Once the student model begins making inferences, TPD requires no further
intervention from the teacher LLM or humans. Through extensive experiments
across eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared
to standard chain-of-thought prompting, TPD significantly improves the student
model's performance, achieving $6.2\%$ improvement on average.",2024-01-24T23:11:33Z
,http://arxiv.org/pdf/2405.15512v1.pdf,ChatGPT Code Detection: Techniques for Uncovering the Source of Code,"In recent times, large language models (LLMs) have made significant strides
in generating computer code, blurring the lines between code created by humans
and code produced by artificial intelligence (AI). As these technologies evolve
rapidly, it is crucial to explore how they influence code generation,
especially given the risk of misuse in areas like higher education. This paper
explores this issue by using advanced classification techniques to
differentiate between code written by humans and that generated by ChatGPT, a
type of LLM. We employ a new approach that combines powerful embedding features
(black-box) with supervised learning algorithms - including Deep Neural
Networks, Random Forests, and Extreme Gradient Boosting - to achieve this
differentiation with an impressive accuracy of 98%. For the successful
combinations, we also examine their model calibration, showing that some of the
models are extremely well calibrated. Additionally, we present white-box
features and an interpretable Bayes classifier to elucidate critical
differences between the code sources, enhancing the explainability and
transparency of our approach. Both approaches work well but provide at most
85-88% accuracy. We also show that untrained humans solve the same task not
better than random guessing. This study is crucial in understanding and
mitigating the potential risks associated with using AI in code generation,
particularly in the context of higher education, software development, and
competitive programming.",2024-05-24T12:56:18Z
10.1145/3613905.3650937,http://arxiv.org/pdf/2404.02213v1.pdf,"Exploring How Multiple Levels of GPT-Generated Programming Hints Support
  or Disappoint Novices","Recent studies have integrated large language models (LLMs) into diverse
educational contexts, including providing adaptive programming hints, a type of
feedback focuses on helping students move forward during problem-solving.
However, most existing LLM-based hint systems are limited to one single hint
type. To investigate whether and how different levels of hints can support
students' problem-solving and learning, we conducted a think-aloud study with
12 novices using the LLM Hint Factory, a system providing four levels of hints
from general natural language guidance to concrete code assistance, varying in
format and granularity. We discovered that high-level natural language hints
alone can be helpless or even misleading, especially when addressing next-step
or syntax-related help requests. Adding lower-level hints, like code examples
with in-line comments, can better support students. The findings open up future
work on customizing help responses from content, format, and granularity levels
to accurately identify and meet students' learning needs.",2024-04-02T18:05:26Z
10.1145/3649217.3653568,http://arxiv.org/pdf/2404.10990v1.pdf,"Automating Personalized Parsons Problems with Customized Contexts and
  Concepts","Parsons problems provide useful scaffolding for introductory programming
students learning to write code. However, generating large numbers of
high-quality Parsons problems that appeal to the diverse range of interests in
a typical introductory course is a significant challenge for educators. Large
language models (LLMs) may offer a solution, by allowing students to produce
on-demand Parsons problems for topics covering the breadth of the introductory
programming curriculum, and targeting thematic contexts that align with their
personal interests. In this paper, we introduce PuzzleMakerPy, an educational
tool that uses an LLM to generate unlimited contextualized drag-and-drop
programming exercises in the form of Parsons Problems, which introductory
programmers can use as a supplemental learning resource. We evaluated
PuzzleMakerPy by deploying it in a large introductory programming course, and
found that the ability to personalize the contextual framing used in problem
descriptions was highly engaging for students, and being able to customize the
programming topics was reported as being useful for their learning.",2024-04-17T02:01:50Z
,http://arxiv.org/pdf/2405.03734v1.pdf,"FOKE: A Personalized and Explainable Education Framework Integrating
  Foundation Models, Knowledge Graphs, and Prompt Engineering","Integrating large language models (LLMs) and knowledge graphs (KGs) holds
great promise for revolutionizing intelligent education, but challenges remain
in achieving personalization, interactivity, and explainability. We propose
FOKE, a Forest Of Knowledge and Education framework that synergizes foundation
models, knowledge graphs, and prompt engineering to address these challenges.
FOKE introduces three key innovations: (1) a hierarchical knowledge forest for
structured domain knowledge representation; (2) a multi-dimensional user
profiling mechanism for comprehensive learner modeling; and (3) an interactive
prompt engineering scheme for generating precise and tailored learning
guidance.
  We showcase FOKE's application in programming education, homework assessment,
and learning path planning, demonstrating its effectiveness and practicality.
Additionally, we implement Scholar Hero, a real-world instantiation of FOKE.
Our research highlights the potential of integrating foundation models,
knowledge graphs, and prompt engineering to revolutionize intelligent education
practices, ultimately benefiting learners worldwide. FOKE provides a principled
and unified approach to harnessing cutting-edge AI technologies for
personalized, interactive, and explainable educational services, paving the way
for further research and development in this critical direction.",2024-05-06T15:11:05Z
,http://arxiv.org/pdf/2308.03312v8.pdf,Exploiting Code Symmetries for Learning Program Semantics,"This paper tackles the challenge of teaching code semantics to Large Language
Models (LLMs) for program analysis by incorporating code symmetries into the
model architecture. We introduce a group-theoretic framework that defines code
symmetries as semantics-preserving transformations, where forming a code
symmetry group enables precise and efficient reasoning of code semantics. Our
solution, SymC, develops a novel variant of self-attention that is provably
equivariant to code symmetries from the permutation group defined over the
program dependence graph. SymC obtains superior performance on five program
analysis tasks, outperforming state-of-the-art code models without any
pre-training. Our results suggest that code LLMs that encode the code
structural prior via the code symmetry group generalize better and faster.",2023-08-07T05:40:58Z
,http://arxiv.org/pdf/2401.06391v2.pdf,"Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code
  Generation","Recent code large language models (LLMs) have shown promising performance in
generating standalone functions but face limitations in repository-level code
generation due to their lack of awareness of repository-level dependencies
(e.g., user-defined attributes), resulting in dependency errors such as
undefined-variable and no-member errors. In this work, we introduce ToolGen, an
approach that integrates autocompletion tools into the code LLM generation
process to address these dependencies. ToolGen comprises two main phases:
Trigger Insertion and Model Fine-tuning (Offline), and Tool-integrated Code
Generation (Online). During the offline phase, ToolGen augments functions
within a given code corpus with a special mark token, indicating positions to
trigger autocompletion tools. These augmented functions, along with their
corresponding docstrings, are then used to fine-tune a selected code LLM. In
the online phase, ToolGen iteratively generates functions by predicting tokens
step-by-step using the fine-tuned LLM. Whenever a mark token is encountered,
ToolGen invokes the autocompletion tool to suggest code completions and selects
the most appropriate one.
  We conduct comprehensive experiments to evaluate ToolGen's effectiveness in
repository-level code generation. To facilitate this evaluation, we create a
benchmark comprising 680 real-world code repositories and introduce two new
repository-level metrics: Dependency Coverage and Static Validity Rate. The
results demonstrate that ToolGen significantly improves Dependency Coverage by
15.2% to 45.8% and Static Validity Rate by 10.9% to 42.2% across three distinct
code LLMs, while maintaining competitive performance in widely-recognized
similarity metrics. Furthermore, our generalizability evaluation confirms
ToolGen's consistent performance when applied to diverse code LLMs, including
various model architectures and scales.",2024-01-12T06:03:56Z
,http://arxiv.org/pdf/2405.06835v1.pdf,Automating Code Adaptation for MLOps -- A Benchmarking Study on LLMs,"This paper explores the possibilities of the current generation of Large
Language Models for incorporating Machine Learning Operations (MLOps)
functionalities into ML training code bases. We evaluate the performance of
OpenAI (gpt-3.5-turbo) and WizardCoder (open-source, 15B parameters) models on
the automated accomplishment of various MLOps functionalities in different
settings. We perform a benchmarking study that assesses the ability of these
models to: (1) adapt existing code samples (Inlining) with component-specific
MLOps functionality such as MLflow and Weights & Biases for experiment
tracking, Optuna for hyperparameter optimization etc., and (2) perform the task
of Translation from one component of an MLOps functionality to another, e.g.,
translating existing GitPython library based version control code to Data
Version Control library based. We also propose three different approaches that
involve teaching LLMs to comprehend the API documentation of the components as
a reference while accomplishing the Translation tasks. In our evaluations, the
gpt-3.5-turbo model significantly outperforms WizardCoder by achieving
impressive Pass@3 accuracy in model optimization (55% compared to 0% by
WizardCoder), experiment tracking (100%, compared to 62.5% by WizardCoder),
model registration (92% compared to 42% by WizardCoder) and hyperparameter
optimization (83% compared to 58% by WizardCoder) on average, in their best
possible settings, showcasing its superior code adaptability performance in
complex MLOps tasks.",2024-05-10T22:18:43Z
,http://arxiv.org/pdf/2306.10509v2.pdf,"Can We Trust AI-Generated Educational Content? Comparative Analysis of
  Human and AI-Generated Learning Resources","As an increasing number of students move to online learning platforms that
deliver personalized learning experiences, there is a great need for the
production of high-quality educational content. Large language models (LLMs)
appear to offer a promising solution to the rapid creation of learning
materials at scale, reducing the burden on instructors. In this study, we
investigated the potential for LLMs to produce learning resources in an
introductory programming context, by comparing the quality of the resources
generated by an LLM with those created by students as part of a learnersourcing
activity. Using a blind evaluation, students rated the correctness and
helpfulness of resources generated by AI and their peers, after both were
initially provided with identical exemplars. Our results show that the quality
of AI-generated resources, as perceived by students, is equivalent to the
quality of resources generated by their peers. This suggests that AI-generated
resources may serve as viable supplementary material in certain contexts.
Resources generated by LLMs tend to closely mirror the given exemplars, whereas
student-generated resources exhibit greater variety in terms of content length
and specific syntax features used. The study highlights the need for further
research exploring different types of learning resources and a broader range of
subject areas, and understanding the long-term impact of AI-generated resources
on learning outcomes.",2023-06-18T09:49:21Z
,http://arxiv.org/pdf/2403.04449v1.pdf,Feedback-Generation for Programming Exercises With GPT-4,"Ever since Large Language Models (LLMs) and related applications have become
broadly available, several studies investigated their potential for assisting
educators and supporting students in higher education. LLMs such as Codex,
GPT-3.5, and GPT 4 have shown promising results in the context of large
programming courses, where students can benefit from feedback and hints if
provided timely and at scale. This paper explores the quality of GPT-4 Turbo's
generated output for prompts containing both the programming task specification
and a student's submission as input. Two assignments from an introductory
programming course were selected, and GPT-4 was asked to generate feedback for
55 randomly chosen, authentic student programming submissions. The output was
qualitatively analyzed regarding correctness, personalization, fault
localization, and other features identified in the material. Compared to prior
work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For
example, the output is more structured and consistent. GPT-4 Turbo can also
accurately identify invalid casing in student programs' output. In some cases,
the feedback also includes the output of the student program. At the same time,
inconsistent feedback was noted such as stating that the submission is correct
but an error needs to be fixed. The present work increases our understanding of
LLMs' potential, limitations, and how to integrate them into e-assessment
systems, pedagogical scenarios, and instructing students who are using
applications based on GPT-4.",2024-03-07T12:37:52Z
10.1609/aaai.v38i21.30364,http://arxiv.org/pdf/2403.14565v1.pdf,"A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students'
  Formative Assessment Responses in Science","This paper explores the use of large language models (LLMs) to score and
explain short-answer assessments in K-12 science. While existing methods can
score more structured math and computer science assessments, they often do not
provide explanations for the scores. Our study focuses on employing GPT-4 for
automated assessment in middle school Earth Science, combining few-shot and
active learning with chain-of-thought reasoning. Using a human-in-the-loop
approach, we successfully score and provide meaningful explanations for
formative assessment responses. A systematic analysis of our method's pros and
cons sheds light on the potential for human-in-the-loop techniques to enhance
automated grading for open-ended science assessments.",2024-03-21T17:09:08Z
,http://arxiv.org/pdf/2304.09102v1.pdf,"Solving Math Word Problems by Combining Language Models With Symbolic
  Solvers","Automatically generating high-quality step-by-step solutions to math word
problems has many applications in education. Recently, combining large language
models (LLMs) with external tools to perform complex reasoning and calculation
has emerged as a promising direction for solving math word problems, but prior
approaches such as Program-Aided Language model (PAL) are biased towards simple
procedural problems and less effective for problems that require declarative
reasoning. We propose an approach that combines an LLM that can incrementally
formalize word problems as a set of variables and equations with an external
symbolic solver that can solve the equations. Our approach achieves comparable
accuracy to the original PAL on the GSM8K benchmark of math word problems and
outperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more
challenging word problems extracted from Algebra textbooks. Our work highlights
the benefits of using declarative and incremental representations when
interfacing with an external tool for solving complex math word problems. Our
data and prompts are publicly available at
https://github.com/joyheyueya/declarative-math-word-problem.",2023-04-16T04:16:06Z
,http://arxiv.org/pdf/2305.13888v2.pdf,"PaD: Program-aided Distillation Can Teach Small Models Reasoning Better
  than Chain-of-thought Fine-tuning","While large language models (LLMs) excel in various natural language
processing tasks, their huge size and the inaccessibility of parameters present
challenges for practical deployment. Previous studies try to distill
task-specific ability from LLMs to smaller models, using data synthesis and
chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains
faulty reasoning, which deteriorates the quality of distillation, especially in
reasoning capabilities. In this work, we propose Program-aided Distillation
(PaD), which introduces reasoning programs to suppress the errors in distilled
data, and thus achieves better distillation quality for reasoning tasks. In
PaD, we utilize the reasoning program to substitute the CoT, allowing automated
error checking of synthetic data. Further, through error injecting and further
training, the small distilling model could iteratively self-refine the
reasoning. Moreover, we conduct a step-wise beam search by step-by-step
verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic
reasoning, symbolic reasoning, and general ability. Experimental results
demonstrate that smaller models using PaD can not only outperform certain
LLMs~(e.g., LLaMA-1 13B) but also achieve strong improvement over baselines
with a significantly smaller scale of parameters and data. The source code is
publicly available at https://github.com/Xuekai-Zhu/pad.",2023-05-23T10:11:56Z
10.1145/3657604.3664694,http://arxiv.org/pdf/2405.14713v1.pdf,"Towards Educator-Driven Tutor Authoring: Generative AI Approaches for
  Creating Intelligent Tutor Interfaces","Intelligent Tutoring Systems (ITSs) have shown great potential in delivering
personalized and adaptive education, but their widespread adoption has been
hindered by the need for specialized programming and design skills. Existing
approaches overcome the programming limitations with no-code authoring through
drag and drop, however they assume that educators possess the necessary skills
to design effective and engaging tutor interfaces. To address this assumption
we introduce generative AI capabilities to assist educators in creating tutor
interfaces that meet their needs while adhering to design principles. Our
approach leverages Large Language Models (LLMs) and prompt engineering to
generate tutor layout and contents based on high-level requirements provided by
educators as inputs. However, to allow them to actively participate in the
design process, rather than relying entirely on AI-generated solutions, we
allow generation both at the entire interface level and at the individual
component level. The former provides educators with a complete interface that
can be refined using direct manipulation, while the latter offers the ability
to create specific elements to be added to the tutor interface. A small-scale
comparison shows the potential of our approach to enhance the efficiency of
tutor interface design. Moving forward, we raise critical questions for
assisting educators with generative AI capabilities to create personalized,
effective, and engaging tutors, ultimately enhancing their adoption.",2024-05-23T15:46:10Z
,http://arxiv.org/pdf/2311.05943v1.pdf,Prompt Problems: A New Programming Exercise for the Generative AI Era,"Large Language Models (LLMs) are revolutionizing the field of computing
education with their powerful code-generating capabilities. Traditional
pedagogical practices have focused on code writing tasks, but there is now a
shift in importance towards code reading, comprehension and evaluation of
LLM-generated code. Alongside this shift, an important new skill is emerging --
the ability to solve programming tasks by constructing good prompts for
code-generating models. In this work we introduce a new type of programming
exercise to hone this nascent skill: 'Prompt Problems'. Prompt Problems are
designed to help students learn how to write effective prompts for AI code
generators. A student solves a Prompt Problem by crafting a natural language
prompt which, when provided as input to an LLM, outputs code that successfully
solves a specified programming task. We also present a new web-based tool
called Promptly which hosts a repository of Prompt Problems and supports the
automated evaluation of prompt-generated code. We deploy Promptly for the first
time in one CS1 and one CS2 course and describe our experiences, which include
student perceptions of this new type of activity and their interactions with
the tool. We find that students are enthusiastic about Prompt Problems, and
appreciate how the problems engage their computational thinking skills and
expose them to new programming constructs. We discuss ideas for the future
development of new variations of Prompt Problems, and the need to carefully
study their integration into classroom practice.",2023-11-10T09:01:34Z
10.1145/3545945.3569770,http://arxiv.org/pdf/2210.11630v1.pdf,Using Large Language Models to Enhance Programming Error Messages,"A key part of learning to program is learning to understand programming error
messages. They can be hard to interpret and identifying the cause of errors can
be time-consuming. One factor in this challenge is that the messages are
typically intended for an audience that already knows how to program, or even
for programming environments that then use the information to highlight areas
in code. Researchers have been working on making these errors more novice
friendly since the 1960s, however progress has been slow. The present work
contributes to this stream of research by using large language models to
enhance programming error messages with explanations of the errors and
suggestions on how to fix the error. Large language models can be used to
create useful and novice-friendly enhancements to programming error messages
that sometimes surpass the original programming error messages in
interpretability and actionability. These results provide further evidence of
the benefits of large language models for computing educators, highlighting
their use in areas known to be challenging for students. We further discuss the
benefits and downsides of large language models and highlight future streams of
research for enhancing programming error messages.",2022-10-20T23:17:26Z
10.1145/3613904.3642229,http://arxiv.org/pdf/2402.04975v1.pdf,"ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming
  Learning for Children Aged 6-12","As Computational Thinking (CT) continues to permeate younger age groups in
K-12 education, established CT platforms such as Scratch face challenges in
catering to these younger learners, particularly those in the elementary school
(ages 6-12). Through formative investigation with Scratch experts, we uncover
three key obstacles to children's autonomous Scratch learning: artist's block
in project planning, bounded creativity in asset creation, and inadequate
coding guidance during implementation. To address these barriers, we introduce
ChatScratch, an AI-augmented system to facilitate autonomous programming
learning for young children. ChatScratch employs structured interactive
storyboards and visual cues to overcome artist's block, integrates digital
drawing and advanced image generation technologies to elevate creativity, and
leverages Scratch-specialized Large Language Models (LLMs) for professional
coding guidance. Our study shows that, compared to Scratch, ChatScratch
efficiently fosters autonomous programming learning, and contributes to the
creation of high-quality, personally meaningful Scratch projects for children.",2024-02-07T15:55:51Z
,http://arxiv.org/pdf/2406.09671v1.pdf,"Evaluating ChatGPT-4 Vision on Brazil's National Undergraduate Computer
  Science Exam","The recent integration of visual capabilities into Large Language Models
(LLMs) has the potential to play a pivotal role in science and technology
education, where visual elements such as diagrams, charts, and tables are
commonly used to improve the learning experience. This study investigates the
performance of ChatGPT-4 Vision, OpenAI's most advanced visual model at the
time the study was conducted, on the Bachelor in Computer Science section of
Brazil's 2021 National Undergraduate Exam (ENADE). By presenting the model with
the exam's open and multiple-choice questions in their original image format
and allowing for reassessment in response to differing answer keys, we were
able to evaluate the model's reasoning and self-reflecting capabilities in a
large-scale academic assessment involving textual and visual content. ChatGPT-4
Vision significantly outperformed the average exam participant, positioning
itself within the top 10 best score percentile. While it excelled in questions
that incorporated visual elements, it also encountered challenges with question
interpretation, logical reasoning, and visual acuity. The involvement of an
independent expert panel to review cases of disagreement between the model and
the answer key revealed some poorly constructed questions containing vague or
ambiguous statements, calling attention to the critical need for improved
question design in future exams. Our findings suggest that while ChatGPT-4
Vision shows promise in multimodal academic evaluations, human oversight
remains crucial for verifying the model's accuracy and ensuring the fairness of
high-stakes educational exams. The paper's research materials are publicly
available at https://github.com/nabormendonca/gpt-4v-enade-cs-2021.",2024-06-14T02:42:30Z
,http://arxiv.org/pdf/2308.02432v1.pdf,"Performance of Large Language Models in a Computer Science Degree
  Program","Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and
dominate the current discourse. Their transformative capabilities have led to a
paradigm shift in how we interact with and utilize (text-based) information.
Each day, new possibilities to leverage the capabilities of these models
emerge. This paper presents findings on the performance of different large
language models in a university of applied sciences' undergraduate computer
science degree program. Our primary objective is to assess the effectiveness of
these models within the curriculum by employing them as educational aids. By
prompting the models with lecture material, exercise tasks, and past exams, we
aim to evaluate their proficiency across different computer science domains. We
showcase the strong performance of current large language models while
highlighting limitations and constraints within the context of such a degree
program. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10
tested modules, BingAI achieved 68.4%, and LLaMa, in the 65 billion parameter
variant, 20%. Despite these convincing results, even GPT-4.0 would not pass the
degree program - due to limitations in mathematical calculations.",2023-07-24T14:17:00Z
,http://arxiv.org/pdf/2303.14310v1.pdf,GPT is becoming a Turing machine: Here are some ways to program it,"We demonstrate that, through appropriate prompting, GPT-3 family of models
can be triggered to perform iterative behaviours necessary to execute (rather
than just write or recall) programs that involve loops, including several
popular algorithms found in computer science curricula or software developer
interviews. We trigger execution and description of Iterations by Regimenting
Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong
repetitive structure in an example of an execution path of a target program for
one particular input, 2) Prompting with fragments of execution paths, and 3)
Explicitly forbidding (skipping) self-attention to parts of the generated text.
On a dynamic program execution, IRSA leads to larger accuracy gains than
replacing the model with the much more powerful GPT-4. IRSA has promising
applications in education, as the prompts and responses resemble student
assignments in data structures and algorithms classes. Our findings hold
implications for evaluating LLMs, which typically target the in-context
learning: We show that prompts that may not even cover one full task example
can trigger algorithmic behaviour, allowing solving problems previously thought
of as hard for LLMs, such as logical puzzles. Consequently, prompt design plays
an even more critical role in LLM performance than previously recognized.",2023-03-25T00:43:41Z
,http://arxiv.org/pdf/2309.10085v1.pdf,"Evaluating the Impact of ChatGPT on Exercises of a Software Security
  Course","Along with the development of large language models (LLMs), e.g., ChatGPT,
many existing approaches and tools for software security are changing. It is,
therefore, essential to understand how security-aware these models are and how
these models impact software security practices and education. In exercises of
a software security course at our university, we ask students to identify and
fix vulnerabilities we insert in a web application using state-of-the-art
tools. After ChatGPT, especially the GPT-4 version of the model, we want to
know how the students can possibly use ChatGPT to complete the exercise tasks.
We input the vulnerable code to ChatGPT and measure its accuracy in
vulnerability identification and fixing. In addition, we investigated whether
ChatGPT can provide a proper source of information to support its outputs.
Results show that ChatGPT can identify 20 of the 28 vulnerabilities we inserted
in the web application in a white-box setting, reported three false positives,
and found four extra vulnerabilities beyond the ones we inserted. ChatGPT makes
nine satisfactory penetration testing and fixing recommendations for the ten
vulnerabilities we want students to fix and can often point to related sources
of information.",2023-09-18T18:53:43Z
,http://arxiv.org/pdf/2311.00177v2.pdf,Students' Perspective on AI Code Completion: Benefits and Challenges,"AI Code Completion (e.g., GitHub's Copilot) has revolutionized how computer
science students interact with programming languages. However, AI code
completion has been studied from the developers' perspectives, not the
students' perspectives who represent the future generation of our digital
world. In this paper, we investigated the benefits, challenges, and
expectations of AI code completion from students' perspectives. To facilitate
the study, we first developed an open-source Visual Studio Code Extension tool
AutoAurora, powered by a state-of-the-art large language model StarCoder, as an
AI code completion research instrument. Next, we conduct an interview study
with ten student participants and apply grounded theory to help analyze
insightful findings regarding the benefits, challenges, and expectations of
students on AI code completion. Our findings show that AI code completion
enhanced students' productivity and efficiency by providing correct syntax
suggestions, offering alternative solutions, and functioning as a coding tutor.
However, the over-reliance on AI code completion may lead to a surface-level
understanding of programming concepts, diminishing problem-solving skills and
restricting creativity. In the future, AI code completion should be explainable
and provide best coding practices to enhance the education process.",2023-10-31T22:41:16Z
,http://arxiv.org/pdf/2401.05566v3.pdf,"Sleeper Agents: Training Deceptive LLMs that Persist Through Safety
  Training","Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoor behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoor behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.",2024-01-10T22:14:35Z
10.1145/3501385.3543957,http://arxiv.org/pdf/2206.11861v2.pdf,"Automatic Generation of Programming Exercises and Code Explanations
  using Large Language Models","This article explores the natural language generation capabilities of large
language models with application to the production of two types of learning
resources common in programming courses. Using OpenAI Codex as the large
language model, we create programming exercises (including sample solutions and
test cases) and code explanations, assessing these qualitatively and
quantitatively. Our results suggest that the majority of the automatically
generated content is both novel and sensible, and in some cases ready to use as
is. When creating exercises we find that it is remarkably easy to influence
both the programming concepts and the contextual themes they contain, simply by
supplying keywords as input to the model. Our analysis suggests that there is
significant value in massive generative machine learning models as a tool for
instructors, although there remains a need for some oversight to ensure the
quality of the generated content before it is delivered to students. We further
discuss the implications of OpenAI Codex and similar tools for introductory
programming education and highlight future research streams that have the
potential to improve the quality of the educational experience for both
teachers and students alike.",2022-06-03T11:00:43Z
,http://arxiv.org/pdf/2209.14876v1.pdf,Repairing Bugs in Python Assignments Using Large Language Models,"Students often make mistakes on their introductory programming assignments as
part of their learning process. Unfortunately, providing custom repairs for
these mistakes can require a substantial amount of time and effort from class
instructors. Automated program repair (APR) techniques can be used to
synthesize such fixes. Prior work has explored the use of symbolic and neural
techniques for APR in the education domain. Both types of approaches require
either substantial engineering efforts or large amounts of data and training.
We propose to use a large language model trained on code, such as Codex, to
build an APR system -- MMAPR -- for introductory Python programming
assignments. Our system can fix both syntactic and semantic mistakes by
combining multi-modal prompts, iterative querying, test-case-based selection of
few-shots, and program chunking. We evaluate MMAPR on 286 real student programs
and compare to a baseline built by combining a state-of-the-art Python syntax
repair engine, BIFI, and state-of-the-art Python semantic repair engine for
student assignments, Refactory. We find that MMAPR can fix more programs and
produce smaller patches on average.",2022-09-29T15:41:17Z
,http://arxiv.org/pdf/2403.02078v1.pdf,"Automated Generation of Multiple-Choice Cloze Questions for Assessing
  English Vocabulary Using GPT-turbo 3.5","A common way of assessing language learners' mastery of vocabulary is via
multiple-choice cloze (i.e., fill-in-the-blank) questions. But the creation of
test items can be laborious for individual teachers or in large-scale language
programs. In this paper, we evaluate a new method for automatically generating
these types of questions using large language models (LLM). The VocaTT
(vocabulary teaching and training) engine is written in Python and comprises
three basic steps: pre-processing target word lists, generating sentences and
candidate word options using GPT, and finally selecting suitable word options.
To test the efficiency of this system, 60 questions were generated targeting
academic words. The generated items were reviewed by expert reviewers who
judged the well-formedness of the sentences and word options, adding comments
to items judged not well-formed. Results showed a 75% rate of well-formedness
for sentences and 66.85% rate for suitable word options. This is a marked
improvement over the generator used earlier in our research which did not take
advantage of GPT's capabilities. Post-hoc qualitative analysis reveals several
points for improvement in future work including cross-referencing
part-of-speech tagging, better sentence validation, and improving GPT prompts.",2024-03-04T14:24:47Z
,http://arxiv.org/pdf/2403.09744v1.pdf,"Evaluating the Application of Large Language Models to Generate Feedback
  in Programming Education","This study investigates the application of large language models,
specifically GPT-4, to enhance programming education. The research outlines the
design of a web application that uses GPT-4 to provide feedback on programming
tasks, without giving away the solution. A web application for working on
programming tasks was developed for the study and evaluated with 51 students
over the course of one semester. The results show that most of the feedback
generated by GPT-4 effectively addressed code errors. However, challenges with
incorrect suggestions and hallucinated issues indicate the need for further
improvements.",2024-03-13T23:14:35Z
,http://arxiv.org/pdf/2211.04715v1.pdf,"Robosourcing Educational Resources -- Leveraging Large Language Models
  for Learnersourcing","In this article, we introduce and evaluate the concept of robosourcing for
creating educational content. Robosourcing lies in the intersection of
crowdsourcing and large language models, where instead of a crowd of humans,
requests to large language models replace some of the work traditionally
performed by the crowd. Robosourcing includes a human-in-the-loop to provide
priming (input) as well as to evaluate and potentially adjust the generated
artefacts; these evaluations could also be used to improve the large language
models. We propose a system to outline the robosourcing process. We further
study the feasibility of robosourcing in the context of education by conducting
an evaluation of robosourced and programming exercises, generated using OpenAI
Codex. Our results suggest that robosourcing could significantly reduce human
effort in creating diverse educational content while maintaining quality
similar to human-created content.",2022-11-09T07:13:03Z
,http://arxiv.org/pdf/2303.13375v2.pdf,Capabilities of GPT-4 on Medical Challenge Problems,"Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation across various domains, including
medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art
LLM, on medical competency examinations and benchmark datasets. GPT-4 is a
general-purpose model that is not specialized for medical problems through
training or engineered to solve clinical tasks. Our analysis covers two sets of
official practice materials for the USMLE, a three-step examination program
used to assess clinical competency and grant licensure in the United States. We
also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond
measuring model performance, experiments were conducted to investigate the
influence of test questions containing both text and images on model
performance, probe for memorization of content during training, and study
probability calibration, which is of critical importance in high-stakes
applications like medicine. Our results show that GPT-4, without any
specialized prompt crafting, exceeds the passing score on USMLE by over 20
points and outperforms earlier general-purpose models (GPT-3.5) as well as
models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned
version of Flan-PaLM 540B). In addition, GPT-4 is significantly better
calibrated than GPT-3.5, demonstrating a much-improved ability to predict the
likelihood that its answers are correct. We also explore the behavior of the
model qualitatively through a case study that shows the ability of GPT-4 to
explain medical reasoning, personalize explanations to students, and
interactively craft new counterfactual scenarios around a medical case.
Implications of the findings are discussed for potential uses of GPT-4 in
medical education, assessment, and clinical practice, with appropriate
attention to challenges of accuracy and safety.",2023-03-20T16:18:38Z
,http://arxiv.org/pdf/2312.03728v1.pdf,"Real Customization or Just Marketing: Are Customized Versions of Chat
  GPT Useful?","Large Language Models (LLMs), as the case of OpenAI ChatGPT-4 Turbo, are
revolutionizing several industries, including higher education. In this
context, LLMs can be personalized through a fine-tuning process to meet the
student demands on every particular subject, like statistics. Recently, OpenAI
has launched the possibility to fine-tune their model with a natural language
web interface, enabling the possibility to create customized GPT version
deliberately conditioned to meet the demands of a specific task. The objective
of this research is to assess the potential of the customized GPTs that have
recently been launched by OpenAI. After developing a Business Statistics
Virtual Professor (BSVP), tailored for students at the Universidad Pontificia
Comillas, its behavior was evaluated and compared with that of ChatGPT-4 Turbo.
The results lead to several conclusions. Firstly, a substantial modification in
the style of communication was observed. Following the instructions it was
trained with, BSVP provided responses in a more relatable and friendly tone,
even incorporating a few minor jokes. Secondly, and this is a matter of
relevance, when explicitly asked for something like, ""I would like to practice
a programming exercise similar to those in R practice 4,"" BSVP was capable of
providing a far superior response: having access to contextual documentation,
it could fulfill the request, something beyond ChatGPT-4 Turbo's capabilities.
On the downside, the response times were generally higher. Lastly, regarding
overall performance, quality, depth, and alignment with the specific content of
the course, no statistically significant differences were observed in the
responses between BSVP and ChatGPT-4 Turbo. It appears that customized
assistants trained with prompts present advantages as virtual aids for
students, yet they do not constitute a substantial improvement over ChatGPT-4
Turbo.",2023-11-27T15:46:15Z
,http://arxiv.org/pdf/2405.06681v1.pdf,"Leveraging Lecture Content for Improved Feedback: Explorations with
  GPT-4 and Retrieval Augmented Generation","This paper presents the use of Retrieval Augmented Generation (RAG) to
improve the feedback generated by Large Language Models for programming tasks.
For this purpose, corresponding lecture recordings were transcribed and made
available to the Large Language Model GPT-4 as external knowledge source
together with timestamps as metainformation by using RAG. The purpose of this
is to prevent hallucinations and to enforce the use of the technical terms and
phrases from the lecture. In an exercise platform developed to solve
programming problems for an introductory programming lecture, students can
request feedback on their solutions generated by GPT-4. For this task GPT-4
receives the students' code solution, the compiler output, the result of unit
tests and the relevant passages from the lecture notes available through the
use of RAG as additional context. The feedback generated by GPT-4 should guide
students to solve problems independently and link to the lecture content, using
the time stamps of the transcript as meta-information. In this way, the
corresponding lecture videos can be viewed immediately at the corresponding
positions. For the evaluation, students worked with the tool in a workshop and
decided for each feedback whether it should be extended by RAG or not. First
results based on a questionnaire and the collected usage data show that the use
of RAG can improve feedback generation and is preferred by students in some
situations. Due to the slower speed of feedback generation, the benefits are
situation dependent.",2024-05-05T18:32:06Z
,http://arxiv.org/pdf/2305.02198v1.pdf,Experiences with Remote Examination Formats in Light of GPT-4,"Sudden access to the rapidly improving large language model GPT by open-ai
forces educational institutions worldwide to revisit their exam procedures. In
the pre-GPT era, we successfully applied oral and open-book home exams for two
courses in the third year of our predominantly remote Software Engineering BSc
program. We ask in this paper whether our current open-book exams are still
viable or whether a move back to a legally compliant but less scalable oral
exam is the only workable alternative. We further compare work-effort estimates
between oral and open-book exams and report on differences in throughput and
grade distribution over eight years to better understand the impact of
examination format on the outcome. Examining GPT v4 on the most recent
open-book exams showed that our current Artificial Intelligence and Reactive
Programming exams are not GPT v4 proof. Three potential weaknesses of GPT are
outlined. We also found that grade distributions have largely been unaffected
by the examination format, opening up for a move to oral examinations only if
needed. Throughput was higher for open-book exam course instances (73% vs 64%),
while fail rates were too (12% vs 7%), with teacher workload increasing even
for smaller classes. We also report on our experience regarding effort. Oral
examinations are efficient for smaller groups but come with caveats regarding
intensity and stress.",2023-03-27T19:49:06Z
10.3991/ijep.v13i8.45621,http://arxiv.org/pdf/2311.10737v1.pdf,"AI-enhanced Auto-correction of Programming Exercises: How Effective is
  GPT-3.5?","Timely formative feedback is considered as one of the most important drivers
for effective learning. Delivering timely and individualized feedback is
particularly challenging in large classes in higher education. Recently Large
Language Models such as GPT-3 became available to the public that showed
promising results on various tasks such as code generation and code
explanation. This paper investigates the potential of AI in providing
personalized code correction and generating feedback. Based on existing student
submissions of two different real-world assignments, the correctness of the
AI-aided e-assessment as well as the characteristics such as fault
localization, correctness of hints, and code style suggestions of the generated
feedback are investigated. The results show that 73 % of the submissions were
correctly identified as either correct or incorrect. In 59 % of these cases,
GPT-3.5 also successfully generated effective and high-quality feedback.
Additionally, GPT-3.5 exhibited weaknesses in its evaluation, including
localization of errors that were not the actual errors, or even hallucinated
errors. Implications and potential new usage scenarios are discussed.",2023-10-24T10:35:36Z
,http://arxiv.org/pdf/2406.11104v1.pdf,"Beyond the Hype: A Cautionary Tale of ChatGPT in the Programming
  Classroom","Due to the proliferation of Large Language Models research and the use of
various Artificial Intelligence (AI) tools, the field of information systems
(IS) and computer science (CS) has evolved. The use of tools such as ChatGPT to
complete various student programming exercises (e.g., in Python) and
assignments has gained prominence amongst various academic institutions.
However, recent literature has suggested that the use of ChatGPT in academia is
problematic and the impact on teaching and learning should be further
scrutinized. More specifically, little is known about how ChatGPT can be
practically used with code (programming) writing to complete programming
exercises amongst IS and CS undergraduate university students. Furthermore, the
paper provides insights for academics who teach programming to create more
challenging exercises and how to engage responsibly in the use of ChatGPT to
promote classroom integrity. In this paper, we used Complex Adaptive Systems
(CAS) theory as a theoretical guide to understand the various dynamics through
classroom code demonstrations. Using ChatGPT 3.5, we analyzed the various
practical programming examples from past IS exercises and compared those with
memos created by tutors and lecturers in a university setting. This paper
highlights common ways of assessment, programming errors created by ChatGPT and
the potential consideration for IS academics to ensure the development of
critical programming skills among students.",2024-06-16T23:52:37Z
,http://arxiv.org/pdf/2309.14726v2.pdf,PLMM: Personal Large Language Models on Mobile Devices,"Inspired by Federated Learning, in this paper, we propose personal large
models that are distilled from traditional large language models but more
adaptive to local users' personal information such as education background and
hobbies. We classify the large language models into three levels: the personal
level, expert level and traditional level. The personal level models are
adaptive to users' personal information. They encrypt the users' input and
protect their privacy. The expert level models focus on merging specific
knowledge such as finance, IT and art. The traditional models focus on the
universal knowledge discovery and upgrading the expert models. In such
classifications, the personal models directly interact with the user. For the
whole system, the personal models have users' (encrypted) personal information.
Moreover, such models must be small enough to be performed on personal
computers or mobile devices. Finally, they also have to response in real-time
for better user experience and produce high quality results. The proposed
personal large models can be applied in a wide range of applications such as
language and vision tasks.",2023-09-26T07:36:20Z
10.1145/3643991.3644926,http://arxiv.org/pdf/2403.04013v1.pdf,"Whodunit: Classifying Code as Human Authored or GPT-4 Generated -- A
  case study on CodeChef problems","Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT,
built on large language models like GPT-4, are revolutionizing how programming
tasks are performed, raising questions about whether code is authored by
generative AI models. Such questions are of particular interest to educators,
who worry that these tools enable a new form of academic dishonesty, in which
students submit AI generated code as their own work. Our research explores the
viability of using code stylometry and machine learning to distinguish between
GPT-4 generated and human-authored code. Our dataset comprises human-authored
solutions from CodeChef and AI-authored solutions generated by GPT-4. Our
classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91. A
variant of our classifier that excludes gameable features (e.g., empty lines,
whitespace) still performs well with an F1-score and AUC-ROC score of 0.89. We
also evaluated our classifier with respect to the difficulty of the programming
problem and found that there was almost no difference between easier and
intermediate problems, and the classifier performed only slightly worse on
harder problems. Our study shows that code stylometry is a promising approach
for distinguishing between GPT-4 generated code and human-authored code.",2024-03-06T19:51:26Z
,http://arxiv.org/pdf/2308.02522v1.pdf,Evaluating ChatGPT and GPT-4 for Visual Programming,"Generative AI and large language models have the potential to drastically
improve the landscape of computing education by automatically generating
personalized feedback and content. Recent works have studied the capabilities
of these models for different programming education scenarios; however, these
works considered only text-based programming, in particular, Python
programming. Consequently, they leave open the question of how well these
models would perform in visual programming domains popularly used for K-8
programming education. The main research question we study is: Do
state-of-the-art generative models show advanced capabilities in visual
programming on par with their capabilities in text-based Python programming? In
our work, we evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, in
visual programming domains for various scenarios and assess performance using
expert-based annotations. In particular, we base our evaluation using reference
tasks from the domains of Hour of Code: Maze Challenge by Code-dot-org and
Karel. Our results show that these models perform poorly and struggle to
combine spatial, logical, and programming skills crucial for visual
programming. These results also provide exciting directions for future work on
developing techniques to improve the performance of generative models in visual
programming.",2023-07-30T22:13:20Z
10.3389/fvets.2024.1395934,http://arxiv.org/pdf/2403.14654v1.pdf,"ChatGPT in Veterinary Medicine: A Practical Guidance of Generative
  Artificial Intelligence in Clinics, Education, and Research","ChatGPT, the most accessible generative artificial intelligence (AI) tool,
offers considerable potential for veterinary medicine, yet a dedicated review
of its specific applications is lacking. This review concisely synthesizes the
latest research and practical applications of ChatGPT within the clinical,
educational, and research domains of veterinary medicine. It intends to provide
specific guidance and actionable examples of how generative AI can be directly
utilized by veterinary professionals without a programming background. For
practitioners, ChatGPT can extract patient data, generate progress notes, and
potentially assist in diagnosing complex cases. Veterinary educators can create
custom GPTs for student support, while students can utilize ChatGPT for exam
preparation. ChatGPT can aid in academic writing tasks in research, but
veterinary publishers have set specific requirements for authors to follow.
Despite its transformative potential, careful use is essential to avoid
pitfalls like hallucination. This review addresses ethical considerations,
provides learning resources, and offers tangible examples to guide responsible
implementation. Carefully selected, up-to-date links to platforms that host
large language models are provided for advanced readers with programming
capability. A table of key takeaways was provided to summarize this review. By
highlighting potential benefits and limitations, this review equips
veterinarians, educators, and researchers to harness the power of ChatGPT
effectively.",2024-02-26T02:59:07Z
,http://arxiv.org/pdf/2307.09163v1.pdf,Generative Type Inference for Python,"Python is a popular dynamic programming language, evidenced by its ranking as
the second most commonly used language on GitHub. However, its dynamic type
system can lead to potential type errors, leading researchers to explore
automatic type inference approaches for Python programs. The rule-based type
inference approaches can ensure the accuracy of predicted variable types, but
they suffer from low coverage problems. Supervised type inference approaches,
while feature-agnostic, require large, high-quality annotated datasets and are
limited to pre-defined types. As zero-shot approaches, the cloze-style
approaches reformulate the type inference problem into a fill-in-the-blank
problem. However, their performance is limited.
  This paper introduces TypeGen, a few-shot generative type inference approach
that incorporates static domain knowledge from static analysis. TypeGen creates
chain-of-thought (COT) prompts by translating the type inference steps of
static analysis into prompts based on the type dependency graphs (TDGs),
enabling language models to learn from how static analysis infers types. By
combining COT prompts with code slices and type hints, TypeGen constructs
example prompts from human annotations. TypeGen only requires very few
annotated examples to teach language models to generate similar COT prompts via
in-context learning. Moreover, TypeGen enhances the interpretability of results
through the use of the input-explanation-output strategy. Experiments show that
TypeGen outperforms the best baseline Type4Py by 10.0% for argument type
prediction and 22.5% in return value type prediction in terms of top-1 Exact
Match by using only five examples. Furthermore, TypeGen achieves substantial
improvements of 27% to 84% compared to the zero-shot performance of large
language models with parameter sizes ranging from 1.3B to 175B in terms of
top-1 Exact Match.",2023-07-18T11:40:31Z
,http://arxiv.org/pdf/2306.08997v2.pdf,"Exploring the MIT Mathematics and EECS Curriculum Using Large Language
  Models","We curate a comprehensive dataset of 4,550 questions and solutions from
problem sets, midterm exams, and final exams across all MIT Mathematics and
Electrical Engineering and Computer Science (EECS) courses required for
obtaining a degree. We evaluate the ability of large language models to fulfill
the graduation requirements for any MIT major in Mathematics and EECS. Our
results demonstrate that GPT-3.5 successfully solves a third of the entire MIT
curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate
on a test set excluding questions based on images. We fine-tune an open-source
large language model on this dataset. We employ GPT-4 to automatically grade
model responses, providing a detailed performance breakdown by course,
question, and answer type. By embedding questions in a low-dimensional space,
we explore the relationships between questions, topics, and classes and
discover which questions and classes are required for solving other questions
and classes through few-shot learning. Our analysis offers valuable insights
into course prerequisites and curriculum design, highlighting language models'
potential for learning and improving Mathematics and EECS education.",2023-06-15T09:48:14Z
,http://arxiv.org/pdf/2405.00748v1.pdf,ChatGPT in Data Visualization Education: A Student Perspective,"Unlike traditional educational chatbots that rely on pre-programmed
responses, large-language model-driven chatbots, such as ChatGPT, demonstrate
remarkable versatility and have the potential to serve as a dynamic resource
for addressing student needs from understanding advanced concepts to solving
complex problems. This work explores the impact of such technology on student
learning in an interdisciplinary, project-oriented data visualization course.
Throughout the semester, students engaged with ChatGPT across four distinct
projects, including data visualizations and implementing them using a variety
of tools including Tableau, D3, and Vega-lite. We collected conversation logs
and reflection surveys from the students after each assignment. In addition, we
conducted interviews with selected students to gain deeper insights into their
overall experiences with ChatGPT. Our analysis examined the advantages and
barriers of using ChatGPT, students' querying behavior, the types of assistance
sought, and its impact on assignment outcomes and engagement. Based on the
findings, we discuss design considerations for an educational solution that
goes beyond the basic interface of ChatGPT, specifically tailored for data
visualization education.",2024-05-01T02:40:20Z
,http://arxiv.org/pdf/2404.05752v1.pdf,Physics Event Classification Using Large Language Models,"The 2023 AI4EIC hackathon was the culmination of the third annual AI4EIC
workshop at The Catholic University of America. This workshop brought together
researchers from physics, data science and computer science to discuss the
latest developments in Artificial Intelligence (AI) and Machine Learning (ML)
for the Electron Ion Collider (EIC), including applications for detectors,
accelerators, and experimental control. The hackathon, held on the final day of
the workshop, involved using a chatbot powered by a Large Language Model,
ChatGPT-3.5, to train a binary classifier neutrons and photons in simulated
data from the \textsc{GlueX} Barrel Calorimeter. In total, six teams of up to
four participants from all over the world took part in this intense educational
and research event. This article highlights the hackathon challenge, the
resources and methodology used, and the results and insights gained from
analyzing physics data using the most cutting-edge tools in AI/ML.",2024-04-05T03:52:27Z
,http://arxiv.org/pdf/2306.17156v3.pdf,"Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4,
  and Human Tutors","Generative AI and large language models hold great promise in enhancing
computing education by powering next-generation educational technologies for
introductory programming. Recent works have studied these models for different
scenarios relevant to programming education; however, these works are limited
for several reasons, as they typically consider already outdated models or only
specific scenario(s). Consequently, there is a lack of a systematic study that
benchmarks state-of-the-art models for a comprehensive set of programming
education scenarios. In our work, we systematically evaluate two models,
ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human
tutors for a variety of scenarios. We evaluate using five introductory Python
programming problems and real-world buggy programs from an online platform, and
assess performance using expert-based annotations. Our results show that GPT-4
drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human
tutors' performance for several scenarios. These results also highlight
settings where GPT-4 still struggles, providing exciting future directions on
developing techniques to improve the performance of these models.",2023-06-29T17:57:40Z
,http://arxiv.org/pdf/2405.08213v1.pdf,"Interpreting Latent Student Knowledge Representations in Programming
  Assignments","Recent advances in artificial intelligence for education leverage generative
large language models, including using them to predict open-ended student
responses rather than their correctness only. However, the black-box nature of
these models limits the interpretability of the learned student knowledge
representations. In this paper, we conduct a first exploration into
interpreting latent student knowledge representations by presenting InfoOIRT,
an Information regularized Open-ended Item Response Theory model, which
encourages the latent student knowledge states to be interpretable while being
able to generate student-written code for open-ended programming questions.
InfoOIRT maximizes the mutual information between a fixed subset of latent
knowledge states enforced with simple prior distributions and generated student
code, which encourages the model to learn disentangled representations of
salient syntactic and semantic code features including syntactic styles,
mastery of programming skills, and code structures. Through experiments on a
real-world programming education dataset, we show that InfoOIRT can both
accurately generate student code and lead to interpretable student knowledge
representations.",2024-05-13T22:01:03Z
,http://arxiv.org/pdf/2404.11129v1.pdf,"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales","The remarkable performance of Multimodal Large Language Models (MLLMs) has
unequivocally demonstrated their proficient understanding capabilities in
handling a wide array of visual tasks. Nevertheless, the opaque nature of their
black-box reasoning processes persists as an enigma, rendering them
uninterpretable and struggling with hallucination. Their ability to execute
intricate compositional reasoning tasks is also constrained, culminating in a
stagnation of learning progression for these models. In this work, we introduce
Fact, a novel paradigm designed to generate multimodal rationales that are
faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes
verifiable visual programming to generate executable code guaranteeing
faithfulness and precision. Subsequently, through a series of operations
including pruning, merging, and bridging, the rationale enhances its
conciseness. Furthermore, we filter rationales that can be transferred to
end-to-end paradigms from programming paradigms to guarantee transferability.
Empirical evidence from experiments demonstrates the superiority of our method
across models of varying parameter sizes, significantly enhancing their
compositional reasoning and generalization ability. Our approach also reduces
hallucinations owing to its high correlation between images and text.",2024-04-17T07:20:56Z
10.1145/3613904.3642773,http://arxiv.org/pdf/2401.11314v2.pdf,"CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming
  Assistant that Balances Student and Educator Needs","Timely, personalized feedback is essential for students learning programming.
LLM-powered tools like ChatGPT offer instant support, but reveal direct answers
with code, which may hinder deep conceptual engagement. We developed CodeAid,
an LLM-powered programming assistant delivering helpful, technically correct
responses, without revealing code solutions. CodeAid answers conceptual
questions, generates pseudo-code with line-by-line explanations, and annotates
student's incorrect code with fix suggestions. We deployed CodeAid in a
programming class of 700 students for a 12-week semester. A thematic analysis
of 8,000 usages of CodeAid was performed, further enriched by weekly surveys,
and 22 student interviews. We then interviewed eight programming educators to
gain further insights. Our findings reveal four design considerations for
future educational AI assistants: D1) exploiting AI's unique benefits; D2)
simplifying query formulation while promoting cognitive engagement; D3)
avoiding direct responses while encouraging motivated learning; and D4)
maintaining transparency and control for students to asses and steer AI
responses.",2024-01-20T20:14:42Z
,http://arxiv.org/pdf/2406.05053v1.pdf,"Hints-In-Browser: Benchmarking Language Models for Programming Feedback
  Generation","Generative AI and large language models hold great promise in enhancing
programming education by generating individualized feedback and hints for
learners. Recent works have primarily focused on improving the quality of
generated feedback to achieve human tutors' quality. While quality is an
important performance criterion, it is not the only criterion to optimize for
real-world educational deployments. In this paper, we benchmark language models
for programming feedback generation across several performance criteria,
including quality, cost, time, and data privacy. The key idea is to leverage
recent advances in the new paradigm of in-browser inference that allow running
these models directly in the browser, thereby providing direct benefits across
cost and data privacy. To boost the feedback quality of small models compatible
with in-browser inference engines, we develop a fine-tuning pipeline based on
GPT-4 generated synthetic data. We showcase the efficacy of fine-tuned
Llama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser
inference engine on three different Python programming datasets. We will
release the full implementation along with a web app and datasets to facilitate
further research on in-browser language models.",2024-06-07T16:22:51Z
,http://arxiv.org/pdf/2405.20529v1.pdf,An Automatic Question Usability Evaluation Toolkit,"Evaluating multiple-choice questions (MCQs) involves either labor intensive
human assessments or automated methods that prioritize readability, often
overlooking deeper question design flaws. To address this issue, we introduce
the Scalable Automatic Question Usability Evaluation Toolkit (SAQUET), an
open-source tool that leverages the Item-Writing Flaws (IWF) rubric for a
comprehensive and automated quality evaluation of MCQs. By harnessing the
latest in large language models such as GPT-4, advanced word embeddings, and
Transformers designed to analyze textual complexity, SAQUET effectively
pinpoints and assesses a wide array of flaws in MCQs. We first demonstrate the
discrepancy between commonly used automated evaluation metrics and the human
assessment of MCQ quality. Then we evaluate SAQUET on a diverse dataset of MCQs
across the five domains of Chemistry, Statistics, Computer Science, Humanities,
and Healthcare, showing how it effectively distinguishes between flawed and
flawless questions, providing a level of analysis beyond what is achievable
with traditional metrics. With an accuracy rate of over 94% in detecting the
presence of flaws identified by human evaluators, our findings emphasize the
limitations of existing evaluation methods and showcase potential in improving
the quality of educational assessments.",2024-05-30T23:04:53Z
,http://arxiv.org/pdf/2201.07406v2.pdf,Fooling MOSS Detection with Pretrained Language Models,"As artificial intelligence (AI) technologies become increasingly powerful and
prominent in society, their misuse is a growing concern. In educational
settings, AI technologies could be used by students to cheat on assignments and
exams. In this paper we explore whether transformers can be used to solve
introductory level programming assignments while bypassing commonly used AI
tools to detect similarities between pieces of software. We find that a student
using GPT-J [Wang and Komatsuzaki, 2021] can complete introductory level
programming assignments without triggering suspicion from MOSS [Aiken, 2000], a
widely used software similarity and plagiarism detection tool. This holds
despite the fact that GPT-J was not trained on the problems in question and is
not provided with any examples to work from. We further find that the code
written by GPT-J is diverse in structure, lacking any particular tells that
future plagiarism detection techniques may use to try to identify
algorithmically generated code. We conclude with a discussion of the ethical
and educational implications of large language models and directions for future
research.",2022-01-19T04:00:46Z
10.1145/3643991.3644910,http://arxiv.org/pdf/2402.03777v1.pdf,Improving Automated Code Reviews: Learning from Experience,"Modern code review is a critical quality assurance process that is widely
adopted in both industry and open source software environments. This process
can help newcomers learn from the feedback of experienced reviewers; however,
it often brings a large workload and stress to reviewers. To alleviate this
burden, the field of automated code reviews aims to automate the process,
teaching large language models to provide reviews on submitted code, just as a
human would. A recent approach pre-trained and fine-tuned the code intelligent
language model on a large-scale code review corpus. However, such techniques
did not fully utilise quality reviews amongst the training data. Indeed,
reviewers with a higher level of experience or familiarity with the code will
likely provide deeper insights than the others. In this study, we set out to
investigate whether higher-quality reviews can be generated from automated code
review models that are trained based on an experience-aware oversampling
technique. Through our quantitative and qualitative evaluation, we find that
experience-aware oversampling can increase the correctness, level of
information, and meaningfulness of reviews generated by the current
state-of-the-art model without introducing new data. The results suggest that a
vast amount of high-quality reviews are underutilised with current training
strategies. This work sheds light on resource-efficient ways to boost automated
code review models.",2024-02-06T07:48:22Z
,http://arxiv.org/pdf/2402.03349v1.pdf,"When Geoscience Meets Generative AI and Large Language Models:
  Foundations, Trends, and Future Challenges","Generative Artificial Intelligence (GAI) represents an emerging field that
promises the creation of synthetic data and outputs in different modalities.
GAI has recently shown impressive results across a large spectrum of
applications ranging from biology, medicine, education, legislation, computer
science, and finance. As one strives for enhanced safety, efficiency, and
sustainability, generative AI indeed emerges as a key differentiator and
promises a paradigm shift in the field. This paper explores the potential
applications of generative AI and large language models in geoscience. The
recent developments in the field of machine learning and deep learning have
enabled the generative model's utility for tackling diverse prediction
problems, simulation, and multi-criteria decision-making challenges related to
geoscience and Earth system dynamics. This survey discusses several GAI models
that have been used in geoscience comprising generative adversarial networks
(GANs), physics-informed neural networks (PINNs), and generative pre-trained
transformer (GPT)-based structures. These tools have helped the geoscience
community in several applications, including (but not limited to) data
generation/augmentation, super-resolution, panchromatic sharpening, haze
removal, restoration, and land surface changing. Some challenges still remain
such as ensuring physical interpretation, nefarious use cases, and
trustworthiness. Beyond that, GAI models show promises to the geoscience
community, especially with the support to climate change, urban science,
atmospheric science, marine science, and planetary science through their
extraordinary ability to data-driven modeling and uncertainty quantification.",2024-01-25T12:03:50Z
,http://arxiv.org/pdf/2303.17012v3.pdf,Advances in apparent conceptual physics reasoning in GPT-4,"ChatGPT is built on a large language model trained on an enormous corpus of
human text to emulate human conversation. Despite lacking any explicit
programming regarding the laws of physics, recent work has demonstrated that
GPT-3.5 could pass an introductory physics course at some nominal level and
register something close to a minimal understanding of Newtonian Mechanics on
the Force Concept Inventory. This work replicates those results and also
demonstrates that the latest version, GPT-4, has reached a much higher mark in
the latter context. Indeed, its responses come quite close to perfectly
demonstrating expert-level competence, with a few very notable exceptions and
limitations. We briefly comment on the implications of this for the future of
physics education and pedagogy.",2023-03-29T20:32:40Z
,http://arxiv.org/pdf/2303.08033v1.pdf,"Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions
  about Code","We analyzed effectiveness of three generative pre-trained transformer (GPT)
models in answering multiple-choice question (MCQ) assessments, often involving
short snippets of code, from introductory and intermediate programming courses
at the postsecondary level. This emerging technology stirs countless
discussions of its potential uses (e.g., exercise generation, code explanation)
as well as misuses in programming education (e.g., cheating). However, the
capabilities of GPT models and their limitations to reason about and/or analyze
code in educational settings have been under-explored. We evaluated several
OpenAI's GPT models on formative and summative MCQ assessments from three
Python courses (530 questions). We found that MCQs containing code snippets are
not answered as successfully as those that only contain natural language. While
questions requiring to fill-in a blank in the code or completing a natural
language statement about the snippet are handled rather successfully, MCQs that
require analysis and/or reasoning about the code (e.g., what is true/false
about the snippet, or what is its output) appear to be the most challenging.
These findings can be leveraged by educators to adapt their instructional
practices and assessments in programming courses, so that GPT becomes a
valuable assistant for a learner as opposed to a source of confusion and/or
potential hindrance in the learning process.",2023-03-09T16:52:12Z
,http://arxiv.org/pdf/2401.17647v1.pdf,Generative AI for Data Science 101: Coding Without Learning To Code,"Should one teach coding in a required introductory statistics and data
science class for non-technical students? Many professors advise against it,
considering it a distraction from the important and challenging statistical
topics that need to be covered. By contrast, other professors argue that the
ability to interact flexibly with data will inspire students with a lasting
love of the subject and a continued commitment to the material beyond the
introductory course. With the release of large language models that write code,
we saw an opportunity for a middle ground, which we tried in Fall 2023 in a
required introductory data science course in our school's full-time MBA
program. We taught students how to write English prompts to the AI tool Github
Copilot that could be turned into R code and executed. In this short article,
we report on our experience using this new approach.",2024-01-31T08:03:08Z
,http://arxiv.org/pdf/2401.01089v1.pdf,Quokka: An Open-source Large Language Model ChatBot for Material Science,"This paper presents the development of a specialized chatbot for materials
science, leveraging the Llama-2 language model, and continuing pre-training on
the expansive research articles in the materials science domain from the S2ORC
dataset. The methodology involves an initial pretraining phase on over one
million domain-specific papers, followed by an instruction-tuning process to
refine the chatbot's capabilities. The chatbot is designed to assist
researchers, educators, and students by providing instant, context-aware
responses to queries in the field of materials science. We make the four
trained checkpoints (7B, 13B, with or without chat ability) freely available to
the research community at https://github.com/Xianjun-Yang/Quokka.",2024-01-02T08:14:48Z
10.1109/ICSTW58534.2023.00078,http://arxiv.org/pdf/2302.03287v3.pdf,ChatGPT and Software Testing Education: Promises & Perils,"Over the past decade, predictive language modeling for code has proven to be
a valuable tool for enabling new forms of automation for developers. More
recently, we have seen the advent of general purpose ""large language models"",
based on neural transformer architectures, that have been trained on massive
datasets of human written text spanning code and natural language. However,
despite the demonstrated representational power of such models, interacting
with them has historically been constrained to specific task settings, limiting
their general applicability. Many of these limitations were recently overcome
with the introduction of ChatGPT, a language model created by OpenAI and
trained to operate as a conversational agent, enabling it to answer questions
and respond to a wide variety of commands from end users. The introduction of
models, such as ChatGPT, has already spurred fervent discussion from educators,
ranging from fear that students could use these AI tools to circumvent
learning, to excitement about the new types of learning opportunities that they
might unlock. However, given the nascent nature of these tools, we currently
lack fundamental knowledge related to how well they perform in different
educational settings, and the potential promise (or danger) that they might
pose to traditional forms of instruction. As such, in this paper, we examine
how well ChatGPT performs when tasked with answering common questions in a
popular software testing curriculum. Our findings indicate that ChatGPT can
provide correct or partially correct answers in 55.6% of cases, provide correct
or partially correct explanations of answers in 53.0% of cases, and that
prompting the tool in a shared question context leads to a marginally higher
rate of correct responses. Based on these findings, we discuss the potential
promises and perils related to the use of ChatGPT by students and instructors.",2023-02-07T06:41:02Z
,http://arxiv.org/pdf/2404.19336v2.pdf,"Improving LLM Classification of Logical Errors by Integrating Error
  Relationship into Prompts","LLMs trained in the understanding of programming syntax are now providing
effective assistance to developers and are being used in programming education
such as in generation of coding problem examples or providing code
explanations. A key aspect of programming education is understanding and
dealing with error message. However, 'logical errors' in which the program
operates against the programmer's intentions do not receive error messages from
the compiler. In this study, building on existing research on programming
errors, we first define the types of logical errors that can occur in
programming in general. Based on the definition, we propose an effective
approach for detecting logical errors with LLMs that makes use of relations
among error types in the Chain-of-Thought and Tree-of-Thought prompts. The
experimental results indicate that when such logical error descriptions in the
prompt are used, the average classifition performance is about 21% higher than
the ones without them. We also conducted an experiment for exploiting the
relations among errors in generating a new logical error dataset using LLMs. As
there is very limited dataset for logical errors such benchmark dataset can be
very useful for various programming related applications. We expect that our
work can assist novice programmers in identifying the causes of code errors and
correct them more effectively.",2024-04-30T08:03:22Z
,http://arxiv.org/pdf/2307.00150v1.pdf,"Large Language Models (GPT) for automating feedback on programming
  assignments","Addressing the challenge of generating personalized feedback for programming
assignments is demanding due to several factors, like the complexity of code
syntax or different ways to correctly solve a task. In this experimental study,
we automated the process of feedback generation by employing OpenAI's GPT-3.5
model to generate personalized hints for students solving programming
assignments on an automated assessment platform. Students rated the usefulness
of GPT-generated hints positively. The experimental group (with GPT hints
enabled) relied less on the platform's regular feedback but performed better in
terms of percentage of successful submissions across consecutive attempts for
tasks, where GPT hints were enabled. For tasks where the GPT feedback was made
unavailable, the experimental group needed significantly less time to solve
assignments. Furthermore, when GPT hints were unavailable, students in the
experimental condition were initially less likely to solve the assignment
correctly. This suggests potential over-reliance on GPT-generated feedback.
However, students in the experimental condition were able to correct reasonably
rapidly, reaching the same percentage correct after seven submission attempts.
The availability of GPT hints did not significantly impact students' affective
state.",2023-06-30T21:57:40Z
,http://arxiv.org/pdf/2206.05442v7.pdf,"From Human Days to Machine Seconds: Automatically Answering and
  Generating Machine Learning Final Exams","A final exam in machine learning at a top institution such as MIT, Harvard,
or Cornell typically takes faculty days to write, and students hours to solve.
We demonstrate that large language models pass machine learning finals at a
human level, on finals available online after the models were trained, and
automatically generate new human-quality final exam questions in seconds.
Previous work has developed program synthesis and few-shot learning methods to
solve university-level problem set questions in mathematics and STEM courses.
In this work, we develop and compare methods that solve final exams, which
differ from problem sets in several ways: the questions are longer, have
multiple parts, are more complicated, and span a broader set of topics. We
curate a dataset and benchmark of questions from machine learning final exams
available online and code for answering these questions and generating new
questions. We show how to generate new questions from other questions and
course notes. For reproducibility and future research on this final exam
benchmark, we use automatic checkers for multiple-choice, numeric, and
questions with expression answers. We perform ablation studies comparing
zero-shot learning with few-shot learning and chain-of-thought prompting using
GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that
few-shot learning methods perform best. We highlight the transformative
potential of language models to streamline the writing and solution of
large-scale assessments, significantly reducing the workload from human days to
mere machine seconds. Our results suggest that rather than banning large
language models such as ChatGPT in class, instructors should teach students to
harness them by asking students meta-questions about correctness, completeness,
and originality of the responses generated, encouraging critical thinking in
academic studies.",2022-06-11T06:38:06Z
,http://arxiv.org/pdf/2212.05856v1.pdf,"""I think this is the most disruptive technology"": Exploring Sentiments
  of ChatGPT Early Adopters using Twitter Data","Large language models have recently attracted significant attention due to
their impressive performance on a variety of tasks. ChatGPT developed by OpenAI
is one such implementation of a large, pre-trained language model that has
gained immense popularity among early adopters, where certain users go to the
extent of characterizing it as a disruptive technology in many domains.
Understanding such early adopters' sentiments is important because it can
provide insights into the potential success or failure of the technology, as
well as its strengths and weaknesses. In this paper, we conduct a mixed-method
study using 10,732 tweets from early ChatGPT users. We first use topic
modelling to identify the main topics and then perform an in-depth qualitative
sentiment analysis of each topic. Our results show that the majority of the
early adopters have expressed overwhelmingly positive sentiments related to
topics such as Disruptions to software development, Entertainment and
exercising creativity. Only a limited percentage of users expressed concerns
about issues such as the potential for misuse of ChatGPT, especially regarding
topics such as Impact on educational aspects. We discuss these findings by
providing specific examples for each topic and then detail implications related
to addressing these concerns for both researchers and users.",2022-12-12T12:41:24Z
,http://arxiv.org/pdf/2402.14594v1.pdf,"Improving Assessment of Tutoring Practices using Retrieval-Augmented
  Generation","One-on-one tutoring is an effective instructional method for enhancing
learning, yet its efficacy hinges on tutor competencies. Novice math tutors
often prioritize content-specific guidance, neglecting aspects such as
social-emotional learning. Social-emotional learning promotes equity and
inclusion and nurturing relationships with students, which is crucial for
holistic student development. Assessing the competencies of tutors accurately
and efficiently can drive the development of tailored tutor training programs.
However, evaluating novice tutor ability during real-time tutoring remains
challenging as it typically requires experts-in-the-loop. To address this
challenge, this preliminary study aims to harness Generative Pre-trained
Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess
tutors' ability of using social-emotional tutoring strategies. Moreover, this
study also reports on the financial dimensions and considerations of employing
these models in real-time and at scale for automated assessment. The current
study examined four prompting strategies: two basic Zero-shot prompt
strategies, Tree of Thought prompt, and Retrieval-Augmented Generator (RAG)
based prompt. The results indicate that the RAG prompt demonstrated more
accurate performance (assessed by the level of hallucination and correctness in
the generated assessment texts) and lower financial costs than the other
strategies evaluated. These findings inform the development of personalized
tutor training interventions to enhance the the educational effectiveness of
tutored learning.",2024-02-04T20:42:30Z
,http://arxiv.org/pdf/2310.03780v3.pdf,"Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4
  Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation","Generative AI and large language models hold great promise in enhancing
programming education by automatically generating individualized feedback for
students. We investigate the role of generative AI models in providing human
tutor-style programming hints to help students resolve errors in their buggy
programs. Recent works have benchmarked state-of-the-art models for various
feedback generation scenarios; however, their overall quality is still inferior
to human tutors and not yet ready for real-world deployment. In this paper, we
seek to push the limits of generative AI models toward providing high-quality
programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a
first step, our technique leverages GPT-4 as a ``tutor'' model to generate
hints -- it boosts the generative quality by using symbolic information of
failing test cases and fixes in prompts. As a next step, our technique
leverages GPT-3.5, a weaker model, as a ``student'' model to further validate
the hint quality -- it performs an automatic quality validation by simulating
the potential utility of providing this feedback. We show the efficacy of our
technique via extensive evaluation using three real-world datasets of Python
programs covering a variety of concepts ranging from basic algorithms to
regular expressions and data analysis using pandas library.",2023-10-05T17:02:59Z
,http://arxiv.org/pdf/2203.13411v1.pdf,"Reshaping Robot Trajectories Using Natural Language Commands: A Study of
  Multi-Modal Data Alignment Using Transformers","Natural language is the most intuitive medium for us to interact with other
people when expressing commands and instructions. However, using language is
seldom an easy task when humans need to express their intent towards robots,
since most of the current language interfaces require rigid templates with a
static set of action targets and commands. In this work, we provide a flexible
language-based interface for human-robot collaboration, which allows a user to
reshape existing trajectories for an autonomous agent. We take advantage of
recent advancements in the field of large language models (BERT and CLIP) to
encode the user command, and then combine these features with trajectory
information using multi-modal attention transformers. We train the model using
imitation learning over a dataset containing robot trajectories modified by
language commands, and treat the trajectory generation process as a sequence
prediction problem, analogously to how language generation architectures
operate. We evaluate the system in multiple simulated trajectory scenarios, and
show a significant performance increase of our model over baseline approaches.
In addition, our real-world experiments with a robot arm show that users
significantly prefer our natural language interface over traditional methods
such as kinesthetic teaching or cost-function programming. Our study shows how
the field of robotics can take advantage of large pre-trained language models
towards creating more intuitive interfaces between robots and machines. Project
webpage: https://arthurfenderbucker.github.io/NL_trajectory_reshaper/",2022-03-25T01:36:56Z
,http://arxiv.org/pdf/2311.12668v1.pdf,"From Concept to Manufacturing: Evaluating Vision-Language Models for
  Engineering Design","Engineering Design is undergoing a transformative shift with the advent of
AI, marking a new era in how we approach product, system, and service planning.
Large language models have demonstrated impressive capabilities in enabling
this shift. Yet, with text as their only input modality, they cannot leverage
the large body of visual artifacts that engineers have used for centuries and
are accustomed to. This gap is addressed with the release of multimodal vision
language models, such as GPT-4V, enabling AI to impact many more types of
tasks. In light of these advancements, this paper presents a comprehensive
evaluation of GPT-4V, a vision language model, across a wide spectrum of
engineering design tasks, categorized into four main areas: Conceptual Design,
System-Level and Detailed Design, Manufacturing and Inspection, and Engineering
Education Tasks. Our study assesses GPT-4V's capabilities in design tasks such
as sketch similarity analysis, concept selection using Pugh Charts, material
selection, engineering drawing analysis, CAD generation, topology optimization,
design for additive and subtractive manufacturing, spatial reasoning
challenges, and textbook problems. Through this structured evaluation, we not
only explore GPT-4V's proficiency in handling complex design and manufacturing
challenges but also identify its limitations in complex engineering design
applications. Our research establishes a foundation for future assessments of
vision language models, emphasizing their immense potential for innovating and
enhancing the engineering design and manufacturing landscape. It also
contributes a set of benchmark testing datasets, with more than 1000 queries,
for ongoing advancements and applications in this field.",2023-11-21T15:20:48Z
,http://arxiv.org/pdf/2310.08773v2.pdf,"Examining the Potential and Pitfalls of ChatGPT in Science and
  Engineering Problem-Solving","The study explores the capabilities of OpenAI's ChatGPT in solving different
types of physics problems. ChatGPT (with GPT-4) was queried to solve a total of
40 problems from a college-level engineering physics course. These problems
ranged from well-specified problems, where all data required for solving the
problem was provided, to under-specified, real-world problems where not all
necessary data were given. Our findings show that ChatGPT could successfully
solve 62.5% of the well-specified problems, but its accuracy drops to 8.3% for
under-specified problems. Analysis of the model's incorrect solutions revealed
three distinct failure modes: 1) failure to construct accurate models of the
physical world, 2) failure to make reasonable assumptions about missing data,
and 3) calculation errors. The study offers implications for how to leverage
LLM-augmented instructional materials to enhance STEM education. The insights
also contribute to the broader discourse on AI's strengths and limitations,
serving both educators aiming to leverage the technology and researchers
investigating human-AI collaboration frameworks for problem-solving and
decision-making.",2023-10-12T23:39:28Z
,http://arxiv.org/pdf/2311.04926v1.pdf,"More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve
  Visually Diverse Images of Parsons Problems","The advent of large language models is reshaping computing education. Recent
research has demonstrated that these models can produce better explanations
than students, answer multiple-choice questions at or above the class average,
and generate code that can pass automated tests in introductory courses. These
capabilities have prompted instructors to rapidly adapt their courses and
assessment methods to accommodate changes in learning objectives and the
potential for academic integrity violations. While some scholars have advocated
for the integration of visual problems as a safeguard against the capabilities
of language models, new multimodal language models now have vision and language
capabilities that may allow them to analyze and solve visual problems. In this
paper, we evaluate the performance of two large multimodal models on visual
assignments, with a specific focus on Parsons problems presented across diverse
visual representations. Our results show that GPT-4V solved 96.7\% of these
visual problems, struggling minimally with a single Parsons problem.
Conversely, Bard performed poorly by only solving 69.2\% of problems,
struggling with common issues like hallucinations and refusals. These findings
suggest that merely transitioning to visual programming problems might not be a
panacea to issues of academic integrity in the generative AI era.",2023-11-03T14:47:17Z
,http://arxiv.org/pdf/2311.09518v1.pdf,"From GPT-3 to GPT-4: On the Evolving Efficacy of LLMs to Answer
  Multiple-choice Questions for Programming Classes in Higher Education","We explore the evolving efficacy of three generative pre-trained transformer
(GPT) models in generating answers for multiple-choice questions (MCQ) from
introductory and intermediate Python programming courses in higher education.
We focus on the differences in capabilities of the models prior to the release
of ChatGPT (Nov '22), at the time of the release, and today (i.e., Aug '23).
Recent studies have established that the abilities of the OpenAI's GPT models
to handle assessments originally designed for humans keep increasing as the
newer more capable models are released. However, the qualitative differences in
the capabilities and limitations of these models to reason about and/or analyze
programming MCQs have been under-explored. We evaluated three OpenAI's GPT
models on formative and summative MCQ assessments from three Python courses
(530 questions) focusing on the qualitative differences in the evolving
efficacy of the subsequent models. This study provides further evidence and
insight into the trajectory of the current developments where there already
exists a technology that can be utilized by students to collect passing scores,
with no effort whatsoever, on what today counts as viable programming knowledge
and skills assessments. This study could be leveraged by educators and
institutions to better understand the recent technological developments in
order to adapt the design of programming assessments as well as to fuel the
necessary discussions into how assessments in future programming classes should
be updated.",2023-11-16T02:46:15Z
,http://arxiv.org/pdf/2406.14596v1.pdf,"ICAL: Continual Learning of Multimodal Agents by Transforming
  Trajectories into Actionable Insights","Large-scale generative language and vision-language models (LLMs and VLMs)
excel in few-shot in-context learning for decision making and instruction
following. However, they require high-quality exemplar demonstrations to be
included in their context window. In this work, we ask: Can LLMs and VLMs
generate their own prompt examples from generic, sub-optimal demonstrations? We
propose In-Context Abstraction Learning (ICAL), a method that builds a memory
of multimodal experience insights from sub-optimal demonstrations and human
feedback. Given a noisy demonstration in a new domain, VLMs abstract the
trajectory into a general program by fixing inefficient actions and annotating
cognitive abstractions: task relationships, object state changes, temporal
subgoals, and task construals. These abstractions are refined and adapted
interactively through human feedback while the agent attempts to execute the
trajectory in a similar environment. The resulting abstractions, when used as
exemplars in the prompt, significantly improve decision-making in
retrieval-augmented LLM and VLM agents. Our ICAL agent surpasses the
state-of-the-art in dialogue-based instruction following in TEACh, multimodal
web agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, we
achieve a 12.6% improvement in goal-condition success. In VisualWebArena, our
task success rate improves over the SOTA from 14.3% to 22.7%. In Ego4D action
forecasting, we improve over few-shot GPT-4V and remain competitive with
supervised models. We show finetuning our retrieval-augmented in-context agent
yields additional improvements. Our approach significantly reduces reliance on
expert-crafted examples and consistently outperforms in-context learning from
action plans that lack such insights.",2024-06-20T17:45:02Z
,http://arxiv.org/pdf/2405.15436v1.pdf,"Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented
  Knowledge Graphs and Vector Database for Accreditation Reporting Assistance","In higher education, accreditation is a quality assurance process, where an
institution demonstrates a commitment to delivering high quality programs and
services to their students. For business schools nationally and internationally
the Association to Advance Collegiate Schools of Business (AACSB) accreditation
is the gold standard. For a business school to receive and subsequently
maintain accreditation, the school must undertake a rigorous, time consuming
reporting and peer review process, to demonstrate alignment with the AACSB
Standards. For this project we create a hybrid context retrieval augmented
generation pipeline that can assist in the documentation alignment and
reporting process necessary for accreditation. We implement both a vector
database and knowledge graph, as knowledge stores containing both institutional
data and AACSB Standard data. The output of the pipeline can be used by
institution stakeholders to build their accreditation report, dually grounded
by the context from the knowledge stores. To develop our knowledge graphs we
utilized both a manual construction process as well as an LLM Augmented
Knowledge Graph approach. We evaluated the pipeline using the RAGAs framework
and observed optimal performance on answer relevancy and answer correctness
metrics.",2024-05-24T11:05:45Z
