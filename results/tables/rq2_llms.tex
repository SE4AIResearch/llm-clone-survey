\begin{table}[ht]
    \centering
    \caption{Distribution of LLMs per studied paper.}
    \begin{tabular}{l c p{4cm}}
        \toprule
        \textbf{Level} & \textbf{\# Papers} & \textbf{Papers} \\
        \midrule
		ChatGPT & 67 (52.8\%) & \cite{grevisse2024comparative,sanchez2023assessing,kruger2024performance,reiche2024bridging,wolfer2024qualitative,savelka2024gpt3,sarshartehrani2024enhancing,aviv2024impact,zhang2024assistant,song2024automatic,karnalim2024detecting,strzelecki2024acceptance,estevezayres2024evaluation,bukar2024text,grevisse2024docimological,haindl2024students,berrezuetaguzman2023recommendations,tran2023generating,zastudil2023generative,jalil2023chatgpt,hanifi2023chatgpt,rodriguezecheverria2024analysis,wang2023exploring,ellis2024chatgpt,becker2023programming,macneil2023experiences,lau2023ban,gumina2023teaching,zheng2023chatgpt,leinonen2023comparing,gehringer2024dualsubmission,nguyen2024beginning,abolnejadian2024leveraging,qureshi2023chatgpt,prather2023robots,joshi2024chatgpt,fernandez2024cs1,shen2024implications,jordan2024need,denny2024prompt,kirova2024software,cambaz2024use,sheese2024patterns,budhiraja2024its,freire2024may,piccolo2023evaluating,jost2024impact,dengel2023qualitative,kosar2024computer,ahmed2024potentiality,drori2023human,tu2023should,orenstrakh2023detecting,kiesler2023large,li2023evaluating,savelka2023efficient,anishka2023can,agarwal2024which,tanay2024exploratory,arora2024analyzing,kim2024chatgpt,scholl2024analyzing,oosterwyk2024beyond,vadaparty2024cs1llm,garg2024impact,manley2024examining,sharpe2024can} \\
		GPT-3.5-Turbo & 33 (26.0\%) & \cite{savelka2024gpt3,bakas2024integrating,ma2024teach,farah2023prompting,parker2024large,jin2024teach,nguyen2024beginning,kazemitabaar2024codeaid,rajala2023call,kuramitsu2023kogi,prather2023robots,liu2024beyond,cambaz2024use,balse2023evaluating,liffiton2024codehelp,sheese2024patterns,jury2024evaluating,roest2024nextstep,frankford2024aitutoring,lyu2024evaluating,prakash2024integrating,azaiz2023aienhanced,drori2023human,babe2023studenteval,pankiewicz2023large,li2023evaluating,savelka2023efficient,zhang2023students,prather2024interactions,kumar2024using,xiao2024qacp,raihan2024cseprompts,ta2023exgen} \\
		GPT-4 & 26 (20.5\%) & \cite{kruger2024performance,savelka2024gpt3,ma2024teach,parker2024large,tran2023generating,savelka2023thrilled,nguyen2024beginning,prather2023robots,delcarpiogutierrez2024evaluating,kirova2024software,liu2024teaching,cambaz2024use,doughty2024comparative,feng2024more,cipriano2024llms,denny2024desirable,fan2023exploring,dengel2023qualitative,kiesler2023large,savelka2023efficient,oli2024automated,kumar2024using,xiao2024qacp,azaiz2024feedbackgeneration,jacobs2024evaluating,arora2024analyzing} \\
		CoPilot & 9 (7.1\%) & \cite{denny2023conversing,lau2023ban,prather2023robots,cambaz2024use,venkatesh2023evaluating,rasnayaka2024empirical,bien2024generative,tanay2024exploratory,arora2024analyzing} \\
		GPT3 & 8 (6.3\%) & \cite{li2023potential,cao2023scaffolding,cipriano2023gpt3,balse2023investigating,prather2023robots,liffiton2024codehelp,cipriano2024llms,prakash2024integrating} \\
		Codex & 7 (5.5\%) & \cite{sarsa2022automatic,kazemitabaar2023studying,macneil2023experiences,nguyen2024beginning,cambaz2024use,kazemitabaar2024novices,drori2023human} \\
		BARD & 5 (3.9\%) & \cite{estevezayres2024evaluation,cambaz2024use,cipriano2024llms,dengel2023qualitative,agarwal2024which} \\
		LLaMA-2 & 5 (3.9\%) & \cite{padiyath2024insights,prakash2024integrating,oli2024automated,xiao2024qacp,raihan2024cseprompts} \\
		BingAI & 2 (1.6\%) & \cite{kruger2024performance,arora2024analyzing} \\
		CodeBERT & 2 (1.6\%) & \cite{wan2024automated,oli2024automated} \\
		ChatGPT Plus & 2 (1.6\%) & \cite{padiyath2024insights,rasnayaka2024empirical} \\
		StarCoder & 2 (1.6\%) & \cite{babe2023studenteval,raihan2024cseprompts} \\
		Gemini & 2 (1.6\%) & \cite{xiao2024qacp,arora2024analyzing} \\
		CodeT5 & 1 (0.8\%) & \cite{koutcheme2023training} \\
		LLaMA-65B & 1 (0.8\%) & \cite{kruger2024performance} \\
		StableLM-7B & 1 (0.8\%) & \cite{kruger2024performance} \\
		LLaMA-7B & 1 (0.8\%) & \cite{kruger2024performance} \\
		BERTopic & 1 (0.8\%) & \cite{sterbini2024automated} \\
		ResNet + ChatGPT & 1 (0.8\%) & \cite{wang2024enhancing} \\
		ChatGPT + DL models & 1 (0.8\%) & \cite{hoq2024detecting} \\
		ChatGPT-4 Vision & 1 (0.8\%) & \cite{mendoncca2024evaluating} \\
		SantaCoder & 1 (0.8\%) & \cite{babe2023studenteval} \\
		Replit-Code & 1 (0.8\%) & \cite{babe2023studenteval} \\
		Star-Chat & 1 (0.8\%) & \cite{babe2023studenteval} \\
		GPT-4-Turbo & 1 (0.8\%) & \cite{oli2024automated} \\
		RoBERTa & 1 (0.8\%) & \cite{oli2024automated} \\
		text-da-vinci-003 & 1 (0.8\%) & \cite{prather2024interactions} \\
		Microsoft Copilot & 1 (0.8\%) & \cite{agarwal2024which} \\
		GitHub Copilot Cha & 1 (0.8\%) & \cite{agarwal2024which} \\
		Baichuan & 1 (0.8\%) & \cite{xiao2024qacp} \\
		ChatGLM & 1 (0.8\%) & \cite{xiao2024qacp} \\
		Qwen & 1 (0.8\%) & \cite{xiao2024qacp} \\
		Moonshot8 & 1 (0.8\%) & \cite{xiao2024qacp} \\
		Atom & 1 (0.8\%) & \cite{xiao2024qacp} \\
		Falcon & 1 (0.8\%) & \cite{raihan2024cseprompts} \\
		MPT & 1 (0.8\%) & \cite{raihan2024cseprompts} \\
		Code-Lamma & 1 (0.8\%) & \cite{raihan2024cseprompts} \\
		WizardCoder & 1 (0.8\%) & \cite{raihan2024cseprompts} \\
		Mistral & 1 (0.8\%) & \cite{raihan2024cseprompts} \\
	\bottomrule
    \end{tabular}
    \label{tab:rq2}
\end{table}