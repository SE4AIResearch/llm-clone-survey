@inbook{koutcheme2023training,
    author = "Koutcheme, C.",
    title = "Training Language Models for Programming Feedback Using Automated Repair Tools",
    ISBN = "9783031362729",
    ISSN = "1611-3349",
    url = "http://dx.doi.org/10.1007/978-3-031-36272-9\_79",
    DOI = "10.1007/978-3-031-36272-9\_79",
    booktitle = "Artificial Intelligence in Education",
    publisher = "Springer Nature Switzerland",
    year = "2023",
    pages = "830–835"
}


@InProceedings{grevisse2024comparative,
    author = "Gr{\'e}visse, C.",
    editor = "Florez, Hector and Leon, Marcelo",
    title = "Comparative Quality Analysis of GPT-Based Multiple Choice Question Generation",
    booktitle = "Applied Informatics",
    year = "2024",
    publisher = "Springer Nature Switzerland",
    address = "Cham",
    pages = "435--447",
    abstract = "Assessment is an essential part of education, both for teachers who assess their students as well as learners who auto-evaluate themselves. A popular type of assessment questions are multiple-choice questions (MCQ), as they can be automatically graded and can cover a wide range of learning items. However, the creation of high quality MCQ items is nontrivial. With the advent of Generative Pre-trained Transformer (GPT), considerable effort has been recently made regarding Automatic Question Generation (AQG). While metrics have been applied to evaluate the linguistic quality, an evaluation of generated questions according to the best practices for MCQ creation has been missing so far. In this paper, we propose an analysis of the quality of automatically generated MCQs from 3 different GPT-based services. After producing 150 MCQs in the domain of computer science, we analyse them according to common multiple-choice item writing guidelines and annotate them with identified docimological issues. The dataset of annotated MCQs is available in Moodle XML format. We discuss the different flaws and propose solutions for AQG service developers.",
    isbn = "978-3-031-46813-1"
}


@inbook{sanchez2023assessing,
    author = "Sánchez, M. and Herrera, A.",
    title = "Assessing ChatGPT’s Proficiency in CS1-Level Problem Solving",
    ISBN = "9783031473722",
    ISSN = "1865-0937",
    url = "http://dx.doi.org/10.1007/978-3-031-47372-2\_7",
    DOI = "10.1007/978-3-031-47372-2\_7",
    booktitle = "Advances in Computing",
    publisher = "Springer Nature Switzerland",
    year = "2023",
    month = "November",
    pages = "71–81"
}


@InProceedings{kruger2024performance,
    author = {Kr{\"u}ger, T. and Gref, M.},
    editor = {Nowaczyk, S{\l}awomir and Biecek, Przemys{\l}aw and Chung, Neo Christopher and Vallati, Mauro and Skruch, Pawe{\l} and Jaworek-Korjakowska, Joanna and Parkinson, Simon and Nikitas, Alexandros and Atzm{\"u}ller, Martin and Kliegr, Tom{\'a}{\v{s}} and Schmid, Ute and Bobek, Szymon and Lavrac, Nada and Peeters, Marieke and van Dierendonck, Roland and Robben, Saskia and Mercier-Laurent, Eunika and Kayakutlu, G{\"u}lg{\"u}n and Owoc, Mieczyslaw Lech and Mason, Karl and Wahid, Abdul and Bruno, Pierangela and Calimeri, Francesco and Cauteruccio, Francesco and Terracina, Giorgio and Wolter, Diedrich and Leidner, Jochen L. and Kohlhase, Michael and Dimitrova, Vania},
    title = "Performance of Large Language Models in a Computer Science Degree Program",
    booktitle = "Artificial Intelligence. ECAI 2023 International Workshops",
    year = "2024",
    publisher = "Springer Nature Switzerland",
    address = "Cham",
    pages = "409--424",
    abstract = "Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and dominate the current discourse. Their transformative capabilities have led to a paradigm shift in how we interact with and utilize (text-based) information. Each day, new possibilities to leverage the capabilities of these models emerge. This paper presents findings on the performance of different large language models in a university of applied sciences' undergraduate computer science degree program. Our primary objective is to assess the effectiveness of these models within the curriculum by employing them as educational aids. By prompting the models with lecture material, exercise tasks, and past exams, we aim to evaluate their proficiency across different computer science domains. We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program. We found that ChatGPT-3.5 averaged 79.9{\\%} of the total score in 10 tested modules, BingAI achieved 68.4{\\%}, and LLaMa, in the 65 billion parameter variant, 20{\\%}. Despite these convincing results, even GPT-4.0 would not pass the degree program - due to limitations in mathematical calculations.",
    isbn = "978-3-031-50485-3"
}


@inbook{reiche2024bridging,
    author = "Reiche, M. and Leidner, J. L.",
    title = "Bridging the Programming Skill Gap with ChatGPT: A Machine Learning Project with Business Students",
    ISBN = "9783031504853",
    ISSN = "1865-0937",
    url = "http://dx.doi.org/10.1007/978-3-031-50485-3\_42",
    DOI = "10.1007/978-3-031-50485-3\_42",
    booktitle = "Artificial Intelligence. ECAI 2023 International Workshops",
    publisher = "Springer Nature Switzerland",
    year = "2024",
    pages = "439–446"
}


@InProceedings{wolfer2024qualitative,
    author = "Wolfer, J.",
    editor = "Auer, Michael E. and Cukierman, Uriel R. and Vendrell Vidal, Eduardo and Tovar Caro, Edmundo",
    title = "A Qualitative Assessment of ChatGPT Generated Code in the Computer Science Curriculum",
    booktitle = "Towards a Hybrid, Flexible and Socially Engaged Higher Education",
    year = "2024",
    publisher = "Springer Nature Switzerland",
    address = "Cham",
    pages = "43--53",
    abstract = "The emergence of Large Language Models and their deployment in systems such as ChatGPT are poised to have a major impact on STEM education, particularly Computer Science. These generative large language models can produce program code as well as human language output. This has potentially serious implications for computer science programs and pedagogy. This work provides a qualitative assessment sample code generated by ChatGPT, as an example of an LLM explores implications for computing pedagogy....",
    isbn = "978-3-031-53022-7"
}


@InProceedings{savelka2024gpt3,
    author = "Savelka, J. and Agarwal, A. and Bogart, C. and Sakr, M.",
    editor = "McLaren, Bruce M. and Uhomoibhi, James and Jovanovic, Jelena and Chounta, Irene-Angelica",
    title = "From GPT-3 to GPT-4: On the Evolving Efficacy of LLMs to Answer Multiple-Choice Questions for Programming Classes in Higher Education",
    booktitle = "Computer Supported Education",
    year = "2024",
    publisher = "Springer Nature Switzerland",
    address = "Cham",
    pages = "160--182",
    abstract = "We explore the evolving efficacy of three generative pre-trained transformer (GPT) models in generating answers for multiple-choice questions (MCQ) from introductory and intermediate Python programming courses in higher education. We focus on the differences in capabilities of the models prior to the release of ChatGPT (Nov '22), at the time of the release, and today (i.e., Aug '23). Recent studies have established that the abilities of the OpenAI's GPT models to handle assessments originally designed for humans keep increasing as the newer more capable models are released. However, the qualitative differences in the capabilities and limitations of these models to reason about and/or analyze programming MCQs have been under-explored. We evaluated three OpenAI's GPT models on formative and summative MCQ assessments from three Python courses (530 questions) focusing on the qualitative differences in the evolving efficacy of the subsequent models. This study provides further evidence and insight into the trajectory of the current developments where there already exists a technology that can be utilized by students to collect passing scores, with no effort whatsoever, on what today counts as viable programming knowledge and skills assessments. This study could be leveraged by educators and institutions to better understand the recent technological developments in order to adapt the design of programming assessments as well as to fuel the necessary discussions into how assessments in future programming classes should be updated.",
    isbn = "978-3-031-53656-4"
}


@inbook{bakas2024integrating,
    author = "Bakas, N. P. and Papadaki, M. and Vagianou, E. and Christou, I. and Chatzichristofis, S. A.",
    title = "Integrating LLMs in Higher Education, Through Interactive Problem Solving and Tutoring: Algorithmic Approach and Use Cases",
    ISBN = "9783031564789",
    ISSN = "1865-1356",
    url = "http://dx.doi.org/10.1007/978-3-031-56478-9\_21",
    DOI = "10.1007/978-3-031-56478-9\_21",
    booktitle = "Lecture Notes in Business Information Processing",
    publisher = "Springer Nature Switzerland",
    year = "2024",
    pages = "291–307"
}


@inproceedings{sarshartehrani2024enhancing,
    author = "Sarshartehrani, F. and Mohammadrezaei, E. and Behravan, M. and Gracanin, D.",
    title = "Enhancing E-Learning Experience Through Embodied AI Tutors in Immersive Virtual Environments: A Multifaceted Approach for Personalized Educational Adaptation",
    booktitle = "International Conference on Human-Computer Interaction",
    pages = "272--287",
    year = "2024",
    organization = "Springer"
}


@inproceedings{aviv2024impact,
    author = "Aviv, I. and Leiba, M. and Rika, H. and Shani, Y.",
    title = "The Impact of ChatGPT on Students’ Learning Programming Languages",
    booktitle = "International Conference on Human-Computer Interaction",
    pages = "207--219",
    year = "2024",
    organization = "Springer"
}


@inproceedings{sterbini2024automated,
    author = "Sterbini, A. and Temperini, M.",
    title = "Automated Analysis of Algorithm Descriptions Quality, Through Large Language Models",
    booktitle = "International Conference on Intelligent Tutoring Systems",
    pages = "258--271",
    year = "2024",
    organization = "Springer"
}


@inproceedings{ma2024teach,
    author = "Ma, Q. and Shen, H. and Koedinger, K. and Wu, S. Tongshuang",
    title = "How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging",
    booktitle = "International Conference on Artificial Intelligence in Education",
    pages = "265--279",
    year = "2024",
    organization = "Springer"
}


@InProceedings{zhang2024assistant,
    author = "Zhang, D. and Cao, Q. and Guo, Y. and Wang, L.",
    editor = "Hong, Wenxing and Kanaparan, Geetha",
    title = "Assistant Teaching System for Computer Hardware Courses Based on Large Language Model",
    booktitle = "Computer Science and Education. Computer Science and Technology",
    year = "2024",
    publisher = "Springer Nature Singapore",
    address = "Singapore",
    pages = "301--313",
    abstract = "Recently, Large Language Models (LLMs), represented by ch1ChatGPT, have garnered significant attention in the field of education due to its impressive capabilities in text generation, comprehension, logical reasoning, and conversational abilities. We incorporate LLMs into the theoretical and experiment teaching of our Digital Logic and Computer Organization courses to enhance the teaching process. Specifically, we propose and implement an assistant teaching system consisting of a knowledge-based Question and Answer (Q {\\&}A) system and an assistant debugging and checking system. For the theoretical teaching session, the Q {\\&}A system utilizes historical Q {\\&}A records and ChatGPT to answer students' questions. This system reduces the repetitive workload for teachers by answering similar questions, and allows students to receive answers in time. For the Field-Programmable Gate Array (FPGA)-based experiment teaching session, the assistant debugging and checking system employ debug assistance module to explain error messages for students. Furthermore, a LLM-generated code checking module assists teachers in detecting academic misconduct among students' code submissions.",
    isbn = "978-981-97-0730-0"
}


@InProceedings{song2024automatic,
    author = "Song, T. and Tian, Q. and Xiao, Y. and Liu, S.",
    editor = "Hong, Wenxing and Kanaparan, Geetha",
    title = "Automatic Generation of Multiple-Choice Questions for CS0 and CS1 Curricula Using Large Language Models",
    booktitle = "Computer Science and Education. Computer Science and Technology",
    year = "2024",
    publisher = "Springer Nature Singapore",
    address = "Singapore",
    pages = "314--324",
    abstract = "In the context of increasing attention to formative assessment in universities, Multiple Choice Question (MCQ) has become a vital assessment form for CS0 and CS1 courses due to its advantages of rapid assessment, which has brought about a significant demand for MCQ exercises. However, creating many MCQs takes time and effort for teachers. A practical method is to use large language models (LLMs) to generate MCQs automatically, but when dealing with specific domain problems, the model results may need to be more reliable. This article designs a set of prompt chains to improve the performance of LLM in education. Based on this design, we developed EduCS, which is based on GPT-3.5 and can automatically generate complete MCQs according to the CS0/CS1 course outline. To evaluate the quality of MCQs generated by EduCS, we established a set of evaluation metrics from four aspects about the three components of MCQ and the complete MCQ, and based on this, we utilized expert scoring. The experimental results indicate that while the generated questions require teacher verification before being delivered to students, they show great potential in terms of quality. The EduCS system demonstrates the ability to generate complete MCQs that can complement formative and summative assessments for students at different levels. The EduCS has great promise value in the formative assessment of CS education.",
    isbn = "978-981-97-0730-0"
}


@inbook{farah2023prompting,
    author = "Farah, J. Carlos and Ingram, S. and Spaenlehauer, B. and Lasne, F. Kim-Lan and Gillet, D.",
    title = "Prompting Large Language Models to Power Educational Chatbots",
    ISBN = "9789819983858",
    ISSN = "1611-3349",
    url = "http://dx.doi.org/10.1007/978-981-99-8385-8\_14",
    DOI = "10.1007/978-981-99-8385-8\_14",
    booktitle = "Lecture Notes in Computer Science",
    publisher = "Springer Nature Singapore",
    year = "2023",
    pages = "169–188"
}


@InProceedings{wang2024enhancing,
    author = "Wang, H. and Qiang, P. and Tan, H. and Hu, J.",
    editor = "Liu, Qingshan and Wang, Hanzi and Ma, Zhanyu and Zheng, Weishi and Zha, Hongbin and Chen, Xilin and Wang, Liang and Ji, Rongrong",
    title = "Enhancing Image Comprehension for Computer Science Visual Question Answering",
    booktitle = "Pattern Recognition and Computer Vision",
    year = "2024",
    publisher = "Springer Nature Singapore",
    address = "Singapore",
    pages = "487--498",
    abstract = "Computer science visual question answering is a fundamental task in the intelligent education. However, current models have poor performance in this task. There are mainly two issues in these models. Firstly, they cannot accurately capture the fine-grained objects and relations in images. Secondly, these models lack the computer domain knowledge. To address the issues, we propose an Image Comprehension Enhancing Model. Specifically, it uses object detection technique to capture fine-grained features of images and utilizes Optical Character Recognition (OCR) to transform fine-grained features into the text information. The model adopts the text information to prompt the large language model, which generates the image caption with the computer domain knowledge. The fine-grained features and image caption can enhance the model's image comprehension and compensate the lack of knowledge. Additionally, the model utilizes the cross-modal attention mechanism to integrate the image features and fine-grained features of the image with the text features. The experimental results on the CSDQA dataset demonstrate that our proposed model outperforms the baselines, and the accuracy improves at least 4.80{\\%}.",
    isbn = "978-981-99-8429-9"
}


@article{karnalim2024detecting,
    author = "Karnalim, O. and Toba, H. and Johan, M. Christianti",
    title = "Detecting AI assisted submissions in introductory programming via code anomaly",
    journal = "Education and Information Technologies",
    pages = "1--26",
    year = "2024",
    publisher = "Springer"
}


@article{strzelecki2024acceptance,
    author = "Strzelecki, A. and Cicha, K. and Rizun, M. and Rutecka, P.",
    title = "Acceptance and use of ChatGPT in the academic community",
    ISSN = "1573-7608",
    url = "http://dx.doi.org/10.1007/s10639-024-12765-1",
    DOI = "10.1007/s10639-024-12765-1",
    journal = "Education and Information Technologies",
    publisher = "Springer Science and Business Media LLC",
    year = "2024",
    month = "May"
}


@article{estevezayres2024evaluation,
    author = "Est{\'e}vez-Ayres, I. and Callejo, P. and Hombrados-Herrera, M. {\'A}ngel and Alario-Hoyos, C. and Delgado Kloos, C.",
    title = "Evaluation of LLM Tools for Feedback Generation in a Course on Concurrent Programming",
    journal = "International Journal of Artificial Intelligence in Education",
    pages = "1--17",
    year = "2024",
    publisher = "Springer"
}


@article{parker2024large,
    author = "Parker, M. J. and Anderson, C. and Stone, C. and Oh, Y.",
    title = "A Large Language Model Approach to Educational Survey Feedback Analysis",
    ISSN = "1560-4306",
    url = "http://dx.doi.org/10.1007/s40593-024-00414-0",
    DOI = "10.1007/s40593-024-00414-0",
    journal = "International Journal of Artificial Intelligence in Education",
    publisher = "Springer Science and Business Media LLC",
    year = "2024",
    month = "June"
}


@article{bukar2024text,
    author = "Bukar, U. Ali and Sayeed, M. Shohel and Razak, S. Fatimah Abdul and Yogarayan, S. and Amodu, O. Ahmed and Raja Mahmood, R. Azlina",
    title = "Text Analysis on Early Reactions to ChatGPT as a Tool for Academic Progress or Exploitation",
    volume = "5",
    ISSN = "2661-8907",
    url = "http://dx.doi.org/10.1007/s42979-024-02714-7",
    DOI = "10.1007/s42979-024-02714-7",
    number = "4",
    journal = "SN Computer Science",
    publisher = "Springer Science and Business Media LLC",
    year = "2024",
    month = "March"
}


@article{grevisse2024docimological,
    author = "Grévisse, C. and Pavlou, M. Angeliki S. and Schneider, J. G.",
    title = "Docimological Quality Analysis of LLM-Generated Multiple Choice Questions in Computer Science and Medicine",
    volume = "5",
    ISSN = "2661-8907",
    url = "http://dx.doi.org/10.1007/s42979-024-02963-6",
    DOI = "10.1007/s42979-024-02963-6",
    number = "5",
    journal = "SN Computer Science",
    publisher = "Springer Science and Business Media LLC",
    year = "2024",
    month = "June"
}


@article{haindl2024students,
    author = "Haindl, P. and Weinberger, G.",
    title = "Students’ Experiences of Using ChatGPT in an Undergraduate Programming Course",
    volume = "12",
    ISSN = "2169-3536",
    url = "http://dx.doi.org/10.1109/ACCESS.2024.3380909",
    DOI = "10.1109/access.2024.3380909",
    journal = "IEEE Access",
    publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
    year = "2024",
    pages = "43519–43529"
}


@INPROCEEDINGS{li2023potential,
    author = "Li, H.",
    booktitle = "2023 3rd International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI)",
    title = "The Potential of Large Language Models as Tools for Analyzing Student Textual Evaluation: A Differential Analysis Between CS and Non-CS Students",
    year = "2023",
    volume = "",
    number = "",
    pages = "225-230",
    keywords = "Computer science;Analytical models;Systematics;Correlation;Reviews;Computational modeling;Natural language processing;Sentiment;Large Language Model;Student Textual Evaluation;Computer Science;Course Reviews",
    doi = "10.1109/CEI60616.2023.10527886"
}


@inproceedings{berrezuetaguzman2023recommendations,
    author = "Berrezueta-Guzman, J. and Krusche, S.",
    title = "Recommendations to create programming exercises to overcome ChatGPT",
    booktitle = "2023 IEEE 35th International Conference on Software Engineering Education and Training (CSEE\\&T)",
    pages = "147--151",
    year = "2023",
    organization = "IEEE"
}


@INPROCEEDINGS{tran2023generating,
    author = "Tran, A. and Angelikas, K. and Rama, E. and Okechukwu, C. and Smith, D. H. and MacNeil, S.",
    booktitle = "2023 IEEE Frontiers in Education Conference (FIE)",
    title = "Generating Multiple Choice Questions for Computing Courses Using Large Language Models",
    year = "2023",
    volume = "",
    number = "",
    pages = "1-8",
    keywords = "Codes;Correlation;Computational modeling;Education;Real-time systems;Natural language processing;Recycling;large language models;generative AI;multiple-choice questions;computing education",
    doi = "10.1109/FIE58773.2023.tran2023generating"
}


@inproceedings{zastudil2023generative,
    author = "Zastudil, C. and Rogalska, M. and",
    title = "Generative ai in computing education: Perspectives of students and instructors",
    booktitle = "2023 IEEE Frontiers in Education Conference (FIE)",
    year = "2023"
}


@inproceedings{jalil2023chatgpt,
    author = "Jalil, S. and Rafi, S. and LaToza, T. D. and Moran, K. and Lam, W.",
    title = "ChatGPT and Software Testing Education: Promises \\& Perils",
    url = "http://dx.doi.org/10.1109/ICSTW58534.2023.00078",
    DOI = "10.1109/icstw58534.2023.00078",
    booktitle = "2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",
    publisher = "IEEE",
    year = "2023",
    month = "April"
}


@inproceedings{hanifi2023chatgpt,
    author = "Hanifi, K. and Cetin, O. and Yilmaz, C.",
    title = "On ChatGPT: Perspectives from Software Engineering Students",
    url = "http://dx.doi.org/10.1109/QRS60937.2023.00028",
    DOI = "10.1109/qrs60937.2023.00028",
    booktitle = "2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security (QRS)",
    publisher = "IEEE",
    year = "2023",
    month = "October"
}


@article{rodriguezecheverria2024analysis,
    author = "Rodriguez-Echeverría, R. and Gutiérrez, J. D. and Conejero, J. M. and Prieto, Á. E.",
    title = "Analysis of ChatGPT Performance in Computer Engineering Exams",
    volume = "19",
    ISSN = "2374-0132",
    url = "http://dx.doi.org/10.1109/RITA.2024.3381842",
    DOI = "10.1109/rita.2024.3381842",
    journal = "IEEE Revista Iberoamericana de Tecnologias del Aprendizaje",
    publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
    year = "2024",
    pages = "71–80"
}


@article{wan2024automated,
    author = "Wan, H. and Luo, H. and Li, M. and Luo, X.",
    title = "Automated Program Repair for Introductory Programming Assignments",
    volume = "17",
    ISSN = "2372-0050",
    url = "http://dx.doi.org/10.1109/TLT.2024.3403710",
    DOI = "10.1109/tlt.2024.3403710",
    journal = "IEEE Transactions on Learning Technologies",
    publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
    year = "2024",
    pages = "1745–1760"
}


@inproceedings{wang2023exploring,
    author = "Wang, T. and Díaz, D. Vargas and Brown, C. and Chen, Y.",
    title = "Exploring the Role of AI Assistants in Computer Science Education: Methods, Implications, and Instructor Perspectives",
    url = "http://dx.doi.org/10.1109/VL-HCC57772.2023.00018",
    DOI = "10.1109/vl-hcc57772.2023.00018",
    booktitle = "2023 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)",
    publisher = "IEEE",
    year = "2023",
    month = "October"
}


@article{ellis2024chatgpt,
    author = "Ellis, M. E. and Casey, K. Mike and Hill, G.",
    title = "ChatGPT and Python programming homework",
    volume = "22",
    ISSN = "1540-4609",
    url = "http://dx.doi.org/10.1111/dsji.12306",
    DOI = "10.1111/dsji.12306",
    number = "2",
    journal = "Decision Sciences Journal of Innovative Education",
    publisher = "Wiley",
    year = "2024",
    month = "January",
    pages = "74–87"
}


@inproceedings{sarsa2022automatic,
    author = "Sarsa, S. and Denny, P. and Hellas, A. and Leinonen, J.",
    series = "ICER 2022",
    title = "Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models",
    url = "http://dx.doi.org/10.1145/3501385.3543957",
    DOI = "10.1145/3501385.3543957",
    booktitle = "Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 1",
    publisher = "ACM",
    year = "2022",
    month = "August",
    collection = "ICER 2022"
}


@inproceedings{kazemitabaar2023studying,
    author = "Kazemitabaar, M. and Chow, J. and Ma, C. Ka To and Ericson, B. J. and Weintrop, D. and Grossman, T.",
    title = "Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming",
    year = "2023",
    isbn = "9781450394215",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/kazemitabaar2023studying",
    doi = "kazemitabaar2023studying",
    abstract = "AI code generators like OpenAI Codex have the potential to assist novice programmers by generating code from natural language descriptions, however, over-reliance might negatively impact learning and retention. To explore the implications that AI code generators have on introductory programming, we conducted a controlled experiment with 69 novices (ages 10-17). Learners worked on 45 Python code-authoring tasks, for which half of the learners had access to Codex, each followed by a code-modification task. Our results show that using Codex significantly increased code-authoring performance (1.15x increased completion rate and 1.8x higher scores) while not decreasing performance on manual code-modification tasks. Additionally, learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this difference did not reach statistical significance. Of interest, learners with higher Scratch pre-test scores performed significantly better on retention post-tests, if they had prior access to Codex.",
    booktitle = "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
    articleno = "455",
    numpages = "23",
    keywords = "AI Coding Assistants, AI-Assisted Pair-Programming, ChatGPT, Copilot, GPT-3, Introductory Programming, K-12 Computer Science Education, Large Language Models, OpenAI Codex",
    location = "Hamburg, Germany",
    series = "CHI '23"
}


@inproceedings{becker2023programming,
    author = "Becker, B. A. and Denny, P. and Finnie-Ansley, J. and Luxton-Reilly, A. and Prather, J. and Santos, E. Antonio",
    title = "Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation",
    year = "2023",
    isbn = "9781450394314",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/becker2023programming",
    doi = "becker2023programming",
    abstract = "The introductory programming sequence has been the focus of much research in computing education. The recent advent of several viable and freely-available AI-driven code generation tools present several immediate opportunities and challenges in this domain. In this position paper we argue that the community needs to act quickly in deciding what possible opportunities can and should be leveraged and how, while also working on overcoming otherwise mitigating the possible challenges. Assuming that the effectiveness and proliferation of these tools will continue to progress rapidly, without quick, deliberate, and concerted efforts, educators will lose advantage in helping shape what opportunities come to be, and what challenges will endure. With this paper we aim to seed this discussion within the computing education community.",
    booktitle = "Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "500–506",
    numpages = "7",
    keywords = "ai, alphacode, amazon, artificial intelligence, code generation, codewhisperer, codex, copilot, cs1, cs2, github, google, gpt-3, introductory programming, large language model, llm, machine learning, midjourney, novice programmers, openai, programming, tabnine",
    location = "Toronto ON, Canada",
    series = "SIGCSE 2023"
}


@inproceedings{macneil2023experiences,
    author = "MacNeil, S. and Tran, A. and Hellas, A. and Kim, J. and Sarsa, S. and Denny, P. and Bernstein, S. and Leinonen, J.",
    title = "Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book",
    year = "2023",
    isbn = "9781450394314",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/10.1145/3545945.3569785",
    doi = "10.1145/3545945.3569785",
    abstract = "Advances in natural language processing have resulted in large language models (LLMs) that can generate code and code explanations. In this paper, we report on our experiences generating multiple code explanation types using LLMs and integrating them into an interactive e-book on web software development. Three different types of explanations -- a line-by-line explanation, a list of important concepts, and a high-level summary of the code -- were created. Students could view explanations by clicking a button next to code snippets, which showed the explanation and asked about its utility. Our results show that all explanation types were viewed by students and that the majority of students perceived the code explanations as helpful to them. However, student engagement varied by code snippet complexity, explanation type, and code snippet length. Drawing on our experiences, we discuss future directions for integrating explanations generated by LLMs into CS classrooms.",
    booktitle = "Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "931–937",
    numpages = "7",
    location = "Toronto ON, Canada",
    series = "SIGCSE 2023"
}


@inproceedings{denny2023conversing,
    author = "Denny, P. and Kumar, V. and Giacaman, N.",
    title = "Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language",
    year = "2023",
    isbn = "9781450394314",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/denny2023conversing",
    doi = "denny2023conversing",
    abstract = "GitHub Copilot is an artificial intelligence tool for automatically generating source code from natural language problem descriptions. Since June 2022, Copilot has officially been available for free to all students as a plug-in to development environments like Visual Studio Code. Prior work exploring OpenAI Codex, the underlying model that powers Copilot, has shown it performs well on typical CS1 problems thus raising concerns about its potential impact on how introductory programming courses are taught. However, little is known about the types of problems for which Copilot does not perform well, or about the natural language interactions that a student might have with Copilot when resolving errors. We explore these questions by evaluating the performance of Copilot on a publicly available dataset of 166 programming problems. We find that it successfully solves around half of these problems on its very first attempt, and that it solves 60\% of the remaining problems using only natural language changes to the problem description. We argue that this type of prompt engineering, which we believe will become a standard interaction between human and Copilot when it initially fails, is a potentially useful learning activity that promotes computational thinking skills, and is likely to change the nature of code writing skill development.",
    booktitle = "Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "1136–1142",
    numpages = "7",
    keywords = "artificial intelligence, cs1, foundation models, github copilot, introductory programming, large language models, openai",
    location = "Toronto ON, Canada",
    series = "SIGCSE 2023"
}


@inproceedings{lau2023ban,
    author = "Lau, S. and Guo, P.",
    title = {From "Ban It Till We Understand It" to "Resistance is Futile": How University Programming Instructors Plan to Adapt as More Students Use AI Code Generation and Explanation Tools such as ChatGPT and GitHub Copilot},
    year = "2023",
    isbn = "9781450399760",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/lau2023ban",
    doi = "lau2023ban",
    abstract = "Over the past year (2022–2023), recently-released AI tools such as ChatGPT and GitHub Copilot have gained significant attention from computing educators. Both researchers and practitioners have discovered that these tools can generate correct solutions to a variety of introductory programming assignments and accurately explain the contents of code. Given their current capabilities and likely advances in the coming years, how do university instructors plan to adapt their courses to ensure that students still learn well? To gather a diverse sample of perspectives, we interviewed 20 introductory programming instructors (9 women + 11 men) across 9 countries (Australia, Botswana, Canada, Chile, China, Rwanda, Spain, Switzerland, United States) spanning all 6 populated continents. To our knowledge, this is the first empirical study to gather instructor perspectives about how they plan to adapt to these AI coding tools that more students will likely have access to in the future. We found that, in the short-term, many planned to take immediate measures to discourage AI-assisted cheating. Then opinions diverged about how to work with AI coding tools longer-term, with one side wanting to ban them and continue teaching programming fundamentals, and the other side wanting to integrate them into courses to prepare students for future jobs. Our study findings capture a rare snapshot in time in early 2023 as computing instructors are just starting to form opinions about this fast-growing phenomenon but have not yet converged to any consensus about best practices. Using these findings as inspiration, we synthesized a diverse set of open research questions regarding how to develop, deploy, and evaluate AI coding tools for computing education.",
    booktitle = "Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1",
    pages = "106–121",
    numpages = "16",
    keywords = "AI coding tools, ChatGPT, Copilot, LLM, instructor perspectives",
    location = "Chicago, IL, USA",
    series = "ICER '23"
}


@inproceedings{savelka2023thrilled,
    author = "Savelka, J. and Agarwal, A. and An, M. and Bogart, C. and Sakr, M.",
    series = "ICER 2023",
    title = "Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses",
    url = "http://dx.doi.org/10.1145/3568813.3600142",
    DOI = "10.1145/3568813.3600142",
    booktitle = "Proceedings of the 2023 ACM Conference on International Computing Education Research V.1",
    publisher = "ACM",
    year = "2023",
    month = "August",
    collection = "ICER 2023"
}


@inproceedings{cao2023scaffolding,
    author = "Cao, C.",
    title = "Scaffolding CS1 Courses with a Large Language Model-Powered Intelligent Tutoring System",
    year = "2023",
    isbn = "9798400701078",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/10.1145/3581754.3584111",
    doi = "10.1145/3581754.3584111",
    abstract = "Programming skills are rapidly becoming essential for many educational paths and career opportunities. Yet, for many international students, the traditional approach to teaching introductory programming courses can be a significant challenge due to the complexities of the language, the lack of prior programming knowledge, and the language and cultural barriers. This study explores how large language models and gamification can scaffold coding learning and increase Chinese students’ sense of belonging in introductory programming courses. In this project, a gamification intelligent tutoring system was developed to adapt to Chinese international students’ learning needs and provides scaffolding to support their success in introductory computer programming courses. My research includes three studies: a formative study, a user study of an initial prototype, and a computer simulation study with a user study in progress. Both qualitative and quantitative data were collected through surveys, observations, focus group discussions and computer simulation. The preliminary findings suggest that GPT-3-enhanced gamification has great potential in scaffolding introductory programming learning by providing adaptive and personalised feedback, increasing students’ sense of belonging, and reducing their anxiety about learning programming.",
    booktitle = "Companion Proceedings of the 28th International Conference on Intelligent User Interfaces",
    pages = "229–232",
    numpages = "4",
    location = "Sydney, NSW, Australia",
    series = "IUI '23 Companion"
}


@inproceedings{gumina2023teaching,
    author = "Gumina, S. and Dalton, T. and Gerdes, J.",
    title = "Teaching IT Software Fundamentals: Strategies and Techniques for Inclusion of Large Language Models: Strategies and Techniques for Inclusion of Large Language Models",
    year = "2023",
    isbn = "9798400701306",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/gumina2023teaching",
    doi = "gumina2023teaching",
    abstract = "This paper argues for the inclusion of tools that utilize Artificial Intelligence (AI) Large Language Models (LLMs) in information technology (IT) undergraduate courses that teach the fundamentals of software. LLM tools have become widely available and disrupt traditional methods for teaching software concepts. Learning objectives are compromised when students submit AI-generated code for a classroom assignment without comprehending or validating the code. Since LLM tools including OpenAI Codex, Copilot by GitHub, and ChatGPT are being used in industry for software development, students need to be familiar with their use without compromising student learning. Incorporating LLM tools into the curriculum prepares students for real-world software development. However, students still need to understand software fundamentals including how to write and debug code. There are many challenges associated with the inclusion of AI tools into the IT curriculum that need to be addressed and mitigated. This paper presents strategies and techniques to integrate student use of LLM tools, assist students’ interaction with the tools, and help prepare students for careers that increasingly use AI tools to design, develop, and maintain software.",
    booktitle = "Proceedings of the 24th Annual Conference on Information Technology Education",
    pages = "60–65",
    numpages = "6",
    location = "Marietta, GA, USA",
    series = "SIGITE '23"
}


@inproceedings{zheng2023chatgpt,
    author = "Zheng, Y.",
    series = "SIGITE ’23",
    title = "ChatGPT for Teaching and Learning: An Experience from Data Science Education",
    url = "http://dx.doi.org/10.1145/3585059.3611431",
    DOI = "10.1145/3585059.3611431",
    booktitle = "The 24th Annual Conference on Information Technology Education",
    publisher = "ACM",
    year = "2023",
    month = "October",
    collection = "SIGITE ’23"
}


@inproceedings{leinonen2023comparing,
    author = "Leinonen, J. and Denny, P. and MacNeil, S. and Sarsa, S. and Bernstein, S. and Kim, J. and Tran, A. and Hellas, A.",
    series = "ITiCSE 2023",
    title = "Comparing Code Explanations Created by Students and Large Language Models",
    url = "http://dx.doi.org/10.1145/3587102.3588785",
    DOI = "10.1145/3587102.3588785",
    booktitle = "Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1",
    publisher = "ACM",
    year = "2023",
    month = "June",
    collection = "ITiCSE 2023"
}


@inproceedings{cipriano2023gpt3,
    author = "Cipriano, B. Pereira and Alves, P.",
    title = "GPT-3 vs Object Oriented Programming Assignments: An Experience Report",
    year = "2023",
    isbn = "9798400701382",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/cipriano2023gpt3",
    doi = "cipriano2023gpt3",
    abstract = "Recent studies show that AI-driven code generation tools, such as Large Language Models, are able to solve most of the problems usually presented in introductory programming classes. However, it is still unknown how they cope with Object Oriented Programming assignments, where the students are asked to design and implement several interrelated classes (either by composition or inheritance) that follow a set of best-practices. Since the majority of the exercises in these tools' training dataset are written in English, it is also unclear how well they function with exercises published in other languages.In this paper, we report our experience using GPT-3 to solve 6 real-world tasks used in an Object Oriented Programming course at a Portuguese University and written in Portuguese. Our observations, based on an objective evaluation of the code, performed by an open-source Automatic Assessment Tool, show that GPT-3 is able to interpret and handle direct functional requirements, however it tends not to give the best solution in terms of object oriented design. We perform a qualitative analysis of GPT-3's output, and gather a set of recommendations for computer science educators, since we expect students to use and abuse this tool in their academic work.",
    booktitle = "Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1",
    pages = "61–67",
    numpages = "7",
    keywords = "GPT-3, large language models, object oriented programming, programming assignments, teaching",
    location = "Turku, Finland",
    series = "ITiCSE 2023"
}


@inproceedings{balse2023investigating,
    author = "Balse, R. and Valaboju, B. and Singhal, S. and Warriem, J. Madathil and Prasad, P.",
    title = "Investigating the Potential of GPT-3 in Providing Feedback for Programming Assessments",
    year = "2023",
    isbn = "9798400701382",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/balse2023investigating",
    doi = "balse2023investigating",
    abstract = "Recent advances in artificial intelligence have led to the development of large language models (LLMs), which are able to generate text, images, and source code based on prompts provided by humans. In this paper, we explore the capabilities of an LLM - OpenAI's GPT-3 model to provide feedback for student written code. Specifically, we examine the feasibility of GPT-3 to check, critique and suggest changes to code written by learners in an online programming exam of an undergraduate Python programming course.We collected 1211 student code submissions from 7 questions asked in a programming exam, and provided the GPT-3 model with separate prompts to check, critique and provide suggestions on these submissions. We found that there was a high variability in the accuracy of the model's feedback for student submissions. Across questions, the range for accurately checking the correctness of the code was between 57\% to 79\%, between 41\% to 77\% for accurately critiquing code, and between 32\% and 93\% for suggesting appropriate changes to the code. We also found instances where the model generated incorrect and inconsistent feedback. These findings suggest that models like GPT-3 currently cannot be 'directly' used to provide feedback to students for programming assessments.",
    booktitle = "Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1",
    pages = "292–298",
    numpages = "7",
    keywords = "GPT-3, evaluation, feedback, large language models (LLM), python programming",
    location = "Turku, Finland",
    series = "ITiCSE 2023"
}


@inproceedings{gehringer2024dualsubmission,
    author = "Gehringer, E. F. and Wang, J. George and Jilla, S. Kumar",
    title = "Dual-Submission Homework in Parallel Computer Architecture: An Exploratory Study in the Age of LLMs",
    year = "2024",
    isbn = "9798400702532",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/10.1145/3605507.3610629",
    doi = "10.1145/3605507.3610629",
    abstract = "The traditional model of assigning textbook problems for homework is endangered by the ability of students to find answers to almost any published problem on the web. An alternative is a dual-submission approach, where students submit their work, then receive the solutions, and submit a second metacognitive reflection, explaining any errors they made. Students’ scores can depend on the quality of their second submissions alone or the combined quality of their first and second submissions. We tried this approach in a class on parallel computer architecture. We report students’ personal experience based on their questionnaires responses. In addition, we quantitatively compare students’ performance on test questions related to dual-submission homework against their performance on other questions and previous semesters’ student performance on similar questions. Students overwhelmingly preferred this approach and thought they learned more from it, but evidence about whether it improved their learning was inconclusive. We also analyze the continued viability of this approach in the era of large language models.",
    booktitle = "Proceedings of the Workshop on Computer Architecture Education",
    pages = "41–47",
    numpages = "7",
    location = "Orlando, FL, USA",
    series = "WCAE '23"
}


@inproceedings{jin2024teach,
    author = "Jin, H. and Lee, S. and Shin, H. and Kim, J.",
    title = "Teach AI How to Code: Using Large Language Models as Teachable Agents for Programming Education",
    year = "2024",
    isbn = "9798400703300",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/jin2024teach",
    doi = "jin2024teach",
    abstract = "This work investigates large language models (LLMs) as teachable agents for learning by teaching (LBT). LBT with teachable agents helps learners identify knowledge gaps and discover new knowledge. However, teachable agents require expensive programming of subject-specific knowledge. While LLMs as teachable agents can reduce the cost, LLMs’ expansive knowledge as tutees discourages learners from teaching. We propose a prompting pipeline that restrains LLMs’ knowledge and makes them initiate “why” and “how” questions for effective knowledge-building. We combined these techniques into TeachYou, an LBT environment for algorithm learning, and AlgoBo, an LLM-based tutee chatbot that can simulate misconceptions and unawareness prescribed in its knowledge state. Our technical evaluation confirmed that our prompting pipeline can effectively configure AlgoBo’s problem-solving performance. Through a between-subject study with 40 algorithm novices, we also observed that AlgoBo’s questions led to knowledge-dense conversations (effect size=0.71). Lastly, we discuss design implications, cost-efficiency, and personalization of LLM-based teachable agents.",
    booktitle = "Proceedings of the CHI Conference on Human Factors in Computing Systems",
    articleno = "652",
    numpages = "28",
    keywords = "AI and Education, Generative AI, Human-AI interaction, LLM agents",
    location = "Honolulu, HI, USA",
    series = "CHI '24"
}


@inproceedings{nguyen2024beginning,
    author = "Nguyen, S. and Babe, H. McLean and Zi, Y. and Guha, A. and Anderson, C. Jane and Feldman, M. Q",
    title = "How Beginning Programmers and Code LLMs (Mis)read Each Other",
    year = "2024",
    isbn = "9798400703300",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/nguyen2024beginning",
    doi = "nguyen2024beginning",
    abstract = "Generative AI models, specifically large language models (LLMs), have made strides towards the long-standing goal of text-to-code generation. This progress has invited numerous studies of user interaction. However, less is known about the struggles and strategies of non-experts, for whom each step of the text-to-code problem presents challenges: describing their intent in natural language, evaluating the correctness of generated code, and editing prompts when the generated code is incorrect. This paper presents a large-scale controlled study of how 120 beginning coders across three academic institutions approach writing and editing prompts. A novel experimental design allows us to target specific steps in the text-to-code process and reveals that beginners struggle with writing and editing prompts, even for problems at their skill level and when correctness is automatically determined. Our mixed-methods evaluation provides insight into student processes and perceptions with key implications for non-expert Code LLM use within and outside of education.",
    booktitle = "Proceedings of the CHI Conference on Human Factors in Computing Systems",
    articleno = "651",
    numpages = "26",
    location = "Honolulu, HI, USA",
    series = "CHI '24"
}


@inproceedings{kazemitabaar2024codeaid,
    author = "Kazemitabaar, M. and Ye, R. and Wang, X. and Henley, A. Zachary and Denny, P. and Craig, M. and Grossman, T.",
    series = "CHI ’24",
    title = "CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs",
    url = "http://dx.doi.org/10.1145/3613904.3642773",
    DOI = "10.1145/3613904.3642773",
    booktitle = "Proceedings of the CHI Conference on Human Factors in Computing Systems",
    publisher = "ACM",
    year = "2024",
    month = "May",
    collection = "CHI ’24"
}


@inproceedings{abolnejadian2024leveraging,
    author = "Abolnejadian, M. and Alipour, S. and Taeb, K.",
    title = "Leveraging ChatGPT for Adaptive Learning through Personalized Prompt-based Instruction: A CS1 Education Case Study",
    booktitle = "Extended Abstracts of the CHI Conference on Human Factors in Computing Systems",
    pages = "1--8",
    year = "2024"
}


@inproceedings{qureshi2023chatgpt,
    author = "Qureshi, B.",
    title = "ChatGPT in Computer Science Curriculum Assessment: An analysis of Its Successes and Shortcomings",
    year = "2023",
    isbn = "9798400700415",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/10.1145/3613944.3613946",
    doi = "10.1145/3613944.3613946",
    abstract = "The application of Artificial intelligence for teaching and learning in the academic sphere is a trending subject of interest in computing education. ChatGPT, as an AI-based tool, provides various advantages, such as heightened student involvement, cooperation, accessibility, and availability. This paper addresses the prospects and obstacles associated with utilizing ChatGPT as a tool for learning and assessment in undergraduate Computer Science curriculum in particular to teaching and learning fundamental programming courses. Students having completed the course work for a Data Structures and Algorithms (a sophomore-level course) participated in this study. Two groups of students were given programming challenges to solve within a short period of time. The control group (group A) had access to textbooks and notes of programming courses, however, no Internet access was provided. Group B students were given access to ChatGPT and were encouraged to use it to help solve the programming challenges. The challenge was conducted in a computer lab environment using Programming Contest Control (PC2) environment which is widely used in ACM International Collegiate Programming Contest (ICPC). Each team of students addresses the problem by writing executable code that satisfies a certain number of test cases. Student teams were scored based on their performance in terms of the number of successfully passed test cases. Results show that students using ChatGPT had an advantage in terms of earned scores, however, there were inconsistencies and inaccuracies in the submitted code consequently affecting the overall performance. After a thorough analysis, the paper’s findings indicate that incorporating AI in higher education brings about various opportunities and challenges. Nonetheless, universities can efficiently manage these apprehensions by adopting a proactive and ethical stance toward the implementation of such tools.",
    booktitle = "Proceedings of the 2023 9th International Conference on E-Society, e-Learning and e-Technologies",
    pages = "7–13",
    numpages = "7",
    keywords = "Academic assessment, ChatGPT, Data Structures and Algorithms, programming concepts",
    location = "Portsmouth, United Kingdom",
    series = "ICSLT '23"
}


@inproceedings{rajala2023call,
    author = {Rajala, J. and Hukkanen, J. and Hartikainen, M. and Niemel\"{a}, P.},
    title = {"Call me Kiran" – ChatGPT as a Tutoring Chatbot in a Computer Science Course},
    year = "2023",
    isbn = "9798400708749",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/rajala2023call",
    doi = "rajala2023call",
    abstract = "Natural language processing has taken enormous steps during the last few years. The development of large language models and generative AI has elevated natural language processing to the level that it can output coherent and contextually relevant text for a given natural language prompt. ChatGPT is one incarnation of these steps, and its use in education is a rather new phenomenon. In this paper, we study students’ perception on ChatGPT during a computer science course. On the course, we integrated ChatGPT into Teams private discussion groups. In addition, all the students had freedom to employ ChatGPT and related technologies to help them in their coursework. The results show that the majority of students had at least tested AI-powered chatbots, and that students are using AI-powered chatbots for multiple tasks, e.g., debugging code, tutoring, and enhancing comprehension. The amount of positive implications of using ChatGPT takes over the negative implications, when the implications were considered from an understanding, learning and creativity perspective. Relatively many students reported reliability issues with the outputs and that the iterations with prompts might be necessary for satisfactory outputs. It is important to try to steer the usage of ChatGPT so that it complements students’ learning processes, but does not replace it.",
    booktitle = "Proceedings of the 26th International Academic Mindtrek Conference",
    pages = "83–94",
    numpages = "12",
    keywords = "ChatGPT, artificial intelligence, chatbots, discussion forum, education, generative AI, student perceptions, tutoring",
    location = "Tampere, Finland",
    series = "Mindtrek '23"
}


@inproceedings{kuramitsu2023kogi,
    author = "Kuramitsu, K. and Obara, Y. and Sato, M. and Obara, M.",
    title = "KOGI: A Seamless Integration of ChatGPT into Jupyter Environments for Programming Education",
    year = "2023",
    isbn = "9798400703904",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/10.1145/3622780.3623648",
    doi = "10.1145/3622780.3623648",
    abstract = "The impact of ChatGPT has brought both anxiety and anticipation to schools and universities. Exploring a positive method to improve programming skills with ChatGPT is a new and pressing challenge. In pursuit of this goal, we have developed KOGI, a learning support system that integrates ChatGPT into the Jupyter environment. This paper demonstrates how KOGI enables students to receive timely advice from ChatGPT in response to errors and other questions they encounter. We immediately introduced KOGI in our two introductory courses: Algorithms and Data Science. The introduction of KOGI resulted in a significant decrease in the number of unresolved student errors. In addition, we report on student trends observed in the classroom regarding the type and frequency of help requested. Although our findings are preliminary, they are informative for programming instructors interested in using ChatGPT.",
    booktitle = "Proceedings of the 2023 ACM SIGPLAN International Symposium on SPLASH-E",
    pages = "50–59",
    numpages = "10",
    keywords = "ChatGPT, LLM, classroom experience, programming education",
    location = "Cascais, Portugal",
    series = "SPLASH-E 2023"
}


@inproceedings{prather2023robots,
    author = "Prather, J. and Denny, P. and Leinonen, J. and Becker, B. A. and Albluwi, I. and Craig, M. and Keuning, H. and Kiesler, N. and Kohn, T. and Luxton-Reilly, A. and MacNeil, S. and Petersen, A. and Pettit, R. and Reeves, B. N. and Savelka, J.",
    title = "The Robots Are Here: Navigating the Generative AI Revolution in Computing Education",
    year = "2023",
    isbn = "9798400704055",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/10.1145/3623762.3633499",
    doi = "10.1145/3623762.3633499",
    abstract = "Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80\% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving.There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.",
    booktitle = "Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education",
    pages = "108–159",
    numpages = "52",
    keywords = "ai, artificial intelligence, chatgpt, code generation, codex, computer programming, copilot, cs1, curriculum, generative ai, github, gpt, gpt-3, gpt-4, large language models, llm, llms, novice programming, openai, pedagogical practices, programming",
    location = "Turku, Finland",
    series = "ITiCSE-WGR '23"
}


@inproceedings{liu2024beyond,
    author = "Liu, M. and M'Hiri, F.",
    title = "Beyond Traditional Teaching: Large Language Models as Simulated Teaching Assistants in Computer Science",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/liu2024beyond",
    doi = "liu2024beyond",
    abstract = "As the prominence of Large Language Models (LLMs) grows in various sectors, their potential in education warrants exploration. In this study, we investigate the feasibility of employing GPT-3.5 from OpenAI, as an LLM teaching assistant (TA) or a virtual TA in computer science (CS) courses. The objective is to enhance the accessibility of CS education while maintaining academic integrity by refraining from providing direct solutions to current-semester assignments. Targeting Foundations of Programming (COMP202), an undergraduate course that introduces students to programming with Python, we have developed a virtual TA using the LangChain framework, known for integrating language models with diverse data sources and environments. The virtual TA assists students with their code and clarifies complex concepts. For homework questions, it is designed to guide students with hints rather than giving out direct solutions. We assessed its performance first through a qualitative evaluation, then a survey-based comparative analysis, using a mix of questions commonly asked on the COMP202 discussion board and questions created by the authors. Our preliminary results indicate that the virtual TA outperforms human TAs on clarity and engagement, matching them on accuracy when the question is non-assignment-specific, for which human TAs still proved more reliable. These findings suggest that while virtual TAs, leveraging the capabilities of LLMs, hold great promise towards making CS education experience more accessible and engaging, their optimal use necessitates human supervision. We conclude by identifying several directions that could be explored in future implementations.",
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "743–749",
    numpages = "7",
    keywords = "adaptive teaching, chatgpt, cs education, gpt, llm, machine learning, novice programmers, openai, programming",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{joshi2024chatgpt,
    author = "Joshi, I. and Budhiraja, R. and Dev, H. and Kadia, J. and Ataullah, M. Osama and Mitra, S. and Akolekar, H. D. and Kumar, D.",
    title = "ChatGPT in the Classroom: An Analysis of Its Strengths and Weaknesses for Solving Undergraduate Computer Science Questions",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/joshi2024chatgpt",
    doi = "joshi2024chatgpt",
    abstract = "This research paper aims to analyze the strengths and weaknesses associated with the utilization of ChatGPT as an educational tool in the context of undergraduate computer science education. ChatGPT's usage in tasks such as solving assignments and exams has the potential to undermine students' learning outcomes and compromise academic integrity. This study adopts a quantitative approach to demonstrate the notable unreliability of ChatGPT in providing accurate answers to a wide range of questions within the field of undergraduate computer science. While the majority of existing research has concentrated on assessing the performance of Large Language Models in handling programming assignments, our study adopts a more comprehensive approach. Specifically, we evaluate various types of questions such as true/false, multi-choice, multi-select, short answer, long answer, design-based, and coding-related questions. Our evaluation highlights the potential consequences of students excessively relying on ChatGPT for the completion of assignments and exams, including self-sabotage. We conclude with a discussion on how can students and instructors constructively use ChatGPT and related tools to enhance the quality of instruction and the overall student experience.",
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "625–631",
    numpages = "7",
    keywords = "chatgpt, computer science, education",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{fernandez2024cs1,
    author = "Fernandez, A. S. and Cornell, K. A.",
    title = "CS1 with a Side of AI: Teaching Software Verification for Secure Code in the Era of Generative AI",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/fernandez2024cs1",
    doi = "fernandez2024cs1",
    abstract = {As AI-generated code promises to become an increasingly relied upon tool for software developers, there is a temptation to call for significant changes to early computer science curricula. A move from syntax-focused topics in CS1 toward abstraction and high-level application design seems motivated by the new large language models (LLMs) recently made available. In this position paper however, we advocate for an approach more informed by the AI itself - teaching early CS learners not only how to use the tools but also how to better understand them. Novice programmers leveraging AI-code-generation without proper understanding of syntax or logic can create "black box" code with significant security vulnerabilities. We outline methods for integrating basic AI knowledge and traditional software verification steps into CS1 along with LLMs, which will better prepare students for software development in professional settings.},
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "345–351",
    numpages = "7",
    keywords = "ai, artificial intelligence, code generation, copilot, cs1, gpt-4, introductory programming, large language model, llm, machine learning, novice programmers, programming, prompt engineering, secure code, software verification",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{hoq2024detecting,
    author = "Hoq, M. and Shi, Y. and Leinonen, J. and Babalola, D. and Lynch, C. and Price, T. and Akram, B.",
    title = "Detecting ChatGPT-Generated Code Submissions in a CS1 Course Using Machine Learning Models",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/hoq2024detecting",
    doi = "hoq2024detecting",
    abstract = "The emergence of publicly accessible large language models (LLMs) such as ChatGPT poses unprecedented risks of new types of plagiarism and cheating where students use LLMs to solve exercises for them. Detecting this behavior will be a necessary component in introductory computer science (CS1) courses, and educators should be well-equipped with detection tools when the need arises. However, ChatGPT generates code non-deterministically, and thus, traditional similarity detectors might not suffice to detect AI-created code. In this work, we explore the affordances of Machine Learning (ML) models for the detection task. We used an openly available dataset of student programs for CS1 assignments and had ChatGPT generate code for the same assignments, and then evaluated the performance of both traditional machine learning models and Abstract Syntax Tree-based (AST-based) deep learning models in detecting ChatGPT code from student code submissions. Our results suggest that both traditional machine learning models and AST-based deep learning models are effective in identifying ChatGPT-generated code with accuracy above 90\%. Since the deployment of such models requires ML knowledge and resources that are not always accessible to instructors, we also explore the patterns detected by deep learning models that indicate possible ChatGPT code signatures, which instructors could possibly use to detect LLM-based cheating manually. We also explore whether explicitly asking ChatGPT to impersonate a novice programmer affects the code produced. We further discuss the potential applications of our proposed models for enhancing introductory computer science instruction.",
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "526–532",
    numpages = "7",
    keywords = "artificial intelligence, chatgpt, cheat detection, cs1, introductory programming course, large language model, plagiarism detection",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{delcarpiogutierrez2024evaluating,
    author = "Del Carpio Gutierrez, A. and Denny, P. and Luxton-Reilly, A.",
    title = "Evaluating Automatically Generated Contextualised Programming Exercises",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/delcarpiogutierrez2024evaluating",
    doi = "delcarpiogutierrez2024evaluating",
    abstract = "Introductory programming courses often require students to solve many small programming exercises as part of their learning. Researchers have previously suggested that the context used in the problem description for these exercises is likely to impact student engagement and motivation. Furthermore, supplying programming exercises that use a broad range of contexts or even allowing students to select contexts to personalize their own exercises, may support the interests of a diverse student population. Unfortunately, it is time-consuming for instructors to create large numbers of programming exercises that provide a wide range of contextualized problems. However, recent work has shown that large language models may be able to automate the mass production of programming exercises, reducing the burden on instructors. In this research, we explore the potential of OpenAI's GPT-4 to create high-quality and novel programming exercises that implement various contexts. Finally, through prompt engineering, we compare different prompting strategies used to generate many programming exercises with various contextualized problem descriptions and then evaluate the quality of the exercises generated.",
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "289–295",
    numpages = "7",
    keywords = "chatgpt, cs1, gpt-4, large language models, novice programmers, openai, programming exercises, prompt engineering",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{shen2024implications,
    author = "Shen, Y. and Ai, X. and Soosai Raj, A. Gerald and Leo John, R. Jeffrey and Syamkumar, M.",
    title = "Implications of ChatGPT for Data Science Education",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/shen2024implications",
    doi = "shen2024implications",
    abstract = "ChatGPT is a conversational AI platform that can produce code to solve problems when provided with a natural language prompt. Prior work on similar AI models has shown that they perform well on typical intro-level Computer Science problems. However, little is known about the performance of such tools on Data Science (DS) problems. In this work, we assess the performance of ChatGPT on assignments from three DS courses with varying difficulty levels. First, we apply the raw assignment prompts provided to the students and find that ChatGPT performs well on assignments with dataset(s) descriptions and progressive question prompts, which divide the programming requirements into sub-problems. Then, we perform prompt engineering on the assignments for which ChatGPT had low performance. We find that the following prompt engineering techniques significantly increased ChatGPT's performance: breaking down abstract questions into steps, breaking down steps into multiple prompts, providing descriptions of the dataset(s), including algorithmic details, adding specific instructions to entice specific actions, and removing extraneous information. Finally, we discuss how our findings suggest potential changes to curriculum design of DS courses.",
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "1230–1236",
    numpages = "7",
    keywords = "data science education, large language models, prompt engineering",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{jordan2024need,
    author = "Jordan, M. and Ly, K. and Soosai Raj, A. Gerald",
    title = "Need a Programming Exercise Generated in Your Native Language? ChatGPT's Got Your Back: Automatic Generation of Non-English Programming Exercises Using OpenAI GPT-3.5",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/jordan2024need",
    doi = "jordan2024need",
    abstract = "Large language models (LLMs) like ChatGPT are changing computing education and may create additional barriers to those already faced by non-native English speakers (NNES) learning computing. We investigate an opportunity for a positive impact of LLMs on NNES through multilingual programming exercise generation. Following previous work with LLM exercise generation in English, we prompt OpenAI GPT-3.5 in 4 natural languages (English, Tamil, Spanish, and Vietnamese) to create introductory programming problems, sample solutions, and test cases. We evaluate these problems on their sensibility, readability, translation, sample solution accuracy, topicality, and cultural relevance. We find that problems generated in English, Spanish, and Vietnamese are largely sensible, easily understood, and accurate in their sample solutions. However, Tamil problems are mostly non-sensible and have a much lower passing test rate, indicating that the abilities of LLMs for problem generation are not generalizable across languages. Our analysis suggests that these problems could not be given verbatim to students, but with minimal effort, most errors can be fixed. We further discuss the benefits of these problems despite their flaws, and their opportunities to provide personalized and culturally relevant resources for students in their native languages.",
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "618–624",
    numpages = "7",
    keywords = "introductory programming, large language models, non-native english speakers, problem generation",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{denny2024prompt,
    author = "Denny, P. and Leinonen, J. and Prather, J. and Luxton-Reilly, A. and Amarouche, T. and Becker, B. A. and Reeves, B. N.",
    title = "Prompt Problems: A New Programming Exercise for the Generative AI Era",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/denny2024prompt",
    doi = "denny2024prompt",
    abstract = "Large language models (LLMs) are revolutionizing the field of computing education with their powerful code-generating capabilities. Traditional pedagogical practices have focused on code writing tasks, but there is now a shift in importance towards reading, comprehending and evaluating LLM-generated code. Alongside this shift, an important new skill is emerging -- the ability to solve programming tasks by constructing good prompts for code-generating models. In this work we introduce a new type of programming exercise to hone this nascent skill: 'Prompt Problems'. Prompt Problems are designed to help students learn how to write effective prompts for AI code generators. A student solves a Prompt Problem by crafting a natural language prompt which, when provided as input to an LLM, outputs code that successfully solves a specified programming task. We also present a new web-based tool called Promptly which hosts a repository of Prompt Problems and supports the automated evaluation of prompt-generated code. We deploy Promptly in one CS1 and one CS2 course and describe our experiences, which include student perceptions of this new type of activity and their interactions with the tool. We find that students are enthusiastic about Prompt Problems, and appreciate how the problems engage their computational thinking skills and expose them to new programming constructs. We discuss ideas for the future development of new variations of Prompt Problems, and the need to carefully study their integration into classroom practice.",
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "296–302",
    numpages = "7",
    keywords = "ai code generation, artificial intelligence, generative ai, large language models, llms, prompt engineering, prompt problems",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{kirova2024software,
    author = "Kirova, V. D. and Ku, C. S. and Laracy, J. R. and Marlowe, T. J.",
    title = "Software Engineering Education Must Adapt and Evolve for an LLM Environment",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/kirova2024software",
    doi = "kirova2024software",
    abstract = "In the era of artificial intelligence (AI), generative AI, and Large Language Models (LLMs) in particular, have become increasingly significant in various sectors. LLMs such as GPT expand their applications, from content creation to advanced code completion. They offer unmatched opportunities but pose unique challenges to the software engineering domain. This paper discusses the necessity and urgency for software engineering education to adapt and evolve to prepare software engineers for the emerging LLM environment. While existing literature and social media have investigated AI's integration into various educational spheres, there is a conspicuous gap in examining the specifics of LLMs' implications for software engineering education. We explore the goals of software engineering education, and changes to software engineering, software engineering education, course pedagogy, and ethics. We argue that a holistic approach is needed, combining technical skills, ethical awareness, and adaptable learning strategies. This paper seeks to contribute to the ongoing conversation about the future of software engineering education, emphasizing the importance of adapting and evolving to remain in sync with rapid advancements in AI and LLMs. It is hoped that this exploration will provide valuable insights for educators, curriculum developers, and policymakers in software engineering.",
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "666–672",
    numpages = "7",
    keywords = "chatgpt, generative ai, large language models (llms), responsible ai, software engineering, software engineering education, software engineering ethics, software ethics",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{grover2024teaching,
    author = "Grover, S.",
    title = "Teaching AI to K-12 Learners: Lessons, Issues, and Guidance",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/10.1145/3626252.3630937",
    doi = "10.1145/3626252.3630937",
    abstract = {There is growing recognition of the need to teach artificial intelli- gence (AI) and machine learning (ML) at the school level. This push acknowledges the meteoric growth in the range and diversity of ap- plications of ML in all industries and everyday consumer products, with Large Language Models (LLMs) being only the latest and most compelling example yet. Efforts to bring AI, especially ML educa- tion to school learners are being propelled by substantial industry interest, research efforts, as well as technological developments that make sophisticated ML tools readily available to learners of all ages. These early efforts span a variety of learning goals captured by the AI4K12 "big ideas" framework and employ a plurality of pedagogies.This paper provides a sense for the current state of the field, shares lessons learned from early K-12 AI education as well as CS education efforts that can be leveraged, highlights issues that must be addressed in designing for teaching AI in K-12, and provides guidance for future K-12 AI education efforts and tackle what to many feels like "the next new thing".},
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "422–428",
    numpages = "7",
    keywords = "artificial intelligence, k-12 ai education, k-12 cs education, machine learning",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{liu2024teaching,
    author = "Liu, R. and Zenke, C. and Liu, C. and Holmes, A. and Thornton, P. and Malan, D. J.",
    title = "Teaching CS50 with AI: Leveraging Generative Artificial Intelligence in Computer Science Education",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/liu2024teaching",
    doi = "liu2024teaching",
    abstract = {In Summer 2023, we developed and integrated a suite of AI-based software tools into CS50 at Harvard University. These tools were initially available to approximately 70 summer students, then to thousands of students online, and finally to several hundred on campus during Fall 2023. Per the course's own policy, we encouraged students to use these course-specific tools and limited the use of commercial AI software such as ChatGPT, GitHub Copilot, and the new Bing. Our goal was to approximate a 1:1 teacher-to-student ratio through software, thereby equipping students with a pedagogically-minded subject-matter expert by their side at all times, designed to guide students toward solutions rather than offer them outright. The tools were received positively by students, who noted that they felt like they had "a personal tutor.'' Our findings suggest that integrating AI thoughtfully into educational settings enhances the learning experience by providing continuous, customized support and enabling human educators to address more complex pedagogical issues. In this paper, we detail how AI tools have augmented teaching and learning in CS50, specifically in explaining code snippets, improving code style, and accurately responding to curricular and administrative queries on the course's discussion forum. Additionally, we present our methodological approach, implementation details, and guidance for those considering using these tools or AI generally in education.},
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "750–756",
    numpages = "7",
    keywords = "ai, artificial intelligence, generative ai, large language models, llms",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{cambaz2024use,
    author = "Cambaz, D. and Zhang, X.",
    title = "Use of AI-driven Code Generation Models in Teaching and Learning Programming: a Systematic Literature Review",
    year = "2024",
    isbn = "9798400704239",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/cambaz2024use",
    doi = "cambaz2024use",
    abstract = "The recent emergence of LLM-based code generation models can potentially transform programming education. To pinpoint the current state of research on using LLM-based code generators to support the teaching and learning of programming, we conducted a systematic literature review of 21 papers published since 2018. The review focuses on (1) the teaching and learning practices in programming education that utilized LLM-based code generation models, (2) characteristics and (3) performance indicators of the models, and (4) aspects to consider when utilizing the models in programming education, including the risks and challenges. We found that the most commonly reported uses of LLM-based code generation models for teachers are generating assignments and evaluating student work, while for students, the models function as virtual tutors. We identified that the models exhibit accuracy limitations; generated content often contains minor errors that are manageable by instructors but pose risks for novice learners. Moreover, risks such as academic misconduct and over-reliance on the models are critical when considering integrating these models into education. Overall, LLM-based code generation models can be an assistive tool for both learners and instructors if the risks are mitigated.",
    booktitle = "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
    pages = "172–178",
    numpages = "7",
    keywords = "artificial intelligence in education, code generation models, large language models, programming education, systematic review",
    location = "Portland, OR, USA",
    series = "SIGCSE 2024"
}


@inproceedings{balse2023evaluating,
    author = "Balse, R. and Kumar, V. and Prasad, P. and Warriem, J. Madathil",
    title = "Evaluating the Quality of LLM-Generated Explanations for Logical Errors in CS1 Student Programs",
    year = "2023",
    isbn = "9798400708404",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/balse2023evaluating",
    doi = "balse2023evaluating",
    abstract = "When students in CS1 (Introductory Programming) write erroneous code, course staff can use automated tools to provide various types of helpful feedback. In this paper, we focus on syntactically correct student code containing logical errors. Tools that explain logical errors typically require course staff to invest greater effort than tools that detect such errors. To reduce this effort, prior work has investigated the use of Large Language Models (LLMs) such as GPT-3 to generate explanations. Unfortunately, these explanations can be incomplete or incorrect, and therefore unhelpful if presented to students directly. Nevertheless, LLM-generated explanations may be of adequate quality for Teaching Assistants (TAs) to efficiently craft helpful explanations on their basis. We evaluate the quality of explanations generated by an LLM (GPT-3.5-turbo) in two ways, for 30\&nbsp;buggy student solutions across 6\&nbsp;code-writing problems. First, in a study with 5\&nbsp;undergraduate TAs, we compare TA perception of LLM-generated and peer-generated explanation quality. TAs were unaware which explanations were LLM-generated, but they found them to be comparable in quality to peer-generated explanations. Second, we performed a detailed manual analysis of LLM-generated explanations for all 30\&nbsp;buggy solutions. We found at least one incorrect statement in 15/30 explanations (50\%). However, in 28/30 cases (93\%), the LLM-generated explanation correctly identified at least one logical error. Our results suggest that for large CS1 courses, TAs with adequate training to detect erroneous statements may be able to extract value from such explanations.",
    booktitle = "Proceedings of the 16th Annual ACM India Compute Conference",
    pages = "49–54",
    numpages = "6",
    keywords = "Explanation, GPT-3.5-Turbo, Large language models (LLMs), Logical Errors, Python Programming",
    location = "Hyderabad, India",
    series = "COMPUTE '23"
}


@inproceedings{venkatesh2023evaluating,
    author = "Venkatesh, V. and Venkatesh, V. and Kumar, V.",
    title = "Evaluating Copilot on CS1 Code Writing Problems with Suppressed Specifications",
    booktitle = "Proceedings of the 16th Annual ACM India Compute Conference",
    pages = "104--107",
    year = "2023"
}


@inproceedings{kazemitabaar2024novices,
    author = "Kazemitabaar, M. and Hou, X. and Henley, A. and Ericson, B. Jane and Weintrop, D. and Grossman, T.",
    title = "How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment",
    year = "2024",
    isbn = "9798400716539",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/10.1145/3631802.3631806",
    doi = "10.1145/3631802.3631806",
    abstract = "As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners’ utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners’ use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.",
    booktitle = "Proceedings of the 23rd Koli Calling International Conference on Computing Education Research",
    articleno = "3",
    numpages = "12",
    keywords = "ChatGPT, Copilot, Introductory Programming, Large Language Models, OpenAI Codex, Self-paced Learning, Self-regulation",
    location = "Koli, Finland",
    series = "Koli Calling '23"
}


@inproceedings{liffiton2024codehelp,
    author = "Liffiton, M. and Sheese, B. E and Savelka, J. and Denny, P.",
    title = "CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes",
    year = "2024",
    isbn = "9798400716539",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/liffiton2024codehelp",
    doi = "liffiton2024codehelp",
    abstract = "Computing educators face significant challenges in providing timely support to students, especially in large class settings. Large language models (LLMs) have emerged recently and show great promise for providing on-demand help at a large scale, but there are concerns that students may over-rely on the outputs produced by these models. In this paper, we introduce CodeHelp, a novel LLM-powered tool designed with guardrails to provide on-demand assistance to programming students without directly revealing solutions. We detail the design of the tool, which incorporates a number of useful features for instructors, and elaborate on the pipeline of prompting strategies we use to ensure generated outputs are suitable for students. To evaluate CodeHelp, we deployed it in a first-year computer and data science course with 52 students and collected student interactions over a 12-week period. We examine students’ usage patterns and perceptions of the tool, and we report reflections from the course instructor and a series of recommendations for classroom use. Our findings suggest that CodeHelp is well-received by students who especially value its availability and help with resolving errors, and that for instructors it is easy to deploy and complements, rather than replaces, the support that they provide to students.",
    booktitle = "Proceedings of the 23rd Koli Calling International Conference on Computing Education Research",
    articleno = "8",
    numpages = "11",
    keywords = "Guardrails, Intelligent programming tutors, Intelligent tutoring systems, Large language models, Natural language interfaces, Novice programmers, Programming assistance",
    location = "Koli, Finland",
    series = "Koli Calling '23"
}


@article{padiyath2024insights,
    author = "Padiyath, A. and Hou, X. and Pang, A. and Vargas, D. Viramontes and Gu, X. and Nelson-Fromm, T. and Wu, Z. and Guzdial, M. and Ericson, B.",
    title = "Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course",
    journal = "arXiv preprint arXiv:2406.06451",
    year = "2024"
}


@inproceedings{petrovska2024incorporating,
    author = "Petrovska, O. and Clift, L. and Moller, F. and Pearsall, R.",
    title = "Incorporating Generative AI into Software Development Education",
    year = "2024",
    isbn = "9798400709326",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/petrovska2024incorporating",
    doi = "petrovska2024incorporating",
    abstract = "This paper explores how Generative AI can be incorporated into software development education. We present examples of formative and summative assessments, which explore various aspects of ChatGPT, including its coding capabilities, its ability to construct arguments as well as ethical issues of using ChatGPT and similar tools in education and the workplace. Our work is inspired by the insights from surveys that show that the learners on our Degree Apprenticeship Programme have a great interest in learning about and exploiting emerging AI technology. Similarly, our industrial partners have a clear interest for their employees to be formally prepared to use GenAI in their software engineering roles. In this vein, it is proposed that embedding the use of GenAI tools in a careful and creative way - by developing assessments which encourage learners to critically evaluate AI output - can be beneficial in helping learners understand the subject material being taught without the risk of the AI tools “doing the homework”.",
    booktitle = "Proceedings of the 8th Conference on Computing Education Practice",
    pages = "37–40",
    numpages = "4",
    keywords = "apprenticeship, assessment, education, generative AI, software engineering",
    location = "Durham, United Kingdom",
    series = "CEP '24"
}


@inproceedings{cowan2024enhancing,
    author = "Cowan, B. and Watanobe, Y. and Shirafuji, A.",
    title = "Enhancing Programming Learning with LLMs: Prompt Engineering and Flipped Interaction",
    year = "2024",
    isbn = "9798400708534",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/cowan2024enhancing",
    doi = "cowan2024enhancing",
    abstract = "Due to their robustness, large language models (LLMs) are being utilized in many fields of study, including programming and education. Notably, they can be used by programmers by interfacing with their IDEs to assist with development, and in education by giving students meaningful and immediate feedback. In this paper, we propose and explore the groundwork of a framework designed to combine these two applications of LLMs. The framework acts as a facilitator between the LLM and the student by reading the student’s prompts before filtering and modifying them and sending them to the LLM. The intent is that this will improve the responses from the LLM, thereby improving the student’s learning experience. We discuss the framework in detail and analyze the value of individual responses returned from the LLM as a result of our framework. We conclude that the framework causes the LLM to give helpful responses in comparison to how it would respond without the framework.",
    booktitle = "Proceedings of the 2023 4th Asia Service Sciences and Software Engineering Conference",
    pages = "10–16",
    numpages = "7",
    keywords = "ChatGPT, educational technology, large language models, programming education, prompt engineering",
    location = "Aizu-Wakamatsu City, Japan",
    series = "ASSE '23"
}


@inproceedings{hou2024effects,
    author = "Hou, I. and Mettille, S. and Man, O. and Li, Z. and Zastudil, C. and MacNeil, S.",
    title = "The Effects of Generative AI on Computing Students’ Help-Seeking Preferences",
    year = "2024",
    isbn = "9798400716195",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/hou2024effects",
    doi = "hou2024effects",
    abstract = "Help-seeking is a critical way that students learn new concepts, acquire new skills, and get unstuck when problem-solving in their computing courses. The recent proliferation of generative AI tools, such as ChatGPT, offers students a new source of help that is always available on-demand. However, it is unclear how this new resource compares to existing help-seeking resources along dimensions of perceived quality, latency, and trustworthiness. In this paper, we investigate the help-seeking preferences and experiences of computing students now that generative AI tools are available to them. We collected survey data (n=47) and conducted interviews (n=8) with computing students. Our results suggest that although these models are being rapidly adopted, they have not yet fully eclipsed traditional help resources. The help-seeking resources that students rely on continue to vary depending on the task and other factors. Finally, we observed preliminary evidence about how help-seeking with generative AI is a skill that needs to be developed, with disproportionate benefits for those who are better able to harness the capabilities of LLMs. We discuss potential implications for integrating generative AI into computing classrooms and the future of help-seeking in the era of generative AI.",
    booktitle = "Proceedings of the 26th Australasian Computing Education Conference",
    pages = "39–48",
    numpages = "10",
    keywords = "ChatGPT, Generative AI, computing education, help-seeking",
    location = "Sydney, NSW, Australia",
    series = "ACE '24"
}


@inproceedings{sheese2024patterns,
    author = "Sheese, B. and Liffiton, M. and Savelka, J. and Denny, P.",
    series = "ACE 2024",
    title = "Patterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant",
    url = "http://dx.doi.org/10.1145/3636243.3636249",
    DOI = "10.1145/3636243.3636249",
    booktitle = "Proceedings of the 26th Australasian Computing Education Conference",
    publisher = "ACM",
    year = "2024",
    month = "January",
    collection = "ACE 2024"
}


@inproceedings{jury2024evaluating,
    author = "Jury, B. and Lorusso, A. and Leinonen, J. and Denny, P. and Luxton-Reilly, A.",
    title = "Evaluating LLM-generated Worked Examples in an Introductory Programming Course",
    year = "2024",
    isbn = "9798400716195",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/jury2024evaluating",
    doi = "jury2024evaluating",
    abstract = "Worked examples, which illustrate the process for solving a problem step-by-step, are a well-established pedagogical technique that has been widely studied in computing classrooms. However, creating high-quality worked examples is very time-intensive for educators, and thus learners tend not to have access to a broad range of such examples. The recent emergence of powerful large language models (LLMs), which appear capable of generating high-quality human-like content, may offer a solution. Separate strands of recent work have shown that LLMs can accurately generate code suitable for a novice audience, and that they can generate high-quality explanations of code. Therefore, LLMs may be well suited to creating a broad range of worked examples, overcoming the bottleneck of manual effort that is currently required. In this work, we present a novel tool, ‘WorkedGen’, which uses an LLM to generate interactive worked examples. We evaluate this tool with both an expert assessment of the content, and a user study involving students in a first-year Python programming course (n = \textasciitilde 400). We find that prompt chaining and one-shot learning are useful strategies for optimising the output of an LLM when producing worked examples. Our expert analysis suggests that LLMs generate clear explanations, and our classroom deployment revealed that students find the LLM-generated worked examples useful for their learning. We propose several avenues for future work, including investigating WorkedGen’s value in a range of programming languages, and with more complex questions suitable for more advanced courses.",
    booktitle = "Proceedings of the 26th Australasian Computing Education Conference",
    pages = "77–86",
    numpages = "10",
    keywords = "CS1, GPT-3.5, LLM, chat-GPT, computing education, large language models, worked examples",
    location = "Sydney, NSW, Australia",
    series = "ACE '24"
}


@inproceedings{doughty2024comparative,
    author = "Doughty, J. and Wan, Z. and Bompelli, A. and Qayum, J. and Wang, T. and Zhang, J. and Zheng, Y. and Doyle, A. and Sridhar, P. and Agarwal, A. and Bogart, C. and Keylor, E. and Kultur, C. and Savelka, J. and Sakr, M.",
    series = "ACE 2024",
    title = "A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education",
    url = "http://dx.doi.org/10.1145/3636243.3636256",
    DOI = "10.1145/3636243.3636256",
    booktitle = "Proceedings of the 26th Australasian Computing Education Conference",
    publisher = "ACM",
    year = "2024",
    month = "January",
    collection = "ACE 2024"
}


@inproceedings{budhiraja2024its,
    author = "Budhiraja, R. and Joshi, I. and Challa, J. Sesh and Akolekar, H. D. and Kumar, D.",
    series = "ACE 2024",
    title = "“It’s not like Jarvis, but it’s pretty close!” - Examining ChatGPT’s Usage among Undergraduate Students in Computer Science",
    url = "http://dx.doi.org/10.1145/3636243.3636257",
    DOI = "10.1145/3636243.3636257",
    booktitle = "Proceedings of the 26th Australasian Computing Education Conference",
    publisher = "ACM",
    year = "2024",
    month = "January",
    collection = "ACE 2024"
}


@inproceedings{roest2024nextstep,
    author = "Roest, L. and Keuning, H. and Jeuring, J.",
    title = "Next-Step Hint Generation for Introductory Programming Using Large Language Models",
    year = "2024",
    isbn = "9798400716195",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/roest2024nextstep",
    doi = "roest2024nextstep",
    abstract = "Large Language Models possess skills such as answering questions, writing essays or solving programming exercises. Since these models are easily accessible, researchers have investigated their capabilities and risks for programming education. This work explores how LLMs can contribute to programming education by supporting students with automated next-step hints. We investigate prompt practices that lead to effective next-step hints and use these insights to build our StAP-tutor. We evaluate this tutor by conducting an experiment with students, and performing expert assessments. Our findings show that most LLM-generated feedback messages describe one specific next step and are personalised to the student’s code and approach. However, the hints may contain misleading information and lack sufficient detail when students approach the end of the assignment. This work demonstrates the potential for LLM-generated feedback, but further research is required to explore its practical implementation.",
    booktitle = "Proceedings of the 26th Australasian Computing Education Conference",
    pages = "144–153",
    numpages = "10",
    keywords = "Generative AI, Large Language Models, Next-step hints, automated feedback, learning programming",
    location = "Sydney, NSW, Australia",
    series = "ACE '24"
}


@inproceedings{feng2024more,
    author = "Feng, T. Haoran and Denny, P. and Wuensche, B. and Luxton-Reilly, A. and Hooper, S.",
    title = "More Than Meets the AI: Evaluating the performance of GPT-4 on Computer Graphics assessment questions",
    year = "2024",
    isbn = "9798400716195",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/feng2024more",
    doi = "feng2024more",
    abstract = "Recent studies have showcased the exceptional performance of LLMs (Large Language Models) on assessment questions across various discipline areas. This can be helpful if used to support the learning process, for example by enabling students to quickly generate and contrast alternative solution approaches. However, concerns about student over-reliance and inappropriate use of LLMs in education are common. Understanding the capabilities of LLMs is essential for instructors to make informed decisions on question choices for learning and assessment tasks. In CS (Computer Science), previous evaluations of LLMs have focused on CS1 and CS2 questions, and little is known about how well LLMs perform for assessment questions in upper-level CS courses such as CG (Computer Graphics), which covers a wide variety of concepts and question types. To address this gap, we compiled a dataset of past assessment questions used in a final-year undergraduate course about introductory CG, and evaluated the performance of GPT-4 on this dataset. We also classified assessment questions and evaluated the performance of GPT-4 for different types of questions. We found that the performance tended to be best for simple mathematical questions, and worst for questions requiring creative thinking, and those with complex descriptions and/or images. We share our benchmark dataset with the community and provide new insights into the capabilities of GPT-4 in the context of CG courses. We highlight opportunities for teaching staff to improve student learning by guiding the use of LLMs for CG questions, and inform decisions around question choices for assessment tasks.",
    booktitle = "Proceedings of the 26th Australasian Computing Education Conference",
    pages = "182–191",
    numpages = "10",
    keywords = "Artificial Intelligence, Assessment, Computer Graphics, Computing Education, Evaluation, GPT-4, Large Language Models",
    location = "Sydney, NSW, Australia",
    series = "ACE '24"
}


@inproceedings{freire2024may,
    author = "Freire, A. Pimenta and Cardoso, P. Christina Figueira and Salgado, A. de Lima",
    title = "May We Consult ChatGPT in Our Human-Computer Interaction Written Exam? An Experience Report After a Professor Answered Yes",
    year = "2024",
    isbn = "9798400717154",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/freire2024may",
    doi = "freire2024may",
    abstract = "Using ChatGPT in education presents challenges for evaluating students. It requires distinguishing between original ideas and those generated by the model, assessing critical thinking skills, and gauging subject mastery accurately, which can impact fair assessment practices. The Human-Computer Interaction course described in this experience report has enabled consultation with textbooks, slides and other materials for over five years. This experience report describes reflections regarding using ChatGPT as a source of consultation in a written HCI exam in 2023. The paper describes experiences with analysis of the types of questions ChatGPT was able to solve immediately without mediation and the types of questions that could benefit from ChatGPT’s assistance without compromising the assessment of higher-level learning outcomes that professors want to analyse in teaching HCI. The paper uses Bloom’s taxonomy to analyse different questions and abilities to be evaluated and how they can be solved solely by using ChatGPT. The paper discusses questions that need mediation, previous lived experience in class and understanding of the knowledge acquired in class that cannot be answered directly by copying and pasting questions into ChatGPT. The discussions can raise reflections on the learning outcomes that can be assessed in HCI written exams and how professors should reflect upon their experiences and expectations for exams in the age of growing generative artificial intelligence resources.",
    booktitle = "Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems",
    articleno = "6",
    numpages = "11",
    keywords = "ChatGPT, HCI education, evaluation, open-book exams",
    location = "Macei\'{o}, Brazil",
    series = "IHC '23"
}


@inproceedings{cipriano2024llms,
    author = "Cipriano, B. Pereira and Alves, P.",
    series = "ICSE-SEET ’24",
    title = "LLMs Still Can’t Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4 and Bard’s Capacity to Handle Object-Oriented Programming Assignments",
    url = "http://dx.doi.org/10.1145/3639474.3640052",
    DOI = "10.1145/3639474.3640052",
    booktitle = "Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training",
    publisher = "ACM",
    year = "2024",
    month = "April",
    collection = "ICSE-SEET ’24"
}


@inproceedings{frankford2024aitutoring,
    author = "Frankford, E. and Sauerwein, C. and Bassner, P. and Krusche, S. and Breu, R.",
    series = "ICSE-SEET ’24",
    title = "AI-Tutoring in Software Engineering Education",
    url = "http://dx.doi.org/10.1145/3639474.3640061",
    DOI = "10.1145/3639474.3640061",
    booktitle = "Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training",
    publisher = "ACM",
    year = "2024",
    month = "April",
    collection = "ICSE-SEET ’24"
}


@article{denny2024desirable,
    author = "Denny, P. and MacNeil, S. and Savelka, J. and Porter, L. and Luxton-Reilly, A.",
    title = "Desirable Characteristics for AI Teaching Assistants in Programming Education",
    journal = "arXiv preprint arXiv:2405.14178",
    year = "2024"
}


@article{lyu2024evaluating,
    author = "Lyu, W. and Wang, Y. and Chung, T. Rachel and Sun, Y. and Zhang, Y.",
    title = "Evaluating the Effectiveness of LLMs in Introductory Computer Science Education: A Semester-Long Field Study",
    journal = "arXiv preprint arXiv:2404.13414",
    year = "2024"
}


@inproceedings{mezzaro2024empirical,
    author = "Mezzaro, S. and Gambi, A. and Fraser, G.",
    title = "An Empirical Study on How Large Language Models Impact Software Testing Learning",
    year = "2024",
    isbn = "9798400717017",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/mezzaro2024empirical",
    doi = "mezzaro2024empirical",
    abstract = "Software testing is a challenging topic in software engineering education and requires creative approaches to engage learners. For example, the Code Defenders game has students compete over a Java class under test by writing effective tests and mutants. While such gamified approaches deal with problems of motivation and engagement, students may nevertheless require help to put testing concepts into practice. The recent widespread diffusion of Generative AI and Large Language Models raises the question of whether and how these disruptive technologies could address this problem, for example, by providing explanations of unclear topics and guidance for writing tests. However, such technologies might also be misused or produce inaccurate answers, which would negatively impact learning. To shed more light on this situation, we conducted the first empirical study investigating how students learn and practice new software testing concepts in the context of the Code Defenders testing game, supported by a smart assistant based on a widely known, commercial Large Language Model. Our study shows that students had unrealistic expectations about the smart assistant, “blindly” trusting any output it generated, and often trying to use it to obtain solutions for testing exercises directly. Consequently, students who resorted to the smart assistant more often were less effective and efficient than those who did not. For instance, they wrote 8.6\% fewer tests, and their tests were not useful in 78.0\% of the cases. We conclude that giving unrestricted and unguided access to Large Language Models might generally impair learning. Thus, we believe our study helps to raise awareness about the implications of using Generative AI and Large Language Models in Computer Science Education and provides guidance towards developing better and smarter learning tools.",
    booktitle = "Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering",
    pages = "555–564",
    numpages = "10",
    keywords = "ChatGPT, Computer Science Education, Generative AI, Smart Learning Assistant",
    location = "Salerno, Italy",
    series = "EASE '24"
}


@inproceedings{prakash2024integrating,
    author = "Prakash, K. and Rao, S. and Hamza, R. and Lukich, J. and Chaudhari, V. and Nandi, A.",
    title = "Integrating LLMs into Database Systems Education",
    year = "2024",
    isbn = "9798400706783",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/prakash2024integrating",
    doi = "prakash2024integrating",
    abstract = "Large Language Models (LLMs) have sparked a drastic improvement in the ways computers can understand, process, and generate language. As LLM-based offerings become mainstream, we explore the incorporation of such LLMs into introductory or undergraduate database systems education. Students and instructors are both faced with the calculator dilemma: while the use of LLM-based tools may “solve” tasks such as assignments and exams, do they impede or accelerate the learning itself? We review deficiencies of using existing off-the-shelf tools for learning, and further articulate the differentiated needs of database systems students as opposed to trained data practitioners. Building on our exploration, we outline a vision that integrates LLMs into database education in a principled manner, keeping pedagogical best practices in mind. If implemented correctly, we posit that LLMs can drastically amplify the impact of existing instruction, minimizing costs and barriers towards learning database systems fundamentals.",
    booktitle = "Proceedings of the 3rd International Workshop on Data Systems Education: Bridging Education Practice with Education Research",
    pages = "33–39",
    numpages = "7",
    keywords = "ChatGPT, database systems education, foundation models, intro to db, large language models, llm, undergrad databases",
    location = "Santiago, AA, Chile",
    series = "DataEd '24"
}


@article{mendoncca2024evaluating,
    author = "Mendon\c{c}a, N. C.",
    title = "Evaluating ChatGPT-4 Vision on Brazil’s National Undergraduate Computer Science Exam",
    year = "2024",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/mendoncca2024evaluating",
    doi = "mendoncca2024evaluating",
    abstract = "The recent integration of visual capabilities into Large Language Models (LLMs) has the potential to play a pivotal role in science and technology education, where visual elements such as diagrams, charts, and tables are commonly used to improve the learning experience. This study investigates the performance of ChatGPT-4 Vision, OpenAI’s most advanced visual model at the time the study was conducted, on the Bachelor in Computer Science section of Brazil’s 2021 National Undergraduate Exam (ENADE). By presenting the model with the exam’s open and multiple-choice questions in their original image format and allowing for reassessment in response to differing answer keys, we were able to evaluate the model’s reasoning and self-reflecting capabilities in a large-scale academic assessment involving textual and visual content. ChatGPT-4 Vision significantly outperformed the average exam participant, positioning itself within the top 10 best score percentile. While it excelled in questions that incorporated visual elements, it also encountered challenges with question interpretation, logical reasoning, and visual acuity. A positive correlation between the model’s performance in multiple-choice questions and the performance distribution of the human participants suggests multimodal LLMs can provide a useful tool for question testing and refinement. However, the involvement of an independent expert panel to review cases of disagreement between the model and the answer key revealed some poorly constructed questions containing vague or ambiguous statements, calling attention to the critical need for improved question design in future exams. Our findings suggest that while ChatGPT-4 Vision shows promise in multimodal academic evaluations, human oversight remains crucial for verifying the model’s accuracy and ensuring the fairness of high-stakes educational exams. The paper’s research materials are publicly available at .",
    note = "Just Accepted",
    journal = "ACM Trans. Comput. Educ.",
    month = "jun",
    keywords = "Multimodal Generative AI, ChatGPT-4 Vision, Educational Assessment, Computer Science Education"
}


@article{piccolo2023evaluating,
    author = "Piccolo, S. R. and Denny, P. and Luxton-Reilly, A. and Payne, S. H. and Ridge, P. G.",
    doi = "10.1371/journal.pcbi.1011511",
    journal = "PLOS Computational Biology",
    publisher = "Public Library of Science",
    title = "Evaluating a large language model’s ability to solve programming exercises from an introductory bioinformatics course",
    year = "2023",
    month = "09",
    volume = "19",
    url = "https://doi.org/10.1371/journal.pcbi.1011511",
    pages = "1-16",
    abstract = "Computer programming is a fundamental tool for life scientists, allowing them to carry out essential research tasks. However, despite various educational efforts, learning to write code can be a challenging endeavor for students and researchers in life-sciences disciplines. Recent advances in artificial intelligence have made it possible to translate human-language prompts to functional code, raising questions about whether these technologies can aid (or replace) life scientists’ efforts to write code. Using 184 programming exercises from an introductory-bioinformatics course, we evaluated the extent to which one such tool—OpenAI’s ChatGPT—could successfully complete programming tasks. ChatGPT solved 139 (75.5\%) of the exercises on its first attempt. For the remaining exercises, we provided natural-language feedback to the model, prompting it to try different approaches. Within 7 or fewer attempts, ChatGPT solved 179 (97.3\%) of the exercises. These findings have implications for life-sciences education and research. Instructors may need to adapt their pedagogical approaches and assessment techniques to account for these new capabilities that are available to the general public. For some programming tasks, researchers may be able to work in collaboration with machine-learning models to produce functional code.",
    number = "9"
}


@inproceedings{fan2023exploring,
    author = "Fan, A. and Zhang, H. and Paquette, L. and Zhang, R.",
    editor = "Bouamor, Houda and Pino, Juan and Bali, Kalika",
    title = "Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = "December",
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.496",
    doi = "10.18653/v1/2023.findings-emnlp.496",
    pages = "7406--7421",
    abstract = "In this paper, we explore the application of large language models (LLMs) for generating code-tracing questions in introductory programming courses. We designed targeted prompts for GPT4, guiding it to generate code-tracing questions based on code snippets and descriptions. We established a set of human evaluation metrics to assess the quality of questions produced by the model compared to those created by human experts. Our analysis provides insights into the capabilities and potential of LLMs in generating diverse code-tracing questions. Additionally, we present a unique dataset of human and LLM-generated tracing questions, serving as a valuable resource for both the education and NLP research communities. This work contributes to the ongoing dialogue on the potential uses of LLMs in educational settings."
}


@article{jost2024impact,
    author = "Jošt, G. and Taneski, V. and Karakatič, S.",
    title = "The Impact of Large Language Models on Programming Education and Student Learning Outcomes",
    volume = "14",
    ISSN = "2076-3417",
    url = "http://dx.doi.org/10.3390/app14104115",
    DOI = "10.3390/app14104115",
    number = "10",
    journal = "Applied Sciences",
    publisher = "MDPI AG",
    year = "2024",
    month = "May",
    pages = "4115"
}


@Article{dengel2023qualitative,
    AUTHOR = "Dengel, A. and Gehrlein, R. and Fernes, D. and Görlich, S. and Maurer, J. and Pham, H. Hoang and Großmann, G. and Eisermann, N. Dietrich genannt",
    TITLE = "Qualitative Research Methods for Large Language Models: Conducting Semi-Structured Interviews with ChatGPT and BARD on Computer Science Education",
    JOURNAL = "Informatics",
    VOLUME = "10",
    YEAR = "2023",
    NUMBER = "4",
    ARTICLE-NUMBER = "78",
    URL = "https://www.mdpi.com/2227-9709/10/4/78",
    ISSN = "2227-9709",
    ABSTRACT = "In the current era of artificial intelligence, large language models such as ChatGPT and BARD are being increasingly used for various applications, such as language translation, text generation, and human-like conversation. The fact that these models consist of large amounts of data, including many different opinions and perspectives, could introduce the possibility of a new qualitative research approach: Due to the probabilistic character of their answers, “interviewing” these large language models could give insights into public opinions in a way that otherwise only interviews with large groups of subjects could deliver. However, it is not yet clear if qualitative content analysis research methods can be applied to interviews with these models. Evaluating the applicability of qualitative research methods to interviews with large language models could foster our understanding of their abilities and limitations. In this paper, we examine the applicability of qualitative content analysis research methods to interviews with ChatGPT in English, ChatGPT in German, and BARD in English on the relevance of computer science in K-12 education, which was used as an exemplary topic. We found that the answers produced by these models strongly depended on the provided context, and the same model could produce heavily differing results for the same questions. From these results and the insights throughout the process, we formulated guidelines for conducting and analyzing interviews with large language models. Our findings suggest that qualitative content analysis research methods can indeed be applied to interviews with large language models, but with careful consideration of contextual factors that may affect the responses produced by these models. The guidelines we provide can aid researchers and practitioners in conducting more nuanced and insightful interviews with large language models. From an overall view of our results, we generally do not recommend using interviews with large language models for research purposes, due to their highly unpredictable results. However, we suggest using these models as exploration tools for gaining different perspectives on research topics and for testing interview guidelines before conducting real-world interviews.",
    DOI = "10.3390/informatics10040078"
}


@article{kosar2024computer,
    author = "Kosar, T. and Ostoji{\'c}, D. and Liu, Y. David and Mernik, M.",
    title = "Computer Science Education in ChatGPT Era: Experiences from an Experiment in a Programming Course for Novice Programmers",
    journal = "Mathematics",
    volume = "12",
    number = "5",
    pages = "629",
    year = "2024",
    publisher = "MDPI"
}


@article{ahmed2024potentiality,
    author = "Ahmed, Z. and Shanto, S. Sadat and Jony, A. Islam",
    title = "Potentiality of generative AI tools in higher education: Evaluating ChatGPT's viability as a teaching assistant for introductory programming courses",
    journal = "STEM Education",
    volume = "4",
    number = "3",
    pages = "165--182",
    year = "2024"
}


@article{azaiz2023aienhanced,
    author = "Azaiz, I. and Deckarm, O. and Strickroth, S.",
    title = "AI-Enhanced Auto-Correction of Programming Exercises: How Effective is GPT-3.5?",
    volume = "13",
    ISSN = "2192-4880",
    url = "http://dx.doi.org/10.3991/ijep.v13i8.45621",
    DOI = "10.3991/ijep.v13i8.45621",
    number = "8",
    journal = "International Journal of Engineering Pedagogy (iJEP)",
    publisher = "International Association of Online Engineering (IAOE)",
    year = "2023",
    month = "December",
    pages = "67–83"
}


@inproceedings{drori2023human,
    author = "Drori, I. and Zhang, S. J. and Shuttleworth, R. and Zhang, S. and Tyser, K. and Chin, Z. and Lantigua, P. and Surbehera, S. and Hunter, G. and Austin, D. and Tang, L. and Hicke, Y. and Simhon, S. and Karnik, S. and Granberry, D. and Udell, M.",
    title = "From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams",
    year = "2023",
    isbn = "9798400701030",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    url = "https://doi.org/10.1145/3580305.3599827",
    doi = "10.1145/3580305.3599827",
    abstract = "A final exam in machine learning at a top institution such as MIT, Harvard, or Cornell typically takes faculty days to write, and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level on finals available online and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work, we develop and compare methods that solve final exams, which differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark, we use automatic checkers for multiple-choice, numeric, and questions with expression answers. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments, significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class, instructors should teach students to harness them by asking students meta-questions about correctness, completeness, and originality of the responses generated, encouraging critical thinking in academic studies.",
    booktitle = "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
    pages = "3947–3955",
    numpages = "9",
    keywords = "quantitative reasoning, program synthesis, machine learning, large language models, few-shot learning",
    location = "Long Beach, CA, USA",
    series = "KDD '23"
}


@misc{babe2023studenteval,
    author = "Babe, H. McLean and Nguyen, S. and Zi, Y. and Guha, A. and Feldman, M. Q and Anderson, C. Jane",
    doi = "10.48550/ARXIV.2306.04556",
    url = "https://arxiv.org/abs/2306.04556",
    keywords = "Machine Learning (cs.LG), Human-Computer Interaction (cs.HC), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code",
    publisher = "arXiv",
    year = "2023",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{pankiewicz2023large,
    author = "Pankiewicz, M. and Baker, R. S.",
    doi = "10.48550/ARXIV.2307.00150",
    url = "https://arxiv.org/abs/2307.00150",
    keywords = "Human-Computer Interaction (cs.HC), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Large Language Models (GPT) for automating feedback on programming assignments",
    publisher = "arXiv",
    year = "2023",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@article{tu2023should,
    author = "Tu, X. and Zou, J. and Su, W. J and Zhang, L.",
    title = "What should data science education do with large language models",
    journal = "arXiv preprint arXiv:2307.02792",
    volume = "3",
    year = "2023"
}


@misc{orenstrakh2023detecting,
    author = "Orenstrakh, M. Sheinman and Karnalim, O. and Suarez, C. Anibal and Liut, M.",
    doi = "10.48550/ARXIV.2307.07411",
    url = "https://arxiv.org/abs/2307.07411",
    keywords = "Computation and Language (cs.CL), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases",
    publisher = "arXiv",
    year = "2023",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{kiesler2023large,
    author = "Kiesler, N. and Schiffner, D.",
    doi = "10.48550/ARXIV.2308.08572",
    url = "https://arxiv.org/abs/2308.08572",
    keywords = "Software Engineering (cs.SE), Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Large Language Models in Introductory Programming Education: ChatGPT's Performance and Implications for Assessments",
    publisher = "arXiv",
    year = "2023",
    copyright = "Creative Commons Attribution Non Commercial No Derivatives 4.0 International"
}


@misc{li2023evaluating,
    author = "Li, J. and Meland, P. Håkon and Notland, J. Svennevik and Storhaug, A. and Tysse, J. Hjortland",
    doi = "10.48550/ARXIV.2309.10085",
    url = "https://arxiv.org/abs/2309.10085",
    keywords = "Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Evaluating the Impact of ChatGPT on Exercises of a Software Security Course",
    publisher = "arXiv",
    year = "2023",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{savelka2023efficient,
    author = "Savelka, J. and Denny, P. and Liffiton, M. and Sheese, B.",
    doi = "10.48550/ARXIV.2310.20105",
    url = "https://arxiv.org/abs/2310.20105",
    keywords = "Computers and Society (cs.CY), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models",
    publisher = "arXiv",
    year = "2023",
    copyright = "Creative Commons Attribution 4.0 International"
}


@misc{anishka2023can,
    author = "{Anishka} and Mehta, A. and Gupta, N. and Balachandran, A. and Kumar, D. and Jalote, P.",
    doi = "10.48550/ARXIV.2312.07343",
    url = "https://arxiv.org/abs/2312.07343",
    keywords = "Human-Computer Interaction (cs.HC), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Can ChatGPT Play the Role of a Teaching Assistant in an Introductory Programming Course?",
    publisher = "arXiv",
    year = "2023",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{zhang2023students,
    author = "Zhang, Z. and Dong, Z. and Shi, Y. and Matsuda, N. and Price, T. and Xu, D.",
    doi = "10.48550/ARXIV.2312.11567",
    url = "https://arxiv.org/abs/2312.11567",
    keywords = "Human-Computer Interaction (cs.HC), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Students' Perceptions and Preferences of Generative Artificial Intelligence Feedback for Programming",
    publisher = "arXiv",
    year = "2023",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{oli2024automated,
    author = "Oli, P. and Banjade, R. and Chapagain, J. and Rus, V.",
    doi = "10.48550/ARXIV.2401.05399",
    url = "https://arxiv.org/abs/2401.05399",
    keywords = "Computers and Society (cs.CY), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Automated Assessment of Students' Code Comprehension using LLMs",
    publisher = "arXiv",
    year = "2024",
    copyright = "Creative Commons Attribution 4.0 International"
}


@misc{prather2024interactions,
    author = "Prather, J. and Denny, P. and Leinonen, J. and Smith, D. H. and Reeves, B. N. and MacNeil, S. and Becker, B. A. and Luxton-Reilly, A. and Amarouche, T. and Kimmel, B.",
    doi = "10.48550/ARXIV.2401.10759",
    url = "https://arxiv.org/abs/2401.10759",
    keywords = "Human-Computer Interaction (cs.HC), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models",
    publisher = "arXiv",
    year = "2024",
    copyright = "Creative Commons Attribution 4.0 International"
}


@misc{rasnayaka2024empirical,
    author = "Rasnayaka, S. and Wang, G. and Shariffdeen, R. and Iyer, G. Neelakanta",
    doi = "10.48550/ARXIV.2401.16186",
    url = "https://arxiv.org/abs/2401.16186",
    keywords = "Software Engineering (cs.SE), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences, D.2.3",
    title = "An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project",
    publisher = "arXiv",
    year = "2024",
    copyright = "Creative Commons Attribution 4.0 International"
}


@article{bien2024generative,
    author = "Bien, J. and Mukherjee, G.",
    title = "Generative AI for Data Science 101: Coding Without Learning To Code",
    journal = "arXiv preprint arXiv:2401.17647",
    year = "2024"
}


@misc{agarwal2024which,
    author = "Agarwal, V. and Garg, M. Krishan and Dharmavaram, S. and Kumar, D.",
    doi = "10.48550/ARXIV.2402.01687",
    url = "https://arxiv.org/abs/2402.01687",
    keywords = "Computers and Society (cs.CY), Human-Computer Interaction (cs.HC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = {"Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students},
    publisher = "arXiv",
    year = "2024",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{kumar2024using,
    author = "Kumar, N. Ashok and Lan, A.",
    doi = "10.48550/ARXIV.2402.07081",
    url = "https://arxiv.org/abs/2402.07081",
    keywords = "Computation and Language (cs.CL), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education",
    publisher = "arXiv",
    year = "2024",
    copyright = "Creative Commons Attribution 4.0 International"
}


@misc{xiao2024qacp,
    author = "Xiao, R. and Han, L. and Zhou, X. and Wang, J. and Zong, N. and Zhang, P.",
    doi = "10.48550/ARXIV.2402.07913",
    url = "https://arxiv.org/abs/2402.07913",
    keywords = "Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners",
    publisher = "arXiv",
    year = "2024",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{azaiz2024feedbackgeneration,
    author = "Azaiz, I. and Kiesler, N. and Strickroth, S.",
    doi = "10.48550/ARXIV.2403.04449",
    url = "https://arxiv.org/abs/2403.04449",
    keywords = "Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Feedback-Generation for Programming Exercises With GPT-4",
    publisher = "arXiv",
    year = "2024",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{jacobs2024evaluating,
    author = "Jacobs, S. and Jaschke, S.",
    doi = "10.48550/ARXIV.2403.09744",
    url = "https://arxiv.org/abs/2403.09744",
    keywords = "Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Evaluating the Application of Large Language Models to Generate Feedback in Programming Education",
    publisher = "arXiv",
    year = "2024",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{tanay2024exploratory,
    author = "Tanay, B. Arie and Arinze, L. and Joshi, S. S. and Davis, K. A. and Davis, J. C.",
    doi = "10.48550/ARXIV.2403.18679",
    url = "https://arxiv.org/abs/2403.18679",
    keywords = "Software Engineering (cs.SE), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "An Exploratory Study on Upper-Level Computing Students' Use of Large Language Models as Tools in a Semester-Long Project",
    publisher = "arXiv",
    year = "2024",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{raihan2024cseprompts,
    author = "Raihan, N. and Goswami, D. and Puspo, S. Sayara Chowdhury and Newman, C. and Ranasinghe, T. and Zampieri, M.",
    doi = "10.48550/ARXIV.2404.02540",
    url = "https://arxiv.org/abs/2404.02540",
    keywords = "Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "CSEPrompts: A Benchmark of Introductory Computer Science Prompts",
    publisher = "arXiv",
    year = "2024",
    copyright = "Creative Commons Attribution Non Commercial No Derivatives 4.0 International"
}


@misc{arora2024analyzing,
    author = "Arora, C. and Venaik, U. and Singh, P. and Goyal, S. and Tyagi, J. and Goel, S. and Singhal, U. and Kumar, D.",
    doi = "10.48550/ARXIV.2404.04603",
    url = "https://arxiv.org/abs/2404.04603",
    keywords = "Human-Computer Interaction (cs.HC), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Analyzing LLM Usage in an Advanced Computing Class in India",
    publisher = "arXiv",
    year = "2024",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@article{kim2024chatgpt,
    author = "Kim, N. Wook and Ko, H. and Myers, G. and Bach, B.",
    title = "ChatGPT in Data Visualization Education: A Student Perspective",
    journal = "arXiv preprint arXiv:2405.00748",
    year = "2024"
}


@misc{vierhauser2024towards,
    author = "Vierhauser, M. and Groher, I. and Antensteiner, T. and Sauerwein, C.",
    doi = "10.48550/ARXIV.2405.18062",
    url = "https://arxiv.org/abs/2405.18062",
    keywords = "Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Towards Integrating Emerging AI Applications in SE Education",
    publisher = "arXiv",
    year = "2024",
    copyright = "Creative Commons Attribution Non Commercial No Derivatives 4.0 International"
}


@misc{scholl2024analyzing,
    author = "Scholl, A. and Schiffner, D. and Kiesler, N.",
    doi = "10.48550/ARXIV.2405.19132",
    url = "https://arxiv.org/abs/2405.19132",
    keywords = "Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Analyzing Chat Protocols of Novice Programmers Solving Introductory Programming Tasks with ChatGPT",
    publisher = "arXiv",
    year = "2024",
    copyright = "Creative Commons Attribution Non Commercial Share Alike 4.0 International"
}


@article{oosterwyk2024beyond,
    author = "Oosterwyk, G. and Tsibolane, P. and Kautondokwa, P. and Canani, A.",
    doi = "10.48550/ARXIV.2406.11104",
    url = "https://arxiv.org/abs/2406.11104",
    keywords = "Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "Beyond the Hype: A Cautionary Tale of ChatGPT in the Programming Classroom",
    publisher = "arXiv",
    year = "2024",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@misc{vadaparty2024cs1llm,
    author = "Vadaparty, A. and Zingaro, D. and Smith, D. H. and Padala, M. and Alvarado, C. and Benario, J. Gorson and Porter, L.",
    doi = "10.48550/ARXIV.2406.15379",
    url = "https://arxiv.org/abs/2406.15379",
    keywords = "Computers and Society (cs.CY), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences",
    title = "CS1-LLM: Integrating LLMs into CS1 Instruction",
    publisher = "arXiv",
    year = "2024",
    copyright = "arXiv.org perpetual, non-exclusive license"
}


@conference{garg2024impact,
    author = "Garg, A. and Rajendran, R.",
    title = "The Impact of Structured Prompt-Driven Generative AI on Learning Data Analysis in Engineering Students",
    booktitle = "Proceedings of the 16th International Conference on Computer Supported Education - Volume 2: CSEDU",
    year = "2024",
    pages = "270-277",
    publisher = "SciTePress",
    organization = "INSTICC",
    doi = "10.5220/0012693000003693",
    isbn = "978-989-758-697-2",
    issn = "2184-5026"
}


@article{manley2024examining,
    author = "Manley, E. D. and Urness, T. and Migunov, A. and Reza, M. Alimoor",
    title = "Examining Student Use of AI in CS1 and CS2",
    year = "2024",
    issue_date = "April 2024",
    publisher = "Consortium for Computing Sciences in Colleges",
    address = "Evansville, IN, USA",
    volume = "39",
    number = "6",
    issn = "1937-4771",
    abstract = "The launch of ChatGPT in November 2022 marked a seismic disruption to many disciplines and industries, including higher education. For the first time, students everywhere have widely available access to a Large Language Model (LLM) capable of generating content - including solutions to programming assignments in CS1 and CS2 - that can pass as the work of a high-achieving student while making traditional plagiarism-detection obsolete. This has spurred various responses in higher education, including a shift to more in-class and unplugged assessments. At the same time, LLMs are transforming the way that many people work, including professional software developers, and students similarly might be able to use them to enhance their learning. In this paper, we report on our experiences with a permissive policy towards the use of ChatGPT and other artificial intelligence (AI) tools for assisting students with their programming assignments in CS1 and CS2 courses in the Spring 2023 semester. Students were allowed to use these tools however they wished as long as they submitted a form which included a transcript of their chat and a reflection on what they learned, if anything, through the interaction. We found that students largely approached the AI in positive ways and that they seemed to genuinely learn from the experience. We also document some things that did not go well and that remain challenges to using AI in programming courses, along with our recommendations on how these might be dealt with in the future.",
    journal = "J. Comput. Sci. Coll.",
    month = "may",
    pages = "41–51",
    numpages = "11"
}


@article{sharpe2024can,
    author = "Sharpe, J. S. and Dougherty, R. E. and Smith, S. J.",
    title = "Can ChatGPT Pass a CS1 Python Course?",
    year = "2024",
    issue_date = "April 2024",
    publisher = "Consortium for Computing Sciences in Colleges",
    address = "Evansville, IN, USA",
    volume = "39",
    number = "8",
    issn = "1937-4771",
    abstract = {In this paper we determine whether an LLM-ChatGPT in this case-can successfully complete the assignments in our CS1 course as if it were a "real" student. Our study contains a two-stage approach, involving reprompts to the LLM in the cases of either not successfully completing the assignment, or using concepts that are more advanced than are taught in our course. We find that LLMs can in fact can either perfectly solve, or almost perfectly solve, every assignment in our CS1 course.},
    journal = "J. Comput. Sci. Coll.",
    month = "may",
    pages = "128–142",
    numpages = "15"
}


@article{ta2023exgen,
    author = "TA, N. Binh Duong and NGUYEN, H. Gia Phuc and Swapna, G.",
    title = "ExGen: Ready-to-use exercise generation in introductory programming courses",
    year = "2023",
    publisher = "Asia-Pacific Society for Computers in Education"
}


