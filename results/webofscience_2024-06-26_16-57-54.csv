Publication Type,Authors,Book Authors,Book Editors,Book Group Authors,Author Full Names,Book Author Full Names,Group Authors,Article Title,Source Title,Book Series Title,Book Series Subtitle,Language,Document Type,Conference Title,Conference Date,Conference Location,Conference Sponsor,Conference Host,Author Keywords,Keywords Plus,Abstract,Addresses,Affiliations,Reprint Addresses,Email Addresses,Researcher Ids,ORCIDs,Funding Orgs,Funding Name Preferred,Funding Text,Cited References,Cited Reference Count,"Times Cited, WoS Core","Times Cited, All Databases",180 Day Usage Count,Since 2013 Usage Count,Publisher,Publisher City,Publisher Address,ISSN,eISSN,ISBN,Journal Abbreviation,Journal ISO Abbreviation,Publication Date,Publication Year,Volume,Issue,Part Number,Supplement,Special Issue,Meeting Abstract,Start Page,End Page,Article Number,DOI,DOI Link,Book DOI,Early Access Date,Number of Pages,WoS Categories,Web of Science Index,Research Areas,IDS Number,Pubmed Id,Open Access Designations,Highly Cited Status,Hot Paper Status,Date of Export,UT (Unique WOS ID),Web of Science Record
C,"Kirova, VD; Ku, CS; Laracy, JR; Marlowe, TJ",,,Assoc Computing Machinery,"Kirova, Vassilka D.; Ku, Cyril S.; Laracy, Joseph R.; Marlowe, Thomas J.",,,Software Engineering Education Must Adapt and Evolve for an LLM (Large Language Model) Environment,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"In the era of artificial intelligence (AI), generative AI, and Large Language Models (LLMs) in particular, have become increasingly significant in various sectors. LLMs such as GPT expand their applications, from content creation to advanced code completion. They offer unmatched opportunities but pose unique challenges to the software engineering domain. This paper discusses the necessity and urgency for software engineering education to adapt and evolve to prepare software engineers for the emerging LLM environment. While existing literature and social media have investigated AI's integration into various educational spheres, there is a conspicuous gap in examining the specifics of LLMs' implications for software engineering education. We explore the goals of software engineering education, and changes to software engineering, software engineering education, course pedagogy, and ethics. We argue that a holistic approach is needed, combining technical skills, ethical awareness, and adaptable learning strategies. This paper seeks to contribute to the ongoing conversation about the future of software engineering education, emphasizing the importance of adapting and evolving to remain in sync with rapid advancements in AI and LLMs. It is hoped that this exploration will provide valuable insights for educators, curriculum developers, and policymakers in software engineering.",,,,,,,,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,666,672,,10.1145/3626252.3630927,http://dx.doi.org/10.1145/3626252.3630927,,,,,,,,,,,,,WOS:001181240800098,View Full Record in Web of Science
C,"Liu, MQ; M'hiri, F",,,Assoc Computing Machinery,"Liu, Mengqi; M'hiri, Faten",,,Beyond Traditional Teaching: Large Language Models as Simulated Teaching Assistants in Computer Science,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"As the prominence of Large Language Models (LLMs) grows in various sectors, their potential in education warrants exploration. In this study, we investigate the feasibility of employing GPT-3.5 from OpenAI, as an LLM teaching assistant (TA) or a virtual TA in computer science (CS) courses. The objective is to enhance the accessibility of CS education while maintaining academic integrity by refraining from providing direct solutions to current-semester assignments. Targeting Foundations of Programming (COMP202), an undergraduate course that introduces students to programming with Python, we have developed a virtual TA using the LangChain framework, known for integrating language models with diverse data sources and environments. The virtual TA assists students with their code and clarifies complex concepts. For homework questions, it is designed to guide students with hints rather than giving out direct solutions. We assessed its performance first through a qualitative evaluation, then a survey-based comparative analysis, using a mix of questions commonly asked on the COMP202 discussion board and questions created by the authors. Our preliminary results indicate that the virtual TA outperforms human TAs on clarity and engagement, matching them on accuracy when the question is non-assignment-specific, for which human TAs still proved more reliable. These findings suggest that while virtual TAs, leveraging the capabilities of LLMs, hold great promise towards making CS education experience more accessible and engaging, their optimal use necessitates human supervision. We conclude by identifying several directions that could be explored in future implementations.",,,,,,,,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,743,749,,10.1145/3626252.3630789,http://dx.doi.org/10.1145/3626252.3630789,,,,,,,,,,,,,WOS:001181240800109,View Full Record in Web of Science
C,"Wang, TJ; Díaz, DV; Brown, C; Chen, Y",,,IEEE,"Wang, Tianjia; Diaz, Daniel Vargas; Brown, Chris; Chen, Yan",,,"Exploring the Role of AI Assistants in Computer Science Education: Methods, Implications, and Instructor Perspectives","2023 IEEE SYMPOSIUM ON VISUAL LANGUAGES AND HUMAN-CENTRIC COMPUTING, VL/HCC",Symposium on Visual Languages and Human Centric Computing VL HCC,,,,IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC),"OCT 02-06, 2023","Martin Luther King Jr Memorial Lib, Washington, DC","IEEE,IEEE Comp Soc",Martin Luther King Jr Memorial Lib,,,"The use of AI assistants, along with the challenges they present, has sparked significant debate within the community of computer science education. While these tools demonstrate the potential to support students' learning and instructors' teaching, they also raise concerns about enabling unethical uses by students. Previous research has suggested various strategies aimed at addressing these issues. However, they concentrate on introductory programming courses and focus on one specific type of problem. The present research evaluated the performance of ChatGPT, a state-of-the-art AI assistant, at solving 187 problems spanning three distinct types that were collected from six undergraduate computer science. The selected courses covered different topics and targeted different program levels. We then explored methods to modify these problems to adapt them to ChatGPT's capabilities to reduce potential misuse by students. Finally, we conducted semi-structured interviews with 11 computer science instructors. The aim was to gather their opinions on our problem modification methods, understand their perspectives on the impact of AI assistants on computer science education, and learn their strategies for adapting their courses to leverage these AI capabilities for educational improvement. The results revealed issues ranging from academic fairness to long-term impact on students' mental models. From our results, we derived design implications and recommended tools to help instructors design and create future course material that could more effectively adapt to AI assistants' capabilities.",,,,,,,,,,,,,,,,,,,1943-6092,,979-8-3503-2946-9,,,,2023,,,,,,,92,102,,10.1109/VL-HCC57772.2023.00018,http://dx.doi.org/10.1109/VL-HCC57772.2023.00018,,,,,,,,,,,,,WOS:001103187300010,View Full Record in Web of Science
C,"Jury, B; Lorusso, A; Leinonen, J; Denny, P; Luxton-Reilly, A",,,ACM,"Jury, Breanna; Lorusso, Angela; Leinonen, Juho; Denny, Paul; Luxton-Reilly, Andrew",,,Evaluating LLM-generatedWorked Examples in an Introductory Programming Course,"PROCEEDINGS OF THE 26TH AUSTRALASIAN COMPUTING EDUCATION CONFERENCE, ACE 2024",,,,,26th Australasian Computing Education Conference (ACE),"JAN 29-FEB 02, 2024","Sydney, AUSTRALIA",,,,,"Worked examples, which illustrate the process for solving a problem step-by-step, are a well-established pedagogical technique that has been widely studied in computing classrooms. However, creating high-quality worked examples is very time-intensive for educators, and thus learners tend not to have access to a broad range of such examples. The recent emergence of powerful large language models (LLMs), which appear capable of generating high-quality humanlike content, may offer a solution. Separate strands of recent work have shown that LLMs can accurately generate code suitable for a novice audience, and that they can generate high-quality explanations of code. Therefore, LLMs may be well suited to creating a broad range of worked examples, overcoming the bottleneck of manual effort that is currently required. In this work, we present a novel tool, 'WorkedGen', which uses an LLM to generate interactive worked examples. We evaluate this tool with both an expert assessment of the content, and a user study involving students in a first-year Python programming course ( n = similar to 400). We find that prompt chaining and one-shot learning are useful strategies for optimising the output of an LLM when producing worked examples. Our expert analysis suggests that LLMs generate clear explanations, and our classroom deployment revealed that students find the LLM-generated worked examples useful for their learning. We propose several avenues for future work, including investigating WorkedGen's value in a range of programming languages, and with more complex questions suitable for more advanced courses.",,,,,"Leinonen, Juho/D-2162-2018","Leinonen, Juho/0000-0001-6829-9449",,,,,,,,,,,,,,,979-8-4007-1619-5,,,,2024,,,,,,,77,86,,10.1145/3636243.3636252,http://dx.doi.org/10.1145/3636243.3636252,,,,,,,,,,,,,WOS:001166861800009,View Full Record in Web of Science
C,"Cambaz, D; Zhang, XL",,,Assoc Computing Machinery,"Cambaz, Doga; Zhang, Xiaoling",,,Use of AI-driven Code Generation Models in Teaching and Learning Programming: a Systematic Literature Review,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"The recent emergence of LLM-based code generation models can potentially transform programming education. To pinpoint the current state of research on using LLM-based code generators to support the teaching and learning of programming, we conducted a systematic literature review of 21 papers published since 2018. The review focuses on (1) the teaching and learning practices in programming education that utilized LLM-based code generation models, (2) characteristics and (3) performance indicators of the models, and (4) aspects to consider when utilizing the models in programming education, including the risks and challenges. We found that the most commonly reported uses of LLM-based code generation models for teachers are generating assignments and evaluating student work, while for students, the models function as virtual tutors. We identified that the models exhibit accuracy limitations; generated content often contains minor errors that are manageable by instructors but pose risks for novice learners. Moreover, risks such as academic misconduct and over-reliance on the models are critical when considering integrating these models into education. Overall, LLM-based code generation models can be an assistive tool for both learners and instructors if the risks are mitigated.",,,,,,"Zhang, Xiaoling/0000-0003-0951-0771",,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,172,178,,10.1145/3626252.3630958,http://dx.doi.org/10.1145/3626252.3630958,,,,,,,,,,,,,WOS:001181240800027,View Full Record in Web of Science
C,"Sheese, B; Liffiton, M; Savelka, J; Denny, P",,,ACM,"Sheese, Brad; Liffiton, Mark; Savelka, Jaromir; Denny, Paul",,,Patterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant,"PROCEEDINGS OF THE 26TH AUSTRALASIAN COMPUTING EDUCATION CONFERENCE, ACE 2024",,,,,26th Australasian Computing Education Conference (ACE),"JAN 29-FEB 02, 2024","Sydney, AUSTRALIA",,,,,"Providing personalized assistance at scale is a long-standing challenge for computing educators, but a new generation of tools powered by large language models (LLMs) offers immense promise. Such tools can, in theory, provide on-demand help in large class settings and be configured with appropriate guardrails to prevent misuse and mitigate common concerns around learner over-reliance. However, the deployment of LLM-powered tools in authentic classroom settings is still rare, and very little is currently known about howstudents will use them in practice and what type of help they will seek. To address this, we examine students' use of an innovative LLMpowered tool that provides on-demand programming assistance without revealing solutions directly. We deployed the tool for 12 weeks in an introductory computer and data science course (.. = 52), collecting more than 2,500 queries submitted by students throughout the term. We manually categorized all student queries based on the type of assistance sought, and we automatically analyzed several additional query characteristics. We found that most queries requested immediate help with programming assignments, whereas fewer requests asked for help on related concepts or for deepening conceptual understanding. Furthermore, students often provided minimal information to the tool, suggesting this is an area in which targeted instruction would be beneficial. We also found that students who achieved more success in the course tended to have used the tool more frequently overall. Lessons from this research can be leveraged by programming educators and institutions who plan to augment their teaching with emerging LLM-powered tools.",,,,,,"Savelka, Jaromir/0000-0002-3674-5456",,,,,,,,,,,,,,,979-8-4007-1619-5,,,,2024,,,,,,,49,57,,10.1145/3636243.3636249,http://dx.doi.org/10.1145/3636243.3636249,,,,,,,,,,,,,WOS:001166861800006,View Full Record in Web of Science
C,"Kaleemunnisa; Scharff, C; Bathula, KM; Chen, K",,"Capozucca, A; Ebersold, S; Bruel, JM; Meyer, B",,"Kaleemunnisa; Scharff, Christelle; Bathula, Krishna Mohan; Chen, Kaiyin",,,Analyzing Scrum Team Impediments Using NLP,"FRONTIERS IN SOFTWARE ENGINEERING EDUCATION, FISEE 2023",Lecture Notes in Computer Science,,,,2nd International Workshop on Frontiers in Software Engineering Education (FISEE),"JAN 23-25, 2023","Villebrumier, FRANCE",,,,,"In this research, we focus on the impediments encountered by students in capstone projects following the Scrum methodology. Scrummeeting notes were collected in a dataset to permit Scrum roles and instructors to monitor progress and issues. We identified 9 categories of impediments in this dataset: Android, Coding Skills, Debugging, External Factors, Firebase/Database, Git/GitHub, Teamwork, Time Management, and UI/UX Design. We developed a Large Language Model (LLM) to classify these impediments. Natural Language Processing (NLP) has the potential to support software engineering processes. The novelty of this research is that it attempts to identify impediments faced by students' Scrum teams with AI and support students and instructors. The relevance of the approach was discussed with subject matter experts (SME) of the industry. The proposed model is useful in both the academic and industry settings, to identify on-the-fly areas that need attention and, if fixed, would increase team productivity.",,,,,,"Kaleemunnisa, FNU/0000-0002-9656-6775",,,,,,,,,,,,,0302-9743,1611-3349,978-3-031-48638-8; 978-3-031-48639-5,,,,2023,14387,,,,,,42,55,,10.1007/978-3-031-48639-5_4,http://dx.doi.org/10.1007/978-3-031-48639-5_4,,,,,,,,,,,,,WOS:001166822000004,View Full Record in Web of Science
J,"Jost, G; Taneski, V; Karakatic, S",,,,"Jost, Gregor; Taneski, Viktor; Karakatic, Saso",,,The Impact of Large Language Models on Programming Education and Student Learning Outcomes,APPLIED SCIENCES-BASEL,,,,,,,,,,,,"Recent advancements in Large Language Models (LLMs) like ChatGPT and Copilot have led to their integration into various educational domains, including software development education. Regular use of LLMs in the learning process is still not well-researched; thus, this paper intends to fill this gap. The paper explores the nuanced impact of informal LLM usage on undergraduate students' learning outcomes in software development education, focusing on React applications. We carefully designed an experiment involving thirty-two participants over ten weeks where we examined unrestricted but not specifically encouraged LLM use and their correlation with student performance. Our results reveal a significant negative correlation between increased LLM reliance for critical thinking-intensive tasks such as code generation and debugging and lower final grades. Furthermore, a downward trend in final grades is observed with increased average LLM use across all tasks. However, the correlation between the use of LLMs for seeking additional explanations and final grades was not as strong, indicating that LLMs may serve better as a supplementary learning tool. These findings highlight the importance of balancing LLM integration with the cultivation of independent problem-solving skills in programming education.",,,,,,"Taneski, Viktor/0000-0001-5841-9275; Karakatic, Saso/0000-0003-4441-9690",,,,,,,,,,,,,,2076-3417,,,,MAY,2024,14,10,,,,,,,4115,10.3390/app14104115,http://dx.doi.org/10.3390/app14104115,,,,,,,,,,,,,WOS:001232746700001,View Full Record in Web of Science
C,"Budhiraja, R; Joshi, I; Akolekar, H; Challa, JS; Kumar, D",,,ACM,"Budhiraja, Ritvik; Joshi, Ishika; Akolekar, Harshal; Challa, Jagat Sesh; Kumar, Dhruv",,,"It′s not like Jarvis, but it′s pretty close! - Examining ChatGPT′s Usage among Undergraduate Students in Computer Science","PROCEEDINGS OF THE 26TH AUSTRALASIAN COMPUTING EDUCATION CONFERENCE, ACE 2024",,,,,26th Australasian Computing Education Conference (ACE),"JAN 29-FEB 02, 2024","Sydney, AUSTRALIA",,,,,"have garnered significant attention in the academic community. Previous research has evaluated these LLMs for various applications such as generating programming exercises and solutions. However, these evaluations have predominantly been conducted by instructors and researchers, not considering the actual usage of LLMs by students. This study adopts a student-first approach to comprehensively understand how undergraduate computer science students utilize ChatGPT, a popular LLM, released by OpenAI. We employ a combination of student surveys and interviews to obtain valuable insights into the benefits, challenges, and suggested improvements related to ChatGPT. Our findings suggest that a majority of students (over 57%) have a convincingly positive outlook towards adopting ChatGPT as an aid in coursework-related tasks. However, our research also highlights various challenges that must be resolved for long-term acceptance of ChatGPT amongst students. The findings from this investigation have broader implications and may be applicable to other LLMs and their role in computing education.",,,,,,"Akolekar, Harshal/0000-0002-3178-2987",,,,,,,,,,,,,,,979-8-4007-1619-5,,,,2024,,,,,,,124,133,,10.1145/3636243.3636257,http://dx.doi.org/10.1145/3636243.3636257,,,,,,,,,,,,,WOS:001166861800014,View Full Record in Web of Science
C,"Fan, A; Gokkaya, B; Harman, M; Lyubarskiy, M; Sengupta, S; Yoo, S; Zhang, JM",,,IEEE,"Fan, Angela; Gokkaya, Beliz; Harman, Mark; Lyubarskiy, Mitya; Sengupta, Shubho; Yoo, Shin; Zhang, Jie M.",,,Large Language Models for Software Engineering: Survey and Open Problems,"2023 IEEE/ACM INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING: FUTURE OF SOFTWARE ENGINEERING, ICSE-FOSE",,,,,IEEE/ACM International Conference on Software Engineering - Future of Software Engineering (ICSE-FoSE),"MAY 14-20, 2023","Melbourne, AUSTRALIA","IEEE,Assoc Comp Machinery,IEEE Comp Soc",,,,"This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.",,,,,,,,,,,,,,,,,,,,,979-8-3503-2496-9,,,,2023,,,,,,,31,53,,10.1109/ICSE-FoSE59343.2023.00008,http://dx.doi.org/10.1109/ICSE-FoSE59343.2023.00008,,,,,,,,,,,,,WOS:001181095900004,View Full Record in Web of Science
C,"Huang, Q; Wan, ZY; Xing, ZC; Chen, JJ; Chen, JS; Xu, XW; Lu, QH",,,IEEE,"Huang, Qing; Wan, Zhenyu; Xing, Zhenchang; Wang, Changjing; Chen, Jieshan; Xu, Xiwei; Lu, Qinghua",,,"Let's Chat to Find the APIs: Connecting Human, LLM and Knowledge Graph through AI Chain","2023 38TH IEEE/ACM INTERNATIONAL CONFERENCE ON AUTOMATED SOFTWARE ENGINEERING, ASE",IEEE ACM International Conference on Automated Software Engineering,,,,38th IEEE/ACM International Conference on Automated Software Engineering (ASE),"SEP 11-15, 2023","Echternach, LUXEMBOURG","IEEE,Assoc Comp Machinery,IEEE Comp Soc",,,,"API recommendation methods have evolved from literal and semantic keyword matching to query expansion and query clarification. The latest query clarification method is knowledge graph (KG)-based, but limitations include out-of-vocabulary (OOV) failures and rigid question templates. To address these limitations, we propose a novel knowledge-guided query clarification approach for API recommendation that leverages a large language model (LLM) guided by KG. We utilize the LLM as a neural knowledge base to overcome OOV failures, generating fluent and appropriate clarification questions and options. We also leverage the structured API knowledge and entity relationships stored in the KG to filter out noise, and transfer the optimal clarification path from KG to the LLM, increasing the efficiency of the clarification process. Our approach is designed as an AI chain that consists of five steps, each handled by a separate LLM call, to improve accuracy, efficiency, and fluency for query clarification in API recommendation. We verify the usefulness of each unit in our AI chain, which all received high scores close to a perfect 5. When compared to the baselines, our approach shows a significant improvement in MRR, with a maximum increase of 63.9% higher when the query statement is covered in KG and 37.2% when it is not. Ablation experiments reveal that the guidance of knowledge in the KG and the knowledge-guided pathfinding strategy are crucial for our approach's performance, resulting in a 19.0% and 22.2% increase in MAP, respectively. Our approach demonstrates a way to bridge the gap between KG and LLM, effectively compensating for the strengths and weaknesses of both.",,,,,"Wan, Zhenyu/HNJ-3060-2023; Chen, Jieshan/AAA-5470-2022; Xu, Xiwei/AAD-6098-2020; Lu, Qinghua/AAG-3378-2021","Chen, Jieshan/0000-0002-2700-7478;",,,,,,,,,,,,,1527-1366,,979-8-3503-2996-4,,,,2023,,,,,,,471,483,,10.1109/ASE56229.2023.00075,http://dx.doi.org/10.1109/ASE56229.2023.00075,,,,,,,,,,,,,WOS:001103357200038,View Full Record in Web of Science
C,"Fernandez, AS; Cornell, KA",,,Assoc Computing Machinery,"Fernandez, Amanda S.; Cornell, Kimberly A.",,,CS1 with a Side of AI: Teaching Software Verification for Secure Code in the Era of Generative AI,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"As AI-generated code promises to become an increasingly relied upon tool for software developers, there is a temptation to call for significant changes to early computer science curricula. A move from syntax-focused topics in CS1 toward abstraction and high-level application design seems motivated by the new large language models (LLMs) recently made available. In this position paper however, we advocate for an approach more informed by the AI itself teaching early CS learners not only how to use the tools but also how to better understand them. Novice programmers leveraging AI-code-generation without proper understanding of syntax or logic can create black box code with significant security vulnerabilities. We outline methods for integrating basic AI knowledge and traditional software verification steps into CS1 along with LLMs, which will better prepare students for software development in professional settings.",,,,,"Fernandez, Amanda/AAU-6475-2020","Fernandez, Amanda/0000-0003-2397-0838; Cornell, Kimberly/0000-0001-9551-9689",,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,345,351,,10.1145/3626252.3630817,http://dx.doi.org/10.1145/3626252.3630817,,,,,,,,,,,,,WOS:001181240800052,View Full Record in Web of Science
J,"Sauvola, J; Tarkoma, S; Klemettinen, M; Riekki, J; Doermann, D",,,,"Sauvola, Jaakko; Tarkoma, Sasu; Klemettinen, Mika; Riekki, Jukka; Doermann, David",,,Future of software development with generative AI,AUTOMATED SOFTWARE ENGINEERING,,,,,,,,,,,,"Generative AI is regarded as a major disruption to software development. Platforms, repositories, clouds, and the automation of tools and processes have been proven to improve productivity, cost, and quality. Generative AI, with its rapidly expanding capabilities, is a major step forward in this field. As a new key enabling technology, it can be used for many purposes, from creative dimensions to replacing repetitive and manual tasks. The number of opportunities increases with the capabilities of large-language models (LLMs). This has raised concerns about ethics, education, regulation, intellectual property, and even criminal activities. We analyzed the potential of generative AI and LLM technologies for future software development paths. We propose four primary scenarios, model trajectories for transitions between them, and reflect against relevant software development operations. The motivation for this research is clear: the software development industry needs new tools to understand the potential, limitations, and risks of generative AI, as well as guidelines for using it.",,,,,,,,,,,,,,,,,,,0928-8910,1573-7535,,,,MAY,2024,31,1,,,,,,,26,10.1007/s10515-024-00426-z,http://dx.doi.org/10.1007/s10515-024-00426-z,,,,,,,,,,,,,WOS:001179890300001,View Full Record in Web of Science
C,"Wang, HY; Qiang, PP; Tan, HY; Hu, JC",,"Liu, Q; Wang, H; Ma, Z; Zheng, W; Zha, H; Chen, X; Wang, L; Ji, R",,"Wang, Hongyu; Qiang, Pengpeng; Tan, Hongye; Hu, Jingchang",,,Enhancing Image Comprehension for Computer Science Visual Question Answering,"PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2023, PT I",Lecture Notes in Computer Science,,,,6th Chinese Conference on Pattern Recognition and Computer Vision (PRCV),"OCT 13-15, 2023","Xiamen Univ, Xiamen, PEOPLES R CHINA","Chinese Assoc Artificial Intelligence,China Comp Federat,Chinese Assoc Automat,China Soc Image & Graph",Xiamen Univ,,,"Computer science visual question answering is a fundamental task in the intelligent education. However, current models have poor performance in this task. There are mainly two issues in these models. Firstly, they cannot accurately capture the fine-grained objects and relations in images. Secondly, these models lack the computer domain knowledge. To address the issues, we propose an Image Comprehension Enhancing Model. Specifically, it uses object detection technique to capture fine-grained features of images and utilizes Optical Character Recognition (OCR) to transform fine-grained features into the text information. The model adopts the text information to prompt the large language model, which generates the image caption with the computer domain knowledge. The fine-grained features and image caption can enhance the model's image comprehension and compensate the lack of knowledge. Additionally, the model utilizes the cross-modal attention mechanism to integrate the image features and fine-grained features of the image with the text features. The experimental results on the CSDQA dataset demonstrate that our proposed model outperforms the baselines, and the accuracy improves at least 4.80%.",,,,,,,,,,,,,,,,,,,0302-9743,1611-3349,978-981-99-8428-2; 978-981-99-8429-9,,,,2024,14425,,,,,,487,498,,10.1007/978-981-99-8429-9_39,http://dx.doi.org/10.1007/978-981-99-8429-9_39,,,,,,,,,,,,,WOS:001155015700039,View Full Record in Web of Science
C,"Neumann, M; Rauschenberger, M; Schön, EM",,,IEEE,"Neumann, Michael; Rauschenberger, Maria; Schoen, Eva-Maria",,,We Need To Talk About ChatGPT: The Future of AI and Higher Education,"2023 IEEE/ACM 5TH INTERNATIONAL WORKSHOP ON SOFTWARE ENGINEERING EDUCATION FOR THE NEXT GENERATION, SEENG",,,,,IEEE/ACM 5th International Workshop on Software Engineering Education for the Next Generation (SEENG),"MAY 16, 2023","Melbourne, AUSTRALIA","IEEE,Assoc Comp Machinery,IEEE Comp Soc",,,,"On November 30th, 2022, OpenAI released the large language model ChatGPT, an extension of GPT-3. The AI chatbot provides real-time communication in response to users' requests. The quality of ChatGPT's natural speaking answers marks a major shift in how we will use AI-generated information in our day-to-day lives. For a software engineering student, the use cases for ChatGPT are manifold: assessment preparation, translation, and creation of specified source code, to name a few. It can even handle more complex aspects of scientific writing, such as summarizing literature and paraphrasing text. Hence, this position paper addresses the need for discussion of potential approaches for integrating ChatGPT into higher education. Therefore, we focus on articles that address the effects of ChatGPT on higher education in the areas of software engineering and scientific writing. As ChatGPT was only recently released, there have been no peer-reviewed articles on the subject. Thus, we performed a structured grey literature review using Google Scholar to identify preprints of primary studies. In total, five out of 55 preprints are used for our analysis. Furthermore, we held informal discussions and talks with other lecturers and researchers and took into account the authors' test results from using ChatGPT. We present five challenges and three opportunities for the higher education context that emerge from the release of ChatGPT. The main contribution of this paper is a proposal for how to integrate ChatGPT into higher education in four main areas.",,,,,"Rauschenberger, Maria/AAL-8319-2020","Rauschenberger, Maria/0000-0001-5722-576X",,,,,,,,,,,,,,,979-8-3503-0186-1,,,,2023,,,,,,,29,32,,10.1109/SEENG59157.2023.00010,http://dx.doi.org/10.1109/SEENG59157.2023.00010,,,,,,,,,,,,,WOS:001042092500006,View Full Record in Web of Science
J,"Rodriguez-Echeverría, R; Gutiérrez, JD; Conejero, JM; Prieto, AE",,,,"Rodriguez-Echeverria, Roberto; Gutierrez, Juan D.; Conejero, Jose M.; Prieto, Alvaro E.",,,Analysis of ChatGPT Performance in Computer Engineering Exams,IEEE REVISTA IBEROAMERICANA DE TECNOLOGIAS DEL APRENDIZAJE-IEEE RITA,,,,,,,,,,,,"The appearance of ChatGPT at the end of 2022 was a milestone in the field of Generative Artificial Intelligence. However, it also caused a shock in the academic world. For the first time, a simple interface allowed anyone to access a large language model and use it to generate text. These capabilities have a relevant impact on teaching-learning methodologies and assessment methods. This work aims to obtain an objective measure of ChatGPT's possible performance in solving exams related to computer engineering. For this purpose, it has been tested with actual exams of 15 subjects of the Software Engineering branch of a Spanish university. All the questions of these exams have been extracted and adapted to a text format to obtain an answer. Furthermore, the exams have been rewritten to be corrected by the teaching staff. In light of the results, ChatGPT can achieve relevant performance in these exams; it can pass many questions and problems of different natures in multiple subjects. A detailed study of the results by typology of questions and problems is provided as a fundamental contribution, allowing recommendations to be considered in the design of assessment methods. In addition, an analysis of the impact of the non-deterministic aspect of ChatGPT on the answers to test questions is presented, and the need to use a strategy to reduce this effect for performance analysis is concluded.",,,,,"Rodriguez-Echeverria, Roberto/B-4964-2014","Rodriguez-Echeverria, Roberto/0000-0002-6545-0913",,,,,,,,,,,,,,1932-8540,,,,MAR,2024,19,1,,,,,71,80,,10.1109/RITA.2024.3381842,http://dx.doi.org/10.1109/RITA.2024.3381842,,,,,,,,,,,,,WOS:001232684500001,View Full Record in Web of Science
C,"Kuramitsu, K; Obara, Y; Sato, M; Obara, M",,"Feldman, MQ; Hilton, M",,"Kuramitsu, Kimio; Obara, Yui; Sato, Miyu; Obara, Momoka",,,KOGI: A Seamless Integration of ChatGPT into Jupyter Environments for Programming Education,"PROCEEDINGS OF THE 2023 ACM SIGPLAN INTERNATIONAL SYMPOSIUM ON SPLASH-E, SPLASH-E 2023",,,,,ACM SIGPLAN International Symposium on SPLASH-E (SPLASH-E),"OCT 25, 2023","Cascais, PORTUGAL","Assoc Comp Machinery,ACM SIGPLAN,ACM SIGAda",,,,"The impact of ChatGPT has brought both anxiety and anticipation to schools and universities. Exploring a positive method to improve programming skills with ChatGPT is a new and pressing challenge. In pursuit of this goal, we have developed KOGI, a learning support system that integrates ChatGPT into the Jupyter environment. This paper demonstrates how KOGI enables students to receive timely advice from ChatGPT in response to errors and other questions they encounter. We immediately introduced KOGI in our two introductory courses: Algorithms and Data Science. The introduction of KOGI resulted in a significant decrease in the number of unresolved student errors. In addition, we report on student trends observed in the classroom regarding the type and frequency of help requested. Although our findings are preliminary, they are informative for programming instructors interested in using ChatGPT.",,,,,,,,,,,,,,,,,,,,,979-8-4007-0390-4,,,,2023,,,,,,,50,59,,10.1145/3622780.3623648,http://dx.doi.org/10.1145/3622780.3623648,,,,,,,,,,,,,WOS:001142841700005,View Full Record in Web of Science
J,"Estévez-Ayres, I; Callejo, P; Hombrados-Herrera, MA; Alario-Hoyos, C; Kloos, CD",,,,"Estevez-Ayres, Iria; Callejo, Patricia; Hombrados-Herrera, Miguel Angel; Alario-Hoyos, Carlos; Delgado Kloos, Carlos",,,Evaluation of LLM Tools for Feedback Generation in a Course on Concurrent Programming,INTERNATIONAL JOURNAL OF ARTIFICIAL INTELLIGENCE IN EDUCATION,,,,,,,,,,,,"The emergence of Large Language Models (LLMs) has marked a significant change in education. The appearance of these LLMs and their associated chatbots has yielded several advantages for both students and educators, including their use as teaching assistants for content creation or summarisation. This paper aims to evaluate the capacity of LLMs chatbots to provide feedback on student exercises in a university programming course. The complexity of the programming topic in this study (concurrency) makes the need for feedback to students even more important. The authors conducted an assessment of exercises submitted by students. Then, ChatGPT (from OpenAI) and Bard (from Google) were employed to evaluate each exercise, looking for typical concurrency errors, such as starvation, deadlocks, or race conditions. Compared to the ground-truth evaluations performed by expert teachers, it is possible to conclude that none of these two tools can accurately assess the exercises despite the generally positive reception of LLMs within the educational sector. All attempts result in an accuracy rate of 50%, meaning that both tools have limitations in their ability to evaluate these particular exercises effectively, specifically finding typical concurrency errors.",,,,,"Alario-Hoyos, Carlos/K-3451-2014","Alario-Hoyos, Carlos/0000-0002-3082-0814",,,,,,,,,,,,,1560-4292,1560-4306,,,,2024 MAY 15,2024,,,,,,,,,,10.1007/s40593-024-00406-0,http://dx.doi.org/10.1007/s40593-024-00406-0,,MAY 2024,,,,,,,,,,,WOS:001223448800001,View Full Record in Web of Science
J,"Rahman, MM; Watanobe, Y",,,,"Rahman, Md. Mostafizer; Watanobe, Yutaka",,,"ChatGPT for Education and Research: Opportunities, Threats, and Strategies",APPLIED SCIENCES-BASEL,,,,,,,,,,,,"In recent years, the rise of advanced artificial intelligence technologies has had a profound impact on many fields, including education and research. One such technology is ChatGPT, a powerful large language model developed by OpenAI. This technology offers exciting opportunities for students and educators, including personalized feedback, increased accessibility, interactive conversations, lesson preparation, evaluation, and new ways to teach complex concepts. However, ChatGPT poses different threats to the traditional education and research system, including the possibility of cheating on online exams, human-like text generation, diminished critical thinking skills, and difficulties in evaluating information generated by ChatGPT. This study explores the potential opportunities and threats that ChatGPT poses to overall education from the perspective of students and educators. Furthermore, for programming learning, we explore how ChatGPT helps students improve their programming skills. To demonstrate this, we conducted different coding-related experiments with ChatGPT, including code generation from problem descriptions, pseudocode generation of algorithms from texts, and code correction. The generated codes are validated with an online judge system to evaluate their accuracy. In addition, we conducted several surveys with students and teachers to find out how ChatGPT supports programming learning and teaching. Finally, we present the survey results and analysis.",,,,,"Rahman, Md. Mostafizer/HHN-3547-2022","Rahman, Md. Mostafizer/0000-0001-9368-7638; Watanobe, Yutaka/0000-0002-0030-3859",,,,,,,,,,,,,,2076-3417,,,,MAY 8,2023,13,9,,,,,,,5783,10.3390/app13095783,http://dx.doi.org/10.3390/app13095783,,,,,,,,,,,,,WOS:000986657600001,View Full Record in Web of Science
C,"Balse, R; Valaboju, B; Singhal, S; Warriem, JM; Prasad, P",,,ACM,"Balse, Rishabh; Valaboju, Bharath; Singhal, Shreya; Warriem, Jayakrishnan Madathil; Prasad, Prajish",,,Investigating the Potential of GPT-3 in Providing Feedback for Programming Assessments,"PROCEEDINGS OF THE 2023 CONFERENCE ON INNOVATION AND TECHNOLOGY IN COMPUTER SCIENCE EDUCATION, ITICSE 2023, VOL 1",,,,,28th Annual Conference on Innovation and Technology in Computer Science Education (ITiCSE),"JUL 08-12, 2023","Univ Turku, Turku, FINLAND","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ,ACM Europe Council,Informat Europe",Univ Turku,,,"Recent advances in artificial intelligence have led to the development of large language models (LLMs), which are able to generate text, images, and source code based on prompts provided by humans. In this paper, we explore the capabilities of an LLM - OpenAI's GPT3 model to provide feedback for student written code. Specifically, we examine the feasibility of GPT-3 to check, critique and suggest changes to code written by learners in an online programming exam of an undergraduate Python programming course. We collected 1211 student code submissions from 7 questions asked in a programming exam, and provided the GPT-3 model with separate prompts to check, critique and provide suggestions on these submissions. We found that there was a high variability in the accuracy of the model's feedback for student submissions. Across questions, the range for accurately checking the correctness of the code was between 57% to 79%, between 41% to 77% for accurately critiquing code, and between 32% and 93% for suggesting appropriate changes to the code. We also found instances where the model generated incorrect and inconsistent feedback. These findings suggest that models like GPT-3 currently cannot be 'directly' used to provide feedback to students for programming assessments.",,,,,,"Valaboju, Bharath/0009-0001-0070-5823; Madathil Warriem, Jayakrishnan/0000-0002-4266-0467; Prasad, Prajish/0000-0001-7986-6277; , Shreya/0009-0002-5974-8841; Balse, Rishabh/0009-0004-4754-7430",,,,,,,,,,,,,,,979-8-4007-0138-2,,,,2023,,,,,,,292,298,,10.1145/3587102.3588852,http://dx.doi.org/10.1145/3587102.3588852,,,,,,,,,,,,,WOS:001051691300044,View Full Record in Web of Science
C,"Nguyen, H; Allan, V",,,Assoc Computing Machinery,"Ha Nguyen; Allan, Vicki",,,"Using GPT-4 to Provide Tiered, Formative Code Feedback","PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"Large language models (LLMs) have shown promise in generating sensible code explanation and feedback in programming exercises. In this experience report, we discuss the process of using one of these models (OpenAI's GPT-4) to generate individualized feedback for students' Java code and pseudocode. We instructed GPT-4 to generate feedback for 113 submissions to four programming problems in an Algorithms and Data Structures class. We prompted the model with example feedback (few-shot learning) and instruction to (1) give feedback on conceptual understanding, syntax, and time complexity, and (2) suggest follow-up actions based on students' code or provide guiding questions. Overall, GPT-4 provided accurate feedback and successfully built on students' ideas in most submissions. Human evaluators (computer science instructors and tutors) rated GPT-4's hints as useful in guiding students' next steps. Model performance varied with programming problems but not submission quality. We reflect on where the model performed well and fell short, and discuss the potential of integrating LLM-generated, individualized feedback into computer science instruction.",,,,,,,,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,958,964,,10.1145/3626252.3630960,http://dx.doi.org/10.1145/3626252.3630960,,,,,,,,,,,,,WOS:001181240800140,View Full Record in Web of Science
J,"Ellis, ME; Casey, KM; Hill, G",,,,"Ellis, Michael E.; Casey, K. Mike; Hill, Geoffrey",,,ChatGPT and Python programming homework,DECISION SCIENCES-JOURNAL OF INNOVATIVE EDUCATION,,,,,,,,,,,,"Large Language Model (LLM) artificial intelligence tools present a unique challenge for educators who teach programming languages. While LLMs like ChatGPT have been well documented for their ability to complete exams and create prose, there is a noticeable lack of research into their ability to solve problems using high-level programming languages. Like many other university educators, those teaching programming courses would like to detect if students submit assignments generated by an LLM. To investigate grade performance and the likelihood of instructors identifying code generated by artificial intelligence (AI) tools, we compare code generated by students and ChatGPT for introductory Python homework assignments. Our research reveals mixed results on both counts, with ChatGPT performing like a mid-range student on assignments and seasoned instructors struggling to detect AI-generated code. This indicates that although AI-generated results may not always be identifiable, they do not currently yield results approaching those of diligent students. We describe our methodology for selecting and evaluating the code examples, the results of our comparison, and the implications for future classes. We conclude with recommendations for how instructors of programming courses can mitigate student use of LLM tools as well as articulate the inherent value of preserving students' individual creativity in producing programming languages.",,,,,,"Ellis, Michael/0000-0001-8682-3873",,,,,,,,,,,,,1540-4595,1540-4609,,,,APR,2024,22,2,,,,,74,87,,10.1111/dsji.12306,http://dx.doi.org/10.1111/dsji.12306,,JAN 2024,,,,,,,,,,,WOS:001144549700001,View Full Record in Web of Science
C,"Balse, R; Kumar, V; Prasad, P; Warriem, JM",,"Babu, C; Goel, N; Karkare, A",,"Balse, Rishabh; Kumar, Viraj; Prasad, Prajish; Warriem, Jayakrishnan Madathil",,,Evaluating the Quality of LLM-Generated Explanations for Logical Errors in CS1 Student Programs,"PROCEEDINGS OF THE 16TH ANNUAL ACM INDIA COMPUTE CONFERENCE, COMPUTE 2023",,,,,16th Annual ACM India Compute Conference (COMPUTE),"DEC 09-11, 2023","Univ Hyderabad, Hyderabad, INDIA","Indian Inst Technol Madras, BSc Degree,Virtual Labs,Persistent,NPTEL,ACM India I SIGCSE,ACM In Cooperat",Univ Hyderabad,,,"When students in CS1 (Introductory Programming) write erroneous code, course staff can use automated tools to provide various types of helpful feedback. In this paper, we focus on syntactically correct student code containing logical errors. Tools that explain logical errors typically require course staff to invest greater effort than tools that detect such errors. To reduce this effort, prior work has investigated the use of Large Language Models (LLMs) such as GPT3 to generate explanations. Unfortunately, these explanations can be incomplete or incorrect, and therefore unhelpful if presented to students directly. Nevertheless, LLM-generated explanations may be of adequate quality for Teaching Assistants (TAs) to efficiently craft helpful explanations on their basis. We evaluate the quality of explanations generated by an LLM (GPT-3.5-turbo) in two ways, for 30 buggy student solutions across 6 code-writing problems. First, in a study with 5 undergraduate TAs, we compare TA perception of LLM-generated and peer-generated explanation quality. TAs were unaware which explanations were LLM-generated, but they found them to be comparable in quality to peer-generated explanations. Second, we performed a detailed manual analysis of LLM-generated explanations for all 30 buggy solutions. We found at least one incorrect statement in 15/30 explanations (50%). However, in 28/30 cases (93%), the LLM-generated explanation correctly identified at least one logical error. Our results suggest that for large CS1 courses, TAs with adequate training to detect erroneous statements may be able to extract value from such explanations.",,,,,,"Madathil Warriem, Jayakrishnan/0000-0002-4266-0467; Prasad, Prajish/0000-0001-7986-6277",,,,,,,,,,,,,,,979-8-4007-0840-4,,,,2023,,,,,,,49,54,,10.1145/3627217.3627233,http://dx.doi.org/10.1145/3627217.3627233,,,,,,,,,,,,,WOS:001176678700013,View Full Record in Web of Science
C,"Denny, P; Becker, BA; Leinonen, J; Prather, J",,,ACM,"Denny, Paul; Becker, Brett A.; Leinonen, Juho; Prather, James",,,Chat Overflow: Artificially Intelligent Models for Computing Education - renAIssance or apocAIypse?,"PROCEEDINGS OF THE 2023 CONFERENCE ON INNOVATION AND TECHNOLOGY IN COMPUTER SCIENCE EDUCATION, ITICSE 2023, VOL 1",,,,,28th Annual Conference on Innovation and Technology in Computer Science Education (ITiCSE),"JUL 08-12, 2023","Univ Turku, Turku, FINLAND","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ,ACM Europe Council,Informat Europe",Univ Turku,,,"Recent breakthroughs in deep learning have led to the emergence of generative AI models that exhibit extraordinary performance at producing human-like outputs. Using only simple input prompts, it is possible to generate novel text, images, video, music, and source code, as well as tackle tasks such as answering questions and translating and summarising text. However, the potential for these models to impact computing education practice is only just beginning to be explored. For example, novices learning to code can now use free tools that automatically suggest solutions to programming exercises and assignments; yet these tools were not designed with novices in mind and little to nothing is known about how they will impact learning. Furthermore, much attention has focused on the immediate challenges these models present, such as academic integrity concerns. It seems that even in the AI-era a pending apocalypse sells better than a promising renaissance. Generative AI will likely play an increasing role in people's lives in the reasonably foreseeable future. Model performance seems set to continue accelerating while novel uses and new possibilities multiply. Given this, we should devote just as much effort to identifying and exploiting new opportunities as we do to identifying and mitigating challenges. In this talk, we begin by discussing several concrete and researchbacked opportunities for computing educators. Many of these have already shown great promise in positively impacting current practice. We then discuss more short- to medium-term possibilities in areas such as student recruitment, and curricular changes. Finally - against our better judgement - we speculate over the longerterm, including rethinking the very fundamentals of the practice of teaching introductory and advanced computing courses. In these *Randomly ordered by the spin of a roulette wheel, the results of which were eventually confirmed as valid, reliable and replicable by ChatGPT Plus on the fourth attempt (GPT4 March 23, 2023 version). No other artificial intelligence was used in the authoring of this document. discussions we suggest potential research questions and directions. Although making remotely accurate predictions in such a fastchanging landscape is foolhardy, we believe that now is the time to explore and embrace opportunities to help make positive change in as many computing classrooms as possible.",,,,,"Leinonen, Juho/D-2162-2018","Leinonen, Juho/0000-0001-6829-9449; Denny, Paul/0000-0002-5150-9806; Prather, James/0000-0003-2807-6042",,,,,,,,,,,,,,,979-8-4007-0138-2,,,,2023,,,,,,,3,4,,10.1145/3587102.3588773,http://dx.doi.org/10.1145/3587102.3588773,,,,,,,,,,,,,WOS:001051691300002,View Full Record in Web of Science
J,"Kwon, S; Lee, S; Kim, T; Ryu, D; Baik, J",,,,"Kwon, Sunjae; Lee, Sungu; Kim, Taehyoun; Ryu, Duksan; Baik, Jongmoon",,,Exploring LLM-based Automated Repairing of Ansible Script in Edge-Cloud Infrastructures,JOURNAL OF WEB ENGINEERING,,,,,3rd International workshop on Big Data-Driven Edge Cloud Services (BECS),"JUN 06-09, 2023","Alicante, SPAIN",,,,,"Edge-Cloud system requires massive infrastructures located in closer to the user to minimize latencies in handling Big data. Ansible is one of the most popular Infrastructure as Code (IaC) tools crucial for deploying these infrastructures of the Edge-cloud system. However, Ansible also consists of code, and its code quality is critical in ensuring the delivery of high-quality services within the Edge-Cloud system. On the other hand, the Large Langue Model (LLM) has performed remarkably on various Software Engineering (SE) tasks in recent years. One such task is Automated Program Repairing (APR), where LLMs assist developers in proposing code fixes for identified bugs. Nevertheless, prior studies in LLM-based APR have predominantly concentrated on widely used programming languages (PL), such as Java and C, and there has yet to be an attempt to apply it to Ansible. Hence, we explore the applicability of LLM-based APR on Ansible. We assess LLMs' performance (ChatGPT and Bard) on 58 Ansible script revision cases from Open Source Software (OSS). Our findings reveal promising prospects, with LLMs generating helpful responses in 70% of the sampled cases. Nonetheless, further research is necessary to harness this approach's potential fully.",,,,,,,,,,,,,,,,,,,1540-9589,1544-5976,,,,,2023,22,6,,,,,889,912,,10.13052/jwe1540-9589.2263,http://dx.doi.org/10.13052/jwe1540-9589.2263,,,,,,,,,,,,,WOS:001167543200004,View Full Record in Web of Science
C,"Becker, BA; Denny, P; Finnie-Ansley, J; Luxton-Reilly, A; Prather, J; Santos, EA",,,ACM,"Becker, Brett A.; Denny, Paul; Finnie-Ansley, James; Luxton-Reilly, Andrew; Prather, James; Santos, Eddie Antonio",,,Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation,"PROCEEDINGS OF THE 54TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, VOL 1, SIGCSE 2023",,,,,54th Annual ACM SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS),"MAR 15-18, 2023","Toronto, CANADA","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"The introductory programming sequence has been the focus of much research in computing education. The recent advent of several viable and freely-available AI-driven code generation tools present several immediate opportunities and challenges in this domain. In this position paper we argue that the community needs to act quickly in deciding what possible opportunities can and should be leveraged and how, while also working on overcoming otherwise mitigating the possible challenges. Assuming that the effectiveness and proliferation of these tools will continue to progress rapidly, without quick, deliberate, and concerted efforts, educators will lose advantage in helping shape what opportunities come to be, and what challenges will endure. With this paper we aim to seed this discussion within the computing education community.",,,,,"Luxton-Reilly, Andrew/ABC-5342-2021","Luxton-Reilly, Andrew/0000-0001-8269-2909; Denny, Paul/0000-0002-5150-9806; Finnie-Ansley, James/0000-0002-4279-6284",,,,,,,,,,,,,,,978-1-4503-9431-4,,,,2023,,,,,,,500,506,,10.1145/3545945.3569759,http://dx.doi.org/10.1145/3545945.3569759,,,,,,,,,,,,,WOS:001117817800074,View Full Record in Web of Science
J,"Haindl, P; Weinberger, G",,,,"Haindl, Philipp; Weinberger, Gerald",,,Students’ Experiences of Using ChatGPT in an Undergraduate Programming Course,IEEE ACCESS,,,,,,,,,,,,"Increasing use of artificial intelligence tools in programming education calls for a deeper understanding of their effect on students' learning. This paper presents a study that investigates the experiences of part-time undergraduate students using ChatGPT in a five-week Java programming course. After each exercise, students provided feedback via anonymous surveys in which they rated different suitability aspects of ChatGPT. The majority viewed ChatGPT positively and suitable for learning programming concepts. However, its suitability for specific implementation tasks received mixed reviews. Students found it easy to adapt ChatGPT's generated code to the exercises' implementation tasks. The students primarily used it for acquiring background knowledge, learning syntax and programming concepts and suggesting suitable algorithms. Yet, some abstained from using it due to concerns to not garner sufficient programming proficiency, retrieving partially incorrect or misleading generated code, preferring an independent working style, or general skepticism about its benefits. Finally, in response to our findings, we also discuss three perspective directions for improving the suitability of LLM chatbots for students in programming education.",,,,,,"Haindl, Philipp/0000-0001-6075-5286",,,,,,,,,,,,,2169-3536,,,,,,2024,12,,,,,,43519,43529,,10.1109/ACCESS.2024.3380909,http://dx.doi.org/10.1109/ACCESS.2024.3380909,,,,,,,,,,,,,WOS:001193742900001,View Full Record in Web of Science
C,"Roest, L; Keuning, H; Jeuring, J",,,ACM,"Roest, Lianne; Keuning, Hieke; Jeuring, Johan",,,Next-Step Hint Generation for Introductory Programming Using Large Language Models,"PROCEEDINGS OF THE 26TH AUSTRALASIAN COMPUTING EDUCATION CONFERENCE, ACE 2024",,,,,26th Australasian Computing Education Conference (ACE),"JAN 29-FEB 02, 2024","Sydney, AUSTRALIA",,,,,"Large Language Models possess skills such as answering questions, writing essays or solving programming exercises. Since these models are easily accessible, researchers have investigated their capabilities and risks for programming education. This work explores how LLMs can contribute to programming education by supporting students with automated next-step hints. We investigate prompt practices that lead to effective next-step hints and use these insights to build our StAP-tutor. We evaluate this tutor by conducting an experiment with students, and performing expert assessments. Our findings show that most LLM-generated feedback messages describe one specific next step and are personalised to the student ' s code and approach. However, the hints may contain misleading information and lack sufficient detail when students approach the end of the assignment. This work demonstrates the potential for LLM-generated feedback, but further research is required to explore its practical implementation.",,,,,,"Keuning, Hieke/0000-0001-5778-7519",,,,,,,,,,,,,,,979-8-4007-1619-5,,,,2024,,,,,,,144,153,,10.1145/3636243.3636259,http://dx.doi.org/10.1145/3636243.3636259,,,,,,,,,,,,,WOS:001166861800016,View Full Record in Web of Science
C,"Doughty, J; Wan, ZPA; Bompelli, A; Qayum, J; Wang, TZ; Zhang, JR; Zheng, YJ; Doyle, A; Sridhar, P; Agarwal, A; Bogart, C; Keylor, E; Kultur, C; Savelka, J; Sakr, M",,,ACM,"Doughty, Jacob; Wan, Zipiao; Bompelli, Anishka; Qayum, Jubahed; Wang, Taozhi; Zhang, Juran; Zheng, Yujia; Doyle, Aidan; Sridhar, Pragnya; Agarwal, Arav; Bogart, Christopher; Keylor, Eric; Kultur, Can; Savelka, Jaromir; Sakr, Majd",,,A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education,"PROCEEDINGS OF THE 26TH AUSTRALASIAN COMPUTING EDUCATION CONFERENCE, ACE 2024",,,,,26th Australasian Computing Education Conference (ACE),"JAN 29-FEB 02, 2024","Sydney, AUSTRALIA",,,,,"There is a constant need for educators to develop and maintain effective up-to-date assessments. While there is a growing body of research in computing education on utilizing large language models (LLMs) in generation and engagement with coding exercises, the use of LLMs for generating programming MCQs has not been extensively explored. We analyzed the capability of GPT-4 to produce multiple-choice questions (MCQs) aligned with specific learning objectives (LOs) from Python programming classes in higher education. Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs from high-level course context and module-level LOs. We evaluated 651 LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python courses. We found that GPT-4 was capable of producing MCQs with clear language, a single correct choice, and high-quality distractors. We also observed that the generated MCQs appeared to be wellaligned with the LOs. Our findings can be leveraged by educators wishing to take advantage of the state-of-the-art generative models to support MCQ authoring efforts.",,,,,,"Bogart, Christopher/0000-0001-8581-115X; Savelka, Jaromir/0000-0002-3674-5456",,,,,,,,,,,,,,,979-8-4007-1619-5,,,,2024,,,,,,,114,123,,10.1145/3636243.3636256,http://dx.doi.org/10.1145/3636243.3636256,,,,,,,,,,,,,WOS:001166861800013,View Full Record in Web of Science
C,"Dean, M; Bond, RR; McTear, MF; Mulvenna, MD",,,IEEE,"Dean, Max; Bond, Raymond R.; McTear, Michael F.; Mulvenna, Maurice D.",,,ChatPapers: An AI Chatbot for Interacting with Academic Research,"2023 31ST IRISH CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COGNITIVE SCIENCE, AICS",,,,,31st Irish Conference on Artificial Intelligence and Cognitive Science (AICS),"DEC 07-08, 2023","Letterkenny, IRELAND",,,,,"A growing and significant number of computer science related papers are being published; hence it is challenging to keep up with the latest research. This paper describes the development of a large language model (LLM) augmentation chatbot and user interface that provides responses to research queries in the domain of computer science. Around 200,000 computer science research papers from arXiv were embedded, resulting in similar to 11 million vectors (based on 'chunks' from the papers). Each vector is comprised of 384 numbers/dimensions. Technologies used include Langchain, a Vector Database, and Semantic Searching with document / query embeddings. The chatbot was tested using 30 sample questions that could be asked by computer science students across several topics and from different education levels (i.e., BSc, MSc and PhD level). The responses from this chatbot were compared with those from GPT-4. The responses with and without prompting were also compared. Readability metrics (Flesch-Kincaid and Coleman-Liau) were used to compare the responses from this LLM with GPT-4. Retrieval Augmented Generation Assessment (RAGAS), a novel LLM self-evaluation method was used to evaluate the system. We observed that the developed system provides more suitable responses to the user based on the readability level at which the questions were asked.",,,,,,,,,,,,,,,,,,,,,979-8-3503-6021-9,,,,2023,,,,,,,,,,10.1109/AICS60730.2023.10470521,http://dx.doi.org/10.1109/AICS60730.2023.10470521,,,,,,,,,,,,,WOS:001195949100008,View Full Record in Web of Science
C,"Dobslaw, F; Bergh, P",,,ACM,"Dobslaw, Felix; Bergh, Peter",,,Experiences with Remote Examination Formats in Light of GPT-4,"PROCEEDINGS OF THE 5TH EUROPEAN CONFERENCE ON SOFTWARE ENGINEERING EDUCATION, ECSEE 2023",,,,,5th European Conference on Software Engineering Education (ECSEE),"JUN 19-21, 2023","Seeon Monastery, GERMANY",,,,,"Sudden access to the rapidly improving large language model GPT by OpenAI forces educational institutions worldwide to revisit their exam procedures. In the pre-GPT era, we successfully applied oral and open-book home exams for two courses in the third year of our predominantly remote Software Engineering BSc program. We ask in this paper whether our current open-book exams are still viable or whether a move back to a legally compliant but less scalable oral exam is the only workable alternative. We further compare work-effort estimates between oral and open-book exams and report on differences in throughput and grade distribution over eight years to better understand the impact of examination format on the outcome. Examining GPT-4 on the most recent open-book exams showed that our current Artificial Intelligence and Reactive Programming exams are not GPT v4 proof. Three potential weaknesses of GPT are outlined. We also found that grade distributions have largely been unaffected by the examination format, opening up for a move to oral examinations only if needed. Throughput was higher for open-book exam course instances (73% vs 64%), while fail rates were too (12% vs 7%), with teacher workload increasing even for smaller classes. We also report on our experience regarding effort. Oral examinations are efficient for smaller groups but come with caveats regarding intensity and stress.",,,,,,"Dobslaw, Felix/0000-0001-9372-3416",,,,,,,,,,,,,,,978-1-4503-9956-2,,,,2023,,,,,,,220,225,,10.1145/3593663.3593695,http://dx.doi.org/10.1145/3593663.3593695,,,,,,,,,,,,,WOS:001124146500029,View Full Record in Web of Science
C,"Jordan, M; Ly, K; Raj, AGS",,,Assoc Computing Machinery,"Jordan, Mollie; Ly, Kevin; Raj, Adalbert Gerald Soosai",,,Need a Programming Exercise Generated in Your Native Language? ChatGPT's Got Your Back: Automatic Generation of Non-English Programming Exercises Using OpenAI GPT-3.5,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"Large language models (LLMs) like ChatGPT are changing computing education and may create additional barriers to those already faced by non-native English speakers (NNES) learning computing. We investigate an opportunity for a positive impact of LLMs on NNES through multilingual programming exercise generation. Following previous work with LLM exercise generation in English, we prompt OpenAI GPT-3.5 in 4 natural languages (English, Tamil, Spanish, and Vietnamese) to create introductory programming problems, sample solutions, and test cases. We evaluate these problems on their sensibility, readability, translation, sample solution accuracy, topicality, and cultural relevance. We find that problems generated in English, Spanish, and Vietnamese are largely sensible, easily understood, and accurate in their sample solutions. However, Tamil problems are mostly non-sensible and have a much lower passing test rate, indicating that the abilities of LLMs for problem generation are not generalizable across languages. Our analysis suggests that these problems could not be given verbatim to students, but with minimal effort, most errors can be fixed. We further discuss the benefits of these problems despite their flaws, and their opportunities to provide personalized and culturally relevant resources for students in their native languages.",,,,,,"Soosai Raj, Adalbert Gerald/0000-0002-6848-2208",,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,618,624,,10.1145/3626252.3630897,http://dx.doi.org/10.1145/3626252.3630897,,,,,,,,,,,,,WOS:001181240800091,View Full Record in Web of Science
C,"Taylor, A; Vassar, A; Renzella, J; Pearce, H",,,Assoc Computing Machinery,"Taylor, Andrew; Vassar, Alexandra; Renzella, Jake; Pearce, Hammond",,,dcc --help: Transforming the Role of the Compiler by Generating Context-Aware Error Explanations with Large Language Models,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"In the challenging field of introductory programming, high enrolments and failure rates drive us to explore tools and systems to enhance student outcomes, especially automated tools that scale to large cohorts. This paper presents and evaluates the dcc --help tool, an integration of a Large Language Model (LLM) into the Debugging C Compiler (DCC) to generate unique, novice-focused explanations tailored to each error. dcc --help prompts an LLM with contextual information of compile- and run-time error occurrences, including the source code, error location and standard compiler error message. The LLM is instructed to generate novice-focused, actionable error explanations and guidance, designed to help students understand and resolve problems without providing solutions. dcc --help was deployed to our CS1 and CS2 courses, with 2,565 students using the tool over 64,000 times in ten weeks. We analysed a subset of these error/explanation pairs to evaluate their properties, including conceptual correctness, relevancy, and overall quality. We found that the LLM-generated explanations were conceptually accurate in 90% of compile-time and 75% of run-time cases, but often disregarded the instruction not to provide solutions in code. Our findings, observations and reflections following deployment indicate that dcc --help provides novel opportunities for scaffolding students' introduction to programming.",,,,,,"Renzella, Jake/0000-0002-9587-1196; Pearce, Hammond/0000-0002-3488-7004; Vassar, Sasha/0000-0001-8856-2566",,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,1314,1320,,10.1145/3626252.3630822,http://dx.doi.org/10.1145/3626252.3630822,,,,,,,,,,,,,WOS:001181240800190,View Full Record in Web of Science
C,"Hoq, M; Shi, Y; Leinonen, J; Babalola, D; Lynch, C; Price, T; Akram, B",,,Assoc Computing Machinery,"Hoq, Muntasir; Shi, Yang; Leinonen, Juho; Babalola, Damilola; Lynch, Collin; Price, Thomas; Akram, Bita",,,Detecting ChatGPT-Generated Code Submissions in a CS1 Course Using Machine Learning Models,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"The emergence of publicly accessible large language models (LLMs) such as ChatGPT poses unprecedented risks of new types of plagiarism and cheating where students use LLMs to solve exercises for them. Detecting this behavior will be a necessary component in introductory computer science (CS1) courses, and educators should be well-equipped with detection tools when the need arises. However, ChatGPT generates code non-deterministically, and thus, traditional similarity detectors might not suffice to detect AI-created code. In this work, we explore the affordances of Machine Learning (ML) models for the detection task. We used an openly available dataset of student programs for CS1 assignments and had ChatGPT generate code for the same assignments, and then evaluated the performance of both traditional machine learning models and Abstract Syntax Tree-based (AST-based) deep learning models in detecting ChatGPT code from student code submissions. Our results suggest that both traditional machine learning models and AST-based deep learning models are effective in identifying ChatGPT-generated code with accuracy above 90%. Since the deployment of such models requires ML knowledge and resources that are not always accessible to instructors, we also explore the patterns detected by deep learning models that indicate possible ChatGPT code signatures, which instructors could possibly use to detect LLM-based cheating manually. We also explore whether explicitly asking ChatGPT to impersonate a novice programmer affects the code produced. We further discuss the potential applications of our proposed models for enhancing introductory computer science instruction.",,,,,"Leinonen, Juho/D-2162-2018","Leinonen, Juho/0000-0001-6829-9449; Price, Thomas/0000-0001-9375-2292; Shi, Yang/0000-0001-6486-4340",,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,526,532,,10.1145/3626252.3630826,http://dx.doi.org/10.1145/3626252.3630826,,,,,,,,,,,,,WOS:001181240800078,View Full Record in Web of Science
C,"Savelka, J; Agarwal, A; An, M; Bogart, C; Sakr, M",,,ACM,"Savelka, Jaromir; Agarwal, Arav; An, Marshall; Bogart, Chris; Sakr, Majd",,,Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses,"PROCEEDINGS OF THE 2023 ACM CONFERENCE ON INTERNATIONAL COMPUTING EDUCATION RESEARCH V.1, ICER 2023 V1",,,,,19th Annual ACM Conference on International Computing Education Research V.1 (ICER),"AUG 07-11, 2023","Chicago, IL","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"This paper studies recent developments in large language models' (LLM) abilities to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. The emergence of ChatGPT resulted in heated debates of its potential uses (e.g., exercise generation, code explanation) as well as misuses in programming classes (e.g., cheating). Recent studies show that while the technology performs surprisingly well on diverse sets of assessment instruments employed in typical programming classes the performance is usually not sufficient to pass the courses. The release of GPT-4 largely emphasized notable improvements in the capabilities related to handling assessments originally designed for human test-takers. This study is the necessary analysis in the context of this ongoing transition towards mature generative AI systems. Specifically, we report the performance of GPT-4, comparing it to the previous generations of GPT models, on three Python courses with assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Additionally, we analyze the assessments that were not handled well by GPT-4 to understand the current limitations of the model, as well as its capabilities to leverage feedback provided by an auto-grader. We found that the GPT models evolved from completely failing the typical programming class' assessments (the original GPT-3) to confidently passing the courses with no human involvement (GPT-4). While we identified certain limitations in GPT-4's handling of MCQs and coding exercises, the rate of improvement across the recent generations of GPT models strongly suggests their potential to handle almost any type of assessment widely used in higher education programming courses. These findings could be leveraged by educators and institutions to adapt the design of programming assessments as well as to fuel the necessary discussions into how programming classes should be updated to reflect the recent technological developments. This study provides evidence that programming instructors need to prepare for a world in which there is an easy-to-use widely accessible technology that can be utilized by learners to collect passing scores, with no effort whatsoever, on what today counts as viable programming knowledge and skills assessments.",,,,,,"Sakr, Majd/0000-0001-5150-8259; Bogart, Christopher/0000-0001-8581-115X; Agarwal, Arav/0000-0001-9848-1663; An, Haokang/0009-0005-5165-640X; Savelka, Jaromir/0000-0002-3674-5456",,,,,,,,,,,,,,,978-1-4503-9976-0,,,,2023,,,,,,,78,92,,10.1145/3568813.3600142,http://dx.doi.org/10.1145/3568813.3600142,,,,,,,,,,,,,WOS:001141973500006,View Full Record in Web of Science
J,"Wan, H; Luo, HZ; Li, MY; Luo, XY",,,,"Wan, Han; Luo, Hongzhen; Li, Mengying; Luo, Xiaoyan",,,Automated Program Repair for Introductory Programming Assignments,IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES,,,,,,,,,,,,"Automatic program repair (APR) tools are valuable for students to assist them with debugging tasks since program repair captures the code modification to make a buggy program pass the given test-suite. However, the process of manually generating catalogs of code modifications is intricate and time-consuming. This article proposes contextual error model repair (CEMR), an automated program repair tool for introductory programming assignments. CEMR is designed to learn program code modifications from incorrect-correct code pairs automatically. Then, it utilizes these code modifications along with CodeBERT, a generative AI, to repair students' new incorrect programs in the same programming assignment. CEMR builds on the observation that code edits performed by students in pairs of incorrect-correct code can be used as input-output examples for learning code modifications. The key idea of CEMR is to leverage the wisdom of the crowd: it uses the existing code modifications of incorrect-correct student code pairs to repair the new incorrect student attempts. We chose three of the most related APR tools, Refazer, Refactory, and AlphaRepair, as the baselines to compare against CEMR. The experimental results demonstrate that, on public and real classroom datasets, CEMR achieves higher repair rates than the baselines. Through further analysis, CEMR has demonstrated promising effectiveness in addressing semantical and logical errors while its performance in fixing syntactical errors is limited. In terms of time for repairing buggy programs, CEMR costs approximately half as much as AlphaRepair requires. We opine that CEMR not only be seen as a program repair method that achieves good results with incorrect-correct code pairs but also be further utilized to generate hints to better assist students in learning programming.",,,,,,,,,,,,,,,,,,,1939-1382,,,,,,2024,17,,,,,,1745,1760,,10.1109/TLT.2024.3403710,http://dx.doi.org/10.1109/TLT.2024.3403710,,,,,,,,,,,,,WOS:001247287100003,View Full Record in Web of Science
C,"Denny, P; Leinonen, J; Prather, J; Luxton-Reilly, A; Amarouche, T; Becker, BA; Reeves, BN",,,Assoc Computing Machinery,"Denny, Paul; Leinonen, Juho; Prather, James; Luxton-Reilly, Andrew; Amarouche, Thezyrie; Becker, Brett A.; Reeves, Brent N.",,,Prompt Problems: A New Programming Exercise for the Generative AI Era,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"Large language models (LLMs) are revolutionizing the field of computing education with their powerful code-generating capabilities. Traditional pedagogical practices have focused on code writing tasks, but there is now a shift in importance towards reading, comprehending and evaluating LLM-generated code. Alongside this shift, an important new skill is emerging - the ability to solve programming tasks by constructing good prompts for code-generating models. In this work we introduce a new type of programming exercise to hone this nascent skill: 'Prompt Problems'. Prompt Problems are designed to help students learn how to write effective prompts for AI code generators. A student solves a Prompt Problem by crafting a natural language prompt which, when provided as input to an LLM, outputs code that successfully solves a specified programming task. We also present a new web-based tool called Promptly which hosts a repository of Prompt Problems and supports the automated evaluation of prompt-generated code. We deploy Promptly in one CS1 and one CS2 course and describe our experiences, which include student perceptions of this new type of activity and their interactions with the tool. We find that students are enthusiastic about Prompt Problems, and appreciate how the problems engage their computational thinking skills and expose them to new programming constructs. We discuss ideas for the future development of new variations of Prompt Problems, and the need to carefully study their integration into classroom practice.",,,,,"Leinonen, Juho/D-2162-2018","Leinonen, Juho/0000-0001-6829-9449; Prather, James/0000-0003-2807-6042; Reeves, Brent/0000-0001-5781-1136",,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,296,302,,10.1145/3626252.3630909,http://dx.doi.org/10.1145/3626252.3630909,,,,,,,,,,,,,WOS:001181240800045,View Full Record in Web of Science
C,"Prasad, P; Sane, A",,,Assoc Computing Machinery,"Prasad, Prajish; Sane, Aamod",,,A Self-Regulated Learning Framework using Generative AI and its Application in CS Educational Intervention Design,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"Self-regulation refers to the ability to plan, monitor, control and reflect on one's problem-solving process. Prior research has shown that self-regulated learning (SRL) strategies help improve novice performance in solving programming problems. However, with the advent of LLM tools like ChatGPT, novices can generate fairly accurate code by just providing the problem prompt, and hence may forego applying essential self-regulation strategies such as planning and reflection to solve the problem. In this position paper, we discuss challenges and opportunities that generative AI technologies pose for novices' self-regulation strategies in the context of programming problem solving. We believe that the key challenge facing educators is that such technologies may hamper novices' ability to regulate their programming problem solving process. On the other hand, these technologies also open up the possibility to design new interventions that promote better SRL strategies in learners. We draw on generic and domain-specific self-regulated learning theories as the basis of our work, and propose an SRL framework that incorporates use of generative AI tools in programming problem solving. We illustrate how the proposed framework guides exploration of the design space of interventions that integrate generative AI in CS education.",,,,,,"Prasad, Prajish/0000-0001-7986-6277",,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,1070,1076,,10.1145/3626252.3630828,http://dx.doi.org/10.1145/3626252.3630828,,,,,,,,,,,,,WOS:001181240800155,View Full Record in Web of Science
C,"Pankiewicz, M; Baker, RS",,"Shih, JL; Kashihara, A; Chen, W; Ogata, H",,"Pankiewicz, Maciej; Baker, Ryan S.",,,Large Language Models (GPT) for automating feedback on programming assignments,"31ST INTERNATIONAL CONFERENCE ON COMPUTERS IN EDUCATION, ICCE 2023, VOL I",,,,,31st International Conference on Computers in Education (ICCE),"DEC 04-08, 2023","Kyoto Univ, Matsue, JAPAN","Learning & Educ Technologies Res Unit,Res Council Evidence Driven Educ,Asia Pacific Soc Comp Educ,Uchida Yoko Co Ltd,Photron Ltd",Kyoto Univ,,,"Addressing the challenge of generating personalized feedback for programming assignments is demanding due to several factors, like the complexity of code syntax or different ways to correctly solve a task. In this experimental study, we automated the process of feedback generation by employing OpenAI's GPT-3.5 model to generate personalized hints for students solving programming assignments on an automated assessment platform. Students rated the usefulness of GPT-generated hints positively. The experimental group (with GPT hints enabled) relied less on the platform's regular feedback but performed better in terms of percentage of successful submissions across consecutive attempts for tasks, where GPT hints were enabled. For tasks where the GPT feedback was made unavailable, the experimental group needed significantly less time to solve assignments. Furthermore, when GPT hints were unavailable, students in the experimental condition were initially less likely to solve the assignment correctly. This suggests potential over-reliance on GPT-generated feedback. However, students in the experimental condition were able to correct reasonably rapidly, reaching the same percentage correct after seven submission attempts. The availability of GPT hints did not significantly impact students' affective state.",,,,,"Pankiewicz, Maciej/JAN-8763-2023","Pankiewicz, Maciej/0000-0002-6945-0523",,,,,,,,,,,,,,,978-626-968-901-9,,,,2023,,,,,,,68,77,,,,,,,,,,,,,,,,WOS:001195903400010,View Full Record in Web of Science
C,"Prather, J; Denny, P; Leinonen, J; Becker, BA; Albluwi, I; Craig, M; Keuning, H; Kiesler, N; Kohn, T; Luxton-Reilly, A; MacNeil, S; Petersen, A; Pettit, R; Reeves, BN; Savelka, J",,,ACM,"Prather, James; Denny, Paul; Leinonen, Juho; Becker, Brett A.; Albluwi, Ibrahim; Craig, Michelle; Keuning, Hieke; Kiesler, Natalie; Kohn, Tobias; Luxton-Reilly, Andrew; MacNeil, Stephen; Petersen, Andrew; Pettit, Raymond; Reeves, Brent N.; Savelka, Jaromir",,,The Robots are Here: Navigating the Generative AI Revolution in Computing Education,"PROCEEDINGS OF THE 2023 WORKING GROUP REPORTS ON INNOVATION AND TECHNOLOGY IN COMPUTER SCIENCE EDUCATION, ITICSE-WGR 2023",,,,,28th Annual Conference on Working Group Reports on Innovation and Technology in Computer Science Education (ITiCSE-WGR),"JUL 07-12, 2023","Univ Turku, Turku, FINLAND","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",Univ Turku,,,"Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving. There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.",,,,,"Luxton-Reilly, Andrew/ABC-5342-2021; Savelka, Jaromir/GOK-0488-2022; Leinonen, Juho/D-2162-2018; MacNeil, Stephen/HPH-0843-2023; Craig, Michelle/HOF-9433-2023","Luxton-Reilly, Andrew/0000-0001-8269-2909; Savelka, Jaromir/0000-0002-3674-5456; Leinonen, Juho/0000-0001-6829-9449; MacNeil, Stephen/0000-0003-2781-6619; Craig, Michelle/0000-0001-8283-0072; Kiesler, Natalie/0000-0002-6843-2729; Keuning, Hieke/0000-0001-5778-7519; Kohn, Tobias/0000-0002-9251-8944; Prather, James/0000-0003-2807-6042",,,,,,,,,,,,,,,979-8-4007-0405-5,,,,2023,,,,,,,,,108,10.1145/3623762.3633499,http://dx.doi.org/10.1145/3623762.3633499,,,,,,,,,,,,,WOS:001167053500004,View Full Record in Web of Science
J,"Kim, D; Kim, T; Kim, Y; Byun, YH; Yun, TS",,,,"Kim, Daehyun; Kim, Taegu; Kim, Yejin; Byun, Yong-Hoon; Yun, Tae Sup",,,A ChatGPT-MATLAB framework for numerical modeling in geotechnical engineering applications,COMPUTERS AND GEOTECHNICS,,,,,,,,,,,,"ChatGPT has recently emerged as a representative of Large Language Models (LLMs) that have brought evolutionary changes to our society, and the effectiveness of ChatGPT in various applications has been increasingly reported. This study aimed to explore the potential of employing programming performance driven by ChatGPT responses to conversational prompts in the field of geotechnical engineering. The tested examples included the analysis of seepage flow and slope stability, and the image processing of X-ray computed tomographic image for partially saturated sand. For each case, the prompt was initially fed by a narrative explanation of the problem attributes such as geometry, initial conditions, and boundary conditions to generate the MATLAB code that was in turn executed to evaluate the correctness and functionality. Any errors and unanticipated results were further refined by additional prompts until the correct outcome was achieved. ChatGPT was able to generate the numerical code at a considerable level, demonstrating creditable awareness of the refining process, when meticulous prompts were provided based on a comprehensive understanding of given problems. While ChatGPT may not be able to replace the entire process of programming, it can help minimize sloppy syntax errors and assist in designing a basic framework for logical programming.",,,,,,"Kim, Daehyun/0000-0002-5277-0862",,,,,,,,,,,,,0266-352X,1873-7633,,,,MAY,2024,169,,,,,,,,106237,10.1016/j.compgeo.2024.106237,http://dx.doi.org/10.1016/j.compgeo.2024.106237,,MAR 2024,,,,,,,,,,,WOS:001219361200001,View Full Record in Web of Science
C,"Ta, NBD; Nguyen, HGP; Gottipati, S",,"Shih, JL; Kashihara, A; Chen, W; Ogata, H",,"Ta, Nguyen Binh Duong; Nguyen, Hua Gia Phuc; Gottipati, Swapna",,,ExGen: Ready-To-Use Exercise Generation in Introductory Programming Courses,"31ST INTERNATIONAL CONFERENCE ON COMPUTERS IN EDUCATION, ICCE 2023, VOL I",,,,,31st International Conference on Computers in Education (ICCE),"DEC 04-08, 2023","Kyoto Univ, Matsue, JAPAN","Learning & Educ Technologies Res Unit,Res Council Evidence Driven Educ,Asia Pacific Soc Comp Educ,Uchida Yoko Co Ltd,Photron Ltd",Kyoto Univ,,,"In introductory programming courses, students as novice programmers would benefit from doing frequent practices set at a difficulty level and concept suitable for their skills and knowledge. However, setting many good programming exercises for individual learners is very time-consuming for instructors. In this work, we propose an automated exercise generation system, named ExGen, which leverages recent advances in pre-trained large language models (LLMs) to automatically create customized and ready-to-use programming exercises for individual students on-demand. The system integrates seamlessly with Visual Studio Code, a popular development environment for computing students and software engineers. ExGen effectively does the following: 1) maintaining a set of seed exercises in a personalized database stored locally for each student; 2) constructing appropriate prompts from the seed exercises to be sent to a cloud-based LLM deployment for generating candidate exercises; and 3) implementing a novel combination of filtering checks to automatically select only ready-to-use exercises for a student to work on. Extensive evaluation using more than 600 Python exercises demonstrates the effectiveness of ExGen in generating customized, ready-to-use programming exercises for new computing students.",,,,,,,,,,,,,,,,,,,,,978-626-968-901-9,,,,2023,,,,,,,104,113,,,,,,,,,,,,,,,,WOS:001195903400015,View Full Record in Web of Science
C,"Laato, S; Morschheuser, B; Hamari, J; Björne, J",,"Chang, M; Chen, NS; Kuo, R; Rudolph, G; Sampson, DG; Tlili, A",,"Laato, Samuli; Morschheuser, Benedikt; Hamari, Juho; Bjorne, Jari",,,AI-assisted Learning with ChatGPT and Large Language Models: Implications for Higher Education,"2023 IEEE INTERNATIONAL CONFERENCE ON ADVANCED LEARNING TECHNOLOGIES, ICALT",IEEE International Conference on Advanced Learning Technologies,,,,23rd IEEE International Conference on Advanced Learning Technologies (ICALT),"JUL 10-13, 2023","Orem, UT","IEEE,IEEE Comp Soc,IEEE Tech Community Learning Technol",,,,"The recent progress in generative AI models, particularly large language models (LLMs), has brought about a transformation in the field of education. Conversational LLM services, such as Google's Bard and OpenAI's ChatGPT, offer students access to many abilities such as summarization and generation of text and code, and on-demand replies to questions on expert topics. In this paper, we observe ChatGPT to explore how LLM services impact learning and instruction in higher education. First, we mapped the capabilities of the system by reviewing the grey literature on ChatGPT and using the system ourselves for two months. Second, we selected a Bachelor level computer science curriculum from a Finnish university, and examined the impact of ChatGPT on the offered courses. As an outcome of this study, we highlight 13 implications for students' learning in higher education, and discuss the contemporary future of AI-assisted learning in universities and beyond.",,,,,,"Laato, Samuli/0000-0003-4285-0073; Morschheuser, Benedikt/0000-0002-7665-8971",,,,,,,,,,,,,2161-3761,,979-8-3503-0054-3,,,,2023,,,,,,,226,230,,10.1109/ICALT58122.2023.00072,http://dx.doi.org/10.1109/ICALT58122.2023.00072,,,,,,,,,,,,,WOS:001094548800066,View Full Record in Web of Science
C,"Singh, D; Nishane, I; Rajendran, R",,"Shih, JL; Kashihara, A; Chen, W; Ogata, H",,"Singh, Daevesh; Nishane, Indrayani; Rajendran, Ramkumar",,,Catalyzing Python Learning: Assessing an LLM-based Conversational Agent,"31ST INTERNATIONAL CONFERENCE ON COMPUTERS IN EDUCATION, ICCE 2023, VOL II",,,,,31st International Conference on Computers in Education (ICCE),"DEC 04-08, 2023","Kyoto Univ, Matsue, JAPAN","Learning & Educ Technologies Res Unit,Res Council Evidence Driven Educ,Asia Pacific Soc Comp Educ,Uchida Yoko Co Ltd,Photron Ltd",Kyoto Univ,,,"The rapid rise of digital learning platforms has ushered in an era of educational transformation. While these platforms offer the advantage of scalability, they often fall short in facilitating meaningful interaction, which is pivotal for effective learning. Addressing this concern, our study introduces PyGuru 2.0, an innovative online learning environment for Python programming that aligns with the ICAP framework with an advanced conversational agent. We further investigate the interactions between students and a chatbot, employing a qualitative approach to comprehensively explore the diverse ways in which students interact with the chatbot. The interaction categories encompass a wide spectrum, including code assistance, error resolution, and conceptual explanation. In future, we plan to further elaborate on this coding scheme and see its impact on students' learning outcomes.",,,,,,,,,,,,,,,,,,,,,978-626-968-902-6,,,,2023,,,,,,,932,934,,,,,,,,,,,,,,,,WOS:001191920400138,View Full Record in Web of Science
J,"Azaiz, I; Deckarm, O; Strickroth, S",,,,"Azaiz, Imen; Deckarm, Oliver; Strickroth, Sven",,,AI-Enhanced Auto-Correction of Programming Exercises: How Effective is GPT-3.5?,INTERNATIONAL JOURNAL OF ENGINEERING PEDAGOGY,,,,,,,,,,,,"Timely formative feedback is considered as one of the most important drivers for effective learning. Delivering timely and individualized feedback is particularly challenging in large classes in higher education. Recently Large Language Models such as GPT-3 became available to the public that showed promising results on various tasks such as code generation and code explanation. This paper investigates the potential of AI in providing personalized code correction and generating feedback. Based on existing student submissions of two different real-world assignments, the correctness of the AI-aided e-assessment as well as the characteristics such as fault localization, correctness of hints, and code style suggestions of the generated feedback are investigated. The results show that 73% of the submissions were correctly identified as either correct or incorrect. In 59% of these cases, GPT-3.5 also successfully generated effective and high-quality feedback. Additionally, GPT-3.5 exhibited weaknesses in its evaluation, including localization of errors that were not the actual errors, or even hallucinated errors. Implications and potential new usage scenarios are discussed.",,,,,,"Strickroth, Sven/0000-0002-9647-300X; Azaiz, Imen/0009-0005-6458-4169; Deckarm, Oliver/0009-0008-2919-4044",,,,,,,,,,,,,2192-4880,,,,,,2023,13,8,,,,,67,83,,10.3991/ijep.v13i8.45621,http://dx.doi.org/10.3991/ijep.v13i8.45621,,,,,,,,,,,,,WOS:001126694600008,View Full Record in Web of Science
C,"Sarsa, S; Denny, P; Hellas, A; Leinonen, J",,,ACM,"Sarsa, Sami; Denny, Paul; Hellas, Arto; Leinonen, Juho",,,Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models,"PROCEEDINGS OF THE 2022 ACM CONFERENCE ON INTERNATIONAL COMPUTING EDUCATION RESEARCH, ICER 2022, VOL. 1",,,,,18th Annual ACM Conference on International Computing Education Research (ICER),"AUG 07-11, 2022",ELECTR NETWORK,"Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"This article explores the natural language generation capabilities of large language models with application to the production of two types of learning resources common in programming courses. Using OpenAI Codex as the large language model, we create programming exercises (including sample solutions and test cases) and code explanations, assessing these qualitatively and quantitatively. Our results suggest that the majority of the automatically generated content is both novel and sensible, and in some cases ready to use as is. When creating exercises we find that it is remarkably easy to influence both the programming concepts and the contextual themes they contain, simply by supplying keywords as input to the model. Our analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students. We further discuss the implications of OpenAI Codex and similar tools for introductory programming education and highlight future research streams that have the potential to improve the quality of the educational experience for both teachers and students alike.",,,,,"Leinonen, Juho/D-2162-2018","Leinonen, Juho/0000-0001-6829-9449",,,,,,,,,,,,,,,978-1-4503-9194-8,,,,2023,,,,,,,27,43,,,,,,,,,,,,,,,,WOS:001143762100004,View Full Record in Web of Science
C,"Prather, J; Denny, P; Leinonen, J; Becker, BA; Albluwi, I; Caspersen, ME; Craig, M; Keuning, H; Kiesler, N; Kohn, T; Luxton-Reilly, A; MacNeil, S; Petersen, A; Pettit, R; Reeves, BN; Savelka, J",,,ACM,"Prather, James; Denny, Paul; Leinonen, Juho; Becker, Brett A.; Albluwi, Ibrahim; Caspersen, Michael E.; Craig, Michelle; Keuning, Hieke; Kiesler, Natalie; Kohn, Tobias; Luxton-Reilly, Andrew; MacNeil, Stephen; Petersen, Andrew; Pettit, Raymond; Reeves, Brent N.; Savelka, Jaromir",,,Transformed by Transformers: Navigating the AI Coding Revolution for Computing Education An ITiCSE Working Group Conducted by Humans,"PROCEEDINGS OF THE 2023 CONFERENCE ON INNOVATION AND TECHNOLOGY IN COMPUTER SCIENCE EDUCATION, ITICSE 2023, VOL. 2",,,,,28th Annual Conference on Innovation and Technology in Computer Science Education (ITiCSE),"JUL 08-12, 2023","Univ Turku, Turku, FINLAND","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ,ACM Europe Council,Informat Europe",Univ Turku,,,"The recent advent of highly accurate and scalable large language models (LLMs) has taken the world by storm. From art to essays to computer code, LLMs are producing novel content that until recently was thought only humans could produce. Recent work in computing education has sought to understand the capabilities of LLMs for solving tasks such as writing code, explaining code, creating novel coding assignments, interpreting programming error messages, and more. However, these technologies continue to evolve at an astonishing rate leaving educators little time to adapt. This working group seeks to document the state-of-the-art for code generation LLMs, detail current opportunities and challenges related to their use, and present actionable approaches to integrating them into computing curricula.",,,,,"MacNeil, Stephen/HPH-0843-2023; Craig, Michelle/HOF-9433-2023; Leinonen, Juho/D-2162-2018; Luxton-Reilly, Andrew/ABC-5342-2021","MacNeil, Stephen/0000-0003-2781-6619; Craig, Michelle/0000-0001-8283-0072; Leinonen, Juho/0000-0001-6829-9449; Luxton-Reilly, Andrew/0000-0001-8269-2909; Kohn, Tobias/0000-0002-9251-8944; Kiesler, Natalie/0000-0002-6843-2729; Denny, Paul/0000-0002-5150-9806; Prather, James/0000-0003-2807-6042; Keuning, Hieke/0000-0001-5778-7519",,,,,,,,,,,,,,,979-8-4007-0139-9,,,,2023,,,,,,,561,562,,10.1145/3587103.3594206,http://dx.doi.org/10.1145/3587103.3594206,,,,,,,,,,,,,WOS:001054840400001,View Full Record in Web of Science
J,"Neyem, A; González, LA; Mendoza, M; Alcocer, JPS; Centellas, L; Paredes, C",,,,"Neyem, Andres; Gonzalez, Luis A.; Mendoza, Marcelo; Alcocer, Juan Pablo Sandoval; Centellas, Leonardo; Paredes, Carlos",,,Toward an AI Knowledge Assistant for Context-Aware Learning Experiences in Software Capstone Project Development,IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES,,,,,,,,,,,,"Software assistants have significantly impacted software development for both practitioners and students, particularly in capstone projects. The effectiveness of these tools varies based on their knowledge sources; assistants with localized domain-specific knowledge may have limitations, while tools, such as ChatGPT, using broad datasets, might offer recommendations that do not always match the specific objectives of a capstone course. Addressing a gap in current educational technology, this article introduces an AI Knowledge Assistant specifically designed to overcome the limitations of the existing tools by enhancing the quality and relevance of large language models (LLMs). It achieves this through the innovative integration of contextual knowledge from a local lessons learned database tailored to the capstone course. We conducted a study with 150 students using the assistant during their capstone course. Integrated into the Kanban project tracking system, the assistant offered recommendations using different strategies: direct searches in the lessons learned database, direct queries to a generative pretrained transformers (GPT) model, query enrichment with lessons learned before submission to GPT and large language model meta AI (LLaMa) models, and query enhancement with Stack Overflow data before GPT processing. Survey results underscored a strong preference among students for direct LLM queries and those enriched with local repository insights, highlighting the assistant's practical value. Furthermore, our linguistic analysis conclusively demonstrated that texts generated by the LLM closely mirrored the linguistic standards and topical relevance of university course requirements. This alignment not only fosters a deeper understanding of course content but also significantly enhances the material's applicability to real-world scenarios.",,,,,"Sandoval Alcocer, Juan Pablo/CAA-0465-2022","Sandoval Alcocer, Juan Pablo/0000-0002-8335-4351",,,,,,,,,,,,,1939-1382,,,,,,2024,17,,,,,,1639,1654,,10.1109/TLT.2024.3396735,http://dx.doi.org/10.1109/TLT.2024.3396735,,,,,,,,,,,,,WOS:001235546700003,View Full Record in Web of Science
C,"Hellas, A; Leinonen, J; Sarsa, S; Koutcheme, C; Kujanpää, L; Sorva, J",,,ACM,"Hellas, Arto; Leinonen, Juho; Sarsa, Sami; Koutcheme, Charles; Kujanpaa, Lilja; Sorva, Juha",,,Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests,"PROCEEDINGS OF THE 2023 ACM CONFERENCE ON INTERNATIONAL COMPUTING EDUCATION RESEARCH V.1, ICER 2023 V1",,,,,19th Annual ACM Conference on International Computing Education Research V.1 (ICER),"AUG 07-11, 2023","Chicago, IL","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"Background and Context: Over the past year, large language models (LLMs) have taken the world by storm. In computing education, like in other walks of life, many opportunities and threats have emerged as a consequence. Objectives: In this article, we explore such opportunities and threats in a specific area: responding to student programmers' help requests. More specifically, we assess how good LLMs are at identifying issues in problematic code that students request help on. Method: We collected a sample of help requests and code from an online programming course. We then prompted two different LLMs (OpenAI Codex and GPT-3.5) to identify and explain the issues in the students' code and assessed the LLM-generated answers both quantitatively and qualitatively. Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently find at least one actual issue in each student program (GPT-3.5 in 90% of the cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57% of the time). False positives are common (40% chance for GPT-3.5). The advice that the LLMs provide on the issues is often sensible. The LLMs perform better on issues involving program logic rather than on output formatting. Model solutions are frequently provided even when the LLM is prompted not to. LLM responses to prompts in a non-English language are only slightly worse than responses to English prompts. Implications: Our results continue to highlight the utility of LLMs in programming education. At the same time, the results highlight the unreliability of LLMs: LLMs make some of the same mistakes that students do, perhaps especially when formatting output as required by automated assessment systems. Our study informs teachers interested in using LLMs as well as future efforts to customize LLMs for the needs of programming education.",,,,,"Leinonen, Juho/D-2162-2018","Leinonen, Juho/0000-0001-6829-9449; Koutcheme, Charles/0000-0002-2272-2763",,,,,,,,,,,,,,,978-1-4503-9976-0,,,,2023,,,,,,,93,105,,10.1145/3568813.3600139,http://dx.doi.org/10.1145/3568813.3600139,,,,,,,,,,,,,WOS:001141973500007,View Full Record in Web of Science
C,"Lau, S; Guo, PJ",,,ACM,"Lau, Sam; Guo, Philip J.",,,From Ban It TillWe Understand It to Resistance is Futile: How University Programming Instructors Plan to Adapt as More Students Use AI Code Generation and Explanation Tools such as ChatGPT and GitHub Copilot,"PROCEEDINGS OF THE 2023 ACM CONFERENCE ON INTERNATIONAL COMPUTING EDUCATION RESEARCH V.1, ICER 2023 V1",,,,,19th Annual ACM Conference on International Computing Education Research V.1 (ICER),"AUG 07-11, 2023","Chicago, IL","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"Over the past year (2022-2023), recently-released AI tools such as ChatGPT and GitHub Copilot have gained significant attention from computing educators. Both researchers and practitioners have discovered that these tools can generate correct solutions to a variety of introductory programming assignments and accurately explain the contents of code. Given their current capabilities and likely advances in the coming years, how do university instructors plan to adapt their courses to ensure that students still learn well? To gather a diverse sample of perspectives, we interviewed 20 introductory programming instructors (9 women + 11 men) across 9 countries (Australia, Botswana, Canada, Chile, China, Rwanda, Spain, Switzerland, United States) spanning all 6 populated continents. To our knowledge, this is the first empirical study to gather instructor perspectives about how they plan to adapt to these AI coding tools that more students will likely have access to in the future. We found that, in the short-term, many planned to take immediate measures to discourage AI-assisted cheating. Then opinions diverged about how to work with AI coding tools longer-term, with one side wanting to ban them and continue teaching programming fundamentals, and the other side wanting to integrate them into courses to prepare students for future jobs. Our study findings capture a rare snapshot in time in early 2023 as computing instructors are just starting to form opinions about this fast-growing phenomenon but have not yet converged to any consensus about best practices. Using these findings as inspiration, we synthesized a diverse set of open research questions regarding how to develop, deploy, and evaluate AI coding tools for computing education.",,,,,,"Lau, Sam/0000-0002-3160-0151",,,,,,,,,,,,,,,978-1-4503-9976-0,,,,2023,,,,,,,106,121,,10.1145/3568813.3600138,http://dx.doi.org/10.1145/3568813.3600138,,,,,,,,,,,,,WOS:001141973500008,View Full Record in Web of Science
C,"Feng, YH; Vanam, S; Cherukupally, M; Zheng, WJ; Qiu, MK; Chen, HH",,"Shahriar, H; Teranishi, Y; Cuzzocrea, A; Sharmin, M; Towey, D; Majumder, AKMJA; Kashiwazaki, H; Yang, JJ; Takemoto, M; Sakib, N; Banno, R; Ahamed, SI",,"Feng, Yunhe; Vanam, Sreecharan; Cherukupally, Manasa; Zheng, Weijian; Qiu, Meikang; Chen, Haihua",,,Investigating Code Generation Performance of ChatGPT with Crowdsourcing Social Data,"2023 IEEE 47TH ANNUAL COMPUTERS, SOFTWARE, AND APPLICATIONS CONFERENCE, COMPSAC",Proceedings International Computer Software and Applications Conference,,,,"47th IEEE-Computer-Society Annual International Conference on Computers, Software, and Applications (COMPSAC)","JUN 27-29, 2023","Univ Torino, Torino, ITALY","IEEE,IEEE Comp Soc",Univ Torino,,,"The recent advancements in Artificial Intelligence, particularly in large language models and generative models, are reshaping the field of software engineering by enabling innovative ways of performing various tasks, such as programming, debugging, and testing. However, few existing works have thoroughly explored the potential of AI in code generation and users' attitudes toward AI-assisted coding tools. This knowledge gap leaves it unclear how AI is transforming software engineering and programming education. This paper presents a scalable crowdsourcing data-driven framework to investigate the code generation performance of generative large language models from diverse perspectives across multiple social media platforms. Specifically, we utilize ChatGPT, a popular generative large language model, as a representative example to reveal its insights and patterns in code generation. First, we propose a hybrid keyword word expansion method that integrates words suggested by topic modeling and expert knowledge to filter relevant social posts of interest on Twitter and Reddit. Then we collect 316K tweets and 3.2K Reddit posts about ChatGPT's code generation, spanning from Dec. 1, 2022 to January 31, 2023. Our data analytics show that ChatGPT has been used in more than 10 programming languages, with Python and JavaScript being the two most popular, for a diverse range of tasks such as code debugging, interview preparation, and academic assignment solving. Surprisingly, our analysis shows that fear is the dominant emotion associated with ChatGPT's code generation, overshadowing emotions of happiness, anger, surprise, and sadness. Furthermore, we construct a ChatGPT prompt and corresponding code dataset by analyzing the screen-shots of ChatGPT code generation shared on social media. This dataset enables us to evaluate the quality of the generated code, and we have released this dataset to the public. We believe the insights gained from our work will provide valuable guidance for future research on AI-powered code generation.",,,,,"Feng, Yunhe/AAB-8342-2019","Feng, Yunhe/0000-0001-6577-227X; Chen, Haihua/0000-0002-7088-9752",,,,,,,,,,,,,0730-3157,,979-8-3503-2697-0,,,,2023,,,,,,,876,885,,10.1109/COMPSAC57700.2023.00117,http://dx.doi.org/10.1109/COMPSAC57700.2023.00117,,,,,,,,,,,,,WOS:001046484100107,View Full Record in Web of Science
J,"Thida, M",,,,"Thida, Myo",,,Automated Analysis of Job Market Demands using Large Language Model,INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS,,,,,,,,,,,,"This paper presents a comprehensive analysis of labor market demands for Myanmar workers in Japan, and Thailand, focusing on opportunities for individuals without higher education degrees. Leveraging ChatGPT's text classification and summarization capabilities, we extracted vital insights from extensive job advertisements and social media groups. The dataset comprises 152 job advertisements from Thailand and 30 from Japan, collected in 2023. Our research provides a valuable snapshot of skill demands and job opportunities, offering insights for informed decision-making by both job seekers and international non-governmental organizations. The innovative approach of using ChatGPT highlights its efficacy in understanding labor market dynamics. These findings serve as a foundation for tailored interventions to bridge employment challenges faced by marginalized Myanmar youths.",,,,,"thida, myo/O-6173-2018",,,,,,,,,,,,,,2158-107X,2156-5570,,,,AUG,2023,14,8,,,,,937,946,,,,,,,,,,,,,,,,WOS:001064929600001,View Full Record in Web of Science
J,"Tsai, ML; Ong, CW; Chen, CL",,,,"Tsai, Meng -Lin; Ong, Chong Wei; Chen, Cheng-Liang",,,Exploring the use of large language models (LLMs) in chemical engineering education: Building core course problem models with Chat-GPT,EDUCATION FOR CHEMICAL ENGINEERS,,,,,,,,,,,,"This study highlights the potential benefits of integrating Large Language Models (LLMs) into chemical engineering education. In this study, Chat-GPT, a user-friendly LLM, is used as a problem-solving tool. Chemical engineering education has traditionally focused on fundamental knowledge in the classroom with limited opportunities for hands-on problem-solving. To address this issue, our study proposes an LLMs-assisted problemsolving procedure. This approach promotes critical thinking, enhances problem-solving abilities, and facilitates a deeper understanding of core subjects. Furthermore, incorporating programming into chemical engineering education prepares students with vital Industry 4.0 skills for contemporary industrial practices. During our experimental lecture, we introduced a simple example of building a model to calculate steam turbine cycle efficiency, and assigned projects to students for exploring the possible use of LLMs in solving various aspect of chemical engineering problems. Although it received mixed feedback from students, it was found to be an accessible and practical tool for improving problem-solving efficiency. Analyzing the student projects, we identified five common difficulties and misconceptions and provided helpful suggestions for overcoming them. Our course has limitations regarding using advanced tools and addressing complex problems. We further provide two additional examples to better demonstrate how to integrate LLMs into core courses. We emphasize the importance of universities, professors, and students actively embracing and utilizing LLMs as tools for chemical engineering education. Students must develop critical thinking skills and a thorough understanding of the principles behind LLMs, taking responsibility for their use and creations. This study provides valuable insights for enhancing chemical engineering education's learning experience and outcomes by integrating LLMs.",,,,,,"Tsai, Meng-Lin/0009-0008-9861-1139; Ong, Chong Wei/0000-0002-0189-1189",,,,,,,,,,,,,,1749-7728,,,,JUL,2023,44,,,,,,71,95,,10.1016/j.ece.2023.05.001,http://dx.doi.org/10.1016/j.ece.2023.05.001,,MAY 2023,,,,,,,,,,,WOS:001054652000001,View Full Record in Web of Science
J,"French, F; Levi, D; Maczo, C; Simonaityte, A; Triantafyllidis, S; Varda, G",,,,"French, Fiona; Levi, David; Maczo, Csaba; Simonaityte, Aiste; Triantafyllidis, Stefanos; Varda, Gergo",,,Creative Use of OpenAI in Education: Case Studies from Game Development,MULTIMODAL TECHNOLOGIES AND INTERACTION,,,,,,,,,,,,"Educators and students have shown significant interest in the potential for generative artificial intelligence (AI) technologies to support student learning outcomes, for example, by offering personalized experiences, 24 h conversational assistance, text editing and help with problem-solving. We review contemporary perspectives on the value of AI as a tool in an educational context and describe our recent research with undergraduate students, discussing why and how we integrated OpenAI tools ChatGPT and Dall-E into the curriculum during the 2022-2023 academic year. A small cohort of games programming students in the School of Computing and Digital Media at London Metropolitan University was given a research and development assignment that explicitly required them to engage with OpenAI. They were tasked with evaluating OpenAI tools in the context of game development, demonstrating a working solution and reporting on their findings. We present five case studies that showcase some of the outputs from the students and we discuss their work. This mode of assessment was both productive and popular, mapping to students' interests and helping to refine their skills in programming, problem-solving, critical reflection and exploratory design.",,,,,,"French, Fiona/0000-0003-4226-6889",,,,,,,,,,,,,,2414-4088,,,,AUG,2023,7,8,,,,,,,81,10.3390/mti7080081,http://dx.doi.org/10.3390/mti7080081,,,,,,,,,,,,,WOS:001056644500001,View Full Record in Web of Science
C,"MacNeil, S; Denny, P; Tran, A; Leinonen, J; Bernstein, S; Hellas, A; Sarsa, S; Kim, J",,,ACM,"MacNeil, Stephen; Denny, Paul; Tran, Andrew; Leinonen, Juho; Bernstein, Seth; Hellas, Arto; Sarsa, Sami; Kim, Joanne",,,Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models,"PROCEEDINGS OF THE 26TH AUSTRALASIAN COMPUTING EDUCATION CONFERENCE, ACE 2024",,,,,26th Australasian Computing Education Conference (ACE),"JAN 29-FEB 02, 2024","Sydney, AUSTRALIA",,,,,"Identifying and resolving logic errors can be one of the most frustrating challenges for novices programmers. Unlike syntax errors, for which a compiler or interpreter can issue a message, logic errors can be subtle. In certain conditions, buggy code may even exhibit correct behavior - in other cases, the issue might be about how a problem statement has been interpreted. Such errors can be hard to spot when reading the code, and they can also at times be missed by automated tests. There is great educational potential in automatically detecting logic errors, especially when paired with suitable feedback for novices. Large language models (LLMs) have recently demonstrated surprising performance for a range of computing tasks, including generating and explaining code. These capabilities are closely linked to code syntax, which aligns with the next token prediction behavior of LLMs. On the other hand, logic errors relate to the runtime performance of code and thus may not be as well suited to analysis by LLMs. To explore this, we investigate the performance of two popular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly explanation of logic errors. We compare LLM performance with a large cohort of introductory computing students (.. = 964) solving the same error detection task. Through a mixed-methods analysis of student and model responses, we observe significant improvement in logic error identification between the previous and current generation of LLMs, and find that both LLM generations significantly outperform students. We outline how such models could be integrated into computing education tools, and discuss their potential for supporting students when learning programming.",,,,,"MacNeil, Stephen/HPH-0843-2023; Leinonen, Juho/D-2162-2018","MacNeil, Stephen/0000-0003-2781-6619; Leinonen, Juho/0000-0001-6829-9449",,,,,,,,,,,,,,,979-8-4007-1619-5,,,,2024,,,,,,,11,18,,10.1145/3636243.3636245,http://dx.doi.org/10.1145/3636243.3636245,,,,,,,,,,,,,WOS:001166861800002,View Full Record in Web of Science
J,"Zhang, JL; Cambronero, JP; Gulwani, S; Le, V; Piskac, R; Soares, G; Verbruggen, G",,,,"Zhang, Jialu; Cambronero, Jose Pablo; Gulwani, Sumit; Le, Vu; Piskac, Ruzica; Soares, Gustavo; Verbruggen, Gust",,,PyDex: Repairing Bugs in Introductory Python Assignments using LLMs,PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL,,,,,,,,,,,,"Students often make mistakes in their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex (a version of GPT), to build an APR system - PyDex - for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate PyDex on 286 real student programs and compare to three baselines, including one that combines a state-of-the-art Python syntax repair engine, BIFI, and a state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that PyDex can fix more programs and produce smaller patches on average.",,,,,,"Gulwani, Sumit/0000-0002-9226-9634",,,,,,,,,,,,,,2475-1421,,,,APR,2024,8,OOPSLA,,,,,,,133,10.1145/3649850,http://dx.doi.org/10.1145/3649850,,,,,,,,,,,,,WOS:001209927600040,View Full Record in Web of Science
J,"Barambones, J; Moral, C; de Antonio, A; Imbert, R; Martínez-Normand, L; Villalba-Mora, E",,,,"Barambones, Jose; Moral, Cristian; de Antonio, Angelica; Imbert, Ricardo; Martinez-Normand, Loic; Villalba-Mora, Elena",,,ChatGPT for Learning HCI Techniques: A Case Study on Interviews for Personas,IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES,,,,,,,,,,,,"Before interacting with real users, developers must be proficient in human-computer interaction (HCI) so as not to exhaust user patience and availability. For that, substantial training and practice are required, but it is costly to create a variety of high-quality HCI training materials. In this context, chat generative pretrained transformer (ChatGPT) and other chatbots based on large language models (LLMs) offer an opportunity to generate training materials of acceptable quality without foregoing specific human characteristics present in real-world scenarios. Personas is a user-centered design method that encompasses fictitious but believable user archetypes to help designers understand and empathize with their target audience during product design. We conducted an exploratory study on the Personas technique, addressing the validity and believability of interviews designed by HCI trainers and answered by ChatGPT-simulated users, which can be used as training material for persona creation. Specifically, we employed ChatGPT to respond to interviews designed by user experience (UX) experts. Two groups, HCI professors and professionals, then evaluated the validity of the generated materials considering quality, usefulness, UX, and ethics. The results show that both groups rated the interviews as believable and helpful for Personas training. However, some concerns about response repetition and low response variability suggested the need for further research on improved prompt design in order to generate more diverse and well-developed responses. The findings of this study provide insight into how HCI trainers can use ChatGPT to help their students master persona creation skills before working with real users in real-world scenarios for the first time.",,,,,"Villalba Mora, Elena/AAS-7030-2020; Imbert, Ricardo/M-1268-2014","Villalba Mora, Elena/0000-0001-6043-6322; Barambones, Jose/0000-0003-3860-7257; Imbert, Ricardo/0000-0002-7738-4941",,,,,,,,,,,,,1939-1382,,,,,,2024,17,,,,,,1486,1501,,10.1109/TLT.2024.3386095,http://dx.doi.org/10.1109/TLT.2024.3386095,,,,,,,,,,,,,WOS:001214521800004,View Full Record in Web of Science
C,"Leinonen, J; Denny, P; MacNeil, S; Sarsa, S; Bernstein, S; Kim, J; Tran, A; Hellas, A",,,ACM,"Leinonen, Juho; Denny, Paul; MacNeil, Stephen; Sarsa, Sami; Bernstein, Seth; Kim, Joanne; Tran, Andrew; Hellas, Arto",,,Comparing Code Explanations Created by Students and Large Language Models,"PROCEEDINGS OF THE 2023 CONFERENCE ON INNOVATION AND TECHNOLOGY IN COMPUTER SCIENCE EDUCATION, ITICSE 2023, VOL 1",,,,,28th Annual Conference on Innovation and Technology in Computer Science Education (ITiCSE),"JUL 08-12, 2023","Univ Turku, Turku, FINLAND","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ,ACM Europe Council,Informat Europe",Univ Turku,,,"Reasoning about code and explaining its purpose are fundamental skills for computer scientists. There has been extensive research in the field of computing education on the relationship between a student's ability to explain code and other skills such as writing and tracing code. In particular, the ability to describe at a high-level of abstraction how code will behave over all possible inputs correlates strongly with code writing skills. However, developing the expertise to comprehend and explain code accurately and succinctly is a challenge for many students. Existing pedagogical approaches that scaffold the ability to explain code, such as producing exemplar code explanations on demand, do not currently scale well to large classrooms. The recent emergence of powerful large language models (LLMs) may offer a solution. In this paper, we explore the potential of LLMs in generating explanations that can serve as examples to scaffold students' ability to understand and explain code. To evaluate LLM-created explanations, we compare them with explanations created by students in a large course (n approximate to 1000) with respect to accuracy, understandability and length. We find that LLM-created explanations, which can be produced automatically on demand, are rated as being significantly easier to understand and more accurate summaries of code than student-created explanations. We discuss the significance of this finding, and suggest how such models can be incorporated into introductory programming education.",,,,,"Leinonen, Juho/D-2162-2018; MacNeil, Stephen/HPH-0843-2023","Leinonen, Juho/0000-0001-6829-9449; MacNeil, Stephen/0000-0003-2781-6619; Denny, Paul/0000-0002-5150-9806",,,,,,,,,,,,,,,979-8-4007-0138-2,,,,2023,,,,,,,124,130,,10.1145/3587102.3588785,http://dx.doi.org/10.1145/3587102.3588785,,,,,,,,,,,,,WOS:001051691300020,View Full Record in Web of Science
C,"Hou, I; Man, O; Mettille, S; Gutierrez, S; Angelikas, K; MacNeil, S",,,ACM,"Hou, Irene; Man, Owen; Mettille, Sophia; Gutierrez, Sebastian; Angelikas, Kenneth; MacNeil, Stephen",,,More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems,"PROCEEDINGS OF THE 26TH AUSTRALASIAN COMPUTING EDUCATION CONFERENCE, ACE 2024",,,,,26th Australasian Computing Education Conference (ACE),"JAN 29-FEB 02, 2024","Sydney, AUSTRALIA",,,,,"Large language models are reshaping computing education. Based on recent research, these models explain code better than students, answer multiple choice questions at or above the class average, and generate code that can pass automated tests in introductory courses. In response to these capabilities, instructors have quickly adjusted their courses and assessment methods to align with shifting learning goals and the increased risk of academic integrity issues. While some scholars have advocated for the integration of visual problems as a safeguard against the capabilities of language models, new multimodal models now have vision and language capabilities that may allow them to analyze and solve visual problems. In this paper, we compare the large multimodal model (LMMs) GPT-4V with Bard, an LLM that uses Google Lens for text recognition. We find that LMMs, which have learned both pixel features (from images) and text features (from prompts) in the same embedding space, performed substantially better than Bard which uses a piecemeal approach. With a specific focus on Parsons problems presented across diverse visual representations, our results show that GPT-4V solved 96.7% these visual problems, struggling minimally with a single Parsons problem. Conversely, Bard performed poorly by only solving 69.2% of problems, struggling with common issues like hallucinations and refusals. These findings suggest that merely transitioning to visual programming problems might not be a panacea to issues of academic integrity in the generative AI era.",,,,,,,,,,,,,,,,,,,,,979-8-4007-1619-5,,,,2024,,,,,,,29,38,,10.1145/3636243.3636247,http://dx.doi.org/10.1145/3636243.3636247,,,,,,,,,,,,,WOS:001166861800004,View Full Record in Web of Science
C,"Wang, S; Mitchell, J; Piech, C",,,Assoc Computing Machinery,"Wang, Sierra; Mitchell, John; Piech, Chris",,,A Large Scale RCT on Effective Error Messages in CS1,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"In this paper, we evaluate the most effective error message types through a large-scale randomized controlled trial conducted in an open-access, online introductory computer science course with 8,762 students from 146 countries. We assess existing error message enhancement strategies, as well as two novel approaches of our own: (1) generating error messages using OpenAI's GPT in real time and (2) constructing error messages that incorporate the course discussion forum. By examining students' direct responses to error messages, and their behavior throughout the course, we quantitatively evaluate the immediate and longer term efficacy of different error message types. We find that students using GPT generated error messages repeat an error 23.1% less often in the subsequent attempt, and resolve an error in 34.8% fewer additional attempts, compared to students using standard error messages. We also perform an analysis across various demographics to understand any disparities in the impact of different error message types. Our results find no significant difference in the effectiveness of GPT generated error messages for students from varying socioeconomic and demographic backgrounds. Our findings underscore GPT generated error messages as the most helpful error message type, especially as a universally effective intervention across demographics.",,,,,,"Wang, Sierra/0000-0003-1376-8759",,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,1395,1401,,10.1145/3626252.3630764,http://dx.doi.org/10.1145/3626252.3630764,,,,,,,,,,,,,WOS:001181240800202,View Full Record in Web of Science
C,"Woodrow, J; Malik, A; Piech, C",,,Assoc Computing Machinery,"Woodrow, Juliette; Malik, Ali; Piech, Chris",,,"AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course","PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"Teaching students how to write code that is elegant, reusable, and comprehensible is a fundamental part of CS1 education. However, providing this style feedback in a timely manner has proven difficult to scale. In this paper, we present our experience deploying a novel, real-time style feedback tool in Code in Place, a large-scale online CS1 course. Our tool is based on the latest breakthroughs in large-language models (LLMs) and was carefully designed to be safe and helpful for students. We used our Real-Time Style Feedback tool (RTSF) in a class with over 8,000 diverse students from across the globe and ran a randomized control trial to understand its benefits. We show that students who received style feedback in real-time were five times more likely to view and engage with their feedback compared to students who received delayed feedback. Moreover, those who viewed feedback were more likely to make significant style-related edits to their code, with over 79% of these edits directly incorporating their feedback. We also discuss the practicality and dangers of LLM-based tools for feedback, investigating the quality of the feedback generated, LLM limitations, and techniques for consistency, standardization, and safeguarding against demographic bias, all of which are crucial for a tool utilized by students.",,,,,,,,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,1442,1448,,10.1145/3626252.3630773,http://dx.doi.org/10.1145/3626252.3630773,,,,,,,,,,,,,WOS:001181240800209,View Full Record in Web of Science
C,"Grimme, B; Pohl, J; Winkelmann, H; Stampe, L; Grimme, C",,"Ceolin, D; Caselli, T; Tulin, M",,"Grimme, Britta; Pohl, Janina; Winkelmann, Hendrik; Stampe, Lucas; Grimme, Christian",,,Lost in Transformation: Rediscovering LLM-Generated Campaigns in Social Media,"DISINFORMATION IN OPEN ONLINE MEDIA, MISDOOM 2023",Lecture Notes in Computer Science,,,,5th Multidisciplinary International Symposium on Disinformation in Open Online Media (MISDOOM),"NOV 21-22, 2023","Amsterdam, NETHERLANDS",European Res Ctr Informat Syst,,,,"This paper addresses new challenges of detecting campaigns in social media, which emerged with the rise of Large Language Models (LLMs). LLMs particularly challenge algorithms focused on the temporal analysis of topical clusters. Simple similarity measures can no longer capture and map campaigns that were previously broadly similar in content. Herein, we analyze whether the classification of messages over time can be profitably used to rediscover poorly detectable campaigns at the content level. Thus, we evaluate classical classifiers and a new method based on siamese neural networks. Our results show that campaigns can be detected despite the limited reliability of the classifiers as long as they are based on a large amount of simultaneously spread artificial content.",,,,,,"Lutke Stockdiek, Janina/0000-0001-5251-1169; Grimme, Britta/0009-0008-2282-5130",,,,,,,,,,,,,0302-9743,1611-3349,978-3-031-47895-6; 978-3-031-47896-3,,,,2023,14397,,,,,,72,87,,10.1007/978-3-031-47896-3_6,http://dx.doi.org/10.1007/978-3-031-47896-3_6,,,,,,,,,,,,,WOS:001160755400006,View Full Record in Web of Science
J,"Kawahara, T; Sumi, Y",,,,"Kawahara, Tomoki; Sumi, Yuki",,,GPT-4/4V's performance on the Japanese National Medical Licensing Examination,MEDICAL TEACHER,,,,,,,,,,,,"BackgroundRecent advances in Artificial Intelligence (AI) are changing the medical world, and AI will likely replace many of the actions performed by medical professionals. The overall clinical ability of the AI has been evaluated by its ability to answer a text-based national medical examination. This study uniquely assesses the performance of Open AI's ChatGPT against all Japanese National Medical Licensing Examination (NMLE), including images, illustrations, and pictures.MethodsWe obtained the questions of the past six years of the NMLE (112th to 117th) from the Japanese Ministry of Health, Labour and Welfare website. We converted them to JavaScript Object Notation (JSON) format. We created an application programming interface (API) to output correct answers using GPT-4 for questions without images and GPT4-V(ision) or GPT4 console for questions with images.ResultsThe percentage of image questions was 723/2400 (30.1%) over the past six years. In all years, GPT-4/4V exceeded the minimum score the examinee should score. In total, over the six years, the percentage of correct answers for basic medical knowledge questions was 665/905 (73.5%); for clinical knowledge questions, 1143/1531 (74.7%); and for image questions 497/723 (68.7%), respectively.ConclusionsRegarding medical knowledge, GPT-4/4V met the minimum criteria regardless of whether the questions included images, illustrations, and pictures. Our study sheds light on the potential utility of AI in medical education.",,,,,,"Kawahara, Tomoki/0000-0002-9849-3880",,,,,,,,,,,,,0142-159X,1466-187X,,,,2024 APR 19,2024,,,,,,,,,,10.1080/0142159X.2024.2342545,http://dx.doi.org/10.1080/0142159X.2024.2342545,,APR 2024,,,,,,38648547,,,,,WOS:001207033300001,View Full Record in Web of Science
J,"Khan, N; Khan, Z; Koubaa, A; Khan, MK; Salleh, RB",,,,"Khan, Nauman; Khan, Zahid; Koubaa, Anis; Khan, Muhammad Khurram; Salleh, Rosli bin",,,Global insights and the impact of generative AI-ChatGPT on multidisciplinary: a systematic review and bibliometric analysis,CONNECTION SCIENCE,,,,,,,,,,,,"In 2022, OpenAI's unveiling of generative AI Large Language Models (LLMs)- ChatGPT, heralded a significant leap forward in human-machine interaction through cutting-edge AI technologies. With its surging popularity, scholars across various fields have begun to delve into the myriad applications of ChatGPT. While existing literature reviews on LLMs like ChatGPT are available, there is a notable absence of systematic literature reviews (SLRs) and bibliometric analyses assessing the research's multidisciplinary and geographical breadth. This study aims to bridge this gap by synthesising and evaluating how ChatGPT has been integrated into diverse research areas, focussing on its scope and the geographical distribution of studies. Through a systematic review of scholarly articles, we chart the global utilisation of ChatGPT across various scientific domains, exploring its contribution to advancing research paradigms and its adoption trends among different disciplines. Our findings reveal a widespread endorsement of ChatGPT across multiple fields, with significant implementations in healthcare (38.6%), computer science/IT (18.6%), and education/research (17.3%). Moreover, our demographic analysis underscores ChatGPT's global reach and accessibility, indicating participation from 80 unique countries in ChatGPT-related research, with the most frequent countries keyword occurrence, USA (719), China (181), and India (157) leading in contributions. Additionally, our study highlights the leading roles of institutions such as King Saud University, the All India Institute of Medical Sciences, and Taipei Medical University in pioneering ChatGPT research in our dataset. This research not only sheds light on the vast opportunities and challenges posed by ChatGPT in scholarly pursuits but also acts as a pivotal resource for future inquiries. It emphasises that the generative AI (LLM) role is revolutionising every field. The insights provided in this paper are particularly valuable for academics, researchers, and practitioners across various disciplines, as well as policymakers looking to grasp the extensive reach and impact of generative AI technologies like ChatGPT in the global research community.",,,,,,,,,,,,,,,,,,,0954-0091,1360-0494,,,,DEC 31,2024,36,1,,,,,,,2353630,10.1080/09540091.2024.2353630,http://dx.doi.org/10.1080/09540091.2024.2353630,,,,,,,,,,,,,WOS:001226491800001,View Full Record in Web of Science
C,"Bopp, C; Foerst, A; Kellogg, B",,,Assoc Computing Machinery,"Bopp, Chris; Foerst, Anne; Kellogg, Brian",,,The Case for LLMWorkshops: The Responsible Use of Large Language Models by Faculty at Small Liberal Arts Universities,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"Large Language Models (LLMs) are radically changing the academic landscape. Many professors are unaware of how LLMs work and are therefore unsure how to incorporate them in their teaching. This is problematic as students will use them anyway. In this paper, we outline our institution as a case study for a curricular initiative. We develop an intellectual framework for creating workshops for faculty at small liberal arts universities. We base their development on the literature we have analyzed and discussed as a group. Our approach is to address our colleagues across a variety of different disciplines and teach them the responsible use of LLMs in the classroom. We also teach our colleagues how to modify assignments to make them, to some extent, LLM proof. This includes adding personalized elements, and including LLM designed parts explicitly, such as article summaries. We also design a syllabus policy about the responsible use of LLMs. We present philosophical and ethical challenges and teach a list of other actionable items. We ultimately support the use of LLMs in academia but seek to teach our colleagues how they can guide students to use them mindfully and responsibly.",,,,,,,,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,130,136,,10.1145/3626252.3630941,http://dx.doi.org/10.1145/3626252.3630941,,,,,,,,,,,,,WOS:001181240800021,View Full Record in Web of Science
C,"Salewski, L; Koepke, AS; Lensch, HPA; Akata, Z",,"Kothe, U; Rother, C",,"Salewski, Leonard; Koepke, A. Sophia; Lensch, Hendrik P. A.; Akata, Zeynep",,,Zero-Shot Translation of Attention Patterns in VQA Models to Natural Language,"PATTERN RECOGNITION, DAGM GCPR 2023",Lecture Notes in Computer Science,,,,45th Annual Conference of the German-Association-for-Pattern-Recognition (DAGM GCPR),"SEP 19-22, 2023","Heidelberg, GERMANY","German Assoc Pattern Recognit,Carl Zeiss AG,Bayer AG,Robert Bosch GmbH,Qual Match GmbH,Copresence AG",,,,"Converting a model's internals to text can yield human-understandable insights about the model. Inspired by the recent success of training-free approaches for image captioning, we propose ZS-A2T, a zero-shot framework that translates the transformer attention of a given model into natural language without requiring any training. We consider this in the context of Visual Question Answering (VQA). ZS-A2T builds on a pre-trained large language model (LLM), which receives a task prompt, question, and predicted answer, as inputs. The LLM is guided to select tokens which describe the regions in the input image that the VQA model attended to. Crucially, we determine this similarity by exploiting the text-image matching capabilities of the underlying VQA model. Our framework does not require any training and allows the drop-in replacement of different guiding sources (e.g. attribution instead of attention maps), or language models. We evaluate this novel task on textual explanation datasets for VQA, giving state-of-the-art performances for the zero-shot setting on GQA-REX and VQA-X. Our code is available here.",,,,,,,,,,,,,,,,,,,0302-9743,1611-3349,978-3-031-54604-4; 978-3-031-54605-1,,,,2024,14264,,,,,,378,393,,10.1007/978-3-031-54605-1_25,http://dx.doi.org/10.1007/978-3-031-54605-1_25,,,,,,,,,,,,,WOS:001212397400025,View Full Record in Web of Science
J,"Zaleski, AL; Berkowsky, R; Craig, KJT; Pescatello, LS",,,,"Zaleski, Amanda L.; Berkowsky, Rachel; Craig, Kelly Jean Thomas; Pescatello, Linda S.",,,"Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study",JMIR MEDICAL EDUCATION,,,,,,,,,,,,"Background: Regular physical activity is critical for health and disease prevention. Yet, health care providers and patients face barriers to implement evidence-based lifestyle recommendations. The potential to augment care with the increased availability of artificial intelligence (AI) technologies is limitless; however, the suitability of AI-generated exercise recommendations has yet to be explored. Objective: The purpose of this study was to assess the comprehensiveness, accuracy, and readability of individualized exercise recommendations generated by a novel AI chatbot. Methods: A coding scheme was developed to score AI-generated exercise recommendations across ten categories informed by gold-standard exercise recommendations, including (1) health condition-specific benefits of exercise, (2) exercise preparticipation health screening, (3) frequency, (4) intensity, (5) time, (6) type, (7) volume, (8) progression, (9) special considerations, and (10) references to the primary literature. The AI chatbot was prompted to provide individualized exercise recommendations for 26 clinical populations using an open-source application programming interface. Two independent reviewers coded AI-generated content for each category and calculated comprehensiveness (%) and factual accuracy (%) on a scale of 0%-100%. Readability was assessed using the Flesch-Kincaid formula. Qualitative analysis identified and categorized themes from AI-generated output. Results: AI-generated exercise recommendations were 41.2% (107/260) comprehensive and 90.7% (146/161) accurate, with the majority (8/15, 53%) of inaccuracy related to the need for exercise preparticipation medical clearance. Average readability level of AI-generated exercise recommendations was at the college level (mean 13.7, SD 1.7), with an average Flesch reading ease score of 31.1 (SD 7.7). Several recurring themes and observations of AI-generated output included concern for liability and safety, preference for aerobic exercise, and potential bias and direct discrimination against certain age-based populations and individuals with disabilities. Conclusions: There were notable gaps in the comprehensiveness, accuracy, and readability of AI-generated exercise recommendations. Exercise and health care professionals should be aware of these limitations when using and endorsing AI-based technologies as a tool to support lifestyle change involving exercise.",,,,,"Thomas Craig, Kelly Jean/ABE-6181-2021","Thomas Craig, Kelly Jean/0000-0002-9954-2795; Berkowsky, Rachel/0000-0003-1150-1284; Pescatello, Linda/0000-0002-5841-798X; Zaleski, Amanda/0000-0002-0362-819X",,,,,,,,,,,,,2369-3762,,,,,,2024,10,,,,,,,,e51308,10.2196/51308,http://dx.doi.org/10.2196/51308,,,,,,,,38206661,,,,,WOS:001166043700001,View Full Record in Web of Science
J,"Aggrawal, S; Magana, AJ",,,,"Aggrawal, Sakhi; Magana, Alejandra J.",,,Teamwork Conflict Management Training and Conflict Resolution Practice via Large Language Models,FUTURE INTERNET,,,,,,,,,,,,"This study implements a conflict management training approach guided by principles of transformative learning and conflict management practice simulated via an LLM. Transformative learning is more effective when learners are engaged mentally and behaviorally in learning experiences. Correspondingly, the conflict management training approach involved a three-step procedure consisting of a learning phase, a practice phase enabled by an LLM, and a reflection phase. Fifty-six students enrolled in a systems development course were exposed to the transformative learning approach to conflict management so they would be better prepared to address any potential conflicts within their teams as they approached a semester-long software development project. The study investigated the following: (1) How did the training and practice affect students' level of confidence in addressing conflict? (2) Which conflict management styles did students use in the simulated practice? (3) Which strategies did students employ when engaging with the simulated conflict? The findings indicate that: (1) 65% of the students significantly increased in confidence in managing conflict by demonstrating collaborative, compromising, and accommodative approaches; (2) 26% of the students slightly increased in confidence by implementing collaborative and accommodative approaches; and (3) 9% of the students did not increase in confidence, as they were already confident in applying collaborative approaches. The three most frequently used strategies for managing conflict were identifying the root cause of the problem, actively listening, and being specific and objective in explaining their concerns.",,,,,,"Magana, Alejandra/0000-0001-6117-7502; Aggrawal, Sakhi/0000-0002-2274-0152",,,,,,,,,,,,,1999-5903,,,,,MAY,2024,16,5,,,,,,,177,10.3390/fi16050177,http://dx.doi.org/10.3390/fi16050177,,,,,,,,,,,,,WOS:001232821500001,View Full Record in Web of Science
C,"Rogers, MP; Hillberg, HM; Groves, CL",,,Assoc Computing Machinery,"Rogers, Michael P.; Hillberg, Hannah Miller; Groves, Christopher L.",,,Attitudes Towards the Use (and Misuse) of ChatGPT: A Preliminary Study,"PROCEEDINGS OF THE 55TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, SIGCSE 2024, VOL. 1",,,,,55th ACM Technical Symposium on Computer Science Education (SIGCSE),"MAR 20-23, 2024","Portland, OR","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ",,,,"ChatGPT is the front end to a powerful large language model that has garnered widespread attention in many fields of study, including computer science (CS), where it promises to be transformational. As educators, we are just starting to grapple with the ramifications of this new technology, including implications for what we teach, how we teach, and how we grade. The decisions educators make moving forward depend heavily on the prevalence of students' use (and misuse) of ChatGPT in the classroom. Further, predictors of nefarious use could aid educators as well. We conducted an online survey to capture CS student awareness of, experience with, and attitudes toward ChatGPT. Through quantitative and qualitative analysis, we found that awareness of ChatGPT is generally high, and it is more frequently being used as a study tool than to complete students' work for them. Most students are aware of the potential for abuse in academic pursuits, but a notable minority of students admit to using it unscrupulously and to the potential for it to interfere with their learning. We conclude with a discussion of factors to consider as educators modify their approaches and develop guidelines for ChatGPT usage in their classrooms.",,,,,,,,,,,,,,,,,,,,,979-8-4007-0423-9,,,,2024,,,,,,,1147,1153,,10.1145/3626252.3630784,http://dx.doi.org/10.1145/3626252.3630784,,,,,,,,,,,,,WOS:001181240800166,View Full Record in Web of Science
C,"Zhang, P; Jaipersaud, B; Ba, J; Petersen, A; Zhang, L; Zhang, MR",,,ACM,"Zhang, Paul; Jaipersaud, Brandon; Ba, Jimmy; Petersen, Andrew; Zhang, Lisa; Zhang, Michael R.",,,Classifying Course Discussion Board Questions using LLMs,"PROCEEDINGS OF THE 2023 CONFERENCE ON INNOVATION AND TECHNOLOGY IN COMPUTER SCIENCE EDUCATION, ITICSE 2023, VOL. 2",,,,,28th Annual Conference on Innovation and Technology in Computer Science Education (ITiCSE),"JUL 08-12, 2023","Univ Turku, Turku, FINLAND","Assoc Comp Machinery,ACM Special Interest Grp Comp Sci Educ,ACM Europe Council,Informat Europe",Univ Turku,,,"Large language models (LLMs) can be used to answer student questions on course discussion boards, but there is a risk of LLMs answering questions they are unable to address. We propose and evaluate an LLM-based system that classifies student questions into one of four types: conceptual, homework, logistics, and not answerable. We then prompt an LLM using a type-specific prompt. Using GPT-3, we achieve 81% classification accuracy across the four categories. Furthermore, we achieve 93% accuracy on classifying not answerable questions. This indicates that our system effectively ignores questions that it cannot address.",,,,,,"Jaipersaud, Brandon/0009-0007-0478-7356; Petersen, Andrew/0000-0003-1337-7985",,,,,,,,,,,,,,,979-8-4007-0139-9,,,,2023,,,,,,,658,658,,10.1145/3587103.3594202,http://dx.doi.org/10.1145/3587103.3594202,,,,,,,,,,,,,WOS:001054840400063,View Full Record in Web of Science
J,"Fujimoto, S; Takemoto, K",,,,"Fujimoto, Sasuke; Takemoto, Kazuhiro",,,Revisiting the political biases of ChatGPT,FRONTIERS IN ARTIFICIAL INTELLIGENCE,,,,,,,,,,,,"Although ChatGPT promises wide-ranging applications, there is a concern that it is politically biased; in particular, that it has a left-libertarian orientation. Nevertheless, following recent trends in attempts to reduce such biases, this study re-evaluated the political biases of ChatGPT using political orientation tests and the application programming interface. The effects of the languages used in the system as well as gender and race settings were evaluated. The results indicate that ChatGPT manifests less political bias than previously assumed; however, they did not entirely dismiss the political bias. The languages used in the system, and the gender and race settings may induce political biases. These findings enhance our understanding of the political biases of ChatGPT and may be useful for bias evaluation and designing the operational strategy of ChatGPT.",,,,,"Takemoto, Kazuhiro/H-2915-2019","Takemoto, Kazuhiro/0000-0002-6355-1366",,,,,,,,,,,,,,2624-8212,,,,OCT 20,2023,6,,,,,,,,1232003,10.3389/frai.2023.1232003,http://dx.doi.org/10.3389/frai.2023.1232003,,,,,,,,37928447,,,,,WOS:001119479800001,View Full Record in Web of Science
C,"Fantechi, A; Gnesi, S; Semini, L",,"Mendez, D; Moreira, A",,"Fantechi, Alessandro; Gnesi, Stefania; Semini, Laura",,,Exploring LLMs' Ability to Detect Variability in Requirements,"REQUIREMENTS ENGINEERING: FOUNDATION FOR SOFTWARE QUALITY, REFSQ 2024",Lecture Notes in Computer Science,,,,30th International Working Conference on Requirements Engineering - Foundation for Software Quality (REFSQ),"APR 08-11, 2024","Winterthur, SWITZERLAND","Zhaw, Sch Engn, InIT Inst Comp Sci,Fachhochschule Nordwestschweiz,Fortiss,IREB,Springer,XITASO",,,,"In this paper, we address the question of whether general-purpose LLM-based tools may be useful for detecting requirements variability in Natural Language (NL) requirements documents. For this purpose, we conduct a preliminary exploratory study considering OpenAI chatGPT-3.5 and Microsoft Bing. Using two exemplar NL requirements documents, we compare the variability detection capability of the chatbots with that of experts and that of a rule-based NLP tool.",,,,,,"Fantechi, Alessandro/0000-0002-4648-4667",,,,,,,,,,,,,0302-9743,1611-3349,978-3-031-57326-2; 978-3-031-57327-9,,,,2024,14588,,,,,,178,188,,10.1007/978-3-031-57327-9_11,http://dx.doi.org/10.1007/978-3-031-57327-9_11,,,,,,,,,,,,,WOS:001209314200011,View Full Record in Web of Science
C,"Liu, KB; Han, YD; Zhang, JM; Chen, ZP; Sarro, F; Harman, M; Huang, G; Ma, Y",,"Just, R; Fraser, G",,"Liu, Kaibo; Han, Yudong; Zhang, Jie M.; Chen, Zhenpeng; Sarro, Federica; Harman, Mark; Huang, Gang; Ma, Yun",,,Who Judges the Judge: An Empirical Study on Online Judge Tests,"PROCEEDINGS OF THE 32ND ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON SOFTWARE TESTING AND ANALYSIS, ISSTA 2023",,,,,32nd ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA),"JUL 17-21, 2023","Seattle, WA","Assoc Comp Machinery,ACM SIGSOFT,AITO",,,,"Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2% of false positives have perfect (100%) line coverage, 78.9% have perfect branch coverage, and 32.5% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.",,,,,,"Chen, Zhenpeng/0000-0002-4765-1893; Han, Yudong/0000-0003-4846-5803; Liu, Kaibo/0009-0005-9302-4953",,,,,,,,,,,,,,,979-8-4007-0221-1,,,,2023,,,,,,,334,346,,10.1145/3597926.3598060,http://dx.doi.org/10.1145/3597926.3598060,,,,,,,,,,,,,WOS:001122661400028,View Full Record in Web of Science
C,"Giglou, HB; D'Souza, J; Auer, S",,"Payne, TR; Presutti, V; Qi, G; Poveda-Villalon, M; Stoilos, G; Hollink, L; Kaoudi, Z; Cheng, G; Li, J",,"Giglou, Hamed Babaei; D'Souza, Jennifer; Auer, Soeren",,,LLMs4OL: Large Language Models for Ontology Learning,"SEMANTIC WEB, ISWC 2023, PART I",Lecture Notes in Computer Science,,,,22nd International Semantic Web Conference (ISWC),"NOV 06-10, 2023","Athens, GREECE","Create Link,Zhipu Ai,Bosch,IBM Res,Metaphacts,Google,Gesis, Leibniz Inst Social Sci,Ontotext,Ebay,Huawei,Elsevier, Journal of Artificial Intelligence,Qualco Grp,Bupsolutions",,,,"We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text? To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS. The obtained empirical results show that foundational LLMs are not sufficiently suitable for ontology construction that entails a high degree of reasoning skills and domain expertise. Nevertheless, when effectively fine-tuned they just might work as suitable assistants, alleviating the knowledge acquisition bottleneck, for ontology construction.",,,,,,"D'Souza, Jennifer/0000-0002-6616-9509; Auer, Soren/0000-0002-0698-2864; Babaei Giglou, Hamed/0000-0003-3758-1454",,,,,,,,,,,,,0302-9743,1611-3349,978-3-031-47239-8; 978-3-031-47240-4,,,,2023,14265,,,,,,408,427,,10.1007/978-3-031-47240-4_22,http://dx.doi.org/10.1007/978-3-031-47240-4_22,,,,,,,,,,,,,WOS:001160736700022,View Full Record in Web of Science