doi,url,title,abstract,published
,http://arxiv.org/pdf/2212.05113v1.pdf,"Automatically Generating CS Learning Materials with Large Language
  Models","Recent breakthroughs in Large Language Models (LLMs), such as GPT-3 and
Codex, now enable software developers to generate code based on a natural
language prompt. Within computer science education, researchers are exploring
the potential for LLMs to generate code explanations and programming
assignments using carefully crafted prompts. These advances may enable students
to interact with code in new ways while helping instructors scale their
learning materials. However, LLMs also introduce new implications for academic
integrity, curriculum design, and software engineering careers. This workshop
will demonstrate the capabilities of LLMs to help attendees evaluate whether
and how LLMs might be integrated into their pedagogy and research. We will also
engage attendees in brainstorming to consider how LLMs will impact our field.",2022-12-09T20:37:44Z
,http://arxiv.org/pdf/2405.20183v1.pdf,"A Survey Study on the State of the Art of Programming Exercise
  Generation using Large Language Models","This paper analyzes Large Language Models (LLMs) with regard to their
programming exercise generation capabilities. Through a survey study, we
defined the state of the art, extracted their strengths and weaknesses and
finally proposed an evaluation matrix, helping researchers and educators to
decide which LLM is the best fitting for the programming exercise generation
use case. We also found that multiple LLMs are capable of producing useful
programming exercises. Nevertheless, there exist challenges like the ease with
which LLMs might solve exercises generated by LLMs. This paper contributes to
the ongoing discourse on the integration of LLMs in education.",2024-05-30T15:49:34Z
,http://arxiv.org/pdf/2406.13972v1.pdf,"CREF: An LLM-based Conversational Software Repair Framework for
  Programming Tutors","Program repair techniques offer cost-saving benefits for debugging within
software development and programming education scenarios. With the proven
effectiveness of Large Language Models (LLMs) in code-related tasks,
researchers have explored their potential for program repair. However, it is
crucial to recognize that existing repair benchmarks may have influenced LLM
training data, potentially causing data leakage. To evaluate LLMs' realistic
repair capabilities, (1) we introduce an extensive, non-crawled benchmark,
referred to as TutorCode, comprising 1,239 C++ defect codes and associated
information such as tutor guidance, solution description, failing test cases,
and the corrected code. Our work assesses the repair performance of 12 LLMs on
TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision
(RPSR). (2) We then provide a comprehensive investigation into which types of
extra information can help LLMs improve their performance in repairing defects.
Among these types, tutor guidance was found to be the most effective
information in enhancing LLM repair capabilities. To fully harness LLMs'
conversational capabilities and the benefits of augmented information, (3) we
introduce a novel conversational semi-automatic repair framework CREF assisting
human tutor. It demonstrates a remarkable AVG-5 improvement of 17.2%-24.6%
compared to the baseline, achieving an impressive AVG-5 of 76.6% when utilizing
GPT-4. These results highlight the potential for enhancing LLMs' repair
capabilities through interactions with tutors and historical conversations
involving incorrect responses. The successful application of CREF in a
real-world educational setting demonstrates its effectiveness in reducing
tutors' workload and improving students' learning experience, while also
showcasing its promise for facilitating other software engineering tasks, such
as code review.",2024-06-20T03:36:34Z
,http://arxiv.org/pdf/2402.01156v2.pdf,"An Empirical Study on Low Code Programming using Traditional vs Large
  Language Model Support","Low-code programming (LCP) refers to programming using models at higher
levels of abstraction, resulting in less manual and more efficient programming,
and reduced learning effort for amateur developers. Many LCP tools have rapidly
evolved and have benefited from the concepts of visual programming languages
(VPLs) and programming by demonstration (PBD). With huge increase in interest
in using large language models (LLMs) in software engineering, LLM-based LCP
has began to become increasingly important. However, the technical principles
and application scenarios of traditional approaches to LCP and LLM-based LCP
are significantly different. Understanding these key differences and
characteristics in the application of the two approaches to LCP by users is
crucial for LCP providers in improving existing and developing new LCP tools,
and in better assisting users in choosing the appropriate LCP technology. We
conducted an empirical study of both traditional LCP and LLM-based LCP. We
analyzed developers' discussions on Stack Overflow (SO) over the past three
years and then explored the similarities and differences between traditional
LCP and LLM-based LCP features and developer feedback. Our findings reveal that
while traditional LCP and LLM-based LCP share common primary usage scenarios,
they significantly differ in scope, limitations and usage throughout the
software development lifecycle, particularly during the implementation phase.
We also examine how LLMs impact and integrate with LCP, discussing the latest
technological developments in LLM-based LCP, such as its integration with VPLs
and the application of LLM Agents in software engineering.",2024-02-02T05:52:32Z
,http://arxiv.org/pdf/2401.16186v1.pdf,"An Empirical Study on Usage and Perceptions of LLMs in a Software
  Engineering Project","Large Language Models (LLMs) represent a leap in artificial intelligence,
excelling in tasks using human language(s). Although the main focus of
general-purpose LLMs is not code generation, they have shown promising results
in the domain. However, the usefulness of LLMs in an academic software
engineering project has not been fully explored yet. In this study, we explore
the usefulness of LLMs for 214 students working in teams consisting of up to
six members. Notably, in the academic course through which this study is
conducted, students were encouraged to integrate LLMs into their development
tool-chain, in contrast to most other academic courses that explicitly prohibit
the use of LLMs.
  In this paper, we analyze the AI-generated code, prompts used for code
generation, and the human intervention levels to integrate the code into the
code base. We also conduct a perception study to gain insights into the
perceived usefulness, influencing factors, and future outlook of LLM from a
computer science student's perspective. Our findings suggest that LLMs can play
a crucial role in the early stages of software development, especially in
generating foundational code structures, and helping with syntax and error
debugging. These insights provide us with a framework on how to effectively
utilize LLMs as a tool to enhance the productivity of software engineering
students, and highlight the necessity of shifting the educational focus toward
preparing students for successful human-AI collaboration.",2024-01-29T14:32:32Z
,http://arxiv.org/pdf/2405.18062v2.pdf,Towards Integrating Emerging AI Applications in SE Education,"Artificial Intelligence (AI) approaches have been incorporated into modern
learning environments and software engineering (SE) courses and curricula for
several years. However, with the significant rise in popularity of large
language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in
particular in the last year, educators are faced with rapidly changing
classroom environments and disrupted teaching principles. Examples range from
programming assignment solutions that are fully generated via ChatGPT, to
various forms of cheating during exams. However, despite these negative aspects
and emerging challenges, AI tools in general, and LLM applications in
particular, can also provide significant opportunities in a wide variety of SE
courses, supporting both students and educators in meaningful ways. In this
early research paper, we present preliminary results of a systematic analysis
of current trends in the area of AI, and how they can be integrated into
university-level SE curricula, guidelines, and approaches to support both
instructors and learners. We collected both teaching and research papers and
analyzed their potential usage in SE education, using the ACM Computer Science
Curriculum Guidelines CS2023. As an initial outcome, we discuss a series of
opportunities for AI applications and further research areas.",2024-05-28T11:21:45Z
,http://arxiv.org/pdf/2403.18679v2.pdf,"An Exploratory Study on Upper-Level Computing Students' Use of Large
  Language Models as Tools in a Semester-Long Project","Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are
influencing software engineering practice. Software engineering educators must
teach future software engineers how to use such tools well. As of yet, there
have been few studies that report on the use of LLMs in the classroom. It is,
therefore, important to evaluate students' perception of LLMs and possible ways
of adapting the computing curriculum to these shifting paradigms.
  Purpose: The purpose of this study is to explore computing students'
experiences and approaches to using LLMs during a semester-long software
engineering project.
  Design/Method: We collected data from a senior-level software engineering
course at Purdue University. This course uses a project-based learning (PBL)
design. The students used LLMs such as ChatGPT and Copilot in their projects. A
sample of these student teams were interviewed to understand (1) how they used
LLMs in their projects; and (2) whether and how their perspectives on LLMs
changed over the course of the semester. We analyzed the data to identify
themes related to students' usage patterns and learning outcomes.
  Results/Discussion: When computing students utilize LLMs within a project,
their use cases cover both technical and professional applications. In
addition, these students perceive LLMs to be efficient tools in obtaining
information and completion of tasks. However, there were concerns about the
responsible use of LLMs without being detrimental to their own learning
outcomes. Based on our findings, we recommend future research to investigate
the usage of LLM's in lower-level computer engineering courses to understand
whether and how LLMs can be integrated as a learning aid without hurting the
learning outcomes.",2024-03-27T15:21:58Z
,http://arxiv.org/pdf/2310.05292v4.pdf,"How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent
  for Debugging","Large Language Models (LLMs) now excel at generative skills and can create
content at impeccable speeds. However, they are imperfect and still make
various mistakes. In a Computer Science education context, as these models are
widely recognized as ""AI pair programmers,"" it becomes increasingly important
to train students on evaluating and debugging the LLM-generated code. In this
work, we introduce HypoCompass, a novel system to facilitate deliberate
practice on debugging, where human novices play the role of Teaching Assistants
and help LLM-powered teachable agents debug code. We enable effective task
delegation between students and LLMs in this learning-by-teaching environment:
students focus on hypothesizing the cause of code errors, while adjacent skills
like code completion are offloaded to LLM-agents. Our evaluations demonstrate
that HypoCompass generates high-quality training materials (e.g., bugs and
fixes), outperforming human counterparts fourfold in efficiency, and
significantly improves student performance on debugging by 12% in the
pre-to-post test.",2023-10-08T21:39:47Z
,http://arxiv.org/pdf/2308.08102v1.pdf,"ChatLogo: A Large Language Model-Driven Hybrid Natural-Programming
  Language Interface for Agent-based Modeling and Programming","Building on Papert (1980)'s idea of children talking to computers, we propose
ChatLogo, a hybrid natural-programming language interface for agent-based
modeling and programming. We build upon previous efforts to scaffold ABM & P
learning and recent development in leveraging large language models (LLMs) to
support the learning of computational programming. ChatLogo aims to support
conversations with computers in a mix of natural and programming languages,
provide a more user-friendly interface for novice learners, and keep the
technical system from over-reliance on any single LLM. We introduced the main
elements of our design: an intelligent command center, and a conversational
interface to support creative expression. We discussed the presentation format
and future work. Responding to the challenges of supporting open-ended
constructionist learning of ABM & P and leveraging LLMs for educational
purposes, we contribute to the field by proposing the first constructionist
LLM-driven interface to support computational and complex systems thinking.",2023-08-16T02:21:52Z
,http://arxiv.org/pdf/2310.19813v1.pdf,Enhancing Genetic Improvement Mutations Using Large Language Models,"Large language models (LLMs) have been successfully applied to software
engineering tasks, including program repair. However, their application in
search-based techniques such as Genetic Improvement (GI) is still largely
unexplored. In this paper, we evaluate the use of LLMs as mutation operators
for GI to improve the search process. We expand the Gin Java GI toolkit to call
OpenAI's API to generate edits for the JCodec tool. We randomly sample the
space of edits using 5 different edit types. We find that the number of patches
passing unit tests is up to 75% higher with LLM-based edits than with standard
Insert edits. Further, we observe that the patches found with LLMs are
generally less diverse compared to standard edits. We ran GI with local search
to find runtime improvements. Although many improving patches are found by
LLM-enhanced GI, the best improving patch was found by standard GI.",2023-10-18T10:24:14Z
,http://arxiv.org/pdf/2405.05347v1.pdf,Benchmarking Educational Program Repair,"The emergence of large language models (LLMs) has sparked enormous interest
due to their potential application across a range of educational tasks. For
example, recent work in programming education has used LLMs to generate
learning resources, improve error messages, and provide feedback on code.
However, one factor that limits progress within the field is that much of the
research uses bespoke datasets and different evaluation metrics, making direct
comparisons between results unreliable. Thus, there is a pressing need for
standardization and benchmarks that facilitate the equitable comparison of
competing approaches. One task where LLMs show great promise is program repair,
which can be used to provide debugging support and next-step hints to students.
In this article, we propose a novel educational program repair benchmark. We
curate two high-quality publicly available programming datasets, present a
unified evaluation procedure introducing a novel evaluation metric rouge@k for
approximating the quality of repairs, and evaluate a set of five recent models
to establish baseline performance.",2024-05-08T18:23:59Z
,http://arxiv.org/pdf/2303.07263v1.pdf,InferFix: End-to-End Program Repair with LLMs,"Software development life cycle is profoundly influenced by bugs: their
introduction, identification, and eventual resolution account for a significant
portion of software cost. This has motivated software engineering researchers
and practitioners to propose different approaches for automating the
identification and repair of software defects. Large language models have been
adapted to the program repair task through few-shot demonstration learning and
instruction prompting, treating this as an infilling task. However, these
models have only focused on learning general bug-fixing patterns for
uncategorized bugs mined from public repositories. In this paper, we propose
InferFix: a transformer-based program repair framework paired with a
state-of-the-art static analyzer to fix critical security and performance bugs.
InferFix combines a Retriever -- transformer encoder model pretrained via
contrastive learning objective, which aims at searching for semantically
equivalent bugs and corresponding fixes; and a Generator -- a large language
model (Codex Cushman) finetuned on supervised bug-fix data with prompts
augmented via bug type annotations and semantically similar fixes retrieved
from an external non-parametric memory. To train and evaluate our approach, we
curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by
executing the Infer static analyzer on the change histories of thousands of
Java and C# repositories. Our evaluation demonstrates that InferFix outperforms
strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C#
and 76.8% in Java. We discuss the deployment of InferFix alongside Infer at
Microsoft which offers an end-to-end solution for detection, classification,
and localization of bugs, as well as fixing and validation of candidate
patches, integrated in the continuous integration pipeline to automate the
software development workflow.",2023-03-13T16:42:47Z
,http://arxiv.org/pdf/2307.02792v2.pdf,What Should Data Science Education Do with Large Language Models?,"The rapid advances of large language models (LLMs), such as ChatGPT, are
revolutionizing data science and statistics. These state-of-the-art tools can
streamline complex processes. As a result, it reshapes the role of data
scientists. We argue that LLMs are transforming the responsibilities of data
scientists, shifting their focus from hands-on coding, data-wrangling and
conducting standard analyses to assessing and managing analyses performed by
these automated AIs. This evolution of roles is reminiscent of the transition
from a software engineer to a product manager. We illustrate this transition
with concrete data science case studies using LLMs in this paper. These
developments necessitate a meaningful evolution in data science education.
Pedagogy must now place greater emphasis on cultivating diverse skillsets among
students, such as LLM-informed creativity, critical thinking, AI-guided
programming. LLMs can also play a significant role in the classroom as
interactive teaching and learning tools, contributing to personalized
education. This paper discusses the opportunities, resources and open
challenges for each of these directions. As with any transformative technology,
integrating LLMs into education calls for careful consideration. While LLMs can
perform repetitive tasks efficiently, it's crucial to remember that their role
is to supplement human intelligence and creativity, not to replace it.
Therefore, the new era of data science education should balance the benefits of
LLMs while fostering complementary human expertise and innovations. In
conclusion, the rise of LLMs heralds a transformative period for data science
and its education. This paper seeks to shed light on the emerging trends,
potential opportunities, and challenges accompanying this paradigm shift,
hoping to spark further discourse and investigation into this exciting,
uncharted territory.",2023-07-06T06:07:29Z
,http://arxiv.org/pdf/2310.05727v1.pdf,The Program Testing Ability of Large Language Models for Code,"Recent development of large language models (LLMs) for code like CodeX and
CodeT5+ demonstrates tremendous promise in achieving code intelligence. Their
ability of synthesizing code that completes a program for performing a
pre-defined task has been intensively tested and verified on benchmark datasets
including HumanEval and MBPP. Yet, evaluation of these LLMs from more
perspectives (than just program synthesis) is also anticipated, considering
their broad scope of applications in software engineering. In this paper, we
explore the ability of LLMs for testing programs/code. By performing thorough
analyses of recent LLMs for code in program testing, we show a series of
intriguing properties of these models and demonstrate how program testing
ability of LLMs can be improved. Following recent work which utilizes generated
test cases to enhance program synthesis, we further leverage our findings in
improving the quality of the synthesized programs and show +11.77% and +4.22%
higher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline
and the recent state-of-the-art, respectively.",2023-10-09T13:55:45Z
,http://arxiv.org/pdf/2404.03543v2.pdf,"CodeEditorBench: Evaluating Code Editing Capability of Large Language
  Models","Large Language Models (LLMs) for code are rapidly evolving, with code editing
emerging as a critical capability. We introduce CodeEditorBench, an evaluation
framework designed to rigorously assess the performance of LLMs in code editing
tasks, including debugging, translating, polishing, and requirement switching.
Unlike existing benchmarks focusing solely on code generation, CodeEditorBench
emphasizes real-world scenarios and practical aspects of software development.
We curate diverse coding challenges and scenarios from five sources, covering
various programming languages, complexity levels, and editing tasks. Evaluation
of 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and
GPT-4), outperform open-source models in CodeEditorBench, highlighting
differences in model performance based on problem types and prompt
sensitivities. CodeEditorBench aims to catalyze advancements in LLMs by
providing a robust platform for assessing code editing capabilities. We will
release all prompts and datasets to enable the community to expand the dataset
and benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to
the advancement of LLMs in code editing and provide a valuable resource for
researchers and practitioners.",2024-04-04T15:49:49Z
,http://arxiv.org/pdf/2401.05399v1.pdf,Automated Assessment of Students' Code Comprehension using LLMs,"Assessing student's answers and in particular natural language answers is a
crucial challenge in the field of education. Advances in machine learning,
including transformer-based models such as Large Language Models(LLMs), have
led to significant progress in various natural language tasks. Nevertheless,
amidst the growing trend of evaluating LLMs across diverse tasks, evaluating
LLMs in the realm of automated answer assesment has not received much
attention. To address this gap, we explore the potential of using LLMs for
automated assessment of student's short and open-ended answer. Particularly, we
use LLMs to compare students' explanations with expert explanations in the
context of line-by-line explanations of computer programs.
  For comparison purposes, we assess both Large Language Models (LLMs) and
encoder-based Semantic Textual Similarity (STS) models in the context of
assessing the correctness of students' explanation of computer code. Our
findings indicate that LLMs, when prompted in few-shot and chain-of-thought
setting perform comparable to fine-tuned encoder-based models in evaluating
students' short answers in programming domain.",2023-12-19T20:39:12Z
,http://arxiv.org/pdf/2404.02540v2.pdf,CSEPrompts: A Benchmark of Introductory Computer Science Prompts,"Recent advances in AI, machine learning, and NLP have led to the development
of a new generation of Large Language Models (LLMs) that are trained on massive
amounts of data and often have trillions of parameters. Commercial applications
(e.g., ChatGPT) have made this technology available to the general public, thus
making it possible to use LLMs to produce high-quality texts for academic and
professional purposes. Schools and universities are aware of the increasing use
of AI-generated content by students and they have been researching the impact
of this new technology and its potential misuse. Educational programs in
Computer Science (CS) and related fields are particularly affected because LLMs
are also capable of generating programming code in various programming
languages. To help understand the potential impact of publicly available LLMs
in CS education, we introduce CSEPrompts, a framework with hundreds of
programming exercise prompts and multiple-choice questions retrieved from
introductory CS and programming courses. We also provide experimental results
on CSEPrompts to evaluate the performance of several LLMs with respect to
generating Python code and answering basic computer science and programming
questions.",2024-04-03T07:55:57Z
,http://arxiv.org/pdf/2402.14261v1.pdf,Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming,"The integration of Large Language Models (LLMs) into Development Environments
(IDEs) has become a focal point in modern software development. LLMs such as
OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment
developer productivity by serving as intelligent, chat-driven programming
assistants. However, utilizing LLMs out of the box is unlikely to be optimal
for any given scenario. Rather, each system requires the LLM to be honed to its
set of heuristics to ensure the best performance. In this paper, we introduce
the Copilot evaluation harness: a set of data and tools for evaluating
LLM-guided IDE interactions, covering various programming scenarios and
languages. We propose our metrics as a more robust and information-dense
evaluation than previous state of the art evaluation systems. We design and
compute both static and execution based success metrics for scenarios
encompassing a wide range of developer tasks, including code generation from
natural language (generate), documentation generation from code (doc), test
case generation (test), bug-fixing (fix), and workspace understanding and query
resolution (workspace). These success metrics are designed to evaluate the
performance of LLMs within a given IDE and its respective parameter space. Our
learnings from evaluating three common LLMs using these metrics can inform the
development and validation of future scenarios in LLM guided IDEs.",2024-02-22T03:51:34Z
,http://arxiv.org/pdf/2406.10300v1.pdf,"Large Language Models as Software Components: A Taxonomy for
  LLM-Integrated Applications","Large Language Models (LLMs) have become widely adopted recently. Research
explores their use both as autonomous agents and as tools for software
engineering. LLM-integrated applications, on the other hand, are software
systems that leverage an LLM to perform tasks that would otherwise be
impossible or require significant coding effort. While LLM-integrated
application engineering is emerging as new discipline, its terminology,
concepts and methods need to be established. This study provides a taxonomy for
LLM-integrated applications, offering a framework for analyzing and describing
these systems. It also demonstrates various ways to utilize LLMs in
applications, as well as options for implementing such integrations.
  Following established methods, we analyze a sample of recent LLM-integrated
applications to identify relevant dimensions. We evaluate the taxonomy by
applying it to additional cases. This review shows that applications integrate
LLMs in numerous ways for various purposes. Frequently, they comprise multiple
LLM integrations, which we term ``LLM components''. To gain a clear
understanding of an application's architecture, we examine each LLM component
separately. We identify thirteen dimensions along which to characterize an LLM
component, including the LLM skills leveraged, the format of the output, and
more. LLM-integrated applications are described as combinations of their LLM
components. We suggest a concise representation using feature vectors for
visualization.
  The taxonomy is effective for describing LLM-integrated applications. It can
contribute to theory building in the nascent field of LLM-integrated
application engineering and aid in developing such systems. Researchers and
practitioners explore numerous creative ways to leverage LLMs in applications.
Though challenges persist, integrating LLMs may revolutionize the way software
systems are built.",2024-06-13T21:32:56Z
,http://arxiv.org/pdf/2401.12453v1.pdf,"""The teachers are confused as well"": A Multiple-Stakeholder Ethics
  Discussion on Large Language Models in Computing Education","Large Language Models (LLMs) are advancing quickly and impacting people's
lives for better or worse. In higher education, concerns have emerged such as
students' misuse of LLMs and degraded education outcomes. To unpack the ethical
concerns of LLMs for higher education, we conducted a case study consisting of
stakeholder interviews (n=20) in higher education computer science. We found
that students use several distinct mental models to interact with LLMs - LLMs
serve as a tool for (a) writing, (b) coding, and (c) information retrieval,
which differ somewhat in ethical considerations. Students and teachers brought
up ethical issues that directly impact them, such as inaccurate LLM responses,
hallucinations, biases, privacy leakage, and academic integrity issues.
Participants emphasized the necessity of guidance and rules for the use of LLMs
in higher education, including teaching digital literacy, rethinking education,
and having cautious and contextual policies. We reflect on the ethical
challenges and propose solutions.",2024-01-23T02:43:00Z
,http://arxiv.org/pdf/2402.07081v1.pdf,"Using Large Language Models for Student-Code Guided Test Case Generation
  in Computer Science Education","In computer science education, test cases are an integral part of programming
assignments since they can be used as assessment items to test students'
programming knowledge and provide personalized feedback on student-written
code. The goal of our work is to propose a fully automated approach for test
case generation that can accurately measure student knowledge, which is
important for two reasons. First, manually constructing test cases requires
expert knowledge and is a labor-intensive process. Second, developing test
cases for students, especially those who are novice programmers, is
significantly different from those oriented toward professional-level software
developers. Therefore, we need an automated process for test case generation to
assess student knowledge and provide feedback. In this work, we propose a large
language model-based approach to automatically generate test cases and show
that they are good measures of student knowledge, using a publicly available
dataset that contains student-written Java code. We also discuss future
research directions centered on using test cases to help students.",2024-02-11T01:37:48Z
,http://arxiv.org/pdf/2406.15379v1.pdf,CS1-LLM: Integrating LLMs into CS1 Instruction,"The recent, widespread availability of Large Language Models (LLMs) like
ChatGPT and GitHub Copilot may impact introductory programming courses (CS1)
both in terms of what should be taught and how to teach it. Indeed, recent
research has shown that LLMs are capable of solving the majority of the
assignments and exams we previously used in CS1. In addition, professional
software engineers are often using these tools, raising the question of whether
we should be training our students in their use as well. This experience report
describes a CS1 course at a large research-intensive university that fully
embraces the use of LLMs from the beginning of the course. To incorporate the
LLMs, the course was intentionally altered to reduce emphasis on syntax and
writing code from scratch. Instead, the course now emphasizes skills needed to
successfully produce software with an LLM. This includes explaining code,
testing code, and decomposing large problems into small functions that are
solvable by an LLM. In addition to frequent, formative assessments of these
skills, students were given three large, open-ended projects in three separate
domains (data science, image processing, and game design) that allowed them to
showcase their creativity in topics of their choosing. In an end-of-term
survey, students reported that they appreciated learning with the assistance of
the LLM and that they interacted with the LLM in a variety of ways when writing
code. We provide lessons learned for instructors who may wish to incorporate
LLMs into their course.",2024-04-17T14:44:28Z
,http://arxiv.org/pdf/2304.03938v1.pdf,"Comparing Code Explanations Created by Students and Large Language
  Models","Reasoning about code and explaining its purpose are fundamental skills for
computer scientists. There has been extensive research in the field of
computing education on the relationship between a student's ability to explain
code and other skills such as writing and tracing code. In particular, the
ability to describe at a high-level of abstraction how code will behave over
all possible inputs correlates strongly with code writing skills. However,
developing the expertise to comprehend and explain code accurately and
succinctly is a challenge for many students. Existing pedagogical approaches
that scaffold the ability to explain code, such as producing exemplar code
explanations on demand, do not currently scale well to large classrooms. The
recent emergence of powerful large language models (LLMs) may offer a solution.
In this paper, we explore the potential of LLMs in generating explanations that
can serve as examples to scaffold students' ability to understand and explain
code. To evaluate LLM-created explanations, we compare them with explanations
created by students in a large course ($n \approx 1000$) with respect to
accuracy, understandability and length. We find that LLM-created explanations,
which can be produced automatically on demand, are rated as being significantly
easier to understand and more accurate summaries of code than student-created
explanations. We discuss the significance of this finding, and suggest how such
models can be incorporated into introductory programming education.",2023-04-08T06:52:54Z
,http://arxiv.org/pdf/2308.08572v1.pdf,"Large Language Models in Introductory Programming Education: ChatGPT's
  Performance and Implications for Assessments","This paper investigates the performance of the Large Language Models (LLMs)
ChatGPT-3.5 and GPT-4 in solving introductory programming tasks. Based on the
performance, implications for didactic scenarios and assessment formats
utilizing LLMs are derived. For the analysis, 72 Python tasks for novice
programmers were selected from the free site CodingBat. Full task descriptions
were used as input to the LLMs, while the generated replies were evaluated
using CodingBat's unit tests. In addition, the general availability of textual
explanations and program code was analyzed. The results show high scores of
94.4 to 95.8% correct responses and reliable availability of textual
explanations and program code, which opens new ways to incorporate LLMs into
programming education and assessment.",2023-08-15T19:48:31Z
,http://arxiv.org/pdf/2405.02828v1.pdf,"Trojans in Large Language Models of Code: A Critical Review through a
  Trigger-Based Taxonomy","Large language models (LLMs) have provided a lot of exciting new capabilities
in software development. However, the opaque nature of these models makes them
difficult to reason about and inspect. Their opacity gives rise to potential
security risks, as adversaries can train and deploy compromised models to
disrupt the software development process in the victims' organization.
  This work presents an overview of the current state-of-the-art trojan attacks
on large language models of code, with a focus on triggers -- the main design
point of trojans -- with the aid of a novel unifying trigger taxonomy
framework. We also aim to provide a uniform definition of the fundamental
concepts in the area of trojans in Code LLMs. Finally, we draw implications of
findings on how code models learn on trigger design.",2024-05-05T06:43:52Z
,http://arxiv.org/pdf/2406.12513v1.pdf,"Can We Trust Large Language Models Generated Code? A Framework for
  In-Context Learning, Security Patterns, and Code Evaluations Across Diverse
  LLMs","Large Language Models (LLMs) such as ChatGPT and GitHub Copilot have
revolutionized automated code generation in software engineering. However, as
these models are increasingly utilized for software development, concerns have
arisen regarding the security and quality of the generated code. These concerns
stem from LLMs being primarily trained on publicly available code repositories
and internet-based textual data, which may contain insecure code. This presents
a significant risk of perpetuating vulnerabilities in the generated code,
creating potential attack vectors for exploitation by malicious actors. Our
research aims to tackle these issues by introducing a framework for secure
behavioral learning of LLMs through In-Content Learning (ICL) patterns during
the code generation process, followed by rigorous security evaluations. To
achieve this, we have selected four diverse LLMs for experimentation. We have
evaluated these coding LLMs across three programming languages and identified
security vulnerabilities and code smells. The code is generated through ICL
with curated problem sets and undergoes rigorous security testing to evaluate
the overall quality and trustworthiness of the generated code. Our research
indicates that ICL-driven one-shot and few-shot learning patterns can enhance
code security, reducing vulnerabilities in various programming scenarios.
Developers and researchers should know that LLMs have a limited understanding
of security principles. This may lead to security breaches when the generated
code is deployed in production systems. Our research highlights LLMs are a
potential source of new vulnerabilities to the software supply chain. It is
important to consider this when using LLMs for code generation. This research
article offers insights into improving LLM security and encourages proactive
use of LLMs for code generation to ensure software system safety.",2024-06-18T11:29:34Z
,http://arxiv.org/pdf/2401.05319v1.pdf,"Leveraging Print Debugging to Improve Code Generation in Large Language
  Models","Large language models (LLMs) have made significant progress in code
generation tasks, but their performance in tackling programming problems with
complex data structures and algorithms remains suboptimal. To address this
issue, we propose an in-context learning approach that guides LLMs to debug by
using a ""print debugging"" method, which involves inserting print statements to
trace and analysing logs for fixing the bug. We collect a Leetcode problem
dataset and evaluate our method using the Leetcode online judging system.
Experiments with GPT-4 demonstrate the effectiveness of our approach,
outperforming rubber duck debugging in easy and medium-level Leetcode problems
by 1.5% and 17.9%.",2024-01-10T18:37:59Z
,http://arxiv.org/pdf/2308.13507v2.pdf,"Large Language Models Should Ask Clarifying Questions to Increase
  Confidence in Generated Code","Large language models (LLMs) have significantly improved the ability to
perform tasks in the field of code generation. However, there is still a gap
between LLMs being capable coders and being top-tier software engineers. Based
on the observation that toplevel software engineers often ask clarifying
questions to reduce ambiguity in both requirements and coding solutions, I
argue that the same should be applied to LLMs for code generation tasks. By
asking probing questions in various topics before generating the final code,
the challenges of programming with LLMs, such as unclear intent specification,
lack of computational thinking, and undesired code quality, may be alleviated.
This, in turn, increases confidence in the generated code. In this work, I
explore how to leverage better communication skills to achieve greater
confidence in generated code. I propose a communication-centered process that
uses an LLM-generated communicator to identify issues with high ambiguity or
low confidence in problem descriptions and generated code. I then ask
clarifying questions to obtain responses from users for refining the code.",2023-08-25T17:33:05Z
,http://arxiv.org/pdf/2212.11140v1.pdf,"Benchmarking Large Language Models for Automated Verilog RTL Code
  Generation","Automating hardware design could obviate a significant amount of human error
from the engineering process and lead to fewer errors. Verilog is a popular
hardware description language to model and design digital systems, thus
generating Verilog code is a critical first step. Emerging large language
models (LLMs) are able to write high-quality code in other programming
languages. In this paper, we characterize the ability of LLMs to generate
useful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets
collected from GitHub and Verilog textbooks. We construct an evaluation
framework comprising test-benches for functional analysis and a flow to test
the syntax of Verilog code generated in response to problems of varying
difficulty. Our findings show that across our problem scenarios, the
fine-tuning results in LLMs more capable of producing syntactically correct
code (25.9% overall). Further, when analyzing functional correctness, a
fine-tuned open-source CodeGen LLM can outperform the state-of-the-art
commercial Codex LLM (6.5% overall). Training/evaluation scripts and LLM
checkpoints are available: https://github.com/shailja-thakur/VGen.",2022-12-13T16:34:39Z
,http://arxiv.org/pdf/2306.05715v1.pdf,"Exploring the Responses of Large Language Models to Beginner
  Programmers' Help Requests","Background and Context: Over the past year, large language models (LLMs) have
taken the world by storm. In computing education, like in other walks of life,
many opportunities and threats have emerged as a consequence.
  Objectives: In this article, we explore such opportunities and threats in a
specific area: responding to student programmers' help requests. More
specifically, we assess how good LLMs are at identifying issues in problematic
code that students request help on.
  Method: We collected a sample of help requests and code from an online
programming course. We then prompted two different LLMs (OpenAI Codex and
GPT-3.5) to identify and explain the issues in the students' code and assessed
the LLM-generated answers both quantitatively and qualitatively.
  Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently
find at least one actual issue in each student program (GPT-3.5 in 90% of the
cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57%
of the time). False positives are common (40% chance for GPT-3.5). The advice
that the LLMs provide on the issues is often sensible. The LLMs perform better
on issues involving program logic rather than on output formatting. Model
solutions are frequently provided even when the LLM is prompted not to. LLM
responses to prompts in a non-English language are only slightly worse than
responses to English prompts.
  Implications: Our results continue to highlight the utility of LLMs in
programming education. At the same time, the results highlight the
unreliability of LLMs: LLMs make some of the same mistakes that students do,
perhaps especially when formatting output as required by automated assessment
systems. Our study informs teachers interested in using LLMs as well as future
efforts to customize LLMs for the needs of programming education.",2023-06-09T07:19:43Z
,http://arxiv.org/pdf/2406.04817v1.pdf,"Experiences from Integrating Large Language Model Chatbots into the
  Classroom","In the present study, we provided students an unfiltered access to a
state-of-the-art large language model (LLM) chatbot. The chatbot was
intentionally designed to mimic proprietary commercial chatbots such as ChatGPT
where the chatbot has not been tailored for the educational context; the
underlying engine was OpenAI GPT-4. The chatbot was integrated into online
learning materials of three courses. One of the courses focused on software
engineering with LLMs, while the two other courses were not directly related to
LLMs. Our results suggest that only a minority of students engage with the
chatbot in the courses that do not relate to LLMs. At the same time,
unsurprisingly, nearly all students in the LLM-focused course leveraged the
chatbot. In all courses, the majority of the LLM usage came from a few
superusers, whereas the majority of the students did not heavily use the
chatbot even though it was readily available and effectively provided a free
access to the OpenAI GPT-4 model. We also observe that in addition to students
using the chatbot for course-specific purposes, many use the chatbot for their
own purposes. These results suggest that the worst fears of educators -- all
students overrelying on LLMs -- did not materialize even when the chatbot
access was unfiltered. We finally discuss potential reasons for the low usage,
suggesting the need for more tailored and scaffolded LLM experiences targeted
for specific types of student use cases.",2024-06-07T10:37:14Z
,http://arxiv.org/pdf/2403.16159v2.pdf,"Designing Child-Centric AI Learning Environments: Insights from
  LLM-Enhanced Creative Project-Based Learning","Project-based learning (PBL) is an instructional method that is very helpful
in nurturing students' creativity, but it requires significant time and energy
from both students and teachers. Large language models (LLMs) have been proven
to assist in creative tasks, yet much controversy exists regarding their role
in fostering creativity. This paper explores the potential of LLMs in PBL
settings, with a special focus on fostering creativity. We began with an
exploratory study involving 12 middle school students and identified five
design considerations for LLM applications in PBL. Building on this, we
developed an LLM-empowered, 48-hour PBL program and conducted an instructional
experiment with 31 middle school students. Our results indicated that LLMs can
enhance every stage of PBL. Additionally, we also discovered ambivalent
perspectives among students and mentors toward LLM usage. Furthermore, we
explored the challenge and design implications of integrating LLMs into PBL and
reflected on the program. By bridging AI advancements into educational
practice, our work aims to inspire further discourse and investigation into
harnessing AI's potential in child-centric educational settings.",2024-03-24T13:54:05Z
,http://arxiv.org/pdf/2309.14534v3.pdf,"Teach AI How to Code: Using Large Language Models as Teachable Agents
  for Programming Education","This work investigates large language models (LLMs) as teachable agents for
learning by teaching (LBT). LBT with teachable agents helps learners identify
knowledge gaps and discover new knowledge. However, teachable agents require
expensive programming of subject-specific knowledge. While LLMs as teachable
agents can reduce the cost, LLMs' expansive knowledge as tutees discourages
learners from teaching. We propose a prompting pipeline that restrains LLMs'
knowledge and makes them initiate ""why"" and ""how"" questions for effective
knowledge-building. We combined these techniques into TeachYou, an LBT
environment for algorithm learning, and AlgoBo, an LLM-based tutee chatbot that
can simulate misconceptions and unawareness prescribed in its knowledge state.
Our technical evaluation confirmed that our prompting pipeline can effectively
configure AlgoBo's problem-solving performance. Through a between-subject study
with 40 algorithm novices, we also observed that AlgoBo's questions led to
knowledge-dense conversations (effect size=0.71). Lastly, we discuss design
implications, cost-efficiency, and personalization of LLM-based teachable
agents.",2023-09-25T21:20:04Z
,http://arxiv.org/pdf/2401.07994v1.pdf,"A Novel Approach for Automatic Program Repair using Round-Trip
  Translation with Large Language Models","Research shows that grammatical mistakes in a sentence can be corrected by
translating it to another language and back using neural machine translation
with language models. We investigate whether this correction capability of
Large Language Models (LLMs) extends to Automatic Program Repair (APR). Current
generative models for APR are pre-trained on source code and fine-tuned for
repair. This paper proposes bypassing the fine-tuning step and using Round-Trip
Translation (RTT): translation of code from one programming language to another
programming or natural language, and back. We hypothesize that RTT with LLMs
restores the most commonly seen patterns in code during pre-training, i.e.,
performs a regression toward the mean, which removes bugs as they are a form of
noise w.r.t. the more frequent, natural, bug-free code in the training data. To
test this hypothesis, we employ eight recent LLMs pre-trained on code,
including the latest GPT versions, and four common program repair benchmarks in
Java. We find that RTT with English as an intermediate language repaired 101 of
164 bugs with GPT-4 on the HumanEval-Java dataset. Moreover, 46 of these are
unique bugs that are not repaired by other LLMs fine-tuned for APR. Our
findings highlight the viability of round-trip translation with LLMs as a
technique for automated program repair and its potential for research in
software engineering.
  Keywords: automated program repair, large language model, machine translation",2024-01-15T22:36:31Z
,http://arxiv.org/pdf/2309.00029v1.pdf,"Exploring the Potential of Large Language Models to Generate Formative
  Programming Feedback","Ever since the emergence of large language models (LLMs) and related
applications, such as ChatGPT, its performance and error analysis for
programming tasks have been subject to research. In this work-in-progress
paper, we explore the potential of such LLMs for computing educators and
learners, as we analyze the feedback it generates to a given input containing
program code. In particular, we aim at (1) exploring how an LLM like ChatGPT
responds to students seeking help with their introductory programming tasks,
and (2) identifying feedback types in its responses. To achieve these goals, we
used students' programming sequences from a dataset gathered within a CS1
course as input for ChatGPT along with questions required to elicit feedback
and correct solutions. The results show that ChatGPT performs reasonably well
for some of the introductory programming tasks and student errors, which means
that students can potentially benefit. However, educators should provide
guidance on how to use the provided feedback, as it can contain misleading
information for novices.",2023-08-31T15:22:11Z
,http://arxiv.org/pdf/2404.02548v2.pdf,AI-Tutoring in Software Engineering Education,"With the rapid advancement of artificial intelligence (AI) in various
domains, the education sector is set for transformation. The potential of
AI-driven tools in enhancing the learning experience, especially in
programming, is immense. However, the scientific evaluation of Large Language
Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an
AI-Tutor remains largely unexplored. Therefore, there is a need to understand
how students interact with such AI-Tutors and to analyze their experiences. In
this paper, we conducted an exploratory case study by integrating the
GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a
combination of empirical data collection and an exploratory survey, we
identified different user types based on their interaction patterns with the
AI-Tutor. Additionally, the findings highlight advantages, such as timely
feedback and scalability. However, challenges like generic responses and
students' concerns about a learning progress inhibition when using the AI-Tutor
were also evident. This research adds to the discourse on AI's role in
education.",2024-04-03T08:15:08Z
,http://arxiv.org/pdf/2310.17807v3.pdf,Clover: Closed-Loop Verifiable Code Generation,"The use of large language models for code generation is a rapidly growing
trend in software development. However, without effective methods for ensuring
the correctness of generated code, this trend could lead to any number of
undesirable outcomes. In this paper, we lay out a vision for addressing this
challenge: the Clover paradigm, short for Closed-Loop Verifiable Code
Generation, which reduces correctness checking to the more accessible problem
of consistency checking. At the core of Clover lies a checker that performs
consistency checks among code, docstrings, and formal annotations. The checker
is implemented using a novel integration of formal verification tools and large
language models. We provide a theoretical analysis to support our thesis that
Clover should be effective at consistency checking. We also empirically
investigate its feasibility on a hand-designed dataset (CloverBench) featuring
annotated Dafny programs at a textbook level of difficulty. Experimental
results show that for this dataset, (i) LLMs are reasonably successful at
automatically generating formal specifications; and (ii) our consistency
checker achieves a promising acceptance rate (up to 87%) for correct instances
while maintaining zero tolerance for incorrect ones (no false positives).",2023-10-26T22:58:19Z
,http://arxiv.org/pdf/2402.01687v2.pdf,"""Which LLM should I use?"": Evaluating LLMs for tasks performed by
  Undergraduate Computer Science Students","This study evaluates the effectiveness of various large language models
(LLMs) in performing tasks common among undergraduate computer science
students. Although a number of research studies in the computing education
community have explored the possibility of using LLMs for a variety of tasks,
there is a lack of comprehensive research comparing different LLMs and
evaluating which LLMs are most effective for different tasks. Our research
systematically assesses some of the publicly available LLMs such as Google
Bard, ChatGPT(3.5), GitHub Copilot Chat, and Microsoft Copilot across diverse
tasks commonly encountered by undergraduate computer science students in India.
These tasks include code explanation and documentation, solving class
assignments, technical interview preparation, learning new concepts and
frameworks, and email writing. Evaluation for these tasks was carried out by
pre-final year and final year undergraduate computer science students and
provides insights into the models' strengths and limitations. This study aims
to guide students as well as instructors in selecting suitable LLMs for any
specific task and offers valuable insights on how LLMs can be used
constructively by students and instructors.",2024-01-22T15:11:36Z
,http://arxiv.org/pdf/2402.09299v1.pdf,"Trained Without My Consent: Detecting Code Inclusion In Language Models
  Trained on Code","Code auditing ensures that the developed code adheres to standards,
regulations, and copyright protection by verifying that it does not contain
code from protected sources. The recent advent of Large Language Models (LLMs)
as coding assistants in the software development process poses new challenges
for code auditing. The dataset for training these models is mainly collected
from publicly available sources. This raises the issue of intellectual property
infringement as developers' codes are already included in the dataset.
Therefore, auditing code developed using LLMs is challenging, as it is
difficult to reliably assert if an LLM used during development has been trained
on specific copyrighted codes, given that we do not have access to the training
datasets of these models. Given the non-disclosure of the training datasets,
traditional approaches such as code clone detection are insufficient for
asserting copyright infringement. To address this challenge, we propose a new
approach, TraWiC; a model-agnostic and interpretable method based on membership
inference for detecting code inclusion in an LLM's training dataset. We extract
syntactic and semantic identifiers unique to each program to train a classifier
for detecting code inclusion. In our experiments, we observe that TraWiC is
capable of detecting 83.87% of codes that were used to train an LLM. In
comparison, the prevalent clone detection tool NiCad is only capable of
detecting 47.64%. In addition to its remarkable performance, TraWiC has low
resource overhead in contrast to pair-wise clone detection that is conducted
during the auditing process of tools like CodeWhisperer reference tracker,
across thousands of code snippets.",2024-02-14T16:41:35Z
,http://arxiv.org/pdf/2402.08147v2.pdf,"VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large
  Language Model, and Tree Search","Large Language Models (LLMs) can generate useful code, but often the code
they generate cannot be trusted to be sound. In this paper, we present VerMCTS,
an approach to begin to resolve this issue by generating verified programs in
Dafny and Coq. VerMCTS uses a logical verifier in concert with an LLM to guide
a modified Monte Carlo Tree Search (MCTS). This approach leverages the verifier
to gain intermediate feedback inside the search algorithm by checking partial
programs at each step to estimate an upper bound on the value function. To
measure the performance of VerMCTS, we develop a new suite of multi-step
verified programming problems in Dafny and Coq. In terms of pass@T, a new
metric which computes the pass rate given a budget of T tokens sampled from the
LLM, VerMCTS leads to more than a 30% absolute increase in average pass@5000
across the suite over repeated sampling from the base language model. Our code
and benchmarks are available at
https://github.com/namin/llm-verified-with-monte-carlo-tree-search .",2024-02-13T00:55:14Z
,http://arxiv.org/pdf/2401.08664v3.pdf,"Adapting Large Language Models for Education: Foundational Capabilities,
  Potentials, and Challenges","Online education platforms, leveraging the internet to distribute education
resources, seek to provide convenient education but often fall short in
real-time communication with students. They often struggle to address the
diverse obstacles students encounter throughout their learning journey. Solving
the problems encountered by students poses a significant challenge for
traditional deep learning models, as it requires not only a broad spectrum of
subject knowledge but also the ability to understand what constitutes a
student's individual difficulties. It's challenging for traditional machine
learning models, as they lack the capacity to comprehend students' personalized
needs. Recently, the emergence of large language models (LLMs) offers the
possibility for resolving this issue by comprehending individual requests.
Although LLMs have been successful in various fields, creating an LLM-based
education system is still challenging for the wide range of educational skills
required. This paper reviews the recently emerged LLM research related to
educational capabilities, including mathematics, writing, programming,
reasoning, and knowledge-based question answering, with the aim to explore
their potential in constructing the next-generation intelligent education
system. Specifically, for each capability, we focus on investigating two
aspects. Firstly, we examine the current state of LLMs regarding this
capability: how advanced they have become, whether they surpass human
abilities, and what deficiencies might exist. Secondly, we evaluate whether the
development methods for LLMs in this area are generalizable, that is, whether
these methods can be applied to construct a comprehensive educational
supermodel with strengths across various capabilities, rather than being
effective in only a singular aspect.",2023-12-27T14:37:32Z
,http://arxiv.org/pdf/2305.05176v1.pdf,"FrugalGPT: How to Use Large Language Models While Reducing Cost and
  Improving Performance","There is a rapidly growing number of large language models (LLMs) that users
can query for a fee. We review the cost associated with querying popular LLM
APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have
heterogeneous pricing structures, with fees that can differ by two orders of
magnitude. In particular, using LLMs on large collections of queries and text
can be expensive. Motivated by this, we outline and discuss three types of
strategies that users can exploit to reduce the inference cost associated with
using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As
an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM
cascade which learns which combinations of LLMs to use for different queries in
order to reduce cost and improve accuracy. Our experiments show that FrugalGPT
can match the performance of the best individual LLM (e.g. GPT-4) with up to
98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.
The ideas and findings presented here lay a foundation for using LLMs
sustainably and efficiently.",2023-05-09T05:11:02Z
,http://arxiv.org/pdf/2405.16133v2.pdf,"Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via
  Code Rewriting","Large Language Models (LLMs) have exhibited remarkable proficiency in
generating code. However, the misuse of LLM-generated (Synthetic) code has
prompted concerns within both educational and industrial domains, highlighting
the imperative need for the development of synthetic code detectors. Existing
methods for detecting LLM-generated content are primarily tailored for general
text and often struggle with code content due to the distinct grammatical
structure of programming languages and massive ""low-entropy"" tokens. Building
upon this, our work proposes a novel zero-shot synthetic code detector based on
the similarity between the code and its rewritten variants. Our method relies
on the intuition that the differences between the LLM-rewritten and original
codes tend to be smaller when the original code is synthetic. We utilize
self-supervised contrastive learning to train a code similarity model and
assess our approach on two synthetic code detection benchmarks. Our results
demonstrate a notable enhancement over existing synthetic content detectors
designed for general texts, with an improvement of 20.5% in the APPS benchmark
and 29.1% in the MBPP benchmark.",2024-05-25T08:57:28Z
,http://arxiv.org/pdf/2307.10236v3.pdf,"Look Before You Leap: An Exploratory Study of Uncertainty Measurement
  for Large Language Models","The recent performance leap of Large Language Models (LLMs) opens up new
opportunities across numerous industrial applications and domains. However,
erroneous generations, such as false predictions, misinformation, and
hallucination made by LLMs, have also raised severe concerns for the
trustworthiness of LLMs', especially in safety-, security- and
reliability-sensitive scenarios, potentially hindering real-world adoptions.
While uncertainty estimation has shown its potential for interpreting the
prediction risks made by general machine learning (ML) models, little is known
about whether and to what extent it can help explore an LLM's capabilities and
counteract its undesired behavior. To bridge the gap, in this paper, we
initiate an exploratory study on the risk assessment of LLMs from the lens of
uncertainty. In particular, we experiment with twelve uncertainty estimation
methods and four LLMs on four prominent natural language processing (NLP) tasks
to investigate to what extent uncertainty estimation techniques could help
characterize the prediction risks of LLMs. Our findings validate the
effectiveness of uncertainty estimation for revealing LLMs'
uncertain/non-factual predictions. In addition to general NLP tasks, we
extensively conduct experiments with four LLMs for code generation on two
datasets. We find that uncertainty estimation can potentially uncover buggy
programs generated by LLMs. Insights from our study shed light on future design
and development for reliable LLMs, facilitating further research toward
enhancing the trustworthiness of LLMs.",2023-07-16T08:28:04Z
,http://arxiv.org/pdf/2405.02213v2.pdf,Automatic Programming: Large Language Models and Beyond,"Automatic programming has seen increasing popularity due to the emergence of
tools like GitHub Copilot which rely on Large Language Models (LLMs). At the
same time, automatically generated code faces challenges during deployment due
to concerns around quality and trust. In this article, we study automated
coding in a general sense and study the concerns around code quality, security
and related issues of programmer responsibility. These are key issues for
organizations while deciding on the usage of automatically generated code. We
discuss how advances in software engineering such as program repair and
analysis can enable automatic programming. We conclude with a forward looking
view, focusing on the programming environment of the near future, where
programmers may need to switch to different roles to fully utilize the power of
automatic programming. Automated repair of automatically generated programs
from LLMs, can help produce higher assurance code from LLMs, along with
evidence of assurance",2024-05-03T16:19:24Z
,http://arxiv.org/pdf/2308.10454v1.pdf,"Elucidating STEM Concepts through Generative AI: A Multi-modal
  Exploration of Analogical Reasoning","This study explores the integration of generative artificial intelligence
(AI), specifically large language models, with multi-modal analogical reasoning
as an innovative approach to enhance science, technology, engineering, and
mathematics (STEM) education. We have developed a novel system that utilizes
the capacities of generative AI to transform intricate principles in
mathematics, physics, and programming into comprehensible metaphors. To further
augment the educational experience, these metaphors are subsequently converted
into visual form. Our study aims to enhance the learners' understanding of STEM
concepts and their learning engagement by using the visual metaphors. We
examine the efficacy of our system via a randomized A/B/C test, assessing
learning gains and motivation shifts among the learners. Our study demonstrates
the potential of applying large language models to educational practice on STEM
subjects. The results will shed light on the design of educational system in
terms of harnessing AI's potential to empower educational stakeholders.",2023-08-21T04:00:56Z
,http://arxiv.org/pdf/2402.11702v2.pdf,"Can ChatGPT Support Developers? An Empirical Evaluation of Large
  Language Models for Code Generation","Large language models (LLMs) have demonstrated notable proficiency in code
generation, with numerous prior studies showing their promising capabilities in
various development scenarios. However, these studies mainly provide
evaluations in research settings, which leaves a significant gap in
understanding how effectively LLMs can support developers in real-world. To
address this, we conducted an empirical analysis of conversations in DevGPT, a
dataset collected from developers' conversations with ChatGPT (captured with
the Share Link feature on platforms such as GitHub). Our empirical findings
indicate that the current practice of using LLM-generated code is typically
limited to either demonstrating high-level concepts or providing examples in
documentation, rather than to be used as production-ready code. These findings
indicate that there is much future work needed to improve LLMs in code
generation before they can be integral parts of modern software development.",2024-02-18T20:48:09Z
,http://arxiv.org/pdf/2312.08055v2.pdf,Breaking the Silence: the Threats of Using LLMs in Software Engineering,"Large Language Models (LLMs) have gained considerable traction within the
Software Engineering (SE) community, impacting various SE tasks from code
completion to test generation, from program repair to code summarization.
Despite their promise, researchers must still be careful as numerous intricate
factors can influence the outcomes of experiments involving LLMs. This paper
initiates an open discussion on potential threats to the validity of LLM-based
research including issues such as closed-source models, possible data leakage
between LLM training data and research evaluation, and the reproducibility of
LLM-based findings. In response, this paper proposes a set of guidelines
tailored for SE researchers and Language Model (LM) providers to mitigate these
concerns. The implications of the guidelines are illustrated using existing
good practices followed by LLM providers and a practical example for SE
researchers in the context of test case generation.",2023-12-13T11:02:19Z
,http://arxiv.org/pdf/2310.10690v3.pdf,"Large Language Models for In-Context Student Modeling: Synthesizing
  Student's Behavior in Visual Programming","Student modeling is central to many educational technologies as it enables
predicting future learning outcomes and designing targeted instructional
strategies. However, open-ended learning domains pose challenges for accurately
modeling students due to the diverse behaviors and a large space of possible
misconceptions. To approach these challenges, we explore the application of
large language models (LLMs) for in-context student modeling in open-ended
learning domains. More concretely, given a particular student's attempt on a
reference task as observation, the objective is to synthesize the student's
attempt on a target task. We introduce a novel framework, LLM for Student
Synthesis (LLM-SS), that leverages LLMs for synthesizing a student's behavior.
Our framework can be combined with different LLMs; moreover, we fine-tune LLMs
to boost their student modeling capabilities. We instantiate several methods
based on LLM-SS framework and evaluate them using an existing benchmark,
StudentSyn, for student attempt synthesis in a visual programming domain.
Experimental results show that our methods perform significantly better than
the baseline method NeurSS provided in the StudentSyn benchmark. Furthermore,
our method using a fine-tuned version of the GPT-3.5 model is significantly
better than using the base GPT-3.5 model and gets close to human tutors'
performance.",2023-10-15T12:56:13Z
,http://arxiv.org/pdf/2310.04474v3.pdf,Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning,"While enabling large language models to implement function calling (known as
APIs) can greatly enhance the performance of Large Language Models (LLMs),
function calling is still a challenging task due to the complicated relations
between different APIs, especially in a context-learning setting without
fine-tuning. This paper introduces ``Reverse Chain'', a controllable,
target-driven approach designed to empower LLMs with the capability to operate
external APIs only via prompts. Recognizing that most LLMs have limited
tool-use capabilities, Reverse Chain limits LLMs to executing simple tasks,
e.g., API Selection and Argument Completion. Furthermore, to manage a
controllable multi-function calling, Reverse Chain adopts a generic rule based
on a backward reasoning process. This rule determines when to do API selection
or Argument completion. To evaluate the multi-tool-use capability of LLMs, we
have released a compositional multi-tool task dataset, available at
\url{https://anonymous.4open.science/r/reverse-chain-8681}. Extensive numerical
experiments validate the remarkable proficiency of Reverse Chain in managing
multiple API calls.",2023-10-06T05:20:18Z
,http://arxiv.org/pdf/2307.14936v1.pdf,"PanGu-Coder2: Boosting Large Language Models for Code with Ranking
  Feedback","Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.",2023-07-27T15:28:29Z
,http://arxiv.org/pdf/2305.14752v1.pdf,"A New Era in Software Security: Towards Self-Healing Software via Large
  Language Models and Formal Verification","In this paper we present a novel solution that combines the capabilities of
Large Language Models (LLMs) with Formal Verification strategies to verify and
automatically repair software vulnerabilities. Initially, we employ Bounded
Model Checking (BMC) to locate the software vulnerability and derive a
counterexample. The counterexample provides evidence that the system behaves
incorrectly or contains a vulnerability. The counterexample that has been
detected, along with the source code, are provided to the LLM engine. Our
approach involves establishing a specialized prompt language for conducting
code debugging and generation to understand the vulnerability's root cause and
repair the code. Finally, we use BMC to verify the corrected version of the
code generated by the LLM. As a proof of concept, we create ESBMC-AI based on
the Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained
Transformer model, specifically gpt-3.5-turbo, to detect and fix errors in C
programs. Our experimentation involved generating a dataset comprising 1000 C
code samples, each consisting of 20 to 50 lines of code. Notably, our proposed
method achieved an impressive success rate of up to 80% in repairing vulnerable
code encompassing buffer overflow and pointer dereference failures. We assert
that this automated approach can effectively incorporate into the software
development lifecycle's continuous integration and deployment (CI/CD) process.",2023-05-24T05:54:10Z
,http://arxiv.org/pdf/2304.06815v3.pdf,"Automatic Semantic Augmentation of Language Model Prompts (for Code
  Summarization)","Large Language Models (LLM) are a new class of computation engines,
""programmed"" via prompt engineering. We are still learning how to best
""program"" these LLMs to help developers. We start with the intuition that
developers tend to consciously and unconsciously have a collection of semantics
facts in mind when working on coding tasks. Mostly these are shallow, simple
facts arising from a quick read. For a function, examples of facts might
include parameter and local variable names, return expressions, simple pre- and
post-conditions, and basic control and data flow, etc.
  One might assume that the powerful multi-layer architecture of
transformer-style LLMs makes them inherently capable of doing this simple level
of ""code analysis"" and extracting such information, implicitly, while
processing code: but are they, really? If they aren't, could explicitly adding
this information help? Our goal here is to investigate this question, using the
code summarization task and evaluate whether automatically augmenting an LLM's
prompt with semantic facts explicitly, actually helps.
  Prior work shows that LLM performance on code summarization benefits from
few-shot samples drawn either from the same-project or from examples found via
information retrieval methods (such as BM25). While summarization performance
has steadily increased since the early days, there is still room for
improvement: LLM performance on code summarization still lags its performance
on natural-language tasks like translation and text summarization.
  We find that adding semantic facts actually does help! This approach improves
performance in several different settings suggested by prior work, including
for two different Large Language Models. In most cases, improvement nears or
exceeds 2 BLEU; for the PHP language in the challenging CodeSearchNet dataset,
this augmentation actually yields performance surpassing 30 BLEU.",2023-04-13T20:49:35Z
,http://arxiv.org/pdf/2401.10759v1.pdf,"Interactions with Prompt Problems: A New Way to Teach Programming with
  Large Language Models","Large Language Models (LLMs) have upended decades of pedagogy in computing
education. Students previously learned to code through \textit{writing} many
small problems with less emphasis on code reading and comprehension. Recent
research has shown that free code generation tools powered by LLMs can solve
introductory programming problems presented in natural language with ease. In
this paper, we propose a new way to teach programming with Prompt Problems.
Students receive a problem visually, indicating how input should be transformed
to output, and must translate that to a prompt for an LLM to decipher. The
problem is considered correct when the code that is generated by the student
prompt can pass all test cases. In this paper we present the design of this
tool, discuss student interactions with it as they learn, and provide insights
into this new class of programming problems as well as the design tools that
integrate LLMs.",2024-01-19T15:32:46Z
,http://arxiv.org/pdf/2402.07913v2.pdf,"QACP: An Annotated Question Answering Dataset for Assisting Chinese
  Python Programming Learners","In online learning platforms, particularly in rapidly growing computer
programming courses, addressing the thousands of students' learning queries
requires considerable human cost. The creation of intelligent assistant large
language models (LLMs) tailored for programming education necessitates distinct
data support. However, in real application scenarios, the data resources for
training such LLMs are relatively scarce. Therefore, to address the data
scarcity in intelligent educational systems for programming, this paper
proposes a new Chinese question-and-answer dataset for Python learners. To
ensure the authenticity and reliability of the sources of the questions, we
collected questions from actual student questions and categorized them
according to various dimensions such as the type of questions and the type of
learners. This annotation principle is designed to enhance the effectiveness
and quality of online programming education, providing a solid data foundation
for developing the programming teaching assists (TA). Furthermore, we conducted
comprehensive evaluations of various LLMs proficient in processing and
generating Chinese content, highlighting the potential limitations of general
LLMs as intelligent teaching assistants in computer programming courses.",2024-01-30T13:11:23Z
,http://arxiv.org/pdf/2312.15223v1.pdf,A Survey on Large Language Models for Software Engineering,"Software Engineering (SE) is the systematic design, development, and
maintenance of software applications, underpinning the digital infrastructure
of our modern mainworld. Very recently, the SE community has seen a rapidly
increasing number of techniques employing Large Language Models (LLMs) to
automate a broad range of SE tasks. Nevertheless, existing information of the
applications, effects, and possible limitations of LLMs within SE is still not
well-studied.
  In this paper, we provide a systematic survey to summarize the current
state-of-the-art research in the LLM-based SE community. We summarize 30
representative LLMs of Source Code across three model architectures, 15
pre-training objectives across four categories, and 16 downstream tasks across
five categories. We then present a detailed summarization of the recent SE
studies for which LLMs are commonly utilized, including 155 studies for 43
specific code-related tasks across four crucial phases within the SE workflow.
Besides, we summarize existing attempts to empirically evaluate LLMs in SE,
such as benchmarks, empirical studies, and exploration of SE education. We also
discuss several critical aspects of optimization and applications of LLMs in
SE, such as security attacks, model tuning, and model compression. Finally, we
highlight several challenges and potential opportunities on applying LLMs for
future SE studies, such as exploring domain LLMs and constructing clean
evaluation datasets. Overall, our work can help researchers gain a
comprehensive understanding about the achievements of the existing LLM-based SE
studies and promote the practical application of these techniques. Our
artifacts are publicly available and will continuously updated at the living
repository: \url{https://github.com/iSEngLab/AwesomeLLM4SE}.",2023-12-23T11:09:40Z
,http://arxiv.org/pdf/2306.03438v2.pdf,"Large Language Models of Code Fail at Completing Code with Potential
  Bugs","Large language models of code (Code-LLMs) have recently brought tremendous
advances to code completion, a fundamental feature of programming assistance
and code intelligence. However, most existing works ignore the possible
presence of bugs in the code context for generation, which are inevitable in
software development. Therefore, we introduce and study the buggy-code
completion problem, inspired by the realistic scenario of real-time code
suggestion where the code context contains potential bugs -- anti-patterns that
can become bugs in the completed program. To systematically study the task, we
introduce two datasets: one with synthetic bugs derived from semantics-altering
operator changes (buggy-HumanEval) and one with realistic bugs derived from
user submissions to coding problems (buggy-FixEval). We find that the presence
of potential bugs significantly degrades the generation performance of the
high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO
on test cases of buggy-HumanEval drop more than 50% given a single potential
bug in the context. Finally, we investigate several post-hoc methods for
mitigating the adverse effect of potential bugs and find that there remains a
significant gap in post-mitigation performance.",2023-06-06T06:35:27Z
,http://arxiv.org/pdf/2404.14824v1.pdf,"Automated Commit Message Generation with Large Language Models: An
  Empirical Study and Beyond","Commit Message Generation (CMG) approaches aim to automatically generate
commit messages based on given code diffs, which facilitate collaboration among
developers and play a critical role in Open-Source Software (OSS). Very
recently, Large Language Models (LLMs) have demonstrated extensive
applicability in diverse code-related task. But few studies systematically
explored their effectiveness using LLMs. This paper conducts the first
comprehensive experiment to investigate how far we have been in applying LLM to
generate high-quality commit messages. Motivated by a pilot analysis, we first
clean the most widely-used CMG dataset following practitioners' criteria.
Afterward, we re-evaluate diverse state-of-the-art CMG approaches and make
comparisons with LLMs, demonstrating the superior performance of LLMs against
state-of-the-art CMG approaches. Then, we further propose four manual metrics
following the practice of OSS, including Accuracy, Integrity, Applicability,
and Readability, and assess various LLMs accordingly. Results reveal that
GPT-3.5 performs best overall, but different LLMs carry different advantages.
To further boost LLMs' performance in the CMG task, we propose an Efficient
Retrieval-based In-Context Learning (ICL) framework, namely ERICommiter, which
leverages a two-step filtering to accelerate the retrieval efficiency and
introduces semantic/lexical-based retrieval algorithm to construct the ICL
examples. Extensive experiments demonstrate the substantial performance
improvement of ERICommiter on various LLMs for code diffs of different
programming languages. Meanwhile, ERICommiter also significantly reduces the
retrieval time while keeping almost the same performance. Our research
contributes to the understanding of LLMs' capabilities in the CMG field and
provides valuable insights for practitioners seeking to leverage these tools in
their workflows.",2024-04-23T08:24:43Z
,http://arxiv.org/pdf/2311.03033v1.pdf,"Beyond Words: A Mathematical Framework for Interpreting Large Language
  Models","Large language models (LLMs) are powerful AI tools that can generate and
comprehend natural language text and other complex information. However, the
field lacks a mathematical framework to systematically describe, compare and
improve LLMs. We propose Hex a framework that clarifies key terms and concepts
in LLM research, such as hallucinations, alignment, self-verification and
chain-of-thought reasoning. The Hex framework offers a precise and consistent
way to characterize LLMs, identify their strengths and weaknesses, and
integrate new findings. Using Hex, we differentiate chain-of-thought reasoning
from chain-of-thought prompting and establish the conditions under which they
are equivalent. This distinction clarifies the basic assumptions behind
chain-of-thought prompting and its implications for methods that use it, such
as self-verification and prompt programming.
  Our goal is to provide a formal framework for LLMs that can help both
researchers and practitioners explore new possibilities for generative AI. We
do not claim to have a definitive solution, but rather a tool for opening up
new research avenues. We argue that our formal definitions and results are
crucial for advancing the discussion on how to build generative AI systems that
are safe, reliable, fair and robust, especially in domains like healthcare and
software engineering.",2023-11-06T11:13:17Z
,http://arxiv.org/pdf/2403.06254v1.pdf,"LLMs Still Can't Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4
  and Bard's Capacity to Handle Object-Oriented Programming Assignments","Large Language Models (LLMs) have emerged as promising tools to assist
students while solving programming assignments. However, object-oriented
programming (OOP), with its inherent complexity involving the identification of
entities, relationships, and responsibilities, is not yet mastered by these
tools. Contrary to introductory programming exercises, there exists a research
gap with regard to the behavior of LLMs in OOP contexts. In this study, we
experimented with three prominent LLMs - GPT-3.5, GPT-4, and Bard - to solve
real-world OOP exercises used in educational settings, subsequently validating
their solutions using an Automatic Assessment Tool (AAT). The findings revealed
that while the models frequently achieved mostly working solutions to the
exercises, they often overlooked the best practices of OOP. GPT-4 stood out as
the most proficient, followed by GPT-3.5, with Bard trailing last. We advocate
for a renewed emphasis on code quality when employing these models and explore
the potential of pairing LLMs with AATs in pedagogical settings. In conclusion,
while GPT-4 showcases promise, the deployment of these models in OOP education
still mandates supervision.",2024-03-10T16:40:05Z
,http://arxiv.org/pdf/2205.12615v1.pdf,Autoformalization with Large Language Models,"Autoformalization is the process of automatically translating from natural
language mathematics to formal specifications and proofs. A successful
autoformalization system could advance the fields of formal verification,
program synthesis, and artificial intelligence. While the long-term goal of
autoformalization seemed elusive for a long time, we show large language models
provide new prospects towards this goal. We make the surprising observation
that LLMs can correctly translate a significant portion ($25.3\%$) of
mathematical competition problems perfectly to formal specifications in
Isabelle/HOL. We demonstrate the usefulness of this process by improving a
previously introduced neural theorem prover via training on these
autoformalized theorems. Our methodology results in a new state-of-the-art
result on the MiniF2F theorem proving benchmark, improving the proof rate from
$29.6\%$ to $35.2\%$.",2022-05-25T09:53:30Z
,http://arxiv.org/pdf/2309.00608v3.pdf,"Copiloting the Copilots: Fusing Large Language Models with Completion
  Engines for Automated Program Repair","During Automated Program Repair (APR), it can be challenging to synthesize
correct patches for real-world systems in general-purpose programming
languages. Recent Large Language Models (LLMs) have been shown to be helpful
""copilots"" in assisting developers with various coding tasks, and have also
been directly applied for patch synthesis. However, most LLMs treat programs as
sequences of tokens, meaning that they are ignorant of the underlying semantics
constraints of the target programming language. This results in plenty of
statically invalid generated patches, impeding the practicality of the
technique. Therefore, we propose Repilot, a general code generation framework
to further copilot the AI ""copilots"" (i.e., LLMs) by synthesizing more valid
patches during the repair process. Our key insight is that many LLMs produce
outputs autoregressively (i.e., token by token), resembling human writing
programs, which can be significantly boosted and guided through a Completion
Engine. Repilot synergistically synthesizes a candidate patch through the
interaction between an LLM and a Completion Engine, which 1) prunes away
infeasible tokens suggested by the LLM and 2) proactively completes the token
based on the suggestions provided by the Completion Engine. Our evaluation on a
subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot
outperforms state-of-the-art techniques by fixing 27% and 47% more bugs,
respectively. Moreover, Repilot produces more valid and correct patches than
the base LLM with the same budget. While we focus on leveraging Repilot for APR
in this work, the overall approach is also generalizable to other code
generation tasks.",2023-09-01T17:54:14Z
,http://arxiv.org/pdf/2406.08216v1.pdf,"A Software Engineering Perspective on Testing Large Language Models:
  Research, Practice, Tools and Benchmarks","Large Language Models (LLMs) are rapidly becoming ubiquitous both as
stand-alone tools and as components of current and future software systems. To
enable usage of LLMs in the high-stake or safety-critical systems of 2030, they
need to undergo rigorous testing. Software Engineering (SE) research on testing
Machine Learning (ML) components and ML-based systems has systematically
explored many topics such as test input generation and robustness. We believe
knowledge about tools, benchmarks, research and practitioner views related to
LLM testing needs to be similarly organized. To this end, we present a taxonomy
of LLM testing topics and conduct preliminary studies of state of the art and
practice approaches to research, open-source tools and benchmarks for LLM
testing, mapping results onto this taxonomy. Our goal is to identify gaps
requiring more research and engineering effort and inspire a clearer
communication between LLM practitioners and the SE research community.",2024-06-12T13:45:45Z
,http://arxiv.org/pdf/2406.06451v1.pdf,"Insights from Social Shaping Theory: The Appropriation of Large Language
  Models in an Undergraduate Programming Course","The capability of large language models (LLMs) to generate, debug, and
explain code has sparked the interest of researchers and educators in
undergraduate programming, with many anticipating their transformative
potential in programming education. However, decisions about why and how to use
LLMs in programming education may involve more than just the assessment of an
LLM's technical capabilities. Using the social shaping of technology theory as
a guiding framework, our study explores how students' social perceptions
influence their own LLM usage. We then examine the correlation of self-reported
LLM usage with students' self-efficacy and midterm performances in an
undergraduate programming course. Triangulating data from an anonymous
end-of-course student survey (n = 158), a mid-course self-efficacy survey
(n=158), student interviews (n = 10), self-reported LLM usage on homework, and
midterm performances, we discovered that students' use of LLMs was associated
with their expectations for their future careers and their perceptions of peer
usage. Additionally, early self-reported LLM usage in our context correlated
with lower self-efficacy and lower midterm scores, while students' perceived
over-reliance on LLMs, rather than their usage itself, correlated with
decreased self-efficacy later in the course.",2024-06-10T16:40:14Z
,http://arxiv.org/pdf/2404.08029v1.pdf,"A Multi-Expert Large Language Model Architecture for Verilog Code
  Generation","Recently, there has been a surging interest in using large language models
(LLMs) for Verilog code generation. However, the existing approaches are
limited in terms of the quality of the generated Verilog code. To address such
limitations, this paper introduces an innovative multi-expert LLM architecture
for Verilog code generation (MEV-LLM). Our architecture uniquely integrates
multiple LLMs, each specifically fine-tuned with a dataset that is categorized
with respect to a distinct level of design complexity. It allows more targeted
learning, directly addressing the nuances of generating Verilog code for each
category. Empirical evidence from experiments highlights notable improvements
in terms of the percentage of generated Verilog outputs that are syntactically
and functionally correct. These findings underscore the efficacy of our
approach, promising a forward leap in the field of automated hardware design
through machine learning.",2024-04-11T16:58:29Z
,http://arxiv.org/pdf/2310.06225v2.pdf,"GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using
  Large Language Models","Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs' performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4's ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models' capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.",2023-10-10T00:39:04Z
,http://arxiv.org/pdf/2406.07573v1.pdf,"Investigating the Potential of Using Large Language Models for
  Scheduling","The inaugural ACM International Conference on AI-powered Software introduced
the AIware Challenge, prompting researchers to explore AI-driven tools for
optimizing conference programs through constrained optimization. We investigate
the use of Large Language Models (LLMs) for program scheduling, focusing on
zero-shot learning and integer programming to measure paper similarity. Our
study reveals that LLMs, even under zero-shot settings, create reasonably good
first drafts of conference schedules. When clustering papers, using only titles
as LLM inputs produces results closer to human categorization than using titles
and abstracts with TFIDF. The code has been made publicly available.",2024-06-04T08:56:56Z
,http://arxiv.org/pdf/2307.07924v5.pdf,ChatDev: Communicative Agents for Software Development,"Software development is a complex task that necessitates cooperation among
multiple members with diverse skills. Numerous studies used deep learning to
improve specific phases in a waterfall model, such as design, coding, and
testing. However, the deep learning model in each phase requires unique
designs, leading to technical inconsistencies across various phases, which
results in a fragmented and ineffective development process. In this paper, we
introduce ChatDev, a chat-powered software development framework in which
specialized agents driven by large language models (LLMs) are guided in what to
communicate (via chat chain) and how to communicate (via communicative
dehallucination). These agents actively contribute to the design, coding, and
testing phases through unified language-based communication, with solutions
derived from their multi-turn dialogues. We found their utilization of natural
language is advantageous for system design, and communicating in programming
language proves helpful in debugging. This paradigm demonstrates how linguistic
communication facilitates multi-agent collaboration, establishing language as a
unifying bridge for autonomous task-solving among LLM agents. The code and data
are available at https://github.com/OpenBMB/ChatDev.",2023-07-16T02:11:34Z
,http://arxiv.org/pdf/2402.15100v1.pdf,Studying LLM Performance on Closed- and Open-source Data,"Large Language models (LLMs) are finding wide use in software engineering
practice. These models are extremely data-hungry, and are largely trained on
open-source (OSS) code distributed with permissive licenses. In terms of actual
use however, a great deal of software development still occurs in the
for-profit/proprietary sphere, where the code under development is not, and
never has been, in the public domain; thus, many developers, do their work, and
use LLMs, in settings where the models may not be as familiar with the code
under development. In such settings, do LLMs work as well as they do for OSS
code? If not, what are the differences? When performance differs, what are the
possible causes, and are there work-arounds? In this paper, we examine this
issue using proprietary, closed-source software data from Microsoft, where most
proprietary code is in C# and C++. We find that performance for C# changes
little from OSS --> proprietary code, but does significantly reduce for C++; we
find that this difference is attributable to differences in identifiers. We
also find that some performance degradation, in some cases, can be ameliorated
efficiently by in-context learning.",2024-02-23T05:17:28Z
,http://arxiv.org/pdf/2405.05253v1.pdf,"Open Source Language Models Can Provide Feedback: Evaluating LLMs'
  Ability to Help Students Using GPT-4-As-A-Judge","Large language models (LLMs) have shown great potential for the automatic
generation of feedback in a wide range of computing contexts. However, concerns
have been voiced around the privacy and ethical implications of sending student
work to proprietary models. This has sparked considerable interest in the use
of open source LLMs in education, but the quality of the feedback that such
open models can produce remains understudied. This is a concern as providing
flawed or misleading generated feedback could be detrimental to student
learning. Inspired by recent work that has utilised very powerful LLMs, such as
GPT-4, to evaluate the outputs produced by less powerful models, we conduct an
automated analysis of the quality of the feedback produced by several open
source models using a dataset from an introductory programming course. First,
we investigate the viability of employing GPT-4 as an automated evaluator by
comparing its evaluations with those of a human expert. We observe that GPT-4
demonstrates a bias toward positively rating feedback while exhibiting moderate
agreement with human raters, showcasing its potential as a feedback evaluator.
Second, we explore the quality of feedback generated by several leading
open-source LLMs by using GPT-4 to evaluate the feedback. We find that some
models offer competitive performance with popular proprietary LLMs, such as
ChatGPT, indicating opportunities for their responsible use in educational
settings.",2024-05-08T17:57:39Z
,http://arxiv.org/pdf/2401.00761v1.pdf,The Earth is Flat? Unveiling Factual Errors in Large Language Models,"Large Language Models (LLMs) like ChatGPT are foundational in various
applications due to their extensive knowledge from pre-training and
fine-tuning. Despite this, they are prone to generating factual and commonsense
errors, raising concerns in critical areas like healthcare, journalism, and
education to mislead users. Current methods for evaluating LLMs' veracity are
limited by test data leakage or the need for extensive human labor, hindering
efficient and accurate error detection. To tackle this problem, we introduce a
novel, automatic testing framework, FactChecker, aimed at uncovering factual
inaccuracies in LLMs. This framework involves three main steps: First, it
constructs a factual knowledge graph by retrieving fact triplets from a
large-scale knowledge database. Then, leveraging the knowledge graph,
FactChecker employs a rule-based approach to generates three types of questions
(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and
multi-hop relations, along with correct answers. Lastly, it assesses the LLMs'
responses for accuracy using tailored matching strategies for each question
type. Our extensive tests on six prominent LLMs, including text-davinci-002,
text-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal
that FactChecker can trigger factual errors in up to 45\% of questions in these
models. Moreover, we demonstrate that FactChecker's test cases can improve
LLMs' factual accuracy through in-context learning and fine-tuning (e.g.,
llama-2-13b-chat's accuracy increase from 35.3\% to 68.5\%). We are making all
code, data, and results available for future research endeavors.",2024-01-01T14:02:27Z
,http://arxiv.org/pdf/2307.07699v1.pdf,Leveraging Large Language Models to Generate Answer Set Programs,"Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated
exceptional performance in various natural language processing tasks and have
shown the ability to solve certain reasoning problems. However, their reasoning
capabilities are limited and relatively shallow, despite the application of
various prompting techniques. In contrast, formal logic is adept at handling
complex reasoning, but translating natural language descriptions into formal
logic is a challenging task that non-experts struggle with. This paper proposes
a neuro-symbolic method that combines the strengths of large language models
and answer set programming. Specifically, we employ an LLM to transform natural
language descriptions of logic puzzles into answer set programs. We carefully
design prompts for an LLM to convert natural language descriptions into answer
set programs in a step by step manner. Surprisingly, with just a few in-context
learning examples, LLMs can generate reasonably complex answer set programs.
The majority of errors made are relatively simple and can be easily corrected
by humans, thus enabling LLMs to effectively assist in the creation of answer
set programs.",2023-07-15T03:40:55Z
,http://arxiv.org/pdf/2310.06266v2.pdf,CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,"Code Large Language Models (Code LLMs) have gained significant attention in
the industry due to their wide applications in the full lifecycle of software
engineering. However, the effectiveness of existing models in understanding
non-English inputs for multi-lingual code-related tasks is still far from well
studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code
LLM. It is specifically designed for code-related tasks with both English and
Chinese prompts and supports over 40 programming languages. CodeFuse achieves
its effectiveness by utilizing a high quality pre-training dataset that is
carefully filtered by program analyzers and optimized during the training
process. Extensive experiments are conducted using real-world usage scenarios,
the industry-standard benchmark HumanEval-x, and the specially designed
CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we
actively collected valuable human feedback from the AntGroup's software
development process where CodeFuse has been successfully deployed. The results
demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%,
positioning it as one of the top multi-lingual code LLMs with similar parameter
sizes. In practical scenarios, such as code generation, code translation, code
comments, and testcase generation, CodeFuse performs better than other models
when confronted with Chinese prompts.",2023-10-10T02:38:44Z
,http://arxiv.org/pdf/2404.08877v1.pdf,Aligning LLMs for FL-free Program Repair,"Large language models (LLMs) have achieved decent results on automated
program repair (APR). However, the next token prediction training objective of
decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction
objective of current infilling-style methods, which impedes LLMs from fully
leveraging pre-trained knowledge for program repair. In addition, while some
LLMs are capable of locating and repairing bugs end-to-end when using the
related artifacts (e.g., test cases) as input, existing methods regard them as
separate tasks and ask LLMs to generate patches at fixed locations. This
restriction hinders LLMs from exploring potential patches beyond the given
locations.
  In this paper, we investigate a new approach to adapt LLMs to program repair.
Our core insight is that LLM's APR capability can be greatly improved by simply
aligning the output to their training objective and allowing them to refine the
whole program without first performing fault localization. Based on this
insight, we designed D4C, a straightforward prompting framework for APR. D4C
can repair 180 bugs correctly in Defects4J, with each patch being sampled only
10 times. This surpasses the SOTA APR methods with perfect fault localization
by 10% and reduces the patch sampling number by 90%. Our findings reveal that
(1) objective alignment is crucial for fully exploiting LLM's pre-trained
capability, and (2) replacing the traditional localize-then-repair workflow
with direct debugging is more effective for LLM-based APR methods. Thus, we
believe this paper introduces a new mindset for harnessing LLMs in APR.",2024-04-13T02:36:40Z
,http://arxiv.org/pdf/2404.18496v1.pdf,AI-powered Code Review with LLMs: Early Results,"In this paper, we present a novel approach to improving software quality and
efficiency through a Large Language Model (LLM)-based model designed to review
code and identify potential issues. Our proposed LLM-based AI agent model is
trained on large code repositories. This training includes code reviews, bug
reports, and documentation of best practices. It aims to detect code smells,
identify potential bugs, provide suggestions for improvement, and optimize the
code. Unlike traditional static code analysis tools, our LLM-based AI agent has
the ability to predict future potential risks in the code. This supports a dual
goal of improving code quality and enhancing developer education by encouraging
a deeper understanding of best practices and efficient coding techniques.
Furthermore, we explore the model's effectiveness in suggesting improvements
that significantly reduce post-release bugs and enhance code review processes,
as evidenced by an analysis of developer sentiment toward LLM feedback. For
future work, we aim to assess the accuracy and efficiency of LLM-generated
documentation updates in comparison to manual methods. This will involve an
empirical study focusing on manually conducted code reviews to identify code
smells and bugs, alongside an evaluation of best practice documentation,
augmented by insights from developer discussions and code reviews. Our goal is
to not only refine the accuracy of our LLM-based tool but also to underscore
its potential in streamlining the software development lifecycle through
proactive code improvement and education.",2024-04-29T08:27:50Z
,http://arxiv.org/pdf/2310.13712v2.pdf,"Impact of Guidance and Interaction Strategies for LLM Use on Learner
  Performance and Perception","Personalized chatbot-based teaching assistants can be crucial in addressing
increasing classroom sizes, especially where direct teacher presence is
limited. Large language models (LLMs) offer a promising avenue, with increasing
research exploring their educational utility. However, the challenge lies not
only in establishing the efficacy of LLMs but also in discerning the nuances of
interaction between learners and these models, which impact learners'
engagement and results. We conducted a formative study in an undergraduate
computer science classroom (N=145) and a controlled experiment on Prolific
(N=356) to explore the impact of four pedagogically informed guidance
strategies on the learners' performance, confidence and trust in LLMs. Direct
LLM answers marginally improved performance, while refining student solutions
fostered trust. Structured guidance reduced random queries as well as instances
of students copy-pasting assignment questions to the LLM. Our work highlights
the role that teachers can play in shaping LLM-supported learning environments.",2023-10-13T01:21:52Z
,http://arxiv.org/pdf/2306.01220v2.pdf,"Do Large Language Models Pay Similar Attention Like Human Programmers
  When Generating Code?","Large Language Models (LLMs) have recently been widely used for code
generation. Due to the complexity and opacity of LLMs, little is known about
how these models generate code. We made the first attempt to bridge this
knowledge gap by investigating whether LLMs attend to the same parts of a task
description as human programmers during code generation. An analysis of six
LLMs, including GPT-4, on two popular code generation benchmarks revealed a
consistent misalignment between LLMs' and programmers' attention. We manually
analyzed 211 incorrect code snippets and found five attention patterns that can
be used to explain many code generation errors. Finally, a user study showed
that model attention computed by a perturbation-based method is often favored
by human programmers. Our findings highlight the need for human-aligned LLMs
for better interpretability and programmer trust.",2023-06-02T00:57:03Z
,http://arxiv.org/pdf/2401.05856v1.pdf,"Seven Failure Points When Engineering a Retrieval Augmented Generation
  System","Software engineers are increasingly adding semantic search capabilities to
applications using a strategy known as Retrieval Augmented Generation (RAG). A
RAG system involves finding documents that semantically match a query and then
passing the documents to a large language model (LLM) such as ChatGPT to
extract the right answer using an LLM. RAG systems aim to: a) reduce the
problem of hallucinated responses from LLMs, b) link sources/references to
generated responses, and c) remove the need for annotating documents with
meta-data. However, RAG systems suffer from limitations inherent to information
retrieval systems and from reliance on LLMs. In this paper, we present an
experience report on the failure points of RAG systems from three case studies
from separate domains: research, education, and biomedical. We share the
lessons learned and present 7 failure points to consider when designing a RAG
system. The two key takeaways arising from our work are: 1) validation of a RAG
system is only feasible during operation, and 2) the robustness of a RAG system
evolves rather than designed in at the start. We conclude with a list of
potential research directions on RAG systems for the software engineering
community.",2024-01-11T12:04:11Z
,http://arxiv.org/pdf/2310.01726v1.pdf,Large Language Models for Test-Free Fault Localization,"Fault Localization (FL) aims to automatically localize buggy lines of code, a
key first step in many manual and automatic debugging tasks. Previous FL
techniques assume the provision of input tests, and often require extensive
program analysis, program instrumentation, or data preprocessing. Prior work on
deep learning for APR struggles to learn from small datasets and produces
limited results on real-world programs. Inspired by the ability of large
language models (LLMs) of code to adapt to new tasks based on very few
examples, we investigate the applicability of LLMs to line level fault
localization. Specifically, we propose to overcome the left-to-right nature of
LLMs by fine-tuning a small set of bidirectional adapter layers on top of the
representations learned by LLMs to produce LLMAO, the first language model
based fault localization approach that locates buggy lines of code without any
test coverage information. We fine-tune LLMs with 350 million, 6 billion, and
16 billion parameters on small, manually curated corpora of buggy programs such
as the Defects4J corpus. We observe that our technique achieves substantially
more confidence in fault localization when built on the larger models, with bug
localization performance scaling consistently with the LLM size. Our empirical
evaluation shows that LLMAO improves the Top-1 results over the
state-of-the-art machine learning fault localization (MLFL) baselines by
2.3%-54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FL
technique trained using a language model architecture that can detect security
vulnerabilities down to the code line level.",2023-10-03T01:26:39Z
,http://arxiv.org/pdf/2401.09042v1.pdf,LLMs for Relational Reasoning: How Far are We?,"Large language models (LLMs) have revolutionized many areas (e.g. natural
language processing, software engineering, etc.) by achieving state-of-the-art
performance on extensive downstream tasks. Aiming to achieve robust and general
artificial intelligence, there has been a surge of interest in investigating
the reasoning ability of the LLMs. Whereas the textual and numerical reasoning
benchmarks adopted by previous works are rather shallow and simple, it is hard
to conclude that the LLMs possess strong reasoning ability by merely achieving
positive results on these benchmarks. Recent efforts have demonstrated that the
LLMs are poor at solving sequential decision-making problems that require
common-sense planning by evaluating their performance on the reinforcement
learning benchmarks. In this work, we conduct an in-depth assessment of several
state-of-the-art LLMs' reasoning ability based on the inductive logic
programming (ILP) benchmark, which is broadly recognized as a representative
and challenging measurement for evaluating logic program induction/synthesis
systems as it requires inducing strict cause-effect logic to achieve robust
deduction on independent and identically distributed (IID) and
out-of-distribution (OOD) test samples. Our evaluations illustrate that
compared with the neural program induction systems which are much smaller in
model size, the state-of-the-art LLMs are much poorer in terms of reasoning
ability by achieving much lower performance and generalization using either
natural language prompting or truth-value matrix prompting.",2024-01-17T08:22:52Z
,http://arxiv.org/pdf/2405.05455v1.pdf,"Automated Program Repair: Emerging trends pose and expose problems for
  benchmarks","Machine learning (ML) now pervades the field of Automated Program Repair
(APR). Algorithms deploy neural machine translation and large language models
(LLMs) to generate software patches, among other tasks. But, there are
important differences between these applications of ML and earlier work.
Evaluations and comparisons must take care to ensure that results are valid and
likely to generalize. A challenge is that the most popular APR evaluation
benchmarks were not designed with ML techniques in mind. This is especially
true for LLMs, whose large and often poorly-disclosed training datasets may
include problems on which they are evaluated.",2024-05-08T23:09:43Z
,http://arxiv.org/pdf/2404.12636v2.pdf,Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs,"Large language models (LLMs) have demonstrated remarkable capabilities on a
broad spectrum of downstream tasks. Within the realm of software engineering,
specialized tasks on code, such as program repair, present unique challenges,
necessitating fine-tuning to unlock state-of-the-art performance. Fine-tuning
approaches proposed in the literature for LLMs on program repair tasks are
however generally overlooking the need to reason about the logic behind code
changes, beyond syntactic patterns in the data. High-performing fine-tuning
experiments also usually come at very high computational costs. With MORepair,
we propose a novel perspective on the learning focus of LLM fine-tuning for
program repair: we not only adapt the LLM parameters to the syntactic nuances
of the task of code transformation (objective 1), but we also specifically
fine-tune the LLM with respect to the logical reason behind the code change in
the training data (objective 2). Such a multi-objective fine-tuning will
instruct LLMs to generate high-quality patches.
  We apply MORepair to fine-tune four open-source LLMs with different sizes and
architectures. Experimental results on C++ and Java repair benchmarks show that
the implemented fine-tuning effectively boosts LLM repair performance by 7.6%
to 10% in Top-10 repair suggestions. We further show that our fine-tuning
strategy yields superior performance compared to the incumbent state-of-the-art
in fine-tuned models for program repair, Fine-tune-CoT and RepairLLaMA.",2024-04-19T05:36:21Z
,http://arxiv.org/pdf/2311.18450v3.pdf,Lessons from Building StackSpot AI: A Contextualized AI Coding Assistant,"With their exceptional natural language processing capabilities, tools based
on Large Language Models (LLMs) like ChatGPT and Co-Pilot have swiftly become
indispensable resources in the software developer's toolkit. While recent
studies suggest the potential productivity gains these tools can unlock, users
still encounter drawbacks, such as generic or incorrect answers. Additionally,
the pursuit of improved responses often leads to extensive prompt engineering
efforts, diverting valuable time from writing code that delivers actual value.
To address these challenges, a new breed of tools, built atop LLMs, is
emerging. These tools aim to mitigate drawbacks by employing techniques like
fine-tuning or enriching user prompts with contextualized information.
  In this paper, we delve into the lessons learned by a software development
team venturing into the creation of such a contextualized LLM-based
application, using retrieval-based techniques, called CodeBuddy. Over a
four-month period, the team, despite lacking prior professional experience in
LLM-based applications, built the product from scratch. Following the initial
product release, we engaged with the development team responsible for the code
generative components. Through interviews and analysis of the application's
issue tracker, we uncover various intriguing challenges that teams working on
LLM-based applications might encounter. For instance, we found three main group
of lessons: LLM-based lessons, User-based lessons, and Technical lessons. By
understanding these lessons, software development teams could become better
prepared to build LLM-based applications.",2023-11-30T10:51:26Z
,http://arxiv.org/pdf/2401.17163v2.pdf,"Learning Agent-based Modeling with LLM Companions: Experiences of
  Novices and Experts Using ChatGPT & NetLogo Chat","Large Language Models (LLMs) have the potential to fundamentally change the
way people engage in computer programming. Agent-based modeling (ABM) has
become ubiquitous in natural and social sciences and education, yet no prior
studies have explored the potential of LLMs to assist it. We designed NetLogo
Chat to support the learning and practice of NetLogo, a programming language
for ABM. To understand how users perceive, use, and need LLM-based interfaces,
we interviewed 30 participants from global academia, industry, and graduate
schools. Experts reported more perceived benefits than novices and were more
inclined to adopt LLMs in their workflow. We found significant differences
between experts and novices in their perceptions, behaviors, and needs for
human-AI collaboration. We surfaced a knowledge gap between experts and novices
as a possible reason for the benefit gap. We identified guidance,
personalization, and integration as major needs for LLM-based interfaces to
support the programming of ABM.",2024-01-30T16:49:50Z
,http://arxiv.org/pdf/2308.03873v1.pdf,"Evaluating and Explaining Large Language Models for Code Using Syntactic
  Structures","Large Language Models (LLMs) for code are a family of high-parameter,
transformer-based neural networks pre-trained on massive datasets of both
natural and programming languages. These models are rapidly being employed in
commercial AI-based developer tools, such as GitHub CoPilot. However, measuring
and explaining their effectiveness on programming tasks is a challenging
proposition, given their size and complexity. The methods for evaluating and
explaining LLMs for code are inextricably linked. That is, in order to explain
a model's predictions, they must be reliably mapped to fine-grained,
understandable concepts. Once this mapping is achieved, new methods for
detailed model evaluations are possible. However, most current explainability
techniques and evaluation benchmarks focus on model robustness or individual
task performance, as opposed to interpreting model predictions.
  To this end, this paper introduces ASTxplainer, an explainability method
specific to LLMs for code that enables both new methods for LLM evaluation and
visualizations of LLM predictions that aid end-users in understanding model
predictions. At its core, ASTxplainer provides an automated method for aligning
token predictions with AST nodes, by extracting and aggregating normalized
model logits within AST structures. To demonstrate the practical benefit of
ASTxplainer, we illustrate the insights that our framework can provide by
performing an empirical evaluation on 12 popular LLMs for code using a curated
dataset of the most popular GitHub projects. Additionally, we perform a user
study examining the usefulness of an ASTxplainer-derived visualization of model
predictions aimed at enabling model users to explain predictions. The results
of these studies illustrate the potential for ASTxplainer to provide insights
into LLM effectiveness, and aid end-users in understanding predictions.",2023-08-07T18:50:57Z
,http://arxiv.org/pdf/2403.16087v1.pdf,LLMs as Compiler for Arabic Programming Language,"In this paper we introduce APL (Arabic Programming Language) that uses Large
language models (LLM) as semi-compiler to covert Arabic text code to python
code then run the code. Designing a full pipeline from the structure of the APL
text then a prompt (using prompt engineering) then running the prodcued python
code using PyRunner. This project has a three parts first python library, a
playground with simple interface and this research paper.",2024-03-24T10:57:08Z
,http://arxiv.org/pdf/2404.14419v1.pdf,"Enhancing Fault Detection for Large Language Models via Mutation-Based
  Confidence Smoothing","Large language models (LLMs) achieved great success in multiple application
domains and attracted huge attention from different research communities
recently. Unfortunately, even for the best LLM, there still exist many faults
that LLM cannot correctly predict. Such faults will harm the usability of LLMs.
How to quickly reveal them in LLMs is important, but challenging. The reasons
are twofold, 1) the heavy labeling effort for preparing the test data, and 2)
accessing closed-source LLMs such as GPT4 is money-required. To handle this
problem, in the traditional deep learning testing field, test selection methods
have been proposed for efficiently testing deep learning models by prioritizing
faults. However, the usefulness of these methods on LLMs is unclear and under
exploration. In this paper, we first study the effectiveness of existing fault
detection methods for LLMs. Experimental results on four different
tasks~(including both code tasks and natural language processing tasks) and
four LLMs (e.g., LLaMA and GPT4) demonstrated that existing fault detection
methods cannot perform well on LLMs (e.g., seven out of eight methods perform
worse than random selection on LLaMA). To enhance existing fault detection
methods, we propose MuCS, a prompt Mutation-based prediction Confidence
Smoothing method for LLMs. Concretely, we mutate the prompts and compute the
average prediction confidence of all mutants as the input of fault detection
methods. The results show that our proposed solution significantly enhances
existing methods with the improvement of test relative coverage by up to
97.64%.",2024-04-14T07:06:12Z
,http://arxiv.org/pdf/2405.03734v1.pdf,"FOKE: A Personalized and Explainable Education Framework Integrating
  Foundation Models, Knowledge Graphs, and Prompt Engineering","Integrating large language models (LLMs) and knowledge graphs (KGs) holds
great promise for revolutionizing intelligent education, but challenges remain
in achieving personalization, interactivity, and explainability. We propose
FOKE, a Forest Of Knowledge and Education framework that synergizes foundation
models, knowledge graphs, and prompt engineering to address these challenges.
FOKE introduces three key innovations: (1) a hierarchical knowledge forest for
structured domain knowledge representation; (2) a multi-dimensional user
profiling mechanism for comprehensive learner modeling; and (3) an interactive
prompt engineering scheme for generating precise and tailored learning
guidance.
  We showcase FOKE's application in programming education, homework assessment,
and learning path planning, demonstrating its effectiveness and practicality.
Additionally, we implement Scholar Hero, a real-world instantiation of FOKE.
Our research highlights the potential of integrating foundation models,
knowledge graphs, and prompt engineering to revolutionize intelligent education
practices, ultimately benefiting learners worldwide. FOKE provides a principled
and unified approach to harnessing cutting-edge AI technologies for
personalized, interactive, and explainable educational services, paving the way
for further research and development in this critical direction.",2024-05-06T15:11:05Z
,http://arxiv.org/pdf/2308.16557v1.pdf,"Effective Test Generation Using Pre-trained Large Language Models and
  Mutation Testing","One of the critical phases in software development is software testing.
Testing helps with identifying potential bugs and reducing maintenance costs.
The goal of automated test generation tools is to ease the development of tests
by suggesting efficient bug-revealing tests. Recently, researchers have
leveraged Large Language Models (LLMs) of code to generate unit tests. While
the code coverage of generated tests was usually assessed, the literature has
acknowledged that the coverage is weakly correlated with the efficiency of
tests in bug detection. To improve over this limitation, in this paper, we
introduce MuTAP for improving the effectiveness of test cases generated by LLMs
in terms of revealing bugs by leveraging mutation testing. Our goal is achieved
by augmenting prompts with surviving mutants, as those mutants highlight the
limitations of test cases in detecting bugs. MuTAP is capable of generating
effective test cases in the absence of natural language descriptions of the
Program Under Test (PUTs). We employ different LLMs within MuTAP and evaluate
their performance on different benchmarks. Our results show that our proposed
method is able to detect up to 28% more faulty human-written code snippets.
Among these, 17% remained undetected by both the current state-of-the-art fully
automated test generation tool (i.e., Pynguin) and zero-shot/few-shot learning
approaches on LLMs. Furthermore, MuTAP achieves a Mutation Score (MS) of 93.57%
on synthetic buggy code, outperforming all other approaches in our evaluation.
Our findings suggest that although LLMs can serve as a useful tool to generate
test cases, they require specific post-processing steps to enhance the
effectiveness of the generated test cases which may suffer from syntactic or
functional errors and may be ineffective in detecting certain types of bugs and
testing corner cases PUTs.",2023-08-31T08:48:31Z
,http://arxiv.org/pdf/2310.09748v1.pdf,Large Language Model-Aware In-Context Learning for Code Generation,"Large language models (LLMs) have shown impressive in-context learning (ICL)
ability in code generation. LLMs take a prompt consisting of requirement-code
examples and a new requirement as input, and output new programs. Existing
studies have found that ICL is highly dominated by the examples and thus arises
research on example selection. However, existing approaches randomly select
examples or only consider the textual similarity of requirements to retrieve,
leading to sub-optimal performance. In this paper, we propose a novel
learning-based selection approach named LAIL (LLM-Aware In-context Learning)
for code generation. Given a candidate example, we exploit LLMs themselves to
estimate it by considering the generation probabilities of ground-truth
programs given a requirement and the example. We then label candidate examples
as positive or negative through the probability feedback. Based on the labeled
data, we import a contrastive learning objective to train an effective
retriever that acquires the preference of LLMs in code generation. We apply
LAIL to three LLMs and evaluate it on three representative datasets (e.g.,
MBJP, MBPP, and MBCPP). LATA outperforms the state-of-the-art baselines by
11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5 in
terms of Pass@1, respectively.",2023-10-15T06:12:58Z
,http://arxiv.org/pdf/2305.17145v1.pdf,Type Prediction With Program Decomposition and Fill-in-the-Type Training,"TypeScript and Python are two programming languages that support optional
type annotations, which are useful but tedious to introduce and maintain. This
has motivated automated type prediction: given an untyped program, produce a
well-typed output program. Large language models (LLMs) are promising for type
prediction, but there are challenges: fill-in-the-middle performs poorly,
programs may not fit into the context window, generated types may not type
check, and it is difficult to measure how well-typed the output program is. We
address these challenges by building OpenTau, a search-based approach for type
prediction that leverages large language models. We propose a new metric for
type prediction quality, give a tree-based program decomposition that searches
a space of generated types, and present fill-in-the-type fine-tuning for LLMs.
We evaluate our work with a new dataset for TypeScript type prediction, and
show that 47.4% of files type check (14.5% absolute improvement) with an
overall rate of 3.3 type errors per file. All code, data, and models are
available at: https://github.com/GammaTauAI/opentau.",2023-05-25T21:16:09Z
,http://arxiv.org/pdf/2403.09032v1.pdf,"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language
  Models to Coding Preferences","Evaluating the alignment of large language models (LLMs) with user-defined
coding preferences is a challenging endeavour that requires assessing intricate
textual LLMs' outputs. By relying on automated metrics and static analysis
tools, existing benchmarks fail to assess nuances in user instructions and LLM
outputs, highlighting the need for large-scale datasets and benchmarks for LLM
preference alignment. In this paper, we introduce CodeUltraFeedback, a
preference dataset of 10,000 complex instructions to tune and align LLMs to
coding preferences through AI feedback. We generate responses to the
instructions using a pool of 14 diverse LLMs, which we then annotate according
to their alignment with five coding preferences using the LLM-as-a-Judge
approach with GPT-3.5, producing both numerical and textual feedback. We also
present CODAL-Bench, a benchmark for assessing LLM alignment with these coding
preferences. Our results show that CodeLlama-7B-Instruct, aligned through
reinforcement learning from AI feedback (RLAIF) with direct preference
optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B
LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference
tuning. Furthermore, we show our DPO-aligned CodeLlama model improves
functional correctness on HumanEval+ compared to the unaligned base model.
Therefore, our contributions bridge the gap in preference tuning of LLMs for
code and set the stage for further advancements in model alignment and RLAIF
for code intelligence. Our code and data are available at
https://github.com/martin-wey/CodeUltraFeedback.",2024-03-14T01:51:35Z
,http://arxiv.org/pdf/2306.03324v2.pdf,Impact of Large Language Models on Generating Software Specifications,"Software specifications are essential for ensuring the reliability of
software systems. Existing specification extraction approaches, however, suffer
from limited generalizability and require manual efforts. The recent emergence
of Large Language Models (LLMs), which have been successfully applied to
numerous software engineering tasks, offers a promising avenue for automating
this process. In this paper, we conduct the first empirical study to evaluate
the capabilities of LLMs for generating software specifications from software
comments or documentation. We evaluate LLMs' performance with Few Shot Learning
(FSL), enabling LLMs to generalize from a small number of examples, as well as
different prompt construction strategies, and compare the performance of LLMs
with traditional approaches. Additionally, we conduct a comparative diagnosis
of the failure cases from both LLMs and traditional methods, identifying their
unique strengths and weaknesses. Lastly, we conduct extensive experiments on 15
state of the art LLMs, evaluating their performance and cost effectiveness for
generating software specifications.
  Our results show that with FSL, LLMs outperform traditional methods (by
5.6%), and more sophisticated prompt construction strategies can further
enlarge this performance gap (up to 5.1 to 10.0%). Yet, LLMs suffer from their
unique challenges, such as ineffective prompts and the lack of domain
knowledge, which together account for 53 to 60% of LLM unique failures. The
strong performance of open source models (e.g., StarCoder) makes closed source
models (e.g., GPT 3 Davinci) less desirable due to size and cost. Our study
offers valuable insights for future research to improve specification
generation.",2023-06-06T00:28:39Z
,http://arxiv.org/pdf/2403.08430v1.pdf,"Search-based Optimisation of LLM Learning Shots for Story Point
  Estimation","One of the ways Large Language Models (LLMs) are used to perform machine
learning tasks is to provide them with a few examples before asking them to
produce a prediction. This is a meta-learning process known as few-shot
learning. In this paper, we use available Search-Based methods to optimise the
number and combination of examples that can improve an LLM's estimation
performance, when it is used to estimate story points for new agile tasks. Our
preliminary results show that our SBSE technique improves the estimation
performance of the LLM by 59.34% on average (in terms of mean absolute error of
the estimation) over three datasets against a zero-shot setting.",2024-03-13T11:29:37Z
,http://arxiv.org/pdf/2404.07940v1.pdf,"InfiCoder-Eval: Systematically Evaluating the Question-Answering
  Capabilities of Code Large Language Models","Large Language Models for understanding and generating code (code LLMs) have
witnessed tremendous progress in recent years. With the rapid development of
code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and
MBPP, have emerged to measure the performance of code LLMs with a particular
focus on code generation tasks. However, they are insufficient to cover the
full range of expected capabilities of code LLMs, which span beyond code
generation to answering diverse coding-related questions. To fill this gap, we
propose InfiCoder-Eval, a large-scale freeform question-answering (QA)
benchmark for code, comprising 234 carefully selected high-quality Stack
Overflow questions that span across 15 programming languages. To evaluate the
response correctness, InfiCoder-Eval supports four types of model-free metrics
and domain experts carefully choose and concretize the criterion for each
question. We conduct a systematic evaluation for more than 80 code LLMs on
InfiCoder-Eval, leading to a series of insightful findings. Furthermore, our
detailed analyses showcase possible directions for further improvement of code
LLMs. InfiCoder-Eval is fully open source at
https://infi-coder.github.io/inficoder-eval/ and continuously maintaining and
expanding to foster more scientific and systematic practices for evaluating
code LLMs.",2024-03-11T02:06:30Z
,http://arxiv.org/pdf/2308.10462v2.pdf,"Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation
  with Large Language Models","Large Language Models (LLMs) demonstrate impressive capabilities to generate
accurate code snippets given natural language intents in zero-shot, i.e.,
without the need for specific fine-tuning. While prior studies have highlighted
the advantages of fine-tuning LLMs, this process incurs high computational
costs, making it impractical in resource-scarce environments, particularly for
models with billions of parameters. To address these challenges, previous
research explored In-Context Learning (ICL) as a strategy to guide the LLM
generative process with task-specific prompt examples. However, ICL introduces
inconveniences, such as the need for designing contextually relevant prompts
and the absence of learning task-specific parameters, thereby limiting
downstream task performance. In this context, we foresee Parameter-Efficient
Fine-Tuning (PEFT) techniques as a promising approach to efficiently specialize
LLMs to task-specific data while maintaining reasonable resource consumption.
In this paper, we deliver a comprehensive study of PEFT techniques for LLMs
under the automated code generation scenario. Our comprehensive investigation
of PEFT techniques for LLMs reveals their superiority and potential over ICL
across a diverse set of LLMs. Additionally, we demonstrate the extended
capabilities of PEFT, showcasing its ability to learn from two distinct
datasets jointly without compromising performance. Furthermore, our study
highlights the potential for tuning larger LLMs and significant reductions in
memory usage by combining PEFT with quantization. Therefore, this study opens
opportunities for broader applications of PEFT in software engineering
scenarios. Our code is available at
https://github.com/martin-wey/peft-llm-code/.",2023-08-21T04:31:06Z
,http://arxiv.org/pdf/2308.10088v2.pdf,"PACE: Improving Prompt with Actor-Critic Editing for Large Language
  Model","Large language models (LLMs) have showcased remarkable potential across
various tasks by conditioning on prompts. However, the quality of different
human-written prompts leads to substantial discrepancies in LLMs' performance,
and improving prompts usually necessitates considerable human effort and
expertise. To this end, this paper proposes Prompt with Actor-Critic Editing
(PACE) for LLMs to enable automatic prompt editing. Drawing inspiration from
the actor-critic algorithm in reinforcement learning, PACE leverages LLMs as
the dual roles of actors and critics, conceptualizing prompt as a type of
policy. PACE refines prompt, taking into account the feedback from both actors
performing prompt and critics criticizing response. This process helps LLMs
better align prompt to a specific task, thanks to real responses and thinking
from LLMs. We conduct extensive experiments on 24 instruction induction tasks
and 21 big-bench tasks. Experimental results indicate that PACE elevates the
relative performance of medium/low-quality human-written prompts by up to 98\%,
which has comparable performance to high-quality human-written prompts.
Moreover, PACE also exhibits notable efficacy for prompt generation.",2023-08-19T18:47:44Z
,http://arxiv.org/pdf/2406.09701v1.pdf,"Towards Effectively Detecting and Explaining Vulnerabilities Using Large
  Language Models","Software vulnerabilities pose significant risks to the security and integrity
of software systems. Prior studies have proposed a series of approaches to
vulnerability detection using deep learning or pre-trained models. However,
there is still a lack of vulnerability's detailed explanation for understanding
apart from detecting its occurrence. Recently, large language models (LLMs)
have shown a remarkable capability in the comprehension of complicated context
and content generation, which brings opportunities for the detection and
explanation of vulnerabilities of LLMs. In this paper, we conduct a
comprehensive study to investigate the capabilities of LLMs in detecting and
explaining vulnerabilities and propose LLMVulExp, a framework that utilizes
LLMs for vulnerability detection and explanation. Under specialized fine-tuning
for vulnerability explanation, LLMVulExp not only detects the types of
vulnerabilities in the code but also analyzes the code context to generate the
cause, location, and repair suggestions for these vulnerabilities. We find that
LLMVulExp can effectively enable the LLMs to perform vulnerability detection
(e.g., over 90% F1 score on SeVC dataset) and explanation. We also explore the
potential of using advanced strategies such as Chain-of-Thought (CoT) to guide
the LLMs concentrating on vulnerability-prone code and achieve promising
results.",2024-06-14T04:01:25Z
,http://arxiv.org/pdf/2208.08289v3.pdf,CCTEST: Testing and Repairing Code Completion Systems,"Code completion, a highly valuable topic in the software development domain,
has been increasingly promoted for use by recent advances in large language
models (LLMs). To date, visible LLM-based code completion frameworks such as
GitHub Copilot and GPT are trained using deep learning over vast quantities of
unstructured text and open source code. As the paramount component and the
cornerstone in daily programming tasks, code completion has largely boosted
professionals' efficiency in building real-world software systems. In contrast
to this flourishing market, we find that code completion systems often output
suspicious results, and to date, an automated testing and enhancement framework
for code completion systems is not available. This research proposes CCTEST, a
framework to test and repair code completion systems in blackbox settings.
CCTEST features a set of novel mutation strategies, namely program
structure-correlated (PSC) mutations, to generate mutated code completion
inputs. Then, it detects inconsistent outputs, representing possibly erroneous
cases, from all the completed code cases. Moreover, CCTEST repairs the code
completion outputs by selecting the output that mostly reflects the ""average""
appearance of all output cases, as the final output of the code completion
systems. We detected a total of 33,540 inputs (with a true positive rate of
86%) that can trigger erroneous cases from eight popular LLM-based code
completion systems. With repairing, we show that the accuracy of code
completion systems is notably increased by 40% and 67% with respect to BLEU
score and Levenshtein edit similarity.",2022-08-17T13:37:03Z
,http://arxiv.org/pdf/2405.15729v2.pdf,Optimizing Large Language Models for OpenAPI Code Completion,"Recent advancements in Large Language Models (LLMs) and their utilization in
code generation tasks have significantly reshaped the field of software
development. Despite the remarkable efficacy of code completion solutions in
mainstream programming languages, their performance lags when applied to less
ubiquitous formats such as OpenAPI definitions. This study evaluates the
OpenAPI completion performance of GitHub Copilot, a prevalent commercial code
completion tool, and proposes a set of task-specific optimizations leveraging
Meta's open-source model Code Llama. A semantics-aware OpenAPI completion
benchmark proposed in this research is used to perform a series of experiments
through which the impact of various prompt-engineering and fine-tuning
techniques on the Code Llama model's performance is analyzed. The fine-tuned
Code Llama model reaches a peak correctness improvement of 55.2% over GitHub
Copilot despite utilizing 25 times fewer parameters than the commercial
solution's underlying Codex model. Additionally, this research proposes an
enhancement to a widely used code infilling training technique, addressing the
issue of underperformance when the model is prompted with context sizes smaller
than those used during training. The dataset, the benchmark, and the model
fine-tuning code are made publicly available.",2024-05-24T17:19:03Z
,http://arxiv.org/pdf/2312.03173v1.pdf,"A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in
  Programming Education","There is a constant need for educators to develop and maintain effective
up-to-date assessments. While there is a growing body of research in computing
education on utilizing large language models (LLMs) in generation and
engagement with coding exercises, the use of LLMs for generating programming
MCQs has not been extensively explored. We analyzed the capability of GPT-4 to
produce multiple-choice questions (MCQs) aligned with specific learning
objectives (LOs) from Python programming classes in higher education.
Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs
from high-level course context and module-level LOs. We evaluated 651
LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python
courses. We found that GPT-4 was capable of producing MCQs with clear language,
a single correct choice, and high-quality distractors. We also observed that
the generated MCQs appeared to be well-aligned with the LOs. Our findings can
be leveraged by educators wishing to take advantage of the state-of-the-art
generative models to support MCQ authoring efforts.",2023-12-05T22:29:43Z
,http://arxiv.org/pdf/2401.10745v1.pdf,"Ethical Artificial Intelligence Principles and Guidelines for the
  Governance and Utilization of Highly Advanced Large Language Models","Given the success of ChatGPT, LaMDA and other large language models (LLMs),
there has been an increase in development and usage of LLMs within the
technology sector and other sectors. While the level in which LLMs has not
reached a level where it has surpassed human intelligence, there will be a time
when it will. Such LLMs can be referred to as advanced LLMs. Currently, there
are limited usage of ethical artificial intelligence (AI) principles and
guidelines addressing advanced LLMs due to the fact that we have not reached
that point yet. However, this is a problem as once we do reach that point, we
will not be adequately prepared to deal with the aftermath of it in an ethical
and optimal way, which will lead to undesired and unexpected consequences. This
paper addresses this issue by discussing what ethical AI principles and
guidelines can be used to address highly advanced LLMs.",2023-12-19T06:28:43Z
,http://arxiv.org/pdf/2403.14668v1.pdf,"Predicting Learning Performance with Large Language Models: A Study in
  Adult Literacy","Intelligent Tutoring Systems (ITSs) have significantly enhanced adult
literacy training, a key factor for societal participation, employment
opportunities, and lifelong learning. Our study investigates the application of
advanced AI models, including Large Language Models (LLMs) like GPT-4, for
predicting learning performance in adult literacy programs in ITSs. This
research is motivated by the potential of LLMs to predict learning performance
based on its inherent reasoning and computational capabilities. By using
reading comprehension datasets from the ITS, AutoTutor, we evaluate the
predictive capabilities of GPT-4 versus traditional machine learning methods in
predicting learning performance through five-fold cross-validation techniques.
Our findings show that the GPT-4 presents the competitive predictive abilities
with traditional machine learning methods such as Bayesian Knowledge Tracing,
Performance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor
factorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained
on local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected
XGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior
performance compared to local machine execution. Moreover, our investigation
into hyper-parameter tuning by GPT-4 versus grid-search suggests comparable
performance, albeit with less stability in the automated approach, using
XGBoost as the case study. Our study contributes to the field by highlighting
the potential of integrating LLMs with traditional machine learning models to
enhance predictive accuracy and personalize adult literacy education, setting a
foundation for future research in applying LLMs within ITSs.",2024-03-04T08:14:07Z
,http://arxiv.org/pdf/2308.00708v1.pdf,VeriGen: A Large Language Model for Verilog Code Generation,"In this study, we explore the capability of Large Language Models (LLMs) to
automate hardware design by generating high-quality Verilog code, a common
language for designing and modeling digital systems. We fine-tune pre-existing
LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We
evaluate the functional correctness of the generated Verilog code using a
specially designed test suite, featuring a custom problem set and testing
benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the
commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase.
Upon testing with a more diverse and complex problem set, we find that the
fine-tuned model shows competitive performance against state-of-the-art
gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41%
improvement in generating syntactically correct Verilog code across various
problem categories compared to its pre-trained counterpart, highlighting the
potential of smaller, in-house LLMs in hardware design automation.",2023-07-28T02:57:14Z
,http://arxiv.org/pdf/2308.10345v1.pdf,Can Large Language Models Find And Fix Vulnerable Software?,"In this study, we evaluated the capability of Large Language Models (LLMs),
particularly OpenAI's GPT-4, in detecting software vulnerabilities, comparing
their performance against traditional static code analyzers like Snyk and
Fortify. Our analysis covered numerous repositories, including those from NASA
and the Department of Defense. GPT-4 identified approximately four times the
vulnerabilities than its counterparts. Furthermore, it provided viable fixes
for each vulnerability, demonstrating a low rate of false positives. Our tests
encompassed 129 code samples across eight programming languages, revealing the
highest vulnerabilities in PHP and JavaScript. GPT-4's code corrections led to
a 90% reduction in vulnerabilities, requiring only an 11% increase in code
lines. A critical insight was LLMs' ability to self-audit, suggesting fixes for
their identified vulnerabilities and underscoring their precision. Future
research should explore system-level vulnerabilities and integrate multiple
static code analyzers for a holistic perspective on LLMs' potential.",2023-08-20T19:33:12Z
,http://arxiv.org/pdf/2401.16797v2.pdf,"Enhancing Translation Validation of Compiler Transformations with Large
  Language Models","This paper presents a framework that integrates Large Language Models (LLMs)
into translation validation, targeting LLVM compiler transformations where
formal verification tools fall short. Our framework first utilizes existing
formal verification tools for translation validation. In this work, we use
Alive2, a well-known tool in LLVM compiler verification, as an example. When
formal verification tools are unable to confirm a transformation's soundness,
our framework employs fine-tuned LLMs for prediction. It then applies fuzzing
to transformations predicted as potentially unsound by the LLMs due to return
values or memory inconsistencies, aiming to find counterexamples. In cases
where transformations are unsound for other reasons or sound, or if no
counterexamples emerge, the framework directly reports these outcomes without
further fuzzing. This methodology has shown effectiveness in complex
application such as deep-learning accelerator designs, where traditional formal
verification tools struggle.",2024-01-30T07:24:04Z
,http://arxiv.org/pdf/2402.02388v1.pdf,"Solution-oriented Agent-based Models Generation with Verifier-assisted
  Iterative In-context Learning","Agent-based models (ABMs) stand as an essential paradigm for proposing and
validating hypothetical solutions or policies aimed at addressing challenges
posed by complex systems and achieving various objectives. This process demands
labor-intensive endeavors and multidisciplinary expertise. Large language
models (LLMs) encapsulating cross-domain knowledge and programming proficiency
could potentially alleviate the difficulty of this process. However, LLMs excel
in handling sequential information, making it challenging for analyzing the
intricate interactions and nonlinear dynamics inherent in ABMs. Additionally,
due to the lack of self-evaluation capability of LLMs, relying solely on LLMs
is insufficient to effectively accomplish this process. In this paper, we
present SAGE, a general solution-oriented ABM generation framework designed for
automatic modeling and generating solutions for targeted problems. Unlike
approaches reliant on expert handcrafting or resource-intensive neural network
training, SAGE establishes a verifier-assisted iterative in-context learning
process employing large language models (LLMs) to leverages their inherent
cross-domain knowledge for tackling intricate demands from diverse domain
scenarios. In SAGE, we introduce an semi-structured conceptual representation
expliciting the intricate structures of ABMs and an objective representation to
guide LLMs in modeling scenarios and proposing hypothetical solutions through
in-context learning. To ensure the model executability and solution
feasibility, SAGE devises a two-level verifier with chain-of-thought prompting
tailored to the complex interactions and non-linear dynamics of ABMs, driving
the iterative generation optimization. Moreover, we construct an evaluation
dataset of solution-oriented ABMs from open sources.It contains practical
models across various domains.",2024-02-04T07:59:06Z
,http://arxiv.org/pdf/2403.15600v1.pdf,"Just another copy and paste? Comparing the security vulnerabilities of
  ChatGPT generated code and StackOverflow answers","Sonatype's 2023 report found that 97% of developers and security leads
integrate generative Artificial Intelligence (AI), particularly Large Language
Models (LLMs), into their development process. Concerns about the security
implications of this trend have been raised. Developers are now weighing the
benefits and risks of LLMs against other relied-upon information sources, such
as StackOverflow (SO), requiring empirical data to inform their choice. In this
work, our goal is to raise software developers awareness of the security
implications when selecting code snippets by empirically comparing the
vulnerabilities of ChatGPT and StackOverflow. To achieve this, we used an
existing Java dataset from SO with security-related questions and answers.
Then, we asked ChatGPT the same SO questions, gathering the generated code for
comparison. After curating the dataset, we analyzed the number and types of
Common Weakness Enumeration (CWE) vulnerabilities of 108 snippets from each
platform using CodeQL. ChatGPT-generated code contained 248 vulnerabilities
compared to the 302 vulnerabilities found in SO snippets, producing 20% fewer
vulnerabilities with a statistically significant difference. Additionally,
ChatGPT generated 19 types of CWE, fewer than the 22 found in SO. Our findings
suggest developers are under-educated on insecure code propagation from both
platforms, as we found 274 unique vulnerabilities and 25 types of CWE. Any code
copied and pasted, created by AI or humans, cannot be trusted blindly,
requiring good software engineering practices to reduce risk. Future work can
help minimize insecure code propagation from any platform.",2024-03-22T20:06:41Z
,http://arxiv.org/pdf/2405.06806v1.pdf,"An Empirical Study on the Effectiveness of Large Language Models for
  SATD Identification and Classification","Self-Admitted Technical Debt (SATD), a concept highlighting sub-optimal
choices in software development documented in code comments or other project
resources, poses challenges in the maintainability and evolution of software
systems. Large language models (LLMs) have demonstrated significant
effectiveness across a broad range of software tasks, especially in software
text generation tasks. Nonetheless, their effectiveness in tasks related to
SATD is still under-researched. In this paper, we investigate the efficacy of
LLMs in both identification and classification of SATD. For both tasks, we
investigate the performance gain from using more recent LLMs, specifically the
Flan-T5 family, across different common usage settings. Our results demonstrate
that for SATD identification, all fine-tuned LLMs outperform the best existing
non-LLM baseline, i.e., the CNN model, with a 4.4% to 7.2% improvement in F1
score. In the SATD classification task, while our largest fine-tuned model,
Flan-T5-XL, still led in performance, the CNN model exhibited competitive
results, even surpassing four of six LLMs. We also found that the largest
Flan-T5 model, i.e., Flan-T5-XXL, when used with a zero-shot in-context
learning (ICL) approach for SATD identification, provides competitive results
with traditional approaches but performs 6.4% to 9.2% worse than fine-tuned
LLMs. For SATD classification, few-shot ICL approach, incorporating examples
and category descriptions in prompts, outperforms the zero-shot approach and
even surpasses the fine-tuned smaller Flan-T5 models. Moreover, our experiments
demonstrate that incorporating contextual information, such as surrounding
code, into the SATD classification task enables larger fine-tuned LLMs to
improve their performance.",2024-05-10T20:39:24Z
,http://arxiv.org/pdf/2406.04379v1.pdf,"VHDL-Eval: A Framework for Evaluating Large Language Models in VHDL Code
  Generation","With the unprecedented advancements in Large Language Models (LLMs), their
application domains have expanded to include code generation tasks across
various programming languages. While significant progress has been made in
enhancing LLMs for popular programming languages, there exists a notable gap in
comprehensive evaluation frameworks tailored for Hardware Description Languages
(HDLs), particularly VHDL. This paper addresses this gap by introducing a
comprehensive evaluation framework designed specifically for assessing LLM
performance in VHDL code generation task. We construct a dataset for evaluating
LLMs on VHDL code generation task. This dataset is constructed by translating a
collection of Verilog evaluation problems to VHDL and aggregating publicly
available VHDL problems, resulting in a total of 202 problems. To assess the
functional correctness of the generated VHDL code, we utilize a curated set of
self-verifying testbenches specifically designed for those aggregated VHDL
problem set. We conduct an initial evaluation of different LLMs and their
variants, including zero-shot code generation, in-context learning (ICL), and
Parameter-efficient fine-tuning (PEFT) methods. Our findings underscore the
considerable challenges faced by existing LLMs in VHDL code generation,
revealing significant scope for improvement. This study emphasizes the
necessity of supervised fine-tuning code generation models specifically for
VHDL, offering potential benefits to VHDL designers seeking efficient code
generation solutions.",2024-06-06T00:06:50Z
,http://arxiv.org/pdf/2404.17739v2.pdf,How LLMs Aid in UML Modeling: An Exploratory Study with Novice Analysts,"Since the emergence of GPT-3, Large Language Models (LLMs) have caught the
eyes of researchers, practitioners, and educators in the field of software
engineering. However, there has been relatively little investigation regarding
the performance of LLMs in assisting with requirements analysis and UML
modeling. This paper explores how LLMs can assist novice analysts in creating
three types of typical UML models: use case models, class diagrams, and
sequence diagrams. For this purpose, we designed the modeling tasks of these
three UML models for 45 undergraduate students who participated in a
requirements modeling course, with the help of LLMs. By analyzing their project
reports, we found that LLMs can assist undergraduate students as novice
analysts in UML modeling tasks, but LLMs also have shortcomings and limitations
that should be considered when using them.",2024-04-27T00:38:20Z
,http://arxiv.org/pdf/2401.15468v1.pdf,"Large Language Model for Vulnerability Detection: Emerging Results and
  Future Directions","Previous learning-based vulnerability detection methods relied on either
medium-sized pre-trained models or smaller neural networks from scratch. Recent
advancements in Large Pre-Trained Language Models (LLMs) have showcased
remarkable few-shot learning capabilities in various tasks. However, the
effectiveness of LLMs in detecting software vulnerabilities is largely
unexplored. This paper aims to bridge this gap by exploring how LLMs perform
with various prompts, particularly focusing on two state-of-the-art LLMs:
GPT-3.5 and GPT-4. Our experimental results showed that GPT-3.5 achieves
competitive performance with the prior state-of-the-art vulnerability detection
approach and GPT-4 consistently outperformed the state-of-the-art.",2024-01-27T17:39:36Z
,http://arxiv.org/pdf/2403.04814v3.pdf,Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks,"We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for
evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM)
task. This benchmark focuses on syntax-aware completions of program structures
such as code blocks and conditional expressions, and includes 17,720 examples
from multiple programming languages, sourced from recent code submissions after
April 2022 to minimize data contamination. SAFIM provides a robust framework
with various prompt designs and novel syntax-aware post-processing techniques,
facilitating accurate and fair comparisons across LLMs. Our comprehensive
evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM
proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our
findings challenge conventional beliefs and suggest that pretraining methods
and data quality have more impact than model size. SAFIM thus serves as a
foundational platform for future research in effective pretraining strategies
for code LLMs. The evaluation toolkit and dataset are available at
https://github.com/gonglinyuan/safim, and the leaderboard is available at
https://safimbenchmark.com.",2024-03-07T05:05:56Z
,http://arxiv.org/pdf/2312.17025v3.pdf,Experiential Co-Learning of Software-Developing Agents,"Recent advancements in large language models (LLMs) have brought significant
changes to various domains, especially through LLM-driven autonomous agents. A
representative scenario is in software development, where LLM agents
demonstrate efficient collaboration, task division, and assurance of software
quality, markedly reducing the need for manual involvement. However, these
agents frequently perform a variety of tasks independently, without benefiting
from past experiences, which leads to repeated mistakes and inefficient
attempts in multi-step task execution. To this end, we introduce Experiential
Co-Learning, a novel LLM-agent learning framework in which instructor and
assistant agents gather shortcut-oriented experiences from their historical
trajectories and use these past experiences for future task execution. The
extensive experiments demonstrate that the framework enables agents to tackle
unseen software-developing tasks more effectively. We anticipate that our
insights will guide LLM agents towards enhanced autonomy and contribute to
their evolutionary growth in cooperative learning. The code and data are
available at https://github.com/OpenBMB/ChatDev.",2023-12-28T13:50:42Z
,http://arxiv.org/pdf/2405.03727v2.pdf,Large Language Models Synergize with Automated Machine Learning,"Recently, program synthesis driven by large language models (LLMs) has become
increasingly popular. However, program synthesis for machine learning (ML)
tasks still poses significant challenges. This paper explores a novel form of
program synthesis, targeting ML programs, by combining LLMs and automated
machine learning (autoML). Specifically, our goal is to fully automate the
generation and optimization of the code of the entire ML workflow, from data
preparation to modeling and post-processing, utilizing only textual
descriptions of the ML tasks. To manage the length and diversity of ML
programs, we propose to break each ML program into smaller, manageable parts.
Each part is generated separately by the LLM, with careful consideration of
their compatibilities. To ensure compatibilities, we design a testing technique
for ML programs. Unlike traditional program synthesis, which typically relies
on binary evaluations (i.e., correct or incorrect), evaluating ML programs
necessitates more than just binary judgments. Therefore, we further assess ML
programs numerically and select the optimal programs from a range of candidates
using AutoML methods. In experiments across various ML tasks, our method
outperforms existing methods in 10 out of 12 tasks for generating ML programs.
In addition, autoML significantly improves the performance of the generated ML
programs. In experiments, given the textual task description, our method,
Text-to-ML, generates the complete and optimized ML program in a fully
autonomous process.",2024-05-06T08:09:46Z
,http://arxiv.org/pdf/2406.07737v1.pdf,The Future of Software Engineering in an AI-Driven World,"A paradigm shift is underway in Software Engineering, with AI systems such as
LLMs gaining increasing importance for improving software development
productivity. This trend is anticipated to persist. In the next five years, we
will likely see an increasing symbiotic partnership between human developers
and AI. The Software Engineering research community cannot afford to overlook
this trend; we must address the key research challenges posed by the
integration of AI into the software development process. In this paper, we
present our vision of the future of software development in an AI-Driven world
and explore the key challenges that our research community should address to
realize this vision.",2024-06-11T21:46:19Z
,http://arxiv.org/pdf/2404.11595v3.pdf,"A Deep Dive into Large Language Models for Automated Bug Localization
  and Repair","Large language models (LLMs) have shown impressive effectiveness in various
software engineering tasks, including automated program repair (APR). In this
study, we take a deep dive into automated bug fixing utilizing LLMs. In
contrast to many deep learning-based APR methods that assume known bug
locations, rely on line-level localization tools, or address bug prediction and
fixing in one step, our approach uniquely employs LLMs to predict bug location
at the token level and subsequently utilizes them for bug fixing. This
methodological separation of bug localization and fixing using different LLMs
enables effective integration of diverse contextual information and improved
incorporation of inductive biases. We introduce Toggle: Token-Granulated Bug
Localization and Repair, a comprehensive program repair framework that
integrates a bug localization model, an adjustment unit, and a bug-fixing
model. Toggle takes a buggy function as input and generates a complete
corrected function. We investigate various styles of prompting to the bug
fixing model to identify the most effective prompts that better utilize the
inductive bias and significantly outperform others. Toggle achieves the new
state-of-the-art (SOTA) performance on the CodeXGLUE code refinement benchmark,
and exhibits better and comparable performance on several other widely-used APR
datasets, including Defects4J.",2024-04-17T17:48:18Z
,http://arxiv.org/pdf/2405.19888v1.pdf,"Parrot: Efficient Serving of LLM-based Applications with Semantic
  Variable","The rise of large language models (LLMs) has enabled LLM-based applications
(a.k.a. AI agents or co-pilots), a new software paradigm that combines the
strength of LLM and conventional software. Diverse LLM applications from
different tenants could design complex workflows using multiple LLM requests to
accomplish one task. However, they have to use the over-simplified
request-level API provided by today's public LLM services, losing essential
application-level information. Public LLM services have to blindly optimize
individual LLM requests, leading to sub-optimal end-to-end performance of LLM
applications.
  This paper introduces Parrot, an LLM service system that focuses on the
end-to-end experience of LLM-based applications. Parrot proposes Semantic
Variable, a unified abstraction to expose application-level knowledge to public
LLM services. A Semantic Variable annotates an input/output variable in the
prompt of a request, and creates the data pipeline when connecting multiple LLM
requests, providing a natural way to program LLM applications. Exposing
Semantic Variables to the public LLM service allows it to perform conventional
data flow analysis to uncover the correlation across multiple LLM requests.
This correlation opens a brand-new optimization space for the end-to-end
performance of LLM-based applications. Extensive evaluations demonstrate that
Parrot can achieve up to an order-of-magnitude improvement for popular and
practical use cases of LLM applications.",2024-05-30T09:46:36Z
,http://arxiv.org/pdf/2402.03396v1.pdf,"UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large
  Language Models for Program Testing","The remarkable capability of large language models (LLMs) in generating
high-quality code has drawn increasing attention in the software testing
community. However, existing code LLMs often demonstrate unsatisfactory
capabilities in generating accurate and complete tests since they were trained
on code snippets collected without differentiating between code for testing
purposes and other code. In this paper, we present a large-scale dataset
UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test
Synthesis. Associating tests with the tested functions is crucial for LLMs to
infer the expected behavior and the logic paths to be verified. By leveraging
Language Server Protocol, UniTSyn achieves the challenging goal of collecting
focal-test pairs without per-project execution setups or per-language
heuristics that tend to be fragile and difficult to scale. It contains 2.7
million focal-test pairs across five mainstream programming languages, making
it possible to be utilized for enhancing the test generation ability of LLMs.
The details of UniTSyn can be found in Table 1. Our experiments demonstrate
that, by building an autoregressive model based on UniTSyn, we can achieve
significant benefits in learning and understanding unit test representations,
resulting in improved generation accuracy and code coverage across all
evaluated programming languages. Code and data will be publicly available.",2024-02-04T22:48:05Z
,http://arxiv.org/pdf/2310.15317v1.pdf,"Exploring the Potential of Large Language Models in Generating
  Code-Tracing Questions for Introductory Programming Courses","In this paper, we explore the application of large language models (LLMs) for
generating code-tracing questions in introductory programming courses. We
designed targeted prompts for GPT4, guiding it to generate code-tracing
questions based on code snippets and descriptions. We established a set of
human evaluation metrics to assess the quality of questions produced by the
model compared to those created by human experts. Our analysis provides
insights into the capabilities and potential of LLMs in generating diverse
code-tracing questions. Additionally, we present a unique dataset of human and
LLM-generated tracing questions, serving as a valuable resource for both the
education and NLP research communities. This work contributes to the ongoing
dialogue on the potential uses of LLMs in educational settings.",2023-10-23T19:35:01Z
,http://arxiv.org/pdf/2404.13414v3.pdf,"Evaluating the Effectiveness of LLMs in Introductory Computer Science
  Education: A Semester-Long Field Study","The integration of AI assistants, especially through the development of Large
Language Models (LLMs), into computer science education has sparked significant
debate. An emerging body of work has looked into using LLMs in education, but
few have examined the impacts of LLMs on students in entry-level programming
courses, particularly in real-world contexts and over extended periods. To
address this research gap, we conducted a semester-long, between-subjects study
with 50 students using CodeTutor, an LLM-powered assistant developed by our
research team. Our study results show that students who used CodeTutor (the
experimental group) achieved statistically significant improvements in their
final scores compared to peers who did not use the tool (the control group).
Within the experimental group, those without prior experience with LLM-powered
tools demonstrated significantly greater performance gain than their
counterparts. We also found that students expressed positive feedback regarding
CodeTutor's capability, though they also had concerns about CodeTutor's limited
role in developing critical thinking skills. Over the semester, students'
agreement with CodeTutor's suggestions decreased, with a growing preference for
support from traditional human teaching assistants. Our analysis further
reveals that the quality of user prompts was significantly correlated with
CodeTutor's response effectiveness. Building upon our results, we discuss the
implications of our findings for integrating Generative AI literacy into
curricula to foster critical thinking skills and turn to examining the temporal
dynamics of user engagement with LLM-powered tools. We further discuss the
discrepancy between the anticipated functions of tools and students' actual
capabilities, which sheds light on the need for tailored strategies to improve
educational outcomes.",2024-04-20T15:58:22Z
,http://arxiv.org/pdf/2404.18864v1.pdf,Performance-Aligned LLMs for Generating Fast Code,"Optimizing scientific software is a difficult task because codebases are
often large and complex, and performance can depend upon several factors
including the algorithm, its implementation, and hardware among others. Causes
of poor performance can originate from disparate sources and be difficult to
diagnose. Recent years have seen a multitude of work that use large language
models (LLMs) to assist in software development tasks. However, these tools are
trained to model the distribution of code as text, and are not specifically
designed to understand performance aspects of code. In this work, we introduce
a reinforcement learning based methodology to align the outputs of code LLMs
with performance. This allows us to build upon the current code modeling
capabilities of LLMs and extend them to generate better performing code. We
demonstrate that our fine-tuned model improves the expected speedup of
generated code over base models for a set of benchmark tasks from 0.9 to 1.6
for serial code and 1.9 to 4.5 for OpenMP code.",2024-04-29T16:52:38Z
,http://arxiv.org/pdf/2402.01706v1.pdf,"MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse
  Worlds","Large Language Model (LLM) alignment aims to ensure that LLM outputs match
with human values. Researchers have demonstrated the severity of alignment
problems with a large spectrum of jailbreak techniques that can induce LLMs to
produce malicious content during conversations. Finding the corresponding
jailbreaking prompts usually requires substantial human intelligence or
computation resources. In this paper, we report that LLMs have different levels
of alignment in various contexts. As such, by systematically constructing many
contexts, called worlds, leveraging a Domain Specific Language describing
possible worlds (e.g., time, location, characters, actions and languages) and
the corresponding compiler, we can cost-effectively expose latent alignment
issues. Given the low cost of our method, we are able to conduct a large scale
study regarding LLM alignment issues in different worlds. Our results show that
our method outperforms the-state-of-the-art jailbreaking techniques on both
effectiveness and efficiency. In addition, our results indicate that existing
LLMs are extremely vulnerable to nesting worlds and programming language
worlds. They imply that existing alignment training focuses on the real-world
and is lacking in various (virtual) worlds where LLMs can be exploited.",2024-01-25T02:57:40Z
,http://arxiv.org/pdf/2404.10779v1.pdf,Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations,"There is a compelling necessity from enterprises for fine tuning LLMs (Large
Language Models) o get them trained on proprietary domain knowledge. The
challenge is to imbibe the LLMs with domain specific knowledge using the most
optimial resource and cost and in the best possible time. Many enterprises rely
on RAG (Retrieval Augmented Generation) which does not need LLMs to be
ine-tuned but they are limited by the quality of vector databases and their
retrieval capabilities rather than the intrinsic capabilities of the LLMs
themselves. In our current work we focus on fine tuning LLaMA, an open source
LLM using proprietary documents and code from an enterprise repository and use
the fine tuned models to evaluate the quality of responses. As part of this
work, we aim to guide beginners on how to start with fine tuning an LLM for
documentation and code by making educated guesses on size of GPU required and
options that are available for formatting the data. We also propose pre
processing recipes for both documentation and code to prepare dataset in
different formats. The proposed methods of data preparation for document
datasets are forming paragraph chunks, forming question and answer pairs and
forming keyword and paragraph chunk pairs. For code dataset we propose forming
summary and function pairs. Further, we qualitatively evaluate the results of
the models for domain specific queries. Finally, we also propose practical
guidelines and recommendations for fine tuning LLMs.",2024-03-23T13:25:01Z
,http://arxiv.org/pdf/2308.04898v1.pdf,"An Empirical Study on Using Large Language Models to Analyze Software
  Supply Chain Security Failures","As we increasingly depend on software systems, the consequences of breaches
in the software supply chain become more severe. High-profile cyber attacks
like those on SolarWinds and ShadowHammer have resulted in significant
financial and data losses, underlining the need for stronger cybersecurity. One
way to prevent future breaches is by studying past failures. However,
traditional methods of analyzing these failures require manually reading and
summarizing reports about them. Automated support could reduce costs and allow
analysis of more failures. Natural Language Processing (NLP) techniques such as
Large Language Models (LLMs) could be leveraged to assist the analysis of
failures. In this study, we assessed the ability of Large Language Models
(LLMs) to analyze historical software supply chain breaches. We used LLMs to
replicate the manual analysis of 69 software supply chain security failures
performed by members of the Cloud Native Computing Foundation (CNCF). We
developed prompts for LLMs to categorize these by four dimensions: type of
compromise, intent, nature, and impact. GPT 3.5s categorizations had an average
accuracy of 68% and Bard had an accuracy of 58% over these dimensions. We
report that LLMs effectively characterize software supply chain failures when
the source articles are detailed enough for consensus among manual analysts,
but cannot yet replace human analysts. Future work can improve LLM performance
in this context, and study a broader range of articles and failures.",2023-08-09T15:35:14Z
,http://arxiv.org/pdf/2406.08316v2.pdf,Is Programming by Example solved by LLMs?,"Programming-by-Examples (PBE) aims to generate an algorithm from input-output
examples. Such systems are practically and theoretically important: from an
end-user perspective, they are deployed to millions of people, and from an AI
perspective, PBE corresponds to a very general form of few-shot inductive
inference. Given the success of Large Language Models (LLMs) in code-generation
tasks, we investigate here the extent to which LLMs can be said to have
`solved' PBE. We experiment on classic domains such as lists and strings, and
an uncommon graphics programming domain not well represented in typical
pretraining data. We find that pretrained models are not effective at PBE, but
that they can be fine-tuned for much higher performance, provided the test
problems are in-distribution. We analyze empirically what causes these models
to succeed and fail, and take steps toward understanding how to achieve better
out-of-distribution generalization. Collectively these results suggest that
LLMs make strong progress toward solving the typical suite of PBE tasks,
potentially increasing the flexibility and applicability of PBE systems, while
also identifying ways in which LLMs still fall short.",2024-06-12T15:16:40Z
,http://arxiv.org/pdf/2401.00757v1.pdf,"A & B == B & A: Triggering Logical Reasoning Failures in Large Language
  Models","Recent advancements in large language models (LLMs) have propelled Artificial
Intelligence (AI) to new heights, enabling breakthroughs in various tasks such
as writing assistance, code generation, and machine translation. A significant
distinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to
""reason."" However, evaluating the reasoning ability of LLMs remains a challenge
as most existing evaluations focus on their accuracy on the downstream tasks
rather than directly assessing their reasoning processes. Efforts have been
made to develop benchmarks and metrics to assess reasoning in LLMs, but they
suffer from data leakage or limited scope. In this paper, we introduce
LogicAsker, an automatic approach that comprehensively evaluates and improves
the logical reasoning abilities of LLMs under a set of atomic reasoning skills
based on propositional and predicate logic. The results provide insights into
LLMs' reasoning abilities and reveal the logical rules the LLMs did not learn
well. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,
ChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases
from LogicAsker can find logical reasoning failures in different LLMs with a
rate of 25\% - 94\%. In addition, the test cases of LogicAsker can be further
used to design demonstration examples for in-context learning, which
effectively improves the logical reasoning ability of LLMs, e.g., 10\% for
GPT-4. As far as we know, our work is the first to create prompts based on
testing results to improve LLMs' formal reasoning ability effectively. All the
code, data, and results will be released for reproduction and future research.",2024-01-01T13:53:53Z
,http://arxiv.org/pdf/2310.06646v1.pdf,"Forgetful Large Language Models: Lessons Learned from Using LLMs in
  Robot Programming","Large language models offer new ways of empowering people to program robot
applications-namely, code generation via prompting. However, the code generated
by LLMs is susceptible to errors. This work reports a preliminary exploration
that empirically characterizes common errors produced by LLMs in robot
programming. We categorize these errors into two phases: interpretation and
execution. In this work, we focus on errors in execution and observe that they
are caused by LLMs being ""forgetful"" of key information provided in user
prompts. Based on this observation, we propose prompt engineering tactics
designed to reduce errors in execution. We then demonstrate the effectiveness
of these tactics with three language models: ChatGPT, Bard, and LLaMA-2.
Finally, we discuss lessons learned from using LLMs in robot programming and
call for the benchmarking of LLM-powered end-user development of robot
applications.",2023-10-10T14:10:39Z
,http://arxiv.org/pdf/2306.00029v1.pdf,CodeTF: One-stop Transformer Library for State-of-the-art Code LLM,"Code intelligence plays a key role in transforming modern software
engineering. Recently, deep learning-based models, especially Transformer-based
large language models (LLMs), have demonstrated remarkable potential in
tackling these tasks by leveraging massive open-source code data and
programming language features. However, the development and deployment of such
models often require expertise in both machine learning and software
engineering, creating a barrier for the model adoption. In this paper, we
present CodeTF, an open-source Transformer-based library for state-of-the-art
Code LLMs and code intelligence. Following the principles of modular design and
extensible framework, we design CodeTF with a unified interface to enable rapid
access and development across different types of models, datasets and tasks.
Our library supports a collection of pretrained Code LLM models and popular
code benchmarks, including a standardized interface to train and serve code
LLMs efficiently, and data features such as language-specific parsers and
utility functions for extracting code attributes. In this paper, we describe
the design principles, the architecture, key modules and components, and
compare with other related library tools. Finally, we hope CodeTF is able to
bridge the gap between machine learning/generative AI and software engineering,
providing a comprehensive open-source solution for developers, researchers, and
practitioners.",2023-05-31T05:24:48Z
,http://arxiv.org/pdf/2307.05950v2.pdf,"Exploring the Effectiveness of LLMs in Automated Logging Generation: An
  Empirical Study","Automated logging statement generation supports developers in documenting
critical software runtime behavior. Given the great success in natural language
generation and programming language comprehension, large language models (LLMs)
might help developers generate logging statements, but this has not yet been
investigated. To fill the gap, this paper performs the first study on exploring
LLMs for logging statement generation.We first build a logging statement
generation dataset, LogBench, with two parts: (1) LogBench-O: logging
statements collected from GitHub repositories, and (2) LogBench-T: the
transformed unseen code from LogBench-O. Then, we leverage LogBench to evaluate
the effectiveness and generalization capabilities (using LogBench-T) of eleven
top-performing LLMs. In addition, we examine the performance of these LLMs
against classical retrieval-based and machine learning-based logging methods
from the era preceding LLMs. We further evaluate LLM's logging generalization
capabilities using unseen data (LogBench-T) derived from code transformation
techniques. While existing LLMs deliver decent predictions on logging levels
and logging variables, our study indicates that they only achieve a maximum
BLEU score of 0.249, thus calling for improvements. The paper also highlights
the importance of prompt constructions and external factors (e.g., programming
contexts and code comments) for LLMs' logging performance. Based on these
findings, we identify five implications and provide practical advice for future
logging research. Our empirical analysis discloses the limitations of current
logging approaches while showcasing the potential of LLM-based logging tools,
and provides actionable guidance for building more practical models.",2023-07-12T06:32:51Z
,http://arxiv.org/pdf/2403.14965v1.pdf,"Comprehensive Evaluation and Insights into the Use of Large Language
  Models in the Automation of Behavior-Driven Development Acceptance Test
  Formulation","Behavior-driven development (BDD) is an Agile testing methodology fostering
collaboration among developers, QA analysts, and stakeholders. In this
manuscript, we propose a novel approach to enhance BDD practices using large
language models (LLMs) to automate acceptance test generation. Our study uses
zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B,
and PaLM-2. The paper presents a detailed methodology that includes the
dataset, prompt techniques, LLMs, and the evaluation process. The results
demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests
with better performance. The few-shot prompt technique highlights its ability
to provide higher accuracy by incorporating examples for in-context learning.
Furthermore, the study examines syntax errors, validation accuracy, and
comparative analysis of LLMs, revealing their effectiveness in enhancing BDD
practices. However, our study acknowledges that there are limitations to the
proposed approach. We emphasize that this approach can support collaborative
BDD processes and create opportunities for future research into automated BDD
acceptance test generation using LLMs.",2024-03-22T05:37:52Z
,http://arxiv.org/pdf/2405.11581v2.pdf,DOLLmC: DevOps for Large Language model Customization,"The rapid integration of Large Language Models (LLMs) into various industries
presents both revolutionary opportunities and unique challenges. This research
aims to establish a scalable and efficient framework for LLM customization,
exploring how DevOps practices should be adapted to meet the specific demands
of LLM customization. By integrating ontologies, knowledge maps, and prompt
engineering into the DevOps pipeline, we propose a robust framework that
enhances continuous learning, seamless deployment, and rigorous version control
of LLMs. This methodology is demonstrated through the development of a
domain-specific chatbot for the agricultural sector, utilizing heterogeneous
data to deliver actionable insights. The proposed methodology, so called
DOLLmC, not only addresses the immediate challenges of LLM customization but
also promotes scalability and operational efficiency. However, the
methodology's primary limitation lies in the need for extensive testing,
validation, and broader adoption across different domains.",2024-05-19T15:20:27Z
,http://arxiv.org/pdf/2405.17743v2.pdf,ORLM: Training Large Language Models for Optimization Modeling,"Large Language Models (LLMs) have emerged as powerful tools for tackling
complex Operations Research (OR) problem by providing the capacity in
automating optimization modeling. However, current methodologies heavily rely
on prompt engineering (e.g., multi-agent cooperation) with proprietary LLMs,
raising data privacy concerns that could be prohibitive in industry
applications. To tackle this issue, we propose training open-source LLMs for
optimization modeling. We identify four critical requirements for the training
dataset of OR LLMs, design and implement OR-Instruct, a semi-automated process
for creating synthetic data tailored to specific requirements. We also
introduce the IndustryOR benchmark, the first industrial benchmark for testing
LLMs on solving real-world OR problems. We apply the data from OR-Instruct to
various open-source LLMs of 7b size (termed as ORLMs), resulting in a
significantly improved capability for optimization modeling. Our
best-performing ORLM achieves state-of-the-art performance on the NL4OPT, MAMO,
and IndustryOR benchmarks. Our code and data are available at
\url{https://github.com/Cardinal-Operations/ORLM}.",2024-05-28T01:55:35Z
,http://arxiv.org/pdf/2305.18365v3.pdf,"What can Large Language Models do in chemistry? A comprehensive
  benchmark on eight tasks","Large Language Models (LLMs) with strong abilities in natural language
processing tasks have emerged and have been applied in various kinds of areas
such as science, finance and software engineering. However, the capability of
LLMs to advance the field of chemistry remains unclear. In this paper, rather
than pursuing state-of-the-art performance, we aim to evaluate capabilities of
LLMs in a wide range of tasks across the chemistry domain. We identify three
key chemistry-related capabilities including understanding, reasoning and
explaining to explore in LLMs and establish a benchmark containing eight
chemistry tasks. Our analysis draws on widely recognized datasets facilitating
a broad exploration of the capacities of LLMs within the context of practical
chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are
evaluated for each chemistry task in zero-shot and few-shot in-context learning
settings with carefully selected demonstration examples and specially crafted
prompts. Our investigation found that GPT-4 outperformed other models and LLMs
exhibit different competitive levels in eight chemistry tasks. In addition to
the key findings from the comprehensive benchmark analysis, our work provides
insights into the limitation of current LLMs and the impact of in-context
learning settings on LLMs' performance across various chemistry tasks. The code
and datasets used in this study are available at
https://github.com/ChemFoundationModels/ChemLLMBench.",2023-05-27T14:17:33Z
,http://arxiv.org/pdf/2404.04603v1.pdf,Analyzing LLM Usage in an Advanced Computing Class in India,"This paper investigates the usage patterns of undergraduate and graduate
students when engaging with large language models (LLMs) to tackle programming
assignments in the context of advanced computing courses. Existing work
predominantly focuses on the influence of LLMs in introductory programming
contexts. Additionally, there is a scarcity of studies analyzing actual
conversations between students and LLMs. Our study provides a comprehensive
quantitative and qualitative analysis of raw interactions between students and
LLMs within an advanced computing course (Distributed Systems) at an Indian
University. We further complement this by conducting student interviews to gain
deeper insights into their usage patterns. Our study shows that students make
use of large language models (LLMs) in various ways: generating code or
debugging code by identifying and fixing errors. They also copy and paste
assignment descriptions into LLM interfaces for specific solutions, ask
conceptual questions about complex programming ideas or theoretical concepts,
and generate test cases to check code functionality and robustness. Our
analysis includes over 4,000 prompts from 411 students and conducting
interviews with 10 students. Our analysis shows that LLMs excel at generating
boilerplate code and assisting in debugging, while students handle the
integration of components and system troubleshooting. This aligns with the
learning objectives of advanced computing courses, which are oriented towards
teaching students how to build systems and troubleshoot, with less emphasis on
generating code from scratch. Therefore, LLM tools can be leveraged to increase
student productivity, as shown by the data we collected. This study contributes
to the ongoing discussion on LLM use in education, advocating for their
usefulness in advanced computing courses to complement higher-level learning
and productivity.",2024-04-06T12:06:56Z
,http://arxiv.org/pdf/2405.01553v1.pdf,"Empirical Studies of Parameter Efficient Methods for Large Language
  Models of Code and Knowledge Transfer to R","Recently, Large Langauge Models (LLMs) have gained a lot of attention in the
Software Engineering (SE) community. LLMs or their variants pre-trained on code
are used for many SE tasks. A main approach for adapting LLMs to the downstream
task is to fine-tune the models. However, with having billions-parameters-LLMs,
fine-tuning the models is not practical. An alternative approach is using
Parameter Efficient Fine Tuning (PEFT), in which the model parameters are
frozen and only a few added parameters are trained. Though the LLMs are used
for programming languages such as Python and Java widely, their capability for
low-resource languages is limited. In this work, we empirically study PEFT
methods, LoRA and Compacter, on CodeT5 and CodeLlama. We will assess their
performance compared to fully fine-tuned models, whether they can be used for
knowledge transfer from natural language models to code (using T5 and Llama
models), and their ability to adapt the learned knowledge to an unseen
language. For the unseen language, we aim to study R, as it has a wide
community. The adaptability with less computational costs makes LLMs accessible
in scenarios where heavy computational resources are not available. Moreover,
studying R opens new opportunities for using LLMs for other languages. We
anticipate our findings to showcase the capabilities of PEFT for code LLMs for
R and reveal the improvement areas.",2024-03-16T03:12:45Z
,http://arxiv.org/pdf/2401.03374v2.pdf,"LLM-Powered Code Vulnerability Repair with Reinforcement Learning and
  Semantic Reward","In software development, the predominant emphasis on functionality often
supersedes security concerns, a trend gaining momentum with AI-driven
automation tools like GitHub Copilot. These tools significantly improve
developers' efficiency in functional code development. Nevertheless, it remains
a notable concern that such tools are also responsible for creating insecure
code, predominantly because of pre-training on publicly available repositories
with vulnerable code. Moreover, developers are called the ""weakest link in the
chain"" since they have very minimal knowledge of code security. Although
existing solutions provide a reasonable solution to vulnerable code, they must
adequately describe and educate the developers on code security to ensure that
the security issues are not repeated. Therefore we introduce a multipurpose
code vulnerability analysis system \texttt{SecRepair}, powered by a large
language model, CodeGen2 assisting the developer in identifying and generating
fixed code along with a complete description of the vulnerability with a code
comment. Our innovative methodology uses a reinforcement learning paradigm to
generate code comments augmented by a semantic reward mechanism. Inspired by
how humans fix code issues, we propose an instruction-based dataset suitable
for vulnerability analysis with LLMs. We further identify zero-day and N-day
vulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findings
underscore that incorporating reinforcement learning coupled with semantic
reward augments our model's performance, thereby fortifying its capacity to
address code vulnerabilities with improved efficacy.",2024-01-07T02:46:39Z
,http://arxiv.org/pdf/2401.02985v1.pdf,"Evaluating Large Language Models on the GMAT: Implications for the
  Future of Business Education","The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.",2024-01-02T03:54:50Z
,http://arxiv.org/pdf/2311.11690v1.pdf,Refactoring Programs Using Large Language Models with Few-Shot Examples,"A less complex and more straightforward program is a crucial factor that
enhances its maintainability and makes writing secure and bug-free programs
easier. However, due to its heavy workload and the risks of breaking the
working programs, programmers are reluctant to do code refactoring, and thus,
it also causes the loss of potential learning experiences. To mitigate this, we
demonstrate the application of using a large language model (LLM), GPT-3.5, to
suggest less complex versions of the user-written Python program, aiming to
encourage users to learn how to write better programs. We propose a method to
leverage the prompting with few-shot examples of the LLM by selecting the
best-suited code refactoring examples for each target programming problem based
on the prior evaluation of prompting with the one-shot example. The
quantitative evaluation shows that 95.68% of programs can be refactored by
generating 10 candidates each, resulting in a 17.35% reduction in the average
cyclomatic complexity and a 25.84% decrease in the average number of lines
after filtering only generated programs that are semantically correct.
Furthermore, the qualitative evaluation shows outstanding capability in code
formatting, while unnecessary behaviors such as deleting or translating
comments are also observed.",2023-11-20T11:43:45Z
,http://arxiv.org/pdf/2212.14834v4.pdf,"Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-Learning
  Libraries via Large Language Models","Detecting bugs in Deep Learning (DL) libraries (e.g., TensorFlow/PyTorch) is
critical for almost all downstream DL systems in ensuring effectiveness/safety
for end users. Meanwhile, traditional fuzzing techniques can be hardly
effective for such a challenging domain since the input DL programs need to
satisfy both the input language (e.g., Python) syntax/semantics and the DL API
input/shape constraints for tensor computations.
  To address these limitations, we propose TitanFuzz - the first approach to
directly leveraging Large Language Models (LLMs) to generate input programs for
fuzzing DL libraries. LLMs are titanic models trained on billions of code
snippets and can auto-regressively generate human-like code snippets. Our key
insight is that modern LLMs can also include numerous code snippets invoking DL
library APIs in their training corpora, and thus can implicitly learn both
language syntax/semantics and intricate DL API constraints for valid DL program
generation. More specifically, we use both generative and infilling LLMs (e.g.,
Codex/InCoder) to generate and mutate valid/diverse input DL programs for
fuzzing. Our experimental results demonstrate that TitanFuzz can achieve
30.38%/50.84% higher code coverage than state-of-the-art fuzzers on
TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 41
already confirmed as previously unknown bugs.
  This paper demonstrates that modern titanic LLMs can be leveraged to directly
perform both generation-based and mutation-based fuzzing studied for decades,
while being fully automated, generalizable, and applicable to domains
challenging for traditional approaches (such as DL systems). We hope TitanFuzz
can stimulate more work in this promising direction of LLMs for fuzzing.",2022-12-30T17:25:11Z
,http://arxiv.org/pdf/2406.12655v1.pdf,"Benchmarks and Metrics for Evaluations of Code Generation: A Critical
  Review","With the rapid development of Large Language Models (LLMs), a large number of
machine learning models have been developed to assist programming tasks
including the generation of program code from natural language input. However,
how to evaluate such LLMs for this task is still an open problem despite of the
great amount of research efforts that have been made and reported to evaluate
and compare them. This paper provides a critical review of the existing work on
the testing and evaluation of these tools with a focus on two key aspects: the
benchmarks and the metrics used in the evaluations. Based on the review,
further research directions are discussed.",2024-06-18T14:25:34Z
,http://arxiv.org/pdf/2405.12750v1.pdf,"Generative AI and Large Language Models for Cyber Security: All Insights
  You Need","This paper provides a comprehensive review of the future of cybersecurity
through Generative AI and Large Language Models (LLMs). We explore LLM
applications across various domains, including hardware design security,
intrusion detection, software engineering, design verification, cyber threat
intelligence, malware detection, and phishing detection. We present an overview
of LLM evolution and its current state, focusing on advancements in models such
as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends
to LLM vulnerabilities, such as prompt injection, insecure output handling,
data poisoning, DDoS attacks, and adversarial instructions. We delve into
mitigation strategies to protect these models, providing a comprehensive look
at potential attack scenarios and prevention techniques. Furthermore, we
evaluate the performance of 42 LLM models in cybersecurity knowledge and
hardware security, highlighting their strengths and weaknesses. We thoroughly
evaluate cybersecurity datasets for LLM training and testing, covering the
lifecycle from data creation to usage and identifying gaps for future research.
In addition, we review new strategies for leveraging LLMs, including techniques
like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human
Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank
Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim
to enhance real-time cybersecurity defenses and improve the sophistication of
LLM applications in threat detection and response. Our paper provides a
foundational understanding and strategic direction for integrating LLMs into
future cybersecurity frameworks, emphasizing innovation and robust model
deployment to safeguard against evolving cyber threats.",2024-05-21T13:02:27Z
,http://arxiv.org/pdf/2311.14722v1.pdf,"Zero-Shot Question Answering over Financial Documents using Large
  Language Models","We introduce a large language model (LLM) based approach to answer complex
questions requiring multi-hop numerical reasoning over financial reports. While
LLMs have exhibited remarkable performance on various natural language and
reasoning tasks, complex reasoning problems often rely on few-shot prompts that
require carefully crafted examples. In contrast, our approach uses novel
zero-shot prompts that guide the LLM to encode the required reasoning into a
Python program or a domain specific language. The generated program is then
executed by a program interpreter, thus mitigating the limitations of LLM in
performing accurate arithmetic calculations.
  We evaluate the proposed approach on three financial datasets using some of
the recently developed generative pretrained transformer (GPT) models and
perform comparisons with various zero-shot baselines. The experimental results
demonstrate that our approach significantly improves the accuracy for all the
LLMs over their respective baselines. We provide a detailed analysis of the
results, generating insights to support our findings. The success of our
approach demonstrates the enormous potential to extract complex domain specific
numerical reasoning by designing zero-shot prompts to effectively exploit the
knowledge embedded in LLMs.",2023-11-19T16:23:34Z
,http://arxiv.org/pdf/2401.07031v2.pdf,"Code Security Vulnerability Repair Using Reinforcement Learning with
  Large Language Models","With the recent advancement of Large Language Models (LLMs), generating
functionally correct code has become less complicated for a wide array of
developers. While using LLMs has sped up the functional development process, it
poses a heavy risk to code security. Code generation with proper security
measures using LLM is a significantly more challenging task than functional
code generation. Security measures may include adding a pair of lines of code
with the original code, consisting of null pointer checking or prepared
statements for SQL injection prevention. Currently, available code repair LLMs
generate code repair by supervised fine-tuning, where the model looks at
cross-entropy loss. However, the original and repaired codes are mostly similar
in functionality and syntactically, except for a few (1-2) lines, which act as
security measures. This imbalance between the lines needed for security
measures and the functional code enforces the supervised fine-tuned model to
prioritize generating functional code without adding proper security measures,
which also benefits the model by resulting in minimal loss. Therefore, in this
work, for security hardening and strengthening of generated code from LLMs, we
propose a reinforcement learning-based method for program-specific repair with
the combination of semantic and syntactic reward mechanisms that focus heavily
on adding security and functional measures in the code, respectively.",2024-01-13T10:19:26Z
,http://arxiv.org/pdf/2403.11671v1.pdf,HDLdebugger: Streamlining HDL debugging with Large Language Models,"In the domain of chip design, Hardware Description Languages (HDLs) play a
pivotal role. However, due to the complex syntax of HDLs and the limited
availability of online resources, debugging HDL codes remains a difficult and
time-intensive task, even for seasoned engineers. Consequently, there is a
pressing need to develop automated HDL code debugging models, which can
alleviate the burden on hardware engineers. Despite the strong capabilities of
Large Language Models (LLMs) in generating, completing, and debugging software
code, their utilization in the specialized field of HDL debugging has been
limited and, to date, has not yielded satisfactory results. In this paper, we
propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which
consists of HDL debugging data generation via a reverse engineering approach, a
search engine for retrieval-augmented generation, and a retrieval-augmented LLM
fine-tuning approach. Through the integration of these components, HDLdebugger
can automate and streamline HDL debugging for chip design. Our comprehensive
experiments, conducted on an HDL code dataset sourced from Huawei, reveal that
HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional
effectiveness in HDL code debugging.",2024-03-18T11:19:37Z
,http://arxiv.org/pdf/2406.04568v1.pdf,"StackSight: Unveiling WebAssembly through Large Language Models and
  Neurosymbolic Chain-of-Thought Decompilation","WebAssembly enables near-native execution in web applications and is
increasingly adopted for tasks that demand high performance and robust
security. However, its assembly-like syntax, implicit stack machine, and
low-level data types make it extremely difficult for human developers to
understand, spurring the need for effective WebAssembly reverse engineering
techniques. In this paper, we propose StackSight, a novel neurosymbolic
approach that combines Large Language Models (LLMs) with advanced program
analysis to decompile complex WebAssembly code into readable C++ snippets.
StackSight visualizes and tracks virtual stack alterations via a static
analysis algorithm and then applies chain-of-thought prompting to harness LLM's
complex reasoning capabilities. Evaluation results show that StackSight
significantly improves WebAssembly decompilation. Our user study also
demonstrates that code snippets generated by StackSight have significantly
higher win rates and enable a better grasp of code semantics.",2024-06-07T01:08:17Z
,http://arxiv.org/pdf/2306.04556v1.pdf,"StudentEval: A Benchmark of Student-Written Prompts for Large Language
  Models of Code","Code LLMs are being rapidly deployed and there is evidence that they can make
professional programmers more productive. Current benchmarks for code
generation measure whether models generate correct programs given an expert
prompt. In this paper, we present a new benchmark containing multiple prompts
per problem, written by a specific population of non-expert prompters:
beginning programmers. StudentEval contains 1,749 prompts for 48 problems,
written by 80 students who have only completed one semester of Python
programming. Our students wrote these prompts while working interactively with
a Code LLM, and we observed very mixed success rates. We use StudentEval to
evaluate 5 Code LLMs and find that StudentEval is a better discriminator of
model performance than existing benchmarks. We analyze the prompts and find
significant variation in students' prompting techniques. We also find that
nondeterministic LLM sampling could mislead students into thinking that their
prompts are more (or less) effective than they actually are, which has
implications for how to teach with Code LLMs.",2023-06-07T16:03:55Z
,http://arxiv.org/pdf/2308.04748v2.pdf,Fuzz4All: Universal Fuzzing with Large Language Models,"Fuzzing has achieved tremendous success in discovering bugs and
vulnerabilities in various software systems. Systems under test (SUTs) that
take in programming or formal language as inputs, e.g., compilers, runtime
engines, constraint solvers, and software libraries with accessible APIs, are
especially important as they are fundamental building blocks of software
development. However, existing fuzzers for such systems often target a specific
language, and thus cannot be easily applied to other languages or even other
versions of the same language. Moreover, the inputs generated by existing
fuzzers are often limited to specific features of the input language, and thus
can hardly reveal bugs related to other or new features. This paper presents
Fuzz4All, the first fuzzer that is universal in the sense that it can target
many different input languages and many different features of these languages.
The key idea behind Fuzz4All is to leverage large language models (LLMs) as an
input generation and mutation engine, which enables the approach to produce
diverse and realistic inputs for any practically relevant language. To realize
this potential, we present a novel autoprompting technique, which creates LLM
prompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop,
which iteratively updates the prompt to create new fuzzing inputs. We evaluate
Fuzz4All on nine systems under test that take in six different languages (C,
C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all six
languages, that universal fuzzing achieves higher coverage than existing,
language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs in
widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit
quantum computing platform, with 64 bugs already confirmed by developers as
previously unknown.",2023-08-09T07:36:21Z
,http://arxiv.org/pdf/2403.17134v1.pdf,"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair","Automated program repair has emerged as a powerful technique to mitigate the
impact of software bugs on system reliability and user experience. This paper
introduces RepairAgent, the first work to address the program repair challenge
through an autonomous agent based on a large language model (LLM). Unlike
existing deep learning-based approaches, which prompt a model with a fixed
prompt or in a fixed feedback loop, our work treats the LLM as an agent capable
of autonomously planning and executing actions to fix bugs by invoking suitable
tools. RepairAgent freely interleaves gathering information about the bug,
gathering repair ingredients, and validating fixes, while deciding which tools
to invoke based on the gathered information and feedback from previous fix
attempts. Key contributions that enable RepairAgent include a set of tools that
are useful for program repair, a dynamically updated prompt format that allows
the LLM to interact with these tools, and a finite state machine that guides
the agent in invoking the tools. Our evaluation on the popular Defects4J
dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164
bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM
imposes an average cost of 270,000 tokens per bug, which, under the current
pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To
the best of our knowledge, this work is the first to present an autonomous,
LLM-based agent for program repair, paving the way for future agent-based
techniques in software engineering.",2024-03-25T19:17:43Z
,http://arxiv.org/pdf/2307.07411v1.pdf,"Detecting LLM-Generated Text in Computing Education: A Comparative Study
  for ChatGPT Cases","Due to the recent improvements and wide availability of Large Language Models
(LLMs), they have posed a serious threat to academic integrity in education.
Modern LLM-generated text detectors attempt to combat the problem by offering
educators with services to assess whether some text is LLM-generated. In this
work, we have collected 124 submissions from computer science students before
the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this
data to evaluate eight publicly-available LLM-generated text detectors through
the measures of accuracy, false positives, and resilience. The purpose of this
work is to inform the community of what LLM-generated text detectors work and
which do not, but also to provide insights for educators to better maintain
academic integrity in their courses. Our results find that CopyLeaks is the
most accurate LLM-generated text detector, GPTKit is the best LLM-generated
text detector to reduce false positives, and GLTR is the most resilient
LLM-generated text detector. We also express concerns over 52 false positives
(of 114 human written submissions) generated by GPTZero. Finally, we note that
all LLM-generated text detectors are less accurate with code, other languages
(aside from English), and after the use of paraphrasing tools (like QuillBot).
Modern detectors are still in need of improvements so that they can offer a
full-proof solution to help maintain academic integrity. Further, their
usability can be improved by facilitating a smooth API integration, providing
clear documentation of their features and the understandability of their
model(s), and supporting more commonly used languages.",2023-07-10T12:18:34Z
,http://arxiv.org/pdf/2310.14053v3.pdf,"Beyond Accuracy: Evaluating Self-Consistency of Code Large Language
  Models with IdentityChain","Code Large Language Models (Code LLMs) are being increasingly employed in
real-life applications, so evaluating them is critical. While the conventional
accuracy evaluates the performance of Code LLMs on a set of individual tasks,
their self-consistency across different tasks is overlooked. Intuitively, a
trustworthy model should be self-consistent when generating natural language
specifications for its own code and generating code for its own specifications.
Failure to preserve self-consistency reveals a lack of understanding of the
shared semantics underlying natural language and programming language, and
therefore undermines the trustworthiness of a model. In this paper, we first
formally define the self-consistency of Code LLMs and then design a framework,
IdentityChain, which effectively and efficiently evaluates the self-consistency
and conventional accuracy of a model at the same time. We study eleven Code
LLMs and show that they fail to preserve self-consistency, which is indeed a
distinct aspect from conventional accuracy. Furthermore, we show that
IdentityChain can be used as a model debugging tool to expose weaknesses of
Code LLMs by demonstrating three major weaknesses that we identify in current
models using IdentityChain. Our code is available at
https://github.com/marcusm117/IdentityChain.",2023-10-21T16:14:56Z
,http://arxiv.org/pdf/2401.16448v1.pdf,"LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware
  Debugging","This paper presents LLM4SecHW, a novel framework for hardware debugging that
leverages domain specific Large Language Model (LLM). Despite the success of
LLMs in automating various software development tasks, their application in the
hardware security domain has been limited due to the constraints of commercial
LLMs and the scarcity of domain specific data. To address these challenges, we
propose a unique approach to compile a dataset of open source hardware design
defects and their remediation steps, utilizing version control data. This
dataset provides a substantial foundation for training machine learning models
for hardware. LLM4SecHW employs fine tuning of medium sized LLMs based on this
dataset, enabling the identification and rectification of bugs in hardware
designs. This pioneering approach offers a reference workflow for the
application of fine tuning domain specific LLMs in other research areas. We
evaluate the performance of our proposed system on various open source hardware
designs, demonstrating its efficacy in accurately identifying and correcting
defects. Our work brings a new perspective on automating the quality control
process in hardware design.",2024-01-28T19:45:25Z
,http://arxiv.org/pdf/2307.10348v1.pdf,Code Detection for Hardware Acceleration Using Large Language Models,"Large language models (LLMs) have been massively applied to many tasks, often
surpassing state-of-the-art approaches. While their effectiveness in code
generation has been extensively studied (e.g., AlphaCode), their potential for
code detection remains unexplored.
  This work presents the first analysis of code detection using LLMs. Our study
examines essential kernels, including matrix multiplication, convolution, and
fast-fourier transform, implemented in C/C++. We propose both a preliminary,
naive prompt and a novel prompting strategy for code detection.
  Results reveal that conventional prompting achieves great precision but poor
accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively)
due to a high number of false positives. Our novel prompting strategy
substantially reduces false positives, resulting in excellent overall accuracy
(91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable
challenge to existing state-of-the-art code detection methods.",2023-07-19T17:21:58Z
,http://arxiv.org/pdf/2312.10055v1.pdf,"Next-Step Hint Generation for Introductory Programming Using Large
  Language Models","Large Language Models possess skills such as answering questions, writing
essays or solving programming exercises. Since these models are easily
accessible, researchers have investigated their capabilities and risks for
programming education. This work explores how LLMs can contribute to
programming education by supporting students with automated next-step hints. We
investigate prompt practices that lead to effective next-step hints and use
these insights to build our StAP-tutor. We evaluate this tutor by conducting an
experiment with students, and performing expert assessments. Our findings show
that most LLM-generated feedback messages describe one specific next step and
are personalised to the student's code and approach. However, the hints may
contain misleading information and lack sufficient detail when students
approach the end of the assignment. This work demonstrates the potential for
LLM-generated feedback, but further research is required to explore its
practical implementation.",2023-12-03T17:51:07Z
,http://arxiv.org/pdf/2403.01131v2.pdf,"LLaMoCo: Instruction Tuning of Large Language Models for Optimization
  Code Generation","Recent research explores optimization using large language models (LLMs) by
either iteratively seeking next-step solutions from LLMs or directly prompting
LLMs for an optimizer. However, these approaches exhibit inherent limitations,
including low operational efficiency, high sensitivity to prompt design, and a
lack of domain-specific knowledge. We introduce LLaMoCo, the first
instruction-tuning framework designed to adapt LLMs for solving optimization
problems in a code-to-code manner. Specifically, we establish a comprehensive
instruction set containing well-described problem prompts and effective
optimization codes. We then develop a novel two-phase learning strategy that
incorporates a contrastive learning-based warm-up procedure before the
instruction-tuning phase to enhance the convergence behavior during model
fine-tuning. The experiment results demonstrate that a CodeGen (350M) model
fine-tuned by our LLaMoCo achieves superior optimization performance compared
to GPT-4 Turbo and the other competitors across both synthetic and realistic
problem sets. The fine-tuned model and the usage instructions are available at
https://anonymous.4open.science/r/LLaMoCo-722A.",2024-03-02T08:21:59Z
,http://arxiv.org/pdf/2404.10990v1.pdf,"Automating Personalized Parsons Problems with Customized Contexts and
  Concepts","Parsons problems provide useful scaffolding for introductory programming
students learning to write code. However, generating large numbers of
high-quality Parsons problems that appeal to the diverse range of interests in
a typical introductory course is a significant challenge for educators. Large
language models (LLMs) may offer a solution, by allowing students to produce
on-demand Parsons problems for topics covering the breadth of the introductory
programming curriculum, and targeting thematic contexts that align with their
personal interests. In this paper, we introduce PuzzleMakerPy, an educational
tool that uses an LLM to generate unlimited contextualized drag-and-drop
programming exercises in the form of Parsons Problems, which introductory
programmers can use as a supplemental learning resource. We evaluated
PuzzleMakerPy by deploying it in a large introductory programming course, and
found that the ability to personalize the contextual framing used in problem
descriptions was highly engaging for students, and being able to customize the
programming topics was reported as being useful for their learning.",2024-04-17T02:01:50Z
,http://arxiv.org/pdf/2403.17218v1.pdf,"A Comprehensive Study of the Capabilities of Large Language Models for
  Vulnerability Detection","Large Language Models (LLMs) have demonstrated great potential for code
generation and other software engineering tasks. Vulnerability detection is of
crucial importance to maintaining the security, integrity, and trustworthiness
of software systems. Precise vulnerability detection requires reasoning about
the code, making it a good case study for exploring the limits of LLMs'
reasoning capabilities. Although recent work has applied LLMs to vulnerability
detection using generic prompting techniques, their full capabilities for this
task and the types of errors they make when explaining identified
vulnerabilities remain unclear.
  In this paper, we surveyed eleven LLMs that are state-of-the-art in code
generation and commonly used as coding assistants, and evaluated their
capabilities for vulnerability detection. We systematically searched for the
best-performing prompts, incorporating techniques such as in-context learning
and chain-of-thought, and proposed three of our own prompting methods. Our
results show that while our prompting methods improved the models' performance,
LLMs generally struggled with vulnerability detection. They reported 0.5-0.63
Balanced Accuracy and failed to distinguish between buggy and fixed versions of
programs in 76% of cases on average. By comprehensively analyzing and
categorizing 287 instances of model reasoning, we found that 57% of LLM
responses contained errors, and the models frequently predicted incorrect
locations of buggy code and misidentified bug types. LLMs only correctly
localized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted
correctly by 70-100% of human participants. These findings suggest that despite
their potential for other tasks, LLMs may fail to properly comprehend critical
code structures and security-related concepts. Our data and code are available
at https://figshare.com/s/78fe02e56e09ec49300b.",2024-03-25T21:47:36Z
,http://arxiv.org/pdf/2310.09690v2.pdf,Configuration Validation with Large Language Models,"Misconfigurations are major causes of software failures. Existing practices
rely on developer-written rules or test cases to validate configurations, which
are expensive. Machine learning (ML) for configuration validation is considered
a promising direction, but has been facing challenges such as the need of
large-scale field data and system-specific models. Recent advances in Large
Language Models (LLMs) show promise in addressing some of the long-lasting
limitations of ML-based configuration validation. We present a first analysis
on the feasibility and effectiveness of using LLMs for configuration
validation. We empirically evaluate LLMs as configuration validators by
developing a generic LLM-based configuration validation framework, named Ciri.
Ciri employs effective prompt engineering with few-shot learning based on both
valid configuration and misconfiguration data. Ciri checks outputs from LLMs
when producing results, addressing hallucination and nondeterminism of LLMs. We
evaluate Ciri's validation effectiveness on eight popular LLMs using
configuration data of ten widely deployed open-source systems. Our analysis (1)
confirms the potential of using LLMs for configuration validation, (2) explores
design space of LLMbased validators like Ciri, and (3) reveals open challenges
such as ineffectiveness in detecting certain types of misconfigurations and
biases towards popular configuration parameters.",2023-10-15T00:50:27Z
,http://arxiv.org/pdf/2311.01020v2.pdf,"Exploring the Problems, their Causes and Solutions of AI Pair
  Programming: A Study with Practitioners of GitHub Copilot","With the recent advancement of Artificial Intelligence (AI) and Large
Language Models (LLMs), AI-based code generation tools become a practical
solution for software development. GitHub Copilot, the AI pair programmer,
utilizes machine learning models trained on a large corpus of code snippets to
generate code suggestions using natural language processing. Despite its
popularity in software development, there is limited empirical evidence on the
actual experiences of practitioners who work with Copilot. To this end, we
conducted an empirical study to understand the problems that practitioners face
when using Copilot, as well as their underlying causes and potential solutions.
We collected data from 476 GitHub issues, 706 GitHub discussions, and 142 Stack
Overflow posts. Our results reveal that (1) Operation Issue and Compatibility
Issue are the most common problems faced by Copilot users, (2) Copilot Internal
Error, Network Connection Error, and Editor/IDE Compatibility Issue are
identified as the most frequent causes, and (3) Bug Fixed by Copilot, Modify
Configuration/Setting, and Use Suitable Version are the predominant solutions.
Based on the results, we discuss the potential areas of Copilot for
enhancement, and provide the implications for the Copilot users, the Copilot
team, and researchers.",2023-11-02T06:24:38Z
,http://arxiv.org/pdf/2308.11873v2.pdf,"Dcc --help: Generating Context-Aware Compiler Error Explanations with
  Large Language Models","In the challenging field of introductory programming, high enrollments and
failure rates drive us to explore tools and systems to enhance student
outcomes, especially automated tools that scale to large cohorts. This paper
presents and evaluates the dcc --help tool, an integration of a Large Language
Model (LLM) into the Debugging C Compiler (DCC) to generate unique,
novice-focused explanations tailored to each error. dcc --help prompts an LLM
with contextual information of compile- and run-time error occurrences,
including the source code, error location and standard compiler error message.
The LLM is instructed to generate novice-focused, actionable error explanations
and guidance, designed to help students understand and resolve problems without
providing solutions. dcc --help was deployed to our CS1 and CS2 courses, with
2,565 students using the tool over 64,000 times in ten weeks. We analysed a
subset of these error/explanation pairs to evaluate their properties, including
conceptual correctness, relevancy, and overall quality. We found that the
LLM-generated explanations were conceptually accurate in 90% of compile-time
and 75% of run-time cases, but often disregarded the instruction not to provide
solutions in code. Our findings, observations and reflections following
deployment indicate that dcc-help provides novel opportunities for scaffolding
students' introduction to programming.",2023-08-23T02:36:19Z
,http://arxiv.org/pdf/2406.07714v2.pdf,LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing,"Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in
programs. However, randomized mutation strategies have limited the fuzzer's
performance on structured data. Specialized fuzzers can handle complex
structured data, but require additional efforts in grammar and suffer from low
throughput.
  In this paper, we explore the potential of utilizing the Large Language Model
to enhance greybox fuzzing for structured data. We utilize the pre-trained
knowledge of LLM about data conversion and format to generate new valid inputs.
We further fine-tuned it with paired mutation seeds to learn structured format
and mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ,
integrates the power of LLM to understand and mutate structured data to
fuzzing. We conduct experiments on the standard bug-based benchmark Magma and a
wide variety of real-world programs. LLAMAFUZZ outperforms our top competitor
by 41 bugs on average. We also identified 47 unique bugs across all trials.
Moreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and
bug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in
real-world program sets on average. We also demonstrate a case study to explain
how LLMs enhance the fuzzing process in terms of code coverage.",2024-06-11T20:48:28Z
,http://arxiv.org/pdf/2404.10304v1.pdf,LLM-Powered Test Case Generation for Detecting Tricky Bugs,"Conventional automated test generation tools struggle to generate test
oracles and tricky bug-revealing test inputs. Large Language Models (LLMs) can
be prompted to produce test inputs and oracles for a program directly, but the
precision of the tests can be very low for complex scenarios (only 6.3% based
on our experiments). To fill this gap, this paper proposes AID, which combines
LLMs with differential testing to generate fault-revealing test inputs and
oracles targeting plausibly correct programs (i.e., programs that have passed
all the existing tests). In particular, AID selects test inputs that yield
diverse outputs on a set of program variants generated by LLMs, then constructs
the test oracle based on the outputs. We evaluate AID on two large-scale
datasets with tricky bugs: TrickyBugs and EvalPlus, and compare it with three
state-of-the-art baselines. The evaluation results show that the recall,
precision, and F1 score of AID outperform the state-of-the-art by up to 1.80x,
2.65x, and 1.66x, respectively.",2024-04-16T06:20:06Z
,http://arxiv.org/pdf/2304.11384v3.pdf,"Large Language Models are Few-Shot Summarizers: Multi-Intent Comment
  Generation via In-Context Learning","Code comment generation aims at generating natural language descriptions for
a code snippet to facilitate developers' program comprehension activities.
Despite being studied for a long time, a bottleneck for existing approaches is
that given a code snippet, they can only generate one comment while developers
usually need to know information from diverse perspectives such as what is the
functionality of this code snippet and how to use it. To tackle this
limitation, this study empirically investigates the feasibility of utilizing
large language models (LLMs) to generate comments that can fulfill developers'
diverse intents. Our intuition is based on the facts that (1) the code and its
pairwise comment are used during the pre-training process of LLMs to build the
semantic connection between the natural language and programming language, and
(2) comments in the real-world projects, which are collected for the
pre-training, usually contain different developers' intents. We thus postulate
that the LLMs can already understand the code from different perspectives after
the pre-training. Indeed, experiments on two large-scale datasets demonstrate
the rationale of our insights: by adopting the in-context learning paradigm and
giving adequate prompts to the LLM (e.g., providing it with ten or more
examples), the LLM can significantly outperform a state-of-the-art supervised
learning approach on generating comments with multiple intents. Results also
show that customized strategies for constructing the prompts and
post-processing strategies for reranking the results can both boost the LLM's
performances, which shed light on future research directions for using LLMs to
achieve comment generation.",2023-04-22T12:26:24Z
,http://arxiv.org/pdf/2305.00948v2.pdf,"Large Linguistic Models: Analyzing theoretical linguistic abilities of
  LLMs","The performance of large language models (LLMs) has recently improved to the
point where the models can perform well on many language tasks. We show here
that for the first time, the models can also generate coherent and valid formal
analyses of linguistic data and illustrate the vast potential of large language
models for analyses of their metalinguistic abilities. LLMs are primarily
trained on language data in the form of text; analyzing and evaluating their
metalinguistic abilities improves our understanding of their general
capabilities and sheds new light on theoretical models in linguistics. In this
paper, we probe into GPT-4's metalinguistic capabilities by focusing on three
subfields of formal linguistics: syntax, phonology, and semantics. We outline a
research program for metalinguistic analyses of large language models, propose
experimental designs, provide general guidelines, discuss limitations, and
offer future directions for this line of research. This line of inquiry also
exemplifies behavioral interpretability of deep learning, where models'
representations are accessed by explicit prompting rather than internal
representations.",2023-05-01T17:09:33Z
,http://arxiv.org/pdf/2310.18355v1.pdf,"Health Disparities through Generative AI Models: A Comparison Study
  Using A Domain Specific large language model","Health disparities are differences in health outcomes and access to
healthcare between different groups, including racial and ethnic minorities,
low-income people, and rural residents. An artificial intelligence (AI) program
called large language models (LLMs) can understand and generate human language,
improving health communication and reducing health disparities. There are many
challenges in using LLMs in human-doctor interaction, including the need for
diverse and representative data, privacy concerns, and collaboration between
healthcare providers and technology experts. We introduce the comparative
investigation of domain-specific large language models such as SciBERT with a
multi-purpose LLMs BERT. We used cosine similarity to analyze text queries
about health disparities in exam rooms when factors such as race are used
alone. Using text queries, SciBERT fails when it doesn't differentiate between
queries text: ""race"" alone and ""perpetuates health disparities."" We believe
clinicians can use generative AI to create a draft response when communicating
asynchronously with patients. However, careful attention must be paid to ensure
they are developed and implemented ethically and equitably.",2023-10-23T21:24:05Z
,http://arxiv.org/pdf/2401.03676v1.pdf,"Assessing AI Detectors in Identifying AI-Generated Code: Implications
  for Education","Educators are increasingly concerned about the usage of Large Language Models
(LLMs) such as ChatGPT in programming education, particularly regarding the
potential exploitation of imperfections in Artificial Intelligence Generated
Content (AIGC) Detectors for academic misconduct. In this paper, we present an
empirical study where the LLM is examined for its attempts to bypass detection
by AIGC Detectors. This is achieved by generating code in response to a given
question using different variants. We collected a dataset comprising 5,069
samples, with each sample consisting of a textual description of a coding
problem and its corresponding human-written Python solution codes. These
samples were obtained from various sources, including 80 from Quescol, 3,264
from Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of
code problem variant prompts, which were used to instruct ChatGPT to generate
the outputs. Subsequently, we assessed the performance of five AIGC detectors.
Our results demonstrate that existing AIGC Detectors perform poorly in
distinguishing between human-written code and AI-generated code.",2024-01-08T05:53:52Z
,http://arxiv.org/pdf/2405.15130v1.pdf,OptLLM: Optimal Assignment of Queries to Large Language Models,"Large Language Models (LLMs) have garnered considerable attention owing to
their remarkable capabilities, leading to an increasing number of companies
offering LLMs as services. Different LLMs achieve different performance at
different costs. A challenge for users lies in choosing the LLMs that best fit
their needs, balancing cost and performance. In this paper, we propose a
framework for addressing the cost-effective query allocation problem for LLMs.
Given a set of input queries and candidate LLMs, our framework, named OptLLM,
provides users with a range of optimal solutions to choose from, aligning with
their budget constraints and performance preferences, including options for
maximizing accuracy and minimizing cost. OptLLM predicts the performance of
candidate LLMs on each query using a multi-label classification model with
uncertainty estimation and then iteratively generates a set of non-dominated
solutions by destructing and reconstructing the current solution. To evaluate
the effectiveness of OptLLM, we conduct extensive experiments on various types
of tasks, including text classification, question answering, sentiment
analysis, reasoning, and log parsing. Our experimental results demonstrate that
OptLLM substantially reduces costs by 2.40% to 49.18% while achieving the same
accuracy as the best LLM. Compared to other multi-objective optimization
algorithms, OptLLM improves accuracy by 2.94% to 69.05% at the same cost or
saves costs by 8.79% and 95.87% while maintaining the highest attainable
accuracy.",2024-05-24T01:05:37Z
,http://arxiv.org/pdf/2406.06025v1.pdf,RepoQA: Evaluating Long Context Code Understanding,"Recent advances have been improving the context windows of Large Language
Models (LLMs). To quantify the real long-context capabilities of LLMs,
evaluators such as the popular Needle in a Haystack have been developed to test
LLMs over a large chunk of raw texts. While effective, current evaluations
overlook the insight of how LLMs work with long-context code, i.e.,
repositories. To this end, we initiate the RepoQA benchmark to evaluate LLMs on
long-context code understanding. Traditional needle testers ask LLMs to
directly retrieve the answer from the context without necessary deep
understanding. In RepoQA, we built our initial task, namely Searching Needle
Function (SNF), which exercises LLMs to search functions given their
natural-language description, i.e., LLMs cannot find the desired function if
they cannot understand the description and code. RepoQA is multilingual and
comprehensive: it includes 500 code search tasks gathered from 50 popular
repositories across 5 modern programming languages. By evaluating 26 general
and code-specific LLMs on RepoQA, we show (i) there is still a small gap
between the best open and proprietary models; (ii) different models are good at
different languages; and (iii) models may understand code better without
comments.",2024-06-10T05:15:30Z
,http://arxiv.org/pdf/2311.16017v1.pdf,"Decoding Logic Errors: A Comparative Study on Bug Detection by Students
  and Large Language Models","Identifying and resolving logic errors can be one of the most frustrating
challenges for novices programmers. Unlike syntax errors, for which a compiler
or interpreter can issue a message, logic errors can be subtle. In certain
conditions, buggy code may even exhibit correct behavior -- in other cases, the
issue might be about how a problem statement has been interpreted. Such errors
can be hard to spot when reading the code, and they can also at times be missed
by automated tests. There is great educational potential in automatically
detecting logic errors, especially when paired with suitable feedback for
novices. Large language models (LLMs) have recently demonstrated surprising
performance for a range of computing tasks, including generating and explaining
code. These capabilities are closely linked to code syntax, which aligns with
the next token prediction behavior of LLMs. On the other hand, logic errors
relate to the runtime performance of code and thus may not be as well suited to
analysis by LLMs. To explore this, we investigate the performance of two
popular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly
explanation of logic errors. We compare LLM performance with a large cohort of
introductory computing students $(n=964)$ solving the same error detection
task. Through a mixed-methods analysis of student and model responses, we
observe significant improvement in logic error identification between the
previous and current generation of LLMs, and find that both LLM generations
significantly outperform students. We outline how such models could be
integrated into computing education tools, and discuss their potential for
supporting students when learning programming.",2023-11-27T17:28:33Z
,http://arxiv.org/pdf/2305.01210v3.pdf,"Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of
  Large Language Models for Code Generation","Program synthesis has been long studied with recent approaches focused on
directly using the power of Large Language Models (LLMs) to generate code.
Programming benchmarks, with curated synthesis problems and test-cases, are
used to measure the performance of various LLMs on code synthesis. However,
these test-cases can be limited in both quantity and quality for fully
assessing the functional correctness of the generated code. Such limitation in
the existing benchmarks begs the following question: In the era of LLMs, is the
code generated really correct? To answer this, we propose EvalPlus -- a code
synthesis evaluation framework to rigorously benchmark the functional
correctness of LLM-synthesized code. EvalPlus augments a given evaluation
dataset with large amounts of test-cases newly produced by an automatic test
input generator, powered by both LLM- and mutation-based strategies. While
EvalPlus is general, we extend the test-cases of the popular HumanEval
benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
catch significant amounts of previously undetected wrong code synthesized by
LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
test insufficiency can lead to mis-ranking. For example, both
WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
while none of them could on HumanEval. Our work not only indicates that prior
popular code synthesis evaluation results do not accurately reflect the true
performance of LLMs for code synthesis, but also opens up a new direction to
improve such programming benchmarks through automated testing. We have
open-sourced our tools, enhanced datasets as well as all LLM-generated code at
https://github.com/evalplus/evalplus to facilitate and accelerate future
LLM-for-code research.",2023-05-02T05:46:48Z
,http://arxiv.org/pdf/2312.09601v1.pdf,"Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large
  Language Models","Binary code summarization, while invaluable for understanding code semantics,
is challenging due to its labor-intensive nature. This study delves into the
potential of large language models (LLMs) for binary code comprehension. To
this end, we present BinSum, a comprehensive benchmark and dataset of over 557K
binary functions and introduce a novel method for prompt synthesis and
optimization. To more accurately gauge LLM performance, we also propose a new
semantic similarity metric that surpasses traditional exact-match approaches.
Our extensive evaluation of prominent LLMs, including ChatGPT, GPT-4, Llama 2,
and Code Llama, reveals 10 pivotal insights. This evaluation generates 4
billion inference tokens, incurred a total expense of 11,418 US dollars and 873
NVIDIA A100 GPU hours. Our findings highlight both the transformative potential
of LLMs in this field and the challenges yet to be overcome.",2023-12-15T08:32:28Z
,http://arxiv.org/pdf/2404.13066v2.pdf,"Leveraging Large Language Model as Simulated Patients for Clinical
  Education","Simulated Patients (SPs) play a crucial role in clinical medical education by
providing realistic scenarios for student practice. However, the high cost of
training and hiring qualified SPs, along with the heavy workload and potential
risks they face in consistently portraying actual patients, limit students'
access to this type of clinical training. Consequently, the integration of
computer program-based simulated patients has emerged as a valuable educational
tool in recent years. With the rapid development of Large Language Models
(LLMs), their exceptional capabilities in conversational artificial
intelligence and role-playing have been demonstrated, making them a feasible
option for implementing Virtual Simulated Patient (VSP). In this paper, we
present an integrated model-agnostic framework called CureFun that harnesses
the potential of LLMs in clinical medical education. This framework facilitates
natural conversations between students and simulated patients, evaluates their
dialogue, and provides suggestions to enhance students' clinical inquiry
skills. Through comprehensive evaluations, our approach demonstrates more
authentic and professional SP-scenario dialogue flows compared to other
LLM-based chatbots, thus proving its proficiency in simulating patients.
Additionally, leveraging CureFun's evaluation ability, we assess several
medical LLMs and discuss the possibilities and limitations of using LLMs as
virtual doctors from the perspective of their diagnostic abilities.",2024-04-13T06:36:32Z
,http://arxiv.org/pdf/2306.00597v2.pdf,Analysis of ChatGPT on Source Code,"This paper explores the use of Large Language Models (LLMs) and in particular
ChatGPT in programming, source code analysis, and code generation. LLMs and
ChatGPT are built using machine learning and artificial intelligence
techniques, and they offer several benefits to developers and programmers.
While these models can save time and provide highly accurate results, they are
not yet advanced enough to replace human programmers entirely. The paper
investigates the potential applications of LLMs and ChatGPT in various areas,
such as code creation, code documentation, bug detection, refactoring, and
more. The paper also suggests that the usage of LLMs and ChatGPT is expected to
increase in the future as they offer unparalleled benefits to the programming
community.",2023-06-01T12:12:59Z
,http://arxiv.org/pdf/2401.16445v3.pdf,OMPGPT: A Generative Pre-trained Transformer Model for OpenMP,"Large language models (LLMs)such as ChatGPT have significantly advanced the
field of Natural Language Processing (NLP). This trend led to the development
of code-based large language models such as StarCoder, WizardCoder, and
CodeLlama, which are trained extensively on vast repositories of code and
programming languages. While the generic abilities of these code LLMs are
useful for many programmers in tasks like code generation, the area of
high-performance computing (HPC) has a narrower set of requirements that make a
smaller and more domain-specific model a smarter choice. This paper presents
OMPGPT, a novel domain-specific model meticulously designed to harness the
inherent strengths of language models for OpenMP pragma generation.
Furthermore, we leverage prompt engineering techniques from the NLP domain to
create Chain-of-OMP, an innovative strategy designed to enhance OMPGPT's
effectiveness. Our extensive evaluations demonstrate that OMPGPT outperforms
existing large language models specialized in OpenMP tasks and maintains a
notably smaller size, aligning it more closely with the typical hardware
constraints of HPC environments. We consider our contribution as a pivotal
bridge, connecting the advantage of language models with the specific demands
of HPC tasks.",2024-01-28T06:06:59Z
,http://arxiv.org/pdf/2401.14423v4.pdf,Prompt Design and Engineering: Introduction and Advanced Methods,"Prompt design and engineering has rapidly become essential for maximizing the
potential of large language models. In this paper, we introduce core concepts,
advanced techniques like Chain-of-Thought and Reflection, and the principles
behind building LLM-based agents. Finally, we provide a survey of tools for
prompt engineers.",2024-01-24T06:20:18Z
,http://arxiv.org/pdf/2306.10509v2.pdf,"Can We Trust AI-Generated Educational Content? Comparative Analysis of
  Human and AI-Generated Learning Resources","As an increasing number of students move to online learning platforms that
deliver personalized learning experiences, there is a great need for the
production of high-quality educational content. Large language models (LLMs)
appear to offer a promising solution to the rapid creation of learning
materials at scale, reducing the burden on instructors. In this study, we
investigated the potential for LLMs to produce learning resources in an
introductory programming context, by comparing the quality of the resources
generated by an LLM with those created by students as part of a learnersourcing
activity. Using a blind evaluation, students rated the correctness and
helpfulness of resources generated by AI and their peers, after both were
initially provided with identical exemplars. Our results show that the quality
of AI-generated resources, as perceived by students, is equivalent to the
quality of resources generated by their peers. This suggests that AI-generated
resources may serve as viable supplementary material in certain contexts.
Resources generated by LLMs tend to closely mirror the given exemplars, whereas
student-generated resources exhibit greater variety in terms of content length
and specific syntax features used. The study highlights the need for further
research exploring different types of learning resources and a broader range of
subject areas, and understanding the long-term impact of AI-generated resources
on learning outcomes.",2023-06-18T09:49:21Z
,http://arxiv.org/pdf/2310.15123v2.pdf,"Branch-Solve-Merge Improves Large Language Model Evaluation and
  Generation","Large Language Models (LLMs) are frequently used for multi-faceted language
generation and evaluation tasks that involve satisfying intricate user
constraints or taking into account multiple aspects and criteria. However,
their performance can fall short, due to the model's lack of coherence and
inability to plan and decompose the problem. We propose Branch-Solve-Merge
(BSM), a Large Language Model program (Schlag et al., 2023) for tackling such
challenging natural language tasks. It consists of branch, solve, and merge
modules that are parameterized with specific prompts to the base LLM. These
three modules plan a decomposition of the task into multiple parallel
sub-tasks, independently solve them, and fuse the solutions to the sub-tasks.
We apply our method to the tasks of LLM response evaluation and constrained
text generation and evaluate its effectiveness with multiple LLMs, including
Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and
consistency for each LLM by enhancing human-LLM agreement by up to 26%,
reducing length and pairwise position biases by up to 50%, and allowing
LLaMA2-chat to match or outperform GPT-4 on most domains. On a constraint story
generation task, BSM improves the coherence of stories while also improving
constraint satisfaction by 12%.",2023-10-23T17:29:48Z
,http://arxiv.org/pdf/2406.16739v1.pdf,Agent-Driven Automatic Software Improvement,"With software maintenance accounting for 50% of the cost of developing
software, enhancing code quality and reliability has become more critical than
ever. In response to this challenge, this doctoral research proposal aims to
explore innovative solutions by focusing on the deployment of agents powered by
Large Language Models (LLMs) to perform software maintenance tasks. The
iterative nature of agents, which allows for continuous learning and
adaptation, can help surpass common challenges in code generation. One distinct
challenge is the last-mile problems, errors at the final stage of producing
functionally and contextually relevant code. Furthermore, this project aims to
surpass the inherent limitations of current LLMs in source code through a
collaborative framework where agents can correct and learn from each other's
errors. We aim to use the iterative feedback in these systems to further
fine-tune the LLMs underlying the agents, becoming better aligned to the task
of automated software improvement. Our main goal is to achieve a leap forward
in the field of automatic software improvement by developing new tools and
frameworks that can enhance the efficiency and reliability of software
development.",2024-06-24T15:45:22Z
,http://arxiv.org/pdf/2302.04662v2.pdf,"Generating High-Precision Feedback for Programming Syntax Errors using
  Large Language Models","Large language models (LLMs), such as Codex, hold great promise in enhancing
programming education by automatically generating feedback for students. We
investigate using LLMs to generate feedback for fixing syntax errors in Python
programs, a key scenario in introductory programming. More concretely, given a
student's buggy program, our goal is to generate feedback comprising a fixed
program along with a natural language explanation describing the errors/fixes,
inspired by how a human tutor would give feedback. While using LLMs is
promising, the critical challenge is to ensure high precision in the generated
feedback, which is imperative before deploying such technology in classrooms.
The main research question we study is: Can we develop LLMs-based feedback
generation techniques with a tunable precision parameter, giving educators
quality control over the feedback that students receive? To this end, we
introduce PyFiXV, our technique to generate high-precision feedback powered by
Codex. The key idea behind PyFiXV is to use a novel run-time validation
mechanism to decide whether the generated feedback is suitable for sharing with
the student; notably, this validation mechanism also provides a precision knob
to educators. We perform an extensive evaluation using two real-world datasets
of Python programs with syntax errors and show the efficacy of PyFiXV in
generating high-precision feedback.",2023-01-24T13:00:25Z
,http://arxiv.org/pdf/2405.00302v3.pdf,"Generating Feedback-Ladders for Logical Errors in Programming using
  Large Language Models","In feedback generation for logical errors in programming assignments, large
language model (LLM)-based methods have shown great promise. These methods ask
the LLM to generate feedback given the problem statement and a student's
(buggy) submission. There are several issues with these types of methods.
First, the generated feedback messages are often too direct in revealing the
error in the submission and thus diminish valuable opportunities for the
student to learn. Second, they do not consider the student's learning context,
i.e., their previous submissions, current knowledge, etc. Third, they are not
layered since existing methods use a single, shared prompt for all student
submissions. In this paper, we explore using LLMs to generate a
""feedback-ladder"", i.e., multiple levels of feedback for the same
problem-submission pair. We evaluate the quality of the generated
feedback-ladder via a user study with students, educators, and researchers. We
have observed diminishing effectiveness for higher-level feedback and
higher-scoring submissions overall in the study. In practice, our method
enables teachers to select an appropriate level of feedback to show to a
student based on their personal learning context, or in a progressive manner to
go more detailed if a higher-level feedback fails to correct the student's
error.",2024-05-01T03:52:39Z
,http://arxiv.org/pdf/2406.06647v2.pdf,"How Efficient is LLM-Generated Code? A Rigorous & High-Standard
  Benchmark","The emergence of large language models (LLMs) has significantly pushed the
frontiers of program synthesis. Advancement of LLM-based program synthesis
calls for a thorough evaluation of LLM-generated code. Most evaluation
frameworks focus on the (functional) correctness of generated code; efficiency,
as an important measure of code quality, has been overlooked in existing
evaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a
rigorous and high-standard benchmark for evaluating the capability of LLMs in
generating efficient code. Firstly, we propose a new efficiency metric called
eff@k, which generalizes the pass@k metric from correctness to efficiency and
appropriately handles right-censored execution time. Furthermore, we derive an
unbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we
also provide a numerically stable implementation for the new estimator.
Secondly, to set a high-standard for efficiency evaluation, we employ a human
expert to design best algorithms and implementations as our reference solutions
of efficiency, many of which are much more efficient than existing canonical
solutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous
evaluation, we employ a human expert to curate strong test case generators to
filter out wrong code and differentiate suboptimal algorithms. An extensive
study across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still
fall short of generating expert-level efficient code. Using two subsets of our
problem set, we demonstrate that such deficiency is because current LLMs
struggle in designing advanced algorithms and are barely aware of
implementation optimization. Our benchmark is publicly available at
https://github.com/q-rz/enamel .",2024-06-10T04:19:20Z
,http://arxiv.org/pdf/2402.10517v4.pdf,"Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs","Recently, considerable efforts have been directed towards compressing Large
Language Models (LLMs), which showcase groundbreaking capabilities across
diverse applications but entail significant deployment costs due to their large
sizes. Meanwhile, much less attention has been given to mitigating the costs
associated with deploying multiple LLMs of varying sizes despite its practical
significance. Thus, this paper introduces \emph{any-precision LLM}, extending
the concept of any-precision DNN to LLMs. Addressing challenges in
any-precision LLM, we propose a lightweight method for any-precision
quantization of LLMs, leveraging a post-training quantization framework, and
develop a specialized software engine for its efficient serving. As a result,
our solution significantly reduces the high costs of deploying multiple,
different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such
as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit
LLM. All the supported LLMs with varying bit-widths demonstrate
state-of-the-art model quality and inference throughput, proving itself to be a
compelling option for deployment of multiple, different-sized LLMs. Our code is
open-sourced and available online.",2024-02-16T09:06:06Z
,http://arxiv.org/pdf/2309.03567v1.pdf,"The Devil is in the Tails: How Long-Tailed Code Distributions Impact
  Large Language Models","Learning-based techniques, especially advanced Large Language Models (LLMs)
for code, have gained considerable popularity in various software engineering
(SE) tasks. However, most existing works focus on designing better
learning-based models and pay less attention to the properties of datasets.
Learning-based models, including popular LLMs for code, heavily rely on data,
and the data's properties (e.g., data distribution) could significantly affect
their behavior. We conducted an exploratory study on the distribution of SE
data and found that such data usually follows a skewed distribution (i.e.,
long-tailed distribution) where a small number of classes have an extensive
collection of samples, while a large number of classes have very few samples.
We investigate three distinct SE tasks and analyze the impacts of long-tailed
distribution on the performance of LLMs for code. Our experimental results
reveal that the long-tailed distribution has a substantial impact on the
effectiveness of LLMs for code. Specifically, LLMs for code perform between
30.0\% and 254.0\% worse on data samples associated with infrequent labels
compared to data samples of frequent labels. Our study provides a better
understanding of the effects of long-tailed distributions on popular LLMs for
code and insights for the future development of SE automation.",2023-09-07T08:53:16Z
,http://arxiv.org/pdf/2404.09384v1.pdf,"Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software
  Verification and Falsification Approaches","Prompting has become one of the main approaches to leverage emergent
capabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al.
TMLR 2022, Wei et al. NeurIPS 2022]. During the last year, researchers and
practitioners have been playing with prompts to see how to make the most of
LLMs. By homogeneously dissecting 80 papers, we investigate in deep how
software testing and verification research communities have been abstractly
architecting their LLM-enabled solutions. More precisely, first, we want to
validate whether downstream tasks are an adequate concept to convey the
blueprint of prompt-based solutions. We also aim at identifying number and
nature of such tasks in solutions. For such goal, we develop a novel downstream
task taxonomy that enables pinpointing some engineering patterns in a rather
varied spectrum of Software Engineering problems that encompasses testing,
fuzzing, debugging, vulnerability detection, static analysis and program
verification approaches.",2024-04-14T23:45:23Z
,http://arxiv.org/pdf/2312.15698v4.pdf,"RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for
  Program Repair","Automated Program Repair (APR) has evolved significantly with the advent of
Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent
avenue of research, with many dimensions which have not been explored. Existing
work mostly fine-tune LLMs with naive code representations and does not scale
to frontier models. To address this problem, we propose RepairLLaMA, a novel
program repair approach that 1) identifies optimal code representations for APR
with fine-tuned models, and 2) pioneers state-of-the-art parameter-efficient
fine-tuning technique (PEFT) for program repair. This results in RepairLLaMA
producing a highly effective `program repair adapter' for fixing bugs with AI.
Our experiments demonstrate the validity of both concepts. First, fine-tuning
adapters with program repair specific code representations enables the model to
use meaningful repair signals and produce better patches. Second,
parameter-efficient fine-tuning helps fine-tuning to converge and clearly
contributes to the effectiveness of RepairLLaMA in fixing bugs outside the
fine-tuning data distribution. Overall, RepairLLaMA correctly fixes 144
Defects4J v2 and 109 HumanEval-Java bugs, outperforming all baselines.",2023-12-25T11:39:46Z
,http://arxiv.org/pdf/2309.17446v2.pdf,"L2CEval: Evaluating Language-to-Code Generation Capabilities of Large
  Language Models","Recently, large language models (LLMs), especially those that are pretrained
on code, have demonstrated strong capabilities in generating programs from
natural language inputs in a few-shot or even zero-shot manner. Despite
promising results, there is a notable lack of a comprehensive evaluation of
these models language-to-code generation capabilities. Existing studies often
focus on specific tasks, model architectures, or learning paradigms, leading to
a fragmented understanding of the overall landscape. In this work, we present
L2CEval, a systematic evaluation of the language-to-code generation
capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing,
math reasoning and Python programming, analyzing the factors that potentially
affect their performance, such as model size, pretraining data, instruction
tuning, and different prompting methods. In addition to assessing model
performance, we measure confidence calibration for the models and conduct human
evaluations of the output programs. This enables us to identify and analyze the
typical failure modes across various tasks and models. L2CEval offers a
comprehensive understanding of the capabilities and limitations of LLMs in
language-to-code generation. We also release the evaluation framework and all
model outputs, hoping to lay the groundwork for further future research in this
domain.",2023-09-29T17:57:00Z
,http://arxiv.org/pdf/2312.15960v2.pdf,"MoTCoder: Elevating Large Language Models with Modular of Thought for
  Challenging Programming Tasks","Large Language Models (LLMs) have showcased impressive capabilities in
handling straightforward programming tasks. However, their performance tends to
falter when confronted with more challenging programming problems. We observe
that conventional models often generate solutions as monolithic code blocks,
restricting their effectiveness in tackling intricate questions. To overcome
this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a
pioneering framework for MoT instruction tuning, designed to promote the
decomposition of tasks into logical sub-tasks and sub-modules. Our
investigations reveal that, through the cultivation and utilization of
sub-modules, MoTCoder significantly improves both the modularity and
correctness of the generated solutions, leading to substantial relative pass@1
improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are
available at https://github.com/dvlab-research/MoTCoder.",2023-12-26T08:49:57Z
,http://arxiv.org/pdf/2402.16896v2.pdf,On Trojan Signatures in Large Language Models of Code,"Trojan signatures, as described by Fields et al. (2021), are noticeable
differences in the distribution of the trojaned class parameters (weights) and
the non-trojaned class parameters of the trojaned model, that can be used to
detect the trojaned model. Fields et al. (2021) found trojan signatures in
computer vision classification tasks with image models, such as, Resnet,
WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in
the classifier layer parameters of large language models of source code.
  Our results suggest that trojan signatures could not generalize to LLMs of
code. We found that trojaned code models are stubborn, even when the models
were poisoned under more explicit settings (finetuned with pre-trained weights
frozen). We analyzed nine trojaned models for two binary classification tasks:
clone and defect detection. To the best of our knowledge, this is the first
work to examine weight-based trojan signature revelation techniques for
large-language models of code and furthermore to demonstrate that detecting
trojans only from the weights in such models is a hard problem.",2024-02-23T22:48:29Z
,http://arxiv.org/pdf/2406.07400v1.pdf,"Guiding LLM Temporal Logic Generation with Explicit Separation of Data
  and Control","Temporal logics are powerful tools that are widely used for the synthesis and
verification of reactive systems. The recent progress on Large Language Models
(LLMs) has the potential to make the process of writing such specifications
more accessible. However, writing specifications in temporal logics remains
challenging for all but the most expert users. A key question in using LLMs for
temporal logic specification engineering is to understand what kind of guidance
is most helpful to the LLM and the users to easily produce specifications.
Looking specifically at the problem of reactive program synthesis, we explore
the impact of providing an LLM with guidance on the separation of control and
data--making explicit for the LLM what functionality is relevant for the
specification, and treating the remaining functionality as an implementation
detail for a series of pre-defined functions and predicates. We present a
benchmark set and find that this separation of concerns improves specification
generation. Our benchmark provides a test set against which to verify future
work in LLM generation of temporal logic specifications.",2024-06-11T16:07:24Z
,http://arxiv.org/pdf/2310.02368v1.pdf,"Reinforcement Learning from Automatic Feedback for High-Quality Unit
  Test Generation","Software testing is a crucial aspect of software development, and the
creation of high-quality tests that adhere to best practices is essential for
effective maintenance. Recently, Large Language Models (LLMs) have gained
popularity for code generation, including the automated creation of test cases.
However, these LLMs are often trained on vast amounts of publicly available
code, which may include test cases that do not adhere to best practices and may
even contain test smells (anti-patterns). To address this issue, we propose a
novel technique called Reinforcement Learning from Static Quality Metrics
(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show
that LLMs can generate undesirable test smells. Thus, we train specific reward
models for each static quality metric, then utilize Proximal Policy
Optimization (PPO) to train models for optimizing a single quality metric at a
time. Furthermore, we amalgamate these rewards into a unified reward model
aimed at capturing different best practices and quality aspects of tests. By
comparing RL-trained models with those trained using supervised learning, we
provide insights into how reliably utilize RL to improve test generation
quality and into the effects of various training strategies. Our experimental
results demonstrate that the RL-optimized model consistently generated
high-quality test cases compared to the base LLM, improving the model by up to
21%, and successfully generates nearly 100% syntactically correct code. RLSQM
also outperformed GPT-4 on four out of seven metrics. This represents a
significant step towards enhancing the overall efficiency and reliability of
software testing through Reinforcement Learning and static quality metrics. Our
data are available at this link: https://figshare.com/s/ded476c8d4c221222849.",2023-10-03T18:48:31Z
,http://arxiv.org/pdf/2405.15512v1.pdf,ChatGPT Code Detection: Techniques for Uncovering the Source of Code,"In recent times, large language models (LLMs) have made significant strides
in generating computer code, blurring the lines between code created by humans
and code produced by artificial intelligence (AI). As these technologies evolve
rapidly, it is crucial to explore how they influence code generation,
especially given the risk of misuse in areas like higher education. This paper
explores this issue by using advanced classification techniques to
differentiate between code written by humans and that generated by ChatGPT, a
type of LLM. We employ a new approach that combines powerful embedding features
(black-box) with supervised learning algorithms - including Deep Neural
Networks, Random Forests, and Extreme Gradient Boosting - to achieve this
differentiation with an impressive accuracy of 98%. For the successful
combinations, we also examine their model calibration, showing that some of the
models are extremely well calibrated. Additionally, we present white-box
features and an interpretable Bayes classifier to elucidate critical
differences between the code sources, enhancing the explainability and
transparency of our approach. Both approaches work well but provide at most
85-88% accuracy. We also show that untrained humans solve the same task not
better than random guessing. This study is crucial in understanding and
mitigating the potential risks associated with using AI in code generation,
particularly in the context of higher education, software development, and
competitive programming.",2024-05-24T12:56:18Z
,http://arxiv.org/pdf/2404.11734v1.pdf,"Let's Ask AI About Their Programs: Exploring ChatGPT's Answers To
  Program Comprehension Questions","Recent research has explored the creation of questions from code submitted by
students. These Questions about Learners' Code (QLCs) are created through
program analysis, exploring execution paths, and then creating code
comprehension questions from these paths and the broader code structure.
Responding to the questions requires reading and tracing the code, which is
known to support students' learning. At the same time, computing education
researchers have witnessed the emergence of Large Language Models (LLMs) that
have taken the community by storm. Researchers have demonstrated the
applicability of these models especially in the introductory programming
context, outlining their performance in solving introductory programming
problems and their utility in creating new learning resources. In this work, we
explore the capability of the state-of-the-art LLMs (GPT-3.5 and GPT-4) in
answering QLCs that are generated from code that the LLMs have created. Our
results show that although the state-of-the-art LLMs can create programs and
trace program execution when prompted, they easily succumb to similar errors
that have previously been recorded for novice programmers. These results
demonstrate the fallibility of these models and perhaps dampen the expectations
fueled by the recent LLM hype. At the same time, we also highlight future
research possibilities such as using LLMs to mimic students as their behavior
can indeed be similar for some specific tasks.",2024-04-17T20:37:00Z
,http://arxiv.org/pdf/2403.09740v1.pdf,Teaching Machines to Code: Smart Contract Translation with LLMs,"The advent of large language models (LLMs) has marked a significant milestone
in the realm of artificial intelligence, with their capabilities often matching
or surpassing human expertise in various domains. Among these achievements,
their adeptness in translation tasks stands out, closely mimicking the
intricate and preliminary processes undertaken by human translators to ensure
the fidelity and quality of the translated content. Despite the advancements in
utilizing LLMs for translating programming code across different languages, the
domain of smart contract translation, particularly into languages not
previously encountered by the LLM, remains largely unexplored. In our research,
we present a pioneering approach, SolMover, which harnesses the synergy of two
distinct LLMs within a unified framework. This framework is designed to grasp
coding principles and apply this understanding to the translation of code into
an unfamiliar language. Our study delves into the capacity of LLMs to mimic
human learning processes, offering an in-depth evaluation of our methodology
for converting smart contracts written in Solidity to Move, a language with
limited resources. The framework employs one LLM to decipher coding conventions
for the new language, creating a blueprint for the second LLM, which, lacking
planning abilities, possesses coding expertise. The empirical evidence from our
experiments suggests that SolMover substantially enhances performance compared
to gpt-3.5-turbo-1106, and achieves superior results over competitors such as
Palm2 and Mixtral-8x7B-Instruct. Additionally, our analysis highlights the
efficacy of our bug mitigation strategy in elevating code quality across all
models, even outside the SolMover framework.",2024-03-13T18:55:20Z
,http://arxiv.org/pdf/2402.15938v3.pdf,"Generalization or Memorization: Data Contamination and Trustworthy
  Evaluation for Large Language Models","Recent statements about the impressive capabilities of large language models
(LLMs) are usually supported by evaluating on open-access benchmarks.
Considering the vast size and wide-ranging sources of LLMs' training data, it
could explicitly or implicitly include test data, leading to LLMs being more
susceptible to data contamination. However, due to the opacity of training
data, the black-box access of models, and the rapid growth of synthetic
training data, detecting and mitigating data contamination for LLMs faces
significant challenges. In this paper, we propose CDD, which stands for
Contamination Detection via output Distribution for LLMs. CDD necessitates only
the sampled texts to detect data contamination, by identifying the peakedness
of LLM's output distribution. To mitigate the impact of data contamination in
evaluation, we also present TED: Trustworthy Evaluation via output
Distribution, based on the correction of LLM's output distribution. To
facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,
for data contamination detection and contamination mitigation evaluation tasks.
Extensive experimental results show that CDD achieves the average relative
improvements of 21.8\%-30.2\% over other contamination detection approaches in
terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect
implicit contamination. TED substantially mitigates performance improvements up
to 66.9\% attributed to data contamination across various contamination setups.
In real-world applications, we reveal that ChatGPT exhibits a high potential to
suffer from data contamination on HumanEval benchmark.",2024-02-24T23:54:41Z
,http://arxiv.org/pdf/2310.03094v3.pdf,"Large Language Model Cascades with Mixture of Thoughts Representations
  for Cost-efficient Reasoning","Large language models (LLMs) such as GPT-4 have exhibited remarkable
performance in a variety of tasks, but this strong performance often comes with
the high expense of using paid API services. In this paper, we are motivated to
study building an LLM cascade to save the cost of using LLMs, particularly for
performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline
follows the intuition that simpler questions can be addressed by a weaker but
more affordable LLM, whereas only the challenging questions necessitate the
stronger and more expensive LLM. To realize this decision-making, we consider
the ""answer consistency"" of the weaker LLM as a signal of the question
difficulty and propose several methods for the answer sampling and consistency
checking, including one leveraging a mixture of two thought representations
(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six
reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and
stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can
achieve performance comparable to using solely the stronger LLM but require
only 40% of its cost.",2023-10-04T18:21:17Z
,http://arxiv.org/pdf/2303.10494v1.pdf,Revisiting the Plastic Surgery Hypothesis via Large Language Models,"Automated Program Repair (APR) aspires to automatically generate patches for
an input buggy program. Traditional APR tools typically focus on specific bug
types and fixes through the use of templates, heuristics, and formal
specifications. However, these techniques are limited in terms of the bug types
and patch variety they can produce. As such, researchers have designed various
learning-based APR tools with recent work focused on directly using Large
Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve
state-of-the-art performance on many repair datasets, the LLMs used for direct
repair are not fully aware of the project-specific information such as unique
variable or method names.
  The plastic surgery hypothesis is a well-known insight for APR, which states
that the code ingredients to fix the bug usually already exist within the same
project. Traditional APR tools have largely leveraged the plastic surgery
hypothesis by designing manual or heuristic-based approaches to exploit such
existing code ingredients. However, as recent APR research starts focusing on
LLM-based approaches, the plastic surgery hypothesis has been largely ignored.
In this paper, we ask the following question: How useful is the plastic surgery
hypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique
opportunity to fully automate the plastic surgery hypothesis via fine-tuning
and prompting. To this end, we propose FitRepair, which combines the direct
usage of LLMs with two domain-specific fine-tuning strategies and one prompting
strategy for more powerful APR. Our experiments on the widely studied Defects4j
1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially
outperforming the best-performing baseline by 15 and 8), respectively,
demonstrating a promising future of the plastic surgery hypothesis in the era
of LLMs.",2023-03-18T20:33:46Z
,http://arxiv.org/pdf/2312.07343v2.pdf,"Can ChatGPT Play the Role of a Teaching Assistant in an Introductory
  Programming Course?","The emergence of Large language models (LLMs) is expected to have a major
impact on education. This paper explores the potential of using ChatGPT, an
LLM, as a virtual Teaching Assistant (TA) in an Introductory Programming
Course. We evaluate ChatGPT's capabilities by comparing its performance with
that of human TAs in some of the important TA functions. The TA functions which
we focus on include (1) grading student code submissions, and (2) providing
feedback to undergraduate students in an introductory programming course.
Firstly, we assess ChatGPT's proficiency in grading student code submissions
using a given grading rubric and compare its performance with the grades
assigned by human TAs. Secondly, we analyze the quality and relevance of the
feedback provided by ChatGPT. This evaluation considers how well ChatGPT
addresses mistakes and offers suggestions for improvement in student solutions
from both code correctness and code quality perspectives. We conclude with a
discussion on the implications of integrating ChatGPT into computing education
for automated grading, personalized learning experiences, and instructional
support.",2023-12-12T15:06:44Z
,http://arxiv.org/pdf/2404.08850v2.pdf,"Assessing Economic Viability: A Comparative Analysis of Total Cost of
  Ownership for Domain-Adapted Large Language Models versus State-of-the-art
  Counterparts in Chip Design Coding Assistance","This paper presents a comparative analysis of total cost of ownership (TCO)
and performance between domain-adapted large language models (LLM) and
state-of-the-art (SoTA) LLMs , with a particular emphasis on tasks related to
coding assistance for chip design. We examine the TCO and performance metrics
of a domain-adaptive LLM, ChipNeMo, against two leading LLMs, Claude 3 Opus and
ChatGPT-4 Turbo, to assess their efficacy in chip design coding generation.
Through a detailed evaluation of the accuracy of the model, training
methodologies, and operational expenditures, this study aims to provide
stakeholders with critical information to select the most economically viable
and performance-efficient solutions for their specific needs. Our results
underscore the benefits of employing domain-adapted models, such as ChipNeMo,
that demonstrate improved performance at significantly reduced costs compared
to their general-purpose counterparts. In particular, we reveal the potential
of domain-adapted LLMs to decrease TCO by approximately 90%-95%, with the cost
advantages becoming increasingly evident as the deployment scale expands. With
expansion of deployment, the cost benefits of ChipNeMo become more pronounced,
making domain-adaptive LLMs an attractive option for organizations with
substantial coding needs supported by LLMs",2024-04-12T23:37:56Z
,http://arxiv.org/pdf/2401.10364v1.pdf,"Using LLM such as ChatGPT for Designing and Implementing a RISC
  Processor: Execution,Challenges and Limitations","This paper discusses the feasibility of using Large Language Models LLM for
code generation with a particular application in designing an RISC. The paper
also reviews the associated steps such as parsing, tokenization, encoding,
attention mechanism, sampling the tokens and iterations during code generation.
The generated code for the RISC components is verified through testbenches and
hardware implementation on a FPGA board. Four metric parameters Correct output
on the first iteration, Number of errors embedded in the code, Number of trials
required to achieve the code and Failure to generate the code after three
iterations, are used to compare the efficiency of using LLM in programming. In
all the cases, the generated code had significant errors and human intervention
was always required to fix the bugs. LLM can therefore be used to complement a
programmer code design.",2024-01-18T20:14:10Z
,http://arxiv.org/pdf/2402.11635v1.pdf,Tool-Augmented LLMs as a Universal Interface for IDEs,"Modern-day Integrated Development Environments (IDEs) have come a long way
from the early text editing utilities to the complex programs encompassing
thousands of functions to help developers. However, with the increasing number
of efficiency-enhancing tools incorporated, IDEs gradually became sophisticated
software with a steep learning curve. The rise of the Large Language Models
(LLMs) capable of both natural language dialogue and code generation leads to a
discourse on the obsolescence of the concept of IDE. In this work, we offer a
view on the place of the LLMs in the IDEs as the universal interface wrapping
the IDE facilities. We envision a model that is able to perform complex actions
involving multiple IDE features upon user command, stripping the user
experience of the tedious work involved in searching through options and
actions. For the practical part of the work, we engage with the works exploring
the ability of LLMs to call for external tools to expedite a given task
execution. We showcase a proof-of-concept of such a tool.",2024-02-18T16:32:28Z
