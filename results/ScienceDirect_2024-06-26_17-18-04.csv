title,journal,volume,pages,year,issn,doi,url,author,keywords,abstract,type,number,note,editor,booktitle,publisher,series,isbn,address,edition
A review of deep learning techniques for speech processing,Information Fusion,99,101869,2023,1566-2535,https://doi.org/10.1016/j.inffus.2023.101869,https://www.sciencedirect.com/science/article/pii/S1566253523001859,Ambuj Mehrish and Navonil Majumder and Rishabh Bharadwaj and Rada Mihalcea and Soujanya Poria,"Deep learning, Speech processing, Transformers, Survey, Trends","The field of speech processing has undergone a transformative shift with the advent of deep learning. The use of multiple processing layers has enabled the creation of models capable of extracting intricate features from speech data. This development has paved the way for unparalleled advancements in speech recognition, text-to-speech synthesis, automatic speech recognition, and emotion recognition, propelling the performance of these tasks to unprecedented heights. The power of deep learning techniques has opened up new avenues for research and innovation in the field of speech processing, with far-reaching implications for a range of industries and applications. This review paper provides a comprehensive overview of the key deep learning models and their applications in speech-processing tasks. We begin by tracing the evolution of speech processing research, from early approaches, such as MFCC and HMM, to more recent advances in deep learning architectures, such as CNNs, RNNs, transformers, conformers, and diffusion models. We categorize the approaches and compare their strengths and weaknesses for solving speech-processing tasks. Furthermore, we extensively cover various speech-processing tasks, datasets, and benchmarks used in the literature and describe how different deep-learning networks have been utilized to tackle these tasks. Additionally, we discuss the challenges and future directions of deep learning in speech processing, including the need for more parameter-efficient, interpretable models and the potential of deep learning for multimodal speech processing. By examining the field’s evolution, comparing and contrasting different approaches, and highlighting future directions and challenges, we hope to inspire further research in this exciting and rapidly advancing field.",article,,,,,,,,,
Existence of solutions for nth-order integro-differential equations in Banach spaces,Computers & Mathematics with Applications,41,597-606,2001,0898-1221,https://doi.org/10.1016/S0898-1221(00)00303-5,https://www.sciencedirect.com/science/article/pii/S0898122100003035,Dajun Guo,"Integro-differential equation, Banach space, Measure of noncompactness, Schauder fixed-point theorem",This paper discusses the existence of solutions of initial value problems for nth-order nonlinear integro-differential equations of mixed type on an infinite interval in a Banach space.,article,5,,,,,,,,
The state of human-centered NLP technology for fact-checking,Information Processing & Management,60,103219,2023,0306-4573,https://doi.org/10.1016/j.ipm.2022.103219,https://www.sciencedirect.com/science/article/pii/S030645732200320X,Anubrata Das and Houjiang Liu and Venelin Kovatchev and Matthew Lease,"Natural Language Processing, Misinformation, Disinformation, Explainability, Human-AI teaming","Misinformation threatens modern society by promoting distrust in science, changing narratives in public health, heightening social polarization, and disrupting democratic elections and financial markets, among a myriad of other societal harms. To address this, a growing cadre of professional fact-checkers and journalists provide high-quality investigations into purported facts. However, these largely manual efforts have struggled to match the enormous scale of the problem. In response, a growing body of Natural Language Processing (NLP) technologies have been proposed for more scalable fact-checking. Despite tremendous growth in such research, however, practical adoption of NLP technologies for fact-checking still remains in its infancy today. In this work, we review the capabilities and limitations of the current NLP technologies for fact-checking. Our particular focus is to further chart the design space for how these technologies can be harnessed and refined in order to better meet the needs of human fact-checkers. To do so, we review key aspects of NLP-based fact-checking: task formulation, dataset construction, modeling, and human-centered strategies, such as explainable models and human-in-the-loop approaches. Next, we review the efficacy of applying NLP-based fact-checking tools to assist human fact-checkers. We recommend that future research include collaboration with fact-checker stakeholders early on in NLP research, as well as incorporation of human-centered design practices in model development, in order to further guide technology development for human use and practical adoption. Finally, we advocate for more research on benchmark development supporting extrinsic evaluation of human-centered fact-checking technologies.",article,2,,,,,,,,
Reconstructive memory in case-based design,Artificial Intelligence in Engineering,11,245-258,1997,0954-1810,https://doi.org/10.1016/S0954-1810(96)00044-1,https://www.sciencedirect.com/science/article/pii/S0954181096000441,B. Kumar and B. Raphael,"case based reasoning, design, artificial intelligence, reconstructive memory, structural design","This paper describes an approach to applications of case-based reasoning in design based on utilising design methods used for solving similar problems in the past. In order to facilitate the implementation of this approach, data structures called MREM (Memory REconstruction Methods) are developed. This is based on Kolodner's theory of reconstructive memory. An MREM based model of design is implemented in a prototype called CADREM (CAse based Design using REconstruction Methods). CADREM's casebase contains cases of conceptual structural design of buildings.",article,3,,,,,,,,
Contents,Procedia Computer Science,221,iii-xiv,2023,1877-0509,https://doi.org/10.1016/S1877-0509(23)00901-8,https://www.sciencedirect.com/science/article/pii/S1877050923009018,,,,article,,Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023),,,,,,,
Compound key word generation from document databases using a hierarchical clustering ART model,Intelligent Data Analysis,1,25-48,1997,1088-467X,https://doi.org/10.1016/S1088-467X(98)00008-0,https://www.sciencedirect.com/science/article/pii/S1088467X98000080,Alberto Muñoz,"Automatic indexing, Knowledge extraction, Information retrieval, Neural fuzzy ART models, Information retrieval","The growing availability of databases on the information highways motivates the development of new processing tools able to deal with a heterogeneous and changing information environment. A highly desirable feature of data processing systems handling this type of information is the ability to automatically extract its own key words. In this paper we address the specific problem of creating semantic term associations from a text database. The proposed method uses a hierarchical model made up of Fuzzy Adaptive Resonance Theory (ART) neural networks. First, the system uses several Fuzzy ART modules to cluster isolated words into semantic classes, starting from the database raw text. Next, this knowledge is used together with coocurrence information to extract semantically meaningful term associations. These associations are asymmetric and one-to-many due to the polisemy phenomenon. The strength of the associations between words can be measured numerically. Besides this, they implicitly define a hierarchy between descriptors. The underlying algorithm is appropriate for employment on large databases. The operation of the system is illustrated on several real databases.",article,1,,,,,,,,
Managing misspelled queries in IR applications,Information Processing & Management,47,263-286,2011,0306-4573,https://doi.org/10.1016/j.ipm.2010.08.004,https://www.sciencedirect.com/science/article/pii/S030645731000066X,Jesús Vilares and Manuel Vilares and Juan Otero,"Misspelled queries, Information retrieval, Spelling correction, Character -grams, Evaluation methodology","Our work concerns the design of robust information retrieval environments that can successfully handle queries containing misspelled words. Our aim is to perform a comparative analysis of the efficacy of two possible strategies that can be adopted. A first strategy involves those approaches based on correcting the misspelled query, thus requiring the integration of linguistic information in the system. This solution has been studied from complementary standpoints, according to whether contextual information of a linguistic nature is integrated in the process or not, the former implying a higher degree of complexity. A second strategy involves the use of character n-grams as the basic indexing unit, which guarantees the robustness of the information retrieval process whilst at the same time eliminating the need for a specific query correction stage. This is a knowledge-light and language-independent solution which requires no linguistic information for its application. Both strategies have been subjected to experimental testing, with Spanish being used as the case in point. This is a language which, unlike English, has a great variety of morphological processes, making it particularly sensitive to spelling errors. The results obtained demonstrate that stemming-based approaches are highly sensitive to misspelled queries, particularly with short queries. However, such a negative impact can be effectively reduced by the use of correction mechanisms during querying, particularly in the case of context-based correction, since more classical approaches introduce too much noise when query length is increased. On the other hand, our n-gram based strategy shows a remarkable robustness, with average performance losses appreciably smaller than those for stemming.",article,2,,,,,,,,
"Digital identity, privacy and the right to identity in the United States of America",Computer Law & Security Review,29,348-358,2013,0267-3649,https://doi.org/10.1016/j.clsr.2013.05.011,https://www.sciencedirect.com/science/article/pii/S0267364913001027,Clare Sullivan,"Digital identity, Transaction identity, Right to privacy, Right to identity","This article analyses digital identity as an emergent legal concept in the United States of America, as a consequence of the move to place all federal government services on-line. The features and functions of digital identity and its legal nature are examined, and the consequences are considered. The analysis reveals that the part of digital identity which is required for transactions has specific legal standing under the scheme. This feature makes digital identity valuable and vulnerable. The identifying information which links the registered digital identity with a person is especially susceptible to error and fraud. Yet the system is designed so that all dealings using the digital identity are automatically attributed to the individual to whom it is registered under the scheme, regardless of whether that person did in fact use the identity to transact. This can result in significant consequences for all users and there are implications for the integrity of the scheme, but the consequences are most immediate and serious for the innocent individual. To date, legal scholarship and jurisprudence in the United States has relied on privacy to protect personal information. This article argues that privacy, by its nature, cannot adequately protect the part of digital identity which is required for transactions because of its essentially public nature. The author argues instead that the right to identity can and should be recognised in the United States to fully protect an individual's rights under this scheme, especially considering the scheme's inherent fallibilities.",article,4,,,,,,,,
Chapter 12 - Assessing and implementing trustworthy AI across multiple dimensions,,,229-257,2024,,https://doi.org/10.1016/B978-0-443-18851-0.00001-9,https://www.sciencedirect.com/science/article/pii/B9780443188510000019,Abigail Goldsteen and Ariel Farkash and Michael Hind,"Ethical AI, trustworthy AI, privacy, AI governance, machine learning, regulations, compliance","Artificial intelligence (AI) systems have become more and more prevalent in everyday life, especially in enterprise settings. These systems have grown increasingly more accurate and efficient, but at the same time more complex and less understandable. Broad adoption of AI systems requires humans to trust them. This depends on the ability to ensure that AI systems are fair, robust, explainable, accountable, and respectful of the privacy of individuals and will cause no harm. To this end, many tools and techniques have been developed for both assessing AI models and mitigating any potential risks they may pose. This chapter surveys the existing approaches and technologies available to tackle each of the dimensions of Trustworthy AI to create more ethical AI systems. Moreover, it touches on the challenges and possible solutions to significantly combine various aspects of these dimensions, indicating areas for further research.",incollection,,,Santi Caballé and Joan Casas-Roma and Jordi Conesa,Ethics in Online AI-based Systems,Academic Press,Intelligent Data-Centric Systems,978-0-443-18851-0,,
D-HRSP: Dataset of helpful reviews for service providers,Telematics and Informatics,82,102001,2023,0736-5853,https://doi.org/10.1016/j.tele.2023.102001,https://www.sciencedirect.com/science/article/pii/S0736585323000655,Jinmo Lee and Eunil Park,"Mobile application review, Service quality, Service provider, Review helpfulness","Most common services are now provided through mobile applications; thus, the importance of mobile application reviews has increased. Service providers and developers seek helpful reviews to find useful information to improve their services. However, with currently existing indicators, e.g., star rating systems, it is difficult to identify reviews that are directly related to the quality of the service. Thus, in this study, we defined helpful mobile application reviews for service providers and developers based on the components of an existing service quality evaluation model. We also provide the D-HRSP (dataset of helpful reviews for service providers), which is a labeled dataset that can be used to examine helpful reviews. We also report the experimental results obtained with simple natural language processing techniques and machine learning and deep learning classification models. The experimental results demonstrate that the proposed definition can help address real-life problems and create opportunities for additional research into the identification of helpful mobile application reviews.",article,,,,,,,,,
ESIE-BERT: Enriching sub-words information explicitly with BERT for intent classification and slot filling,Neurocomputing,591,127725,2024,0925-2312,https://doi.org/10.1016/j.neucom.2024.127725,https://www.sciencedirect.com/science/article/pii/S092523122400496X,Yu Guo and Zhilong Xie and Xingyan Chen and Huangen Chen and Leilei Wang and Huaming Du and Shaopeng Wei and Yu Zhao and Qing Li and Gang Wu,"Wordpiece, Intent classification, Slot filling, BERT, Attention mechanisms","Natural language understanding (NLU) has two core tasks: intent classification and slot filling. The success of pre-training language models resulted in a significant breakthrough in the two tasks. The architecture based on autoencoding (BERT-based model) can optimize the two tasks jointly. However, we note that BERT-based models convert each complex token into multiple sub-tokens by the Wordpiece algorithm, which generates an out-of-alignment between the length of the tokens and the labels. This leads to BERT-based models not performing well in label prediction, limiting model performance improvement. Many existing models can address this issue, but some hidden semantic information is discarded during fine-tuning. We addressed the problem by introducing a novel joint method on top of BERT. This method explicitly models multiple sub-token features after the Wordpiece tokenization, contributing to both tasks. Our proposed method effectively extracts contextual features from complex tokens using the Sub-words Attention Adapter (SAA), preserving overall utterance information. Additionally, we propose an Intent Attention Adapter (IAA) to acquire comprehensive sentence features, assisting users in predicting intent. Experimental results confirm that our proposed model exhibits significant improvements on two public benchmark datasets. Specifically, the slot-filling F1 score improves from 96.5 to 98.2 (an absolute increase of 1.7%) on the Airline Travel Information Systems (ATIS) dataset.",article,,,,,,,,,
Optimal scheduling of independent jobs in multiprocessor systems,Microprocessing and Microprogramming,40,651-672,1994,0165-6074,https://doi.org/10.1016/0165-6074(94)90092-2,https://www.sciencedirect.com/science/article/pii/0165607494900922,,"Scheduling independent jobs, Malleability of jobs, Multiprocessor systems, Completion time, Combinatorial tree, Optimal solution","In this paper, we consider the problem of finding an optimal schedule of several independent jobs in multiprocessor systems, i.e. for a given job list, in what order the jobs should be processed and how to assign these jobs to the processors in a multiprocessor system such that the completion time of the last completed job is minimized. We first discuss two models of this problem, one in which time and processor requirements of a job are fixed and the other in which time requirement of a jobs varies according to the number of processors allocated to the job for execution. We then present efficient algorithms for solving these problem models, which are known to be NP-hard in the strong sense, using a branch and bound strategy. Our algorithms use several new reduction rules to arrive at optimal solutions in an efficient way. Finally, we demonstrate the effectiveness of our algorithms by experimental results.",article,9,,,,,,,,
"JAPPI: An unsupervised endpoint application identification methodology for improved Zero Trust models, risk score calculations and threat detection",Computer Networks,250,110606,2024,1389-1286,https://doi.org/10.1016/j.comnet.2024.110606,https://www.sciencedirect.com/science/article/pii/S1389128624004389,Jenny Heino and Christian Jalio and Antti Hakkala and Seppo Virtanen,"Network security, Intrusion detection, Machine learning, Zero Trust, Risk score","The surge in global digitalization triggered by COVID-19 has led to a significant increase in internet traffic and has precipitated a rapid transformation of the network security landscape. Despite being increasingly difficult, accurate traffic inspection is vital for ensuring productivity while reliably protecting internal assets. Endpoint application identification enables high accuracy inspection and detection by providing network security solutions with specific context on individual connections. However, achieving it in real-time with standard fingerprinting methods based only on client-side traffic has proven to be a challenging problem with no comprehensive solution thus far. In this article, we present a new methodology for identifying endpoint applications from network traffic, utilizing machine learning. Our methodology leverages similarities in the pre-hash string of the JA3 algorithm for fingerprinting application specific TLS Client Hello messages. By utilizing well-known clustering algorithms it is possible to identify the underlying TLS libraries and the application from the traffic remarkably better than with simple string-based matching. Our model can categorize 99,5% of the traffic in a controlled network, and 93,8% in an uncontrolled network, compared to 0,1% and 0,2% using simple string matching. Our methodology is especially effective for enhancing Zero Trust models, calculating a risk score for network events, and improving threat detection accuracy in network security solutions.",article,,,,,,,,,
An algebraic structure for derivations in rewriting systems,Theoretical Computer Science,57,205-224,1988,0304-3975,https://doi.org/10.1016/0304-3975(88)90039-4,https://www.sciencedirect.com/science/article/pii/0304397588900394,Yury Velinov,,"In the present paper a uniform description of the algebraic properties of derivations in a rewriting system, and of their syntax and semantics is proposed on the base of a polycategory structure. Similarity relations between derivations are studied on syntax and semantic levels. Several canonical derivation forms are described.",article,2,,,,,,,,
Russia's new personal data localization regulations: A step forward or a self-imposed sanction?,Computer Law & Security Review,32,128-145,2016,0267-3649,https://doi.org/10.1016/j.clsr.2015.12.003,https://www.sciencedirect.com/science/article/pii/S0267364915001685,Alexander Savelyev,"Personal data, Data localization, Cloud computing, Big data, Transborder data flows, Digital sovereignty","The paper represents one of the first comprehensive analyses of Russian personal data localization regulations, which became effective at September 1, 2015. This work describes in detail the main components of the data localization mechanism: triggers of its application, scope, exemptions and enforcement. It also takes into account the official and non-official interpretations of the law by Russian regulators, some of which were developed with the participation of the author. Special consideration is given to the jurisdictional aspects of the Russian data protection legislation and the criteria of its application to foreign data controllers. The author also reveals the rationale behind the adoption of data localization provisions and analyzes their possible impact on foreign companies operating in Russia and implementation of innovative IT-technologies (Cloud computing, Big Data and Internet of Things). The paper concludes that most of the potential benefits of data localization provisions, i.e. in the area of public law, law enforcement activities and taxation. Nevertheless, data localization provisions may still have medium-term positive impact on privacy, since they force all stakeholders to revisit the basic concepts of existing personal data legislation (the notion of personal data, data controller, processing, etc.), thus serving as a driver for re-shaping existing outdated data privacy regulations and crafting something more suitable for the modern IT-environment.",article,1,,,,,,,,
Beyond traditional interviews: Psychometric analysis of asynchronous video interviews for personality and interview performance evaluation using machine learning,Computers in Human Behavior,154,108128,2024,0747-5632,https://doi.org/10.1016/j.chb.2023.108128,https://www.sciencedirect.com/science/article/pii/S074756322300479X,,"Artificial intelligence, Asynchronous video interview, Personality, Trait activation theory, Algorithmic bias","With the advent of new technology, traditional job interviews have been supplemented by asynchronous video interviews (AVIs). However, research on psychometric properties of AVIs is limited. In this study, 710 participants completed a mock AVI responding to eight personality questions (Extraversion, Conscientiousness). We collected self- and observer reports of personality, interview performance ratings, attractiveness, and AVI meta-information (e.g., professional attire, audio quality). Then, we automatically extracted the words, facial expressions, and voice characteristics from the videos and trained machine learning models to predict the personality traits and interview performance. Our algorithm explained substantially more variance in observer reports of Extraversion and Conscientiousness (average R2 = 0.32) and interview performance (R2 = 0.44), than self-reported Extraversion and Conscientiousness (average R2 = 0.12). Consistent with Trait Activation Theory, the explained variance in personality traits increased when participants responded to trait-relevant, compared to trait-irrelevant, questions. The test-retest reliability of our algorithm was somewhat stable over a time period of seven months, but lower than desired reliability standards in personnel selection. We examined potential sources of bias, including age, gender, and attractiveness, and found some instances of algorithmic bias (e.g., gender differences were often amplified in favor of women).",article,,,,,,,,,
Appendix B - An Introduction to Web Application Security,,,437-484,2005,,https://doi.org/10.1016/B978-193183636-4/50019-1,https://www.sciencedirect.com/science/article/pii/B9781931836364500191,Matt Fisher,,,incollection,,,Johnny Long and Ed Skoudis,Google Hacking for Penetration Testers,Syngress,,978-1-931836-36-4,Burlington,
Chapter 3 - Navigation Bloopers,,,83-119,2003,,https://doi.org/10.1016/B978-155860840-5/50005-3,https://www.sciencedirect.com/science/article/pii/B9781558608405500053,Jeff Johnson,,"Publisher Summary
Successful navigation cues let people know where they are, where they've been, and where they can go. Whether their goal is near or far can also be added in the list. People encounter many obstacles when trying to use the Web. The most pervasive problem is navigating: finding the way to what users are seeking. The main reason for this is inadequate cues. The chapter describes the most common methods Web designers use—unintentionally of course—to hinder, divert, and block Web users from navigating to the content they seek. It also explains the way to avoid using those methods. Websites that reflect a company or agency's organizational chart, do a common mistake. Websites and Web applications must be organized according to the needs of their intended users. It is required to put the Web team at the corporate or agency level. Some navigation bloopers include numerous navigation schemes, deceptive duplicate links, and non-indication of current page. An extremely common navigation blooper is for a Webpage to include an active link to itself. Clicking on such a link merely reloads the page. At best, this wastes people's time as the page reloads. At worst, it can be very disorienting, because users may not recognize the re-displayed page. Some Webpages lack not only an indication of the current page, but any navigation link at all. This is a serious navigation blooper that is unfortunate and fairly common. It forces Web users to use the “back” button or other browser controls to navigate away from the page.",incollection,,,Jeff Johnson,Web Bloopers,Morgan Kaufmann,Interactive Technologies,978-1-55860-840-5,San Francisco,
Creating the Web-based Intensive Care Unit Safety Reporting System,Journal of the American Medical Informatics Association,12,130-139,2005,1067-5027,https://doi.org/10.1197/jamia.M1408,https://www.sciencedirect.com/science/article/pii/S1067502704001902,Christine G. Holzmueller and Peter J. Pronovost and Fern Dickman and David A. Thompson and Albert W. Wu and Lisa H. Lubomski and Maureen Fahey and Donald M. Steinwachs and Lilly Engineer and Ali Jaffrey and Laura L. Morlock and Todd Dorman,,"In an effort to improve patient safety, researchers at the Johns Hopkins University designed and implemented a comprehensive Web-based Intensive Care Unit Safety Reporting System (ICUSRS). The ICUSRS collects data about adverse events and near misses from all staff in the ICU. This report reflects data on 854 reports from 18 diverse ICUs across the United States. Reporting is voluntary, and data collected is confidential, with patient, provider, and reporter information deidentified. Preliminary data include system factors reported, degree of patient harm, reporting times, and evaluations of the system. Qualitative and quantitative data are reported back to the ICU site study teams and frontline staff through monthly reports, case discussions, and a quarterly newsletter.",article,2,,,,,,,,
Is mouse dynamics information credible for user behavior research? An empirical investigation,Computer Standards & Interfaces,90,103849,2024,0920-5489,https://doi.org/10.1016/j.csi.2024.103849,https://www.sciencedirect.com/science/article/pii/S0920548924000187,Eduard Kuric and Peter Demcak and Matus Krajcovic and Peter Nemcek,"Mouse configuration, Measurement, Data validity, User behavior modeling, Biological sex classification, Machine learning","Mouse dynamics, information on user’s interaction with a computer mouse, are in vogue in machine learning for purposes such as recommendations, personalization, prediction of user characteristics and behavioral biometrics. We point out a blind spot in current works involving mouse dynamics that originates in underestimating the gravity of the characteristics of the mouse device and configuration on the data that mouse dynamics are inferred from. In a controlled study with N=32 participants, across three kinds of mouse interaction activities, we collect data for mouse dynamics utilizing a variety of mouse parameter configurations. We show that mouse dynamics commonly used in studies can be significantly altered by differences in mouse parameters. Out of 108 evaluated mouse dynamics metrics, 95 and 84 are affected between two conducted studies. A machine learning model’s performance can be warped by the mouse parameters being used. We demonstrate on a prediction task that mouse parameters cannot be approached uniformly and without consideration. We discuss methodological implications — how mouse dynamics studies should account for the diversity of mouse-related conditions.",article,,,,,,,,,
Dialogue logic,,7,665-704,2006,1874-5857,https://doi.org/10.1016/S1874-5857(06)80035-X,https://www.sciencedirect.com/science/article/pii/S187458570680035X,Erik C.W. Krabbe,,"Publisher Summary
This chapter discusses the history of dialogue logic in the second half of the twentieth century. The chapter focuses on the systems proposed by Paul Lorenzen and by those logicians that were inspired by him. The chapter presents an account of dialectic character of logic at the time of its birth. This chapter also discusses structural rules—that is, the rules that are independent of the logical forms of sentences. The chapter compares between Lorenzen systems and those introduced by Hintikka and by Hamblin. Further developments, such as the introduction of modalities and various applications are discussed in the chapter.",incollection,,,Dov M. Gabbay and John Woods,Logic and the Modalities in the Twentieth Century,North-Holland,Handbook of the History of Logic,,,
"Minimalism, justification and non-monotonicity in deductive databases",Journal of Computer and System Sciences,38,290-325,1989,0022-0000,https://doi.org/10.1016/0022-0000(89)90004-4,https://www.sciencedirect.com/science/article/pii/0022000089900044,Nicole Bidoit and Richard Hull,,"Three formalizations of the closed world assumption (CWA) which accomodate disjunctive information are compared. The semantic approach of Bossu and Siegel, here called “minimalism,” is shown to be equivalent to the syntactic approach based on Reiter's “default logic,” when a specific class of defaults corresponding to the CWA is used. Neither approach generalizes the “negation as failure” (NAF) inference rule of Clark. The three formalizations are synthesized to form “positivism,” a new semantically defined formalization of the CWA. The expressive power of minimalism and positivism is compared, in both static and dynamic contexts. In the dynamic case the comparison shows that positivism and minimalism are “non-monotonic” in different ways. Finally, positivism and “stratification,” an alternative formalization of the CWA which combines minimalism and NAF are briefly compared.",article,2,,,,,,,,
Cartesian spline interpolation for industrial robots,Computer-Aided Design,30,217-224,1998,0010-4485,https://doi.org/10.1016/S0010-4485(97)00061-4,https://www.sciencedirect.com/science/article/pii/S0010448597000614,Thomas Horsch and Bert Jüttler,"rational spline motions, Cartesian spline interpolation, industrial robots","We describe an algorithm for interpolation of positions by a rational spline motion. A reparameterization of the resulting motion is applied in order to achieve the desired distribution of the velocity. For the ease of presentation we discuss trapezoidal velocity profiles, i.e. piecewise constant and linear velocity distribution. The method can be generalized to more general velocity profiles. The whole spline scheme possesses some special features which make it a suitable tool for the control of industrial robots.",article,3,Motion Design and Kinematics,,,,,,,
Editorial Board,Computer Law & Security Review,12,IFC,1996,0267-3649,https://doi.org/10.1016/S0267-3649(96)90151-X,https://www.sciencedirect.com/science/article/pii/S026736499690151X,,,,article,1,,,,,,,,
Knowledge-based Dual External Attention Network for peptide detectability prediction,Knowledge-Based Systems,286,111378,2024,0950-7051,https://doi.org/10.1016/j.knosys.2024.111378,https://www.sciencedirect.com/science/article/pii/S0950705124000133,Xiaocai Zhang and Hui Peng and Tao Tang and Yuansheng Liu and Yang Wang and Jianjia Zhang,"Peptide, Detectability prediction, Knowledge, Attention, Deep learning","Accurate prediction of peptide detectability plays a crucial role in various proteomics data analyses such as peptide identification and protein quantification. To improve the precision of peptide detectability prediction, we propose a novel approach called the Knowledge-based Dual External Attention Network (KDEAN). KDEAN introduces several innovative elements to enhance its representation and prediction capabilities. Firstly, it extracts valuable knowledge-based features from peptide sequences to facilitate the pattern recognition process. Secondly, KDEAN adopted dual networks to separately train peptide sequences from both the forward and backward directions, capturing comprehensive information. Thirdly, an external attention mechanism is utilized to identify and understand the connections between different peptide samples. The structure of KDEAN enables long-term dependency learning from both directions of the peptide sequences. Extensive evaluations on four testing datasets demonstrate that KDEAN outperforms existing methods, achieving a higher average performance in peptide detectability prediction. Additionally, comprehensive ablation studies confirm the effectiveness and advantages of key components in KDEAN, including knowledge-based feature representation, dual network architecture, and external attention.",article,,,,,,,,,
Blockchain and big data integration design for traceability and carbon footprint management in the fishery supply chain,Egyptian Informatics Journal,26,100481,2024,1110-8665,https://doi.org/10.1016/j.eij.2024.100481,https://www.sciencedirect.com/science/article/pii/S1110866524000446,Aslan Alwi and Nugroho Adi Sasongko and  Suprapto and Yaya Suryana and Hendro Subagyo,"Blockchain, Big data, ETL design, Data warehouse, Fisheries, Supply chain, Anonymity, Immutability, Carbon footprint, Sustainability, Traceability","The utilization of blockchain technology in the fishing industry has been extensively studied and implemented to address issues such as illegal fishing and carbon emissions control. However, integrating blockchain with the vast amounts of data in the fishing supply chain poses significant challenges. Challenges include managing extensive data such as photos or videos for product traceability throughout their lifecycle, compounded by the growing complexity of cross-border trade and market expansion. Additionally, blockchain's storage capacity limitations present hurdles in fully accommodating and comprehensively storing detailed supply data from a complex and expanding supply chain. While solutions like the Interplanetary File System (IPFS) have been explored for large data storage on the blockchain, this paper proposes a directly integrated blockchain solution tailored for the challenges of fishing with big data. We introduce a novel big data design that preserves blockchain's anonymity and immutability features, addressing storage limitations while maintaining the architecture's purpose. Furthermore, our proposal integrates product supply chain traceability with carbon footprint tracking, enabling comprehensive assessment based on quality, sustainability, and carbon footprint criteria. Despite the proposed solution needing to be tested in real-life situations, we conducted rigorous testing through simulation, white-box evaluation, and complexity analysis. The results demonstrate the potential of our solution to address challenges faced in fisheries supply chains, providing valuable insights for future practical implementation and validation efforts.",article,,,,,,,,,
Do we really need a “Digital Humanism”? A critique based on post-human philosophy of technology and socio-legal techniques,Journal of Responsible Technology,18,100080,2024,2666-6596,https://doi.org/10.1016/j.jrt.2024.100080,https://www.sciencedirect.com/science/article/pii/S2666659624000064,Federica Buongiorno and Xenia Chiaramonte,"Digital humanism, Posthumanism, AI, Accountability, Hybrids","Few concepts have been subjected to as intense scrutiny in contemporary discourse as that of “humanism.” While these critiques have acknowledged the importance of retaining certain key aspects of humanism, such as rights, freedom, and human dignity, the term has assumed ambivalence, especially in light of post-colonial and gender studies, that cannot be ignored. The “Vienna Manifesto on Digital Humanism,” as well as the recent volume (2022) titled Perspectives on Digital Humanism, bear a complex imprint of this ambivalence. In this contribution, we aim to bring to the forefront and decipher this underlying trace, by considering alternative (non-humanistic) ways to understand human-technologies relations, beyond the dominant neoliberal paradigm (paragraphs 1 and 2); we then analyse those relations within the specific context of legal studies (paragraphs 3 and 4), one in which the interdependency of humans and non-humans shows a specific and complex form of “fundamental ambivalence.”",article,,,,,,,,,
Understanding latent affective bias in large pre-trained neural language models,Natural Language Processing Journal,7,100062,2024,2949-7191,https://doi.org/10.1016/j.nlp.2024.100062,https://www.sciencedirect.com/science/article/pii/S2949719124000104,,"Affective bias in NLP, Fairness in NLP, Pre-trained language models, Textual emotion detection, Deep learning","Groundbreaking inventions and highly significant performance improvements in deep learning based Natural Language Processing are witnessed through the development of transformer based large Pre-trained Language Models (PLMs). The wide availability of unlabeled data within human generated data deluge along with self-supervised learning strategy helps to accelerate the success of large PLMs in language generation, language understanding, etc. But at the same time, latent historical bias/unfairness in human minds towards a particular gender, race, etc., encoded unintentionally/intentionally into the corpora harms and questions the utility and efficacy of large PLMs in many real-world applications, particularly for the protected groups. In this paper, we present an extensive investigation towards understanding the existence of “Affective Bias” in large PLMs to unveil any biased association of emotions such as anger, fear, joy, etc., towards a particular gender, race or religion with respect to the downstream task of textual emotion detection. We conduct our exploration of affective bias from the very initial stage of corpus level affective bias analysis by searching for imbalanced distribution of affective words within a domain, in large scale corpora that are used to pre-train and fine-tune PLMs. Later, to quantify affective bias in model predictions, we perform an extensive set of class-based and intensity-based evaluations using various bias evaluation corpora. Our results show the existence of statistically significant affective bias in the PLM based emotion detection systems, indicating biased association of certain emotions towards a particular gender, race, and religion.",article,,,,,,,,,
"A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management",Information Fusion,108,102369,2024,1566-2535,https://doi.org/10.1016/j.inffus.2024.102369,https://www.sciencedirect.com/science/article/pii/S1566253524001477,Sayed Pedram Haeri Boroujeni and Abolfazl Razi and Sahand Khoshdel and Fatemeh Afghah and Janice L. Coen and Leo O’Neill and Peter Fule and Adam Watts and Nick-Marios T. Kokolakis and Kyriakos G. Vamvoudakis,"Wildfire management, Artificial intelligence (AI), Unmanned aerial vehicle (UAV), Machine learning, Deep learning (DL), Reinforcement learning (RL), Computer vision","Wildfires have emerged as one of the most destructive natural disasters worldwide, causing catastrophic losses. These losses have underscored the urgent need to improve public knowledge and advance existing techniques in wildfire management. Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models, has created an unprecedented momentum to implement and develop more effective wildfire management. Although existing survey papers have explored learning-based approaches in wildfire, drone use in disaster management, and wildfire risk assessment, a comprehensive review emphasizing the application of AI-enabled UAV systems and investigating the role of learning-based methods throughout the overall workflow of multi-stage wildfire management, including pre-fire (e.g., vision-based vegetation fuel measurement), active-fire (e.g., fire growth modeling), and post-fire tasks (e.g., evacuation planning) is notably lacking. This survey synthesizes and integrates state-of-the-science reviews and research at the nexus of wildfire observations and modeling, AI, and UAVs — topics at the forefront of advances in wildfire management, elucidating the role of AI in performing monitoring and actuation tasks from pre-fire, through the active-fire stage, to post-fire management. To this aim, we provide an extensive analysis of the existing remote sensing systems with a particular focus on the UAV advancements, device specifications, and sensor technologies relevant to wildfire management. We also examine the pre-fire and post-fire management approaches, including fuel monitoring, prevention strategies, as well as evacuation planning, damage assessment, and operation strategies. Additionally, we review and summarize a wide range of computer vision techniques in active-fire management, with an emphasis on Machine Learning (ML), Reinforcement Learning (RL), and Deep Learning (DL) algorithms for wildfire classification, segmentation, detection, and monitoring tasks. Ultimately, we underscore the substantial advancement in wildfire modeling through the integration of cutting-edge AI techniques and UAV-based data, providing novel insights and enhanced predictive capabilities to understand dynamic wildfire behavior.",article,,,,,,,,,
Enhanced DSSM (deep semantic structure modelling) technique for job recommendation,Journal of King Saud University - Computer and Information Sciences,34,7790-7802,2022,1319-1578,https://doi.org/10.1016/j.jksuci.2021.07.018,https://www.sciencedirect.com/science/article/pii/S1319157821001853,Ravita Mishra and Sheetal Rathi,"Content Based Filtering (CB), BM25, Collaborative Filtering (CF), DSSM (Deep semantic structure modeling), Hybrid Filtering (HF), Adam Optimizer, LSTM ( Long Short Term Memory ).","Now a day’s recommendation system take care of the issue of the massive amount of information overload problem and it provides the services to the candidates to concentrate on relevant information on job domain only. The job recommender system plays an important role in the recruitment process of fresher as well as experienced today. Existing job recommender system mainly focuses on content-based filtering to extricate profile content and on collaborative filtering to capture the behaviour of the user in the form of rating. Dynamic nature of job market leads cold start and scalability issues. This problem can be addressed by item-based collaborative filtering with a machine learning technique, it learns job embedding vector and finds similar jobs content-wise. Existing model in job recommender domain uses the confining model to address the cold start and scalability issue and provide better recommendation, but they fail to accept the complex relationships between job description and candidate profile. In this paper, we are proposing a Deep Semantic Structure Algorithm that overcome the issue of the existing system. Deep semantic structure modelling (DSSM) system uses the semantic representation of sparse data and it represent the job description and skill entities in character trigram format which increases the efficacy of the system. We are comparing the results to three variation of DSSM model with two different dataset (Naukari.com and CareerBuilder. com) and it gives satisfactory results. Experimental results shows that the DSSM Embedding model and its other variants are provides promising results in solving cold start problem in comparison with several variants of embedding model. We used Xavier initializer to initialise the model parameter and Adam optimizer to optimize the system performance.",article,9,,,,,,,,
Ethics-based AI auditing: A systematic literature review on conceptualizations of ethical principles and knowledge contributions to stakeholders,Information & Management,61,103969,2024,0378-7206,https://doi.org/10.1016/j.im.2024.103969,https://www.sciencedirect.com/science/article/pii/S037872062400051X,Joakim Laine and Matti Minkkinen and Matti Mäntymäki,"Artificial intelligence, Auditing, AI ethics, AI governance, AI auditing, Ethics-based AI auditing, Systematic literature review","This systematic literature review synthesizes the conceptualizations of ethical principles in AI auditing literature and the knowledge contributions to the stakeholders of AI auditing. We explain how the literature discusses fairness, transparency, non-maleficence, responsibility, privacy, trust, beneficence, and freedom/autonomy. Conceptualizations vary along social/technical- and process/outcome-oriented dimensions. The main stakeholders of ethics-based AI auditing are system developers and deployers, the wider public, researchers, auditors, AI system users, and regulators. AI auditing provides three types of knowledge contributions to stakeholders: 1) guidance; 2) methods, tools, and frameworks; and 3) awareness and empowerment.",article,5,,,,,,,,
"Blacks tone's Statutes on Intellectual Property: 3rd Edition, by Andrew Christie & Stephen Gare, 1997, softcover, Blackstone Press Ltd, 439 pp., £15.95, ISBN 1 85431 618 4",Computer Law & Security Review,13,271,1997,0267-3649,https://doi.org/10.1016/S0267-3649(97)88860-7,https://www.sciencedirect.com/science/article/pii/S0267364997888607,,,,article,4,,,,,,,,
A pre-trained multi-representation fusion network for molecular property prediction,Information Fusion,103,102092,2024,1566-2535,https://doi.org/10.1016/j.inffus.2023.102092,https://www.sciencedirect.com/science/article/pii/S1566253523004086,Haohui Zhang and Juntong Wu and Shichao Liu and Shen Han,"Molecular property prediction, Graph neural networks, Multi-modal fusion, Unsupervised pre-training","In the field of machine learning and cheminformatics, the prediction of molecular properties holds significant importance. Molecules can be represented in various formats, including 1D SMILES string, 2D graph, and 3D conformation. Numerous models have been proposed for different representations to accomplish molecular property prediction. However, most recent works have focused on one or two representations or combining embedding vectors from different perspectives in an unsophisticated manner. To address this issue, we present PremuNet, a novel pre-trained multi-representation fusion network for molecular property prediction. PremuNet can extract comprehensive molecular information from multiple views and combine them interactively through pre-training and fine-tuning. The framework of PremuNet consists of two branches: a Transformer-GNN branch that extracts SMILES and graph information, and a Fusion Net branch that extracts topology and geometry information, called PremuNet-L and PremuNet-H respectively. We employ masked self-supervised methods to enable the model to learn information fusion and achieve enhanced performance in downstream tasks. The proposed model has been evaluated on eight molecular property prediction tasks, including five classification and three regression tasks, and attained state-of-the-art performance in most cases. Additionally, we conduct the ablation studies to demonstrate the effect of each view and the branch combination approaches.",article,,,,,,,,,
1 - Power and artificial intelligence: transformation of the global public health ecosystem,,,1-65,2024,,https://doi.org/10.1016/B978-0-443-21597-1.00001-9,https://www.sciencedirect.com/science/article/pii/B9780443215971000019,Dominique J. Monlezun,"Global public health ecosystem, global health, public health, healthcare, artificial intelligence, deglobalization, decolonization, COVID-19, population health, precision public health, system optimization","This chapter introduces the book’s unique value proposition as the first comprehensive global analysis of how artificial intelligence (AI) is transforming modern health generally—and how it specifically can revolutionize the decolonized global public health ecosystem to equitably empower the world to solve our era’s defining challenges undermining the health of humanity, from climate change to conflicts, debt crises to deglobalization, and demographic collapse to arrested development. In addition, this chapter outlines the historical evolution, from public health’s early days focused on premodern quarantines to the 19th century’s early modern vaccines and workplace safety, to the 20th century’s late modern globalization following World War II, and 21st-century global public health as data-driven sustainable development, digitalized and institutionalized by the United Nations, World Health Organization, and related public–private partnerships. This chapter considers the anticolonial and COVID-19 critiques of this process and the current global public health ecosystem, setting the stage for the “Great COVID Reset” to foster a human security–based approach to scale responsible AI globally by respecting diverse identities, agency, and values, locally. This chapter introduces this approach, the Personalist Liberalism, as the person-centered, health-based political economic framework to understand the emergent future of health in the context of our world’s structural power imbalances between peoples, elites, institutions, corporations, and governments. It summarizes these themes and trends amid the emerging primary categories for AI use cases illustrating them, including population health, precision public health, and system optimization, to set the stage for the remainder of the book focusing on the ecosystem’s main constitutive domains. Finally, this chapter outlines the structure of the book—focused on responsible AI reengineering a global public health ecosystem as a common home for all humanity—with these domains as the components of a home: design (financing and integral development), framework (data architecture and political economics), inhabitants (culture and demographics), and foundation (security and ethics).",incollection,,,Dominique J. Monlezun,Responsible Artificial Intelligence Re-engineering the Global Public Health Ecosystem,Morgan Kaufmann,,978-0-443-21597-1,,
Market Value and Environmental Performance of Carbon Management Systems: An International Investigation,Information & Management,,103997,2024,0378-7206,https://doi.org/10.1016/j.im.2024.103997,https://www.sciencedirect.com/science/article/pii/S037872062400079X,Daniel E. Rush and Nigel P. Melville and Christie M Fuller,"Business value, Carbon management, Event study, Green IS, Green IT, Greenhouse gas emissions, International research","This study examines the financial and environmental effects of carbon management systems (CMSs) used in publicly traded companies worldwide. Market reactions to companies that announce the adoption of a CMS are analyzed, as are changes in greenhouse gas (GHG) emissions for CMS adopters. A method for conducting international event studies is introduced, and a Monte Carlo simulation indicates that such a method may be necessary to avoid bias. Empirical results suggest that CMS adoption announcements might not generate positive abnormal returns across a variety of specifications. In contrast, estimation results suggest that adoption of a CMS may mitigate increases in GHG emissions.",article,,,,,,,,,
An interactive graphics system for 2-D drawing and design,Computers & Graphics,6,23-27,1982,0097-8493,https://doi.org/10.1016/0097-8493(82)90012-7,https://www.sciencedirect.com/science/article/pii/0097849382900127,Y.N. Srikant and D. Vidyasagar and L.M. Patnaik,,"This paper is about a software system, GRASS-Graphic Software System for 2-D drawing and design—which has been implemented on a PDP-1135 system with RSX-11M operating system. It is a low cost interactive graphics system for the design of two dimensional drawings and uses a minimum of hardware. It provides comprehensive facilities for creating, editing, storing and retrieving pictures. It has been implemented in the language Pascal and has the potential to be used as a powerful data-imputting tool for a design-automation system. The important features of the system are its low cost, software character generation and a user-trainable character recognizer, which has been included.",article,1,,,,,,,,
Exploring relationship development with social chatbots: A mixed-method study of replika,Computers in Human Behavior,140,107600,2023,0747-5632,https://doi.org/10.1016/j.chb.2022.107600,https://www.sciencedirect.com/science/article/pii/S0747563222004204,Iryna Pentina and Tyler Hancock and Tianling Xie,"Social chatbot, Relationship development, Attachment, Anthropomorphism, Authenticity, Replika","This mixed-method investigation proposes and empirically tests a human-Artificial Intelligence (AI) relationship development model in the context of social chatbots. Utilizing data from representative populations and employing method triangulation, the study uniquely combines existing human-computer interaction theoretical concepts (Computers are Social Actors, Perceived Social Presence, and Parasocial Interaction) with interpersonal relationship theories (Social Penetration and Attachment Theories) to advance an explanatory model of human – AI relationship development mechanism. We identify AI Anthropomorphism and AI Authenticity as antecedents, AI Social Interaction as a mediator, and Attachment to AI as an outcome of this process, moderated by the AI usage motivations. Meaningful theoretical, managerial, and societal implications, as well as suggestions for chatbot designers and future research are provided.",article,,,,,,,,,
A survey on text generation using generative adversarial networks,Pattern Recognition,119,108098,2021,0031-3203,https://doi.org/10.1016/j.patcog.2021.108098,https://www.sciencedirect.com/science/article/pii/S0031320321002855,,"Text generation, Generative adversarial Networks, Machine learning, Language modeling, Natural language processing","This work presents a thorough review concerning recent studies and text generation advancements using Generative Adversarial Networks. The usage of adversarial learning for text generation is promising as it provides alternatives to generate the so-called “natural” language. Nevertheless, adversarial text generation is not a simple task as its foremost architecture, the Generative Adversarial Networks, were designed to cope with continuous information (image) instead of discrete data (text). Thus, most works are based on three possible options, i.e., Gumbel-Softmax differentiation, Reinforcement Learning, and modified training objectives. All alternatives are reviewed in this survey as they present the most recent approaches for generating text using adversarial-based techniques. The selected works were taken from renowned databases, such as Science Direct, IEEEXplore, Springer, Association for Computing Machinery, and arXiv, whereas each selected work has been critically analyzed and assessed to present its objective, methodology, and experimental results.",article,,,,,,,,,
Financial sentiment classification with fresh and hot public opinions,Computers and Electrical Engineering,111,108955,2023,0045-7906,https://doi.org/10.1016/j.compeleceng.2023.108955,https://www.sciencedirect.com/science/article/pii/S0045790623003798,Shiyang Cao and Xiao Ma and Jiangfeng Zeng and Ming Yi,"Financial sentiment analysis, Fresh and hot opinions, Temporal modeling, Fresh-hot bilinear pooling","Financial sentiment analysis aims to extract public opinion about an institution to help financial researchers make better decisions. To predict sentiment more accurately, it is necessary for models to improve their capability to capture long-term temporal information and support multi-user interaction. However, existing methods only analyze sentiment based on one comment from a user, which fails to fully exploit the latent emotions of the public, and they lack effective temporal modeling and interaction capabilities. In this paper, we analyze a company from two perspectives to alleviate the above issues: (1) the fresh opinions can reflect timely public attitudes towards a company, while (2) the hot opinions provide the most influential views. A comprehensive exploration of fresh and hot financial sentiment can help researchers make more accurate determinations. To this end, we propose a novel financial sentiment classification framework (FSCN), that can capture temporal information and interact with the opinions of users to make a more comprehensive decision. Our approach takes into account the inherent temporal dependencies in public opinions and combines both views of information to achieve an accurate classification of financial sentiment. Specifically, the FSCN contains (1) a multi-opinion extractor to filter and extract features from massively fresh and hot opinions, respectively. (2) a fresh-hot bilinear pooling (FHBP) module to effectively fuse fresh and hot features. Additionally, to verify the effectiveness of the proposed method, we crawl data from the Internet and create a real-world public opinion dataset that consists of 79,350 comments from 837 companies. Extensive experiments demonstrate that our framework achieves state-of-the-art results on this real-world dataset and is capable of providing reliable service in the financial system. Codes will be released at https://github.com/zjfgh2015/FSCN.",article,,,,,,,,,
SRL-ACO: A text augmentation framework based on semantic role labeling and ant colony optimization,Journal of King Saud University - Computer and Information Sciences,35,101611,2023,1319-1578,https://doi.org/10.1016/j.jksuci.2023.101611,https://www.sciencedirect.com/science/article/pii/S1319157823001659,Aytuğ Onan,"Data augmentation, Text classification, Deep learning, Sarcasm identification","The process of creating high-quality labeled data is crucial for training machine-learning models, but it can be a time-consuming and labor-intensive process. Moreover, manual annotation by human annotators can lead to varying degrees of competency, training, and experience, which can result in inconsistent labeling and arbitrary standards. To address these challenges, researchers have been exploring automated methods for enhancing training and testing datasets. This paper proposes SRL-ACO, a novel text augmentation framework that leverages Semantic Role Labeling (SRL) and Ant Colony Optimization (ACO) techniques to generate additional training data for natural language processing (NLP) models. The framework uses SRL to identify the semantic roles of words in a sentence and ACO to generate new sentences that preserve these roles. SRL-ACO can enhance the accuracy of NLP models by generating additional data without requiring manual data annotation. The paper presents experimental results demonstrating the effectiveness of SRL-ACO on seven text classification datasets for sentiment analysis, toxic text detection and sarcasm identification. The results show that SRL-ACO improves the performance of a classifier on different NLP tasks. These results demonstrate that SRL-ACO has the potential to enhance the quality and quantity of training data for various NLP tasks.",article,7,,,,,,,,
Generalization of S. K. GODUNOV'S method to the calculation of flows with cleavage,USSR Computational Mathematics and Mathematical Physics,18,253-259,1978,0041-5553,https://doi.org/10.1016/0041-5553(78)90157-X,https://www.sciencedirect.com/science/article/pii/004155537890157X,G.A. Atanov,,,article,6,,,,,,,,
Minimizing information acquisition costs,Decision Support Systems,9,161-181,1993,0167-9236,https://doi.org/10.1016/0167-9236(93)90010-Z,https://www.sciencedirect.com/science/article/pii/016792369390010Z,,"Expert systems design, Optimal systems design, Knowledge representation, Decision support systems, Cost/benefit in system design","Today, many organizations are investing heavily in expert systems. Unfortunately, many of these systems will fail to deliver the maximum possible value to their investors because little attention has been paid to the cost of providing these systems with the information they require to make a decision. In an expert system, the cost of providing the information that the system requires can be substantial. Minimizing information costs without affecting the decisions made by the system can reduce the cost of operating the system and thereby increase value. We develop an algorithm that determines an optimal information acquisition strategy for an existing system and show how a specific information acquisition strategy can be implemented. Because of the computational complexity of the algorithm, we also develop a simpler, heuristic solution to the problem. Our tests indicate that the heuristic performs very well. Prolog implementations for the same problem, on the other hand, perform poorly.",article,2,,,,,,,,
ProFPN: Progressive feature pyramid network with soft proposal assignment for object detection,Knowledge-Based Systems,299,112078,2024,0950-7051,https://doi.org/10.1016/j.knosys.2024.112078,https://www.sciencedirect.com/science/article/pii/S0950705124007123,Junjie Ke and Lihuo He and Bo Han and Jie Li and Xinbo Gao,"Feature pyramid, Object detection, Soft proposal assignment","Benefitting from the development of pyramidal feature learning, current state-of-the-art multi-scale detection paradigm has become proficient in detecting objects of varying scales. However, feature pyramid network (FPN), in spite of constructing multi-scale features with strong semantics, still suffers from limited performance caused by insufficient detail exploitation, information loss, limited receptive fields and hard proposal assignment, which can be mainly categorized into semantic level and instance level. To address these limitations, this paper analyzes the structural components that inhibit multi-scale feature representation and then presents a multi-stage progressive FPN (ProFPN) along with a novel RoI feature representation method called soft proposal assignment. In the semantic level, the bottom-up interaction module is first proposed to address to insufficient exploitation of high resolution features. In the bottom-up interaction module, global context attention blocks are utilized to interact adjacent-level features with detail information in a bottom-up progressive manner. After that, the top-down transfer module is designed to mitigate semantic information loss of high-level features. In the top-down transfer module, multi-branch asymmetric dilated blocks are adopted in a top-down progressive manner, which expands receptive fields to capture more object poses. In the instance level, to overcome the hard assignment of object proposals, a nonparametric strategy named soft proposal assignment is proposed to leverage the scale of each object proposal to generate dynamic weights for RoI features from adjacent levels. Comprehensive experiments conducted on MS COCO dataset demonstrate the superiority of ProFPN. By adding negligible extra FLOPs, the proposed ProFPN outperforms most pyramid-based methods. Moreover, due to the design of inherited feature utilization in ProFPN, transformer-based detectors have witnessed a substantial increase in detecting small objects while simultaneously achieving significant reductions in FLOPs. The source code of the proposed method is available at https://github.com/GingerCohle/ProFPN.",article,,,,,,,,,
A survey of Semantic Reasoning frameworks for robotic systems,Robotics and Autonomous Systems,159,104294,2023,0921-8890,https://doi.org/10.1016/j.robot.2022.104294,https://www.sciencedirect.com/science/article/pii/S092188902200183X,Weiyu Liu and Angel Daruna and Maithili Patel and Kartik Ramachandruni and Sonia Chernova,"Semantic reasoning, Robotics, Knowledge bases","Robots are increasingly transitioning from specialized, single-task machines to general-purpose systems that operate in diverse and dynamic environments. To address the challenges associated with operation in real-world domains, robots must effectively generalize knowledge, learn, and be transparent in their decision making. This survey examines Semantic Reasoning techniques for robotic systems, which enable robots to encode and use semantic knowledge, including concepts, facts, ideas, and beliefs about the world. Continually perceiving, understanding, and generalizing semantic knowledge allows a robot to identify the meaningful patterns shared between problems and environments, and therefore more effectively perform a wide range of real-world tasks. We identify the three common components that make up a computational Semantic Reasoning Framework: knowledge sources, computational frameworks, and world representations. We analyze the existing implementations and the key characteristics of these components, highlight the many interactions that occur between them, and examine their integration for solving robotic tasks related to five aspects of the world, including objects, spaces, agents, tasks, and actions. By analyzing the computational formulation and underlying mechanisms of existing methods, we provide a unified view of the wide range of semantic reasoning techniques and identify open areas for future research.",article,,,,,,,,,
A social network of crime: A review of the use of social networks for crime and the detection of crime,Online Social Networks and Media,30,100211,2022,2468-6964,https://doi.org/10.1016/j.osnem.2022.100211,https://www.sciencedirect.com/science/article/pii/S2468696422000155,Brett Drury and Samuel Morais Drury and Md Arafatur Rahman and Ihsan Ullah,"Social media, Cybercrime, Machine learning, Crime prediction, NLP","Social media is used to commit and detect crimes. With automated methods, it is possible to scale both crime and detection of crime to a large number of people. The ability of criminals to reach large numbers of people has made this area subject to frequent study, and consequently, there have been several surveys that have reviewed specific crimes committed on social platforms. Until now, there has not been a review article that considers all types of crimes on social media, their similarity as well as their detection. The demonstration of similarity between crimes and their detection methods allows for the transfer of techniques and data between domains. This survey, therefore, seeks to document the crimes that have been committed on social media, and demonstrate their similarity through a taxonomy of crimes. Also, this survey documents publicly available datasets. Finally, this survey provides suggestions for further research in this field.",article,,,,,,,,,
AraCovTexFinder: Leveraging the transformer-based language model for Arabic COVID-19 text identification,Engineering Applications of Artificial Intelligence,133,107987,2024,0952-1976,https://doi.org/10.1016/j.engappai.2024.107987,https://www.sciencedirect.com/science/article/pii/S0952197624001453,Md. Rajib Hossain and Mohammed Moshiul Hoque and Nazmul Siddique and M. Ali Akber Dewan,"Natural language processing, Low-resource text identification, Text processing, Language model, Arabic covid text, Ablation study, Late-fusion","In light of the pandemic, the identification and processing of COVID-19-related text have emerged as critical research areas within the field of Natural Language Processing (NLP). With a growing reliance on online portals and social media for information exchange and interaction, a surge in online textual content, comprising disinformation, misinformation, fake news, and rumors has led to the phenomenon of an infodemic on the World Wide Web. Arabic, spoken by over 420 million people worldwide, stands as a significant low-resource language, lacking efficient tools or applications for the detection of COVID-19-related text. Additionally, the identification of COVID-19 text is an essential prerequisite task for detecting fake and toxic content associated with COVID-19. This gap hampers crucial COVID information retrieval and processing necessary for policymakers and health authorities. Addressing this issue, this paper introduces an intelligent Arabic COVID-19 text identification system named ‘AraCovTexFinder,’ leveraging a fine-tuned fusion-based transformer model. Recognizing the challenges posed by a scarcity of related text corpora, substantial morphological variations in the language, and a deficiency of well-tuned hyperparameters, the proposed system aims to mitigate these hurdles. To support the proposed method, two corpora are developed: an Arabic embedding corpus (AraEC) and an Arabic COVID-19 text identification corpus (AraCoV). The study evaluates the performance of six transformer-based language models (mBERT, XML-RoBERTa, mDeBERTa-V3, mDistilBERT, BERT-Arabic, and AraBERT), 12 deep learning models (combining Word2Vec, GloVe, and FastText embedding with CNN, LSTM, VDCNN, and BiLSTM), and the newly introduced model AraCovTexFinder. Through extensive evaluation, AraCovTexFinder achieves a high accuracy of 98.89 ± 0.001%, outperforming other baseline models, including transformer-based language and deep learning models. This research highlights the importance of specialized tools in low-resource languages to combat the infodemic relating to COVID-19, which can assist policymakers and health authorities in making informed decisions.",article,,,,,,,,,
RevOnt: Reverse engineering of competency questions from knowledge graphs via language models,Journal of Web Semantics,82,100822,2024,1570-8268,https://doi.org/10.1016/j.websem.2024.100822,https://www.sciencedirect.com/science/article/pii/S1570826824000088,,"Knowledge engineering, Knowledge graph, Ontology development, Competency question extraction","The process of developing ontologies – a formal, explicit specification of a shared conceptualisation – is addressed by well-known methodologies. As for any engineering development, its fundamental basis is the collection of requirements, which includes the elicitation of competency questions. Competency questions are defined through interacting with domain and application experts or by investigating existing datasets that may be used to populate the ontology i.e. its knowledge graph. The rise in popularity and accessibility of knowledge graphs provides an opportunity to support this phase with automatic tools. In this work, we explore the possibility of extracting competency questions from a knowledge graph. This reverses the traditional workflow in which knowledge graphs are built from ontologies, which in turn are engineered from competency questions. We describe in detail RevOnt, an approach that extracts and abstracts triples from a knowledge graph, generates questions based on triple verbalisations, and filters the resulting questions to yield a meaningful set of competency questions; the WDV dataset. This approach is implemented utilising the Wikidata knowledge graph as a use case, and contributes a set of core competency questions from 20 domains present in the WDV dataset. To evaluate RevOnt, we contribute a new dataset of manually-annotated high-quality competency questions, and compare the extracted competency questions by calculating their BLEU score against the human references. The results for the abstraction and question generation components of the approach show good to high quality. Meanwhile, the accuracy of the filtering component is above 86%, which is comparable to the state-of-the-art classifications.",article,,,,,,,,,
3 - Images of Pages,,,61-89,2005,,https://doi.org/10.1016/B978-155860924-2/50006-2,https://www.sciencedirect.com/science/article/pii/B9781558609242500062,Michael Lesk,,"Publisher Summary
Besides ASCII, digital books also arrive in libraries as scanned pages. Some material is either not available as ASCII code or not appropriate to store that way. Most of such content is stored as images, portraying a picture of the original. This chapter discusses how such image files are created and how they can be accessed and used. Image files are the mode in which most digital libraries today store non-ASCII information. Often, for content such as photography, which is basically pictorial rather than textual, it would be the preferred format for use in upcoming years. Images are often the choice for older material; post-1980 material would be available in machine-readable form, since hot-lead machines would have completely given way to computer typesetting by then. A library wishing to convert old material to machine-readable form, however, is likely to have no economic alternative to scanning, since keystroking costs 10 times as much as scanning. Scanning is so cheap that its cost is comparable with the cost of building shelf space to hold books. Some systems involve both ASCII and image pages, utilizing the advantages of each. Web pages typically encode the images as Graphics Interchange File (GIF) or as JPG, and the text as hyper text markup language (HTML)-encoded ASCII; this lets the user handle them with different software and perform different manipulations on each.",incollection,,,Michael Lesk,Understanding Digital Libraries (Second Edition),Morgan Kaufmann,The Morgan Kaufmann Series in Multimedia Information and Systems,978-1-55860-924-2,San Francisco,Second Edition
"Simulating cyber attacks, defences, and consequences",Computers & Security,18,479-518,1999,0167-4048,https://doi.org/10.1016/S0167-4048(99)80115-1,https://www.sciencedirect.com/science/article/pii/S0167404899801151,Fred Cohen,,,article,6,,,,,,,,
"Who do you trust? Beyond encryption, secure e-business",Decision Support Systems,31,293-301,2001,0167-9236,https://doi.org/10.1016/S0167-9236(00)00140-8,https://www.sciencedirect.com/science/article/pii/S0167923600001408,Mathias Klang,"Electronic commerce, Law, Trust, Reputation","Electronic commerce has added a new complex issue to international trade. It is based upon the assumption that buyers and sellers conduct business with very little information about each other. This paper is on the importance and development of trust in electronic commerce. The importance of these assets in commercial relations is discussed. The paper describes how reputation is protected as a legal asset and how laws or legal principles support trust relationships in trade. Finally, the importance of developing legal guidelines for trust and reputation as a counterbalance to the lack of morality on the Internet is discussed.",article,3,,,,,,,,
Automatic recognition of printed Farsi texts,Pattern Recognition,14,395-403,1981,0031-3203,https://doi.org/10.1016/0031-3203(81)90084-4,https://www.sciencedirect.com/science/article/pii/0031320381900844,B Parhami and M Taraghi,"Character recognition, Computer input, Document input, Farsi, Feature selection, Optical character recognition, Pattern recognition, Persian, Printed text recognition","The automatic recognition of printed Farsi (Persian) texts is complicated by several properties of the Farsi script: (a) connectivity of symbols, (b) similarity of groups of symbols, (c) highly variable widths, (d) subword overlap, and (e) line overlap. In this paper, a technique for the automatic recognition of printed Farsi texts is presented and its steps are discussed as follows: (1) digitization, (2) editing, (3) line separation, (4) subword separation, (5) symbol separation, (6) recognition, and (7) postprocessing. The most notable contributions of this work are in algorithms for steps (5) and (6) above. Practical application of the technique to Farsi newspaper headlines has been 100% successful. However, smaller type fonts, which could not be handled by the coarse digitization hardware used, will no doubt result in less than perfect recognition. The technique is also applicable with little or no modification to printed Arabic and Urdu texts which use the same alphabet as Farsi.",article,1,1980 Conference on Pattern Recognition,,,,,,,
Chapter 2 - The human factor,,,31-91,2024,,https://doi.org/10.1016/B978-0-44-314096-9.00008-7,https://www.sciencedirect.com/science/article/pii/B9780443140969000087,I. Scott MacKenzie,"human senses, perception, motor responses, cognition, memory, chunking, language, reaction time, visual search, skilled performance, attention, human error, outliers","This chapter examines the human factor in interactive computing systems. We begin with a brief review of human sensory, perceptual, cognitive, and motor processes. Human memory is examined from the perspective long-term and short-term memory, with short-term memory limited to about seven (±2) units or chunks. Language, and in particular written language, is examined from an information perspective in terms of entropy and redundancy. Human performance in simple reaction and visual search tasks is studied and illustrated through the setup and results of an experiment. Skilled performance, attention, and human error are presented and discussed using examples in computing.",incollection,,,I. Scott MacKenzie,Human-Computer Interaction (Second Edition),Morgan Kaufmann,,978-0-443-14096-9,,Second Edition
Investigation on auditing principles and rules for PDM/PLM system implementation,Computers in Industry,64,741-753,2013,0166-3615,https://doi.org/10.1016/j.compind.2013.04.007,https://www.sciencedirect.com/science/article/pii/S0166361513000791,Shing-Han Li and Jian-Liang Chen and David C. Yen and Yu-Hui Lin,"Product data management, Product lifecycle management, Computer audit, Information technology audit","To avoid fraud behavior and ensure product data quality throughout the stages of product lifecycle management, the demand for the computer audit is increasing in the engineering and manufacturing industries. Enterprises have paid much attention to the implementation of product data management/product lifecycle management (PDM/PLM) systems in which computer audit mechanism is the critical function concerning the final success of PLM. However, the PDM/PLM systems nowadays usually lack of superior mechanism to audit the information quality of product data (PD) and its impact on product design, manufacture, service, and disposal. The purpose of this study is to find out a set of decisive auditing points and rules necessary for the PDM/PLM systems, which may be a reference basis for firms to audit quality of PD throughout the product lifecycle. Additionally, a case company was chosen to implement the proposed audit points and rules for verifying the research findings, examining the benefits deriving from the new audit system implemented, and providing valuable suggestions for firms to improve PD quality and effectiveness of PLM.",article,6,,,,,,,,
Logical analysis of binary data with missing bits,Artificial Intelligence,107,219-263,1999,0004-3702,https://doi.org/10.1016/S0004-3702(98)00110-6,https://www.sciencedirect.com/science/article/pii/S0004370298001106,Endre Boros and Toshihide Ibaraki and Kazuhisa Makino,"Knowledge discovery, Data mining, Logical analysis of data, Boolean functions, Partially defined Boolean functions, Missing bits, NP-hardness",,article,2,,,,,,,,
FL-OTCSEnc: Towards secure federated learning with deep compressed sensing,Knowledge-Based Systems,291,111534,2024,0950-7051,https://doi.org/10.1016/j.knosys.2024.111534,https://www.sciencedirect.com/science/article/pii/S0950705124001692,Leming Wu and Yaochu Jin and Yuping Yan and Kuangrong Hao,"Federated learning, Deep compressed sensing, Privacy preservation, Homomorphic encryption","In recent years, federated learning has made significant progress in preserving data privacy. In this paradigm, clients train local models without sharing their raw data, thereby substantially mitigating the vulnerability to private data exposure. However, it is still possible to infer clients’ raw data by leveraging the gradient parameters exchanged between the clients and the server. To address this problem, this paper proposes a novel algorithm that introduces deep compressed sensing into federated learning to support one time encryption, called FL-OTCSEnc, to secure the communication data exchanged between the clients and the server. The process starts by creating a dataset of deep learning model parameters and training a system for both encryption and decryption using deep compressed sensing. This system is then used to secure the communication between clients and the server in federated learning, by encrypting and decrypting the data exchanged. To enhance the security of the proposed algorithm, we introduce an assessment method for evaluating the security level of the clients, facilitating the selection of suitable candidates for deployment within distributed training encryption and decryption models that are updated in real time. To enhance the accuracy of the decrypted deep network model, we introduce a tandem loss function in the training process. Moreover, this paper proves that the proposed end-to-end encryption method satisfies additive homomorphic encryption properties. Extensive experiments demonstrate that the deep compressed sensing encryption in federated learning achieves promising results without increasing the computational complexity.",article,,,,,,,,,
Finite element approximations of an optimal control problem associated with the scalar Ginzburg-Landau equation,Computers & Mathematics with Applications,21,123-131,1991,0898-1221,https://doi.org/10.1016/0898-1221(91)90090-Q,https://www.sciencedirect.com/science/article/pii/089812219190090Q,M.D. Gunzburger and L. Hou and T.P. Svobodny,,"We consider finite element approximations of an optimal control problem associated with a scalar version of the Ginzburg-Landau equations of superconductivity. The control is the Neumann data on the boundary and the optimization goal is to obtain a best approximation, in the least squares sense, to some desired state. The existence of optimal solutions is proved. The use of Lagrange multipliers is justified and an optimality system of equations is derived. Then, the regularity of solutions of the optimality system is studied, and finally, finite element algorithms are defined and optimal error estimates are obtained.",article,2,,,,,,,,
A comparative review on multi-modal sensors fusion based on deep learning,Signal Processing,213,109165,2023,0165-1684,https://doi.org/10.1016/j.sigpro.2023.109165,https://www.sciencedirect.com/science/article/pii/S0165168423002396,Qin Tang and Jing Liang and Fangqi Zhu,"Multi-model data fusion, Deep learning, Inference mechanisms","The wide deployment of multi-modal sensors in various areas generates vast amounts of data with characteristics of high volume, wide variety, and high integrity. However, traditional data fusion methods face immense challenges when dealing with multi-modal data containing abundant intermodality and cross-modality information. Deep learning has the ability to automatically extract and understand the potential association of multi-modal information. Despite this, there is a lack of a comprehensive review of the inherent inference mechanisms of deep learning for multi-modal sensor fusion. This work investigates up-to-date developments in multi-modal sensor fusion via deep learning to provide a broad picture of data fusion needs and technologies. It compares the characteristics of multi-modal data for various sensors, summarizes background concepts about data fusion and deep learning, and carefully reviews a large number of investigations in four inference mechanisms: adaptive learning, deep generative, deep discriminative, and algorithms unrolling. The pros and cons of the above methodologies are presented, and several popular application domains are discussed, including medical imaging, autonomous driving, remote sensing, and robotics. A large collection of multi-modal datasets published in recent years is presented, and several tables that quantitatively compare and summarize the performance of fusion algorithms are provided. Finally, by acknowledging the limitations of current research, we establish potential open challenges and future directions as guidance for deep learning-based multi-sensor fusion.",article,,,,,,,,,
Robust and explainable identification of logical fallacies in natural language arguments,Knowledge-Based Systems,266,110418,2023,0950-7051,https://doi.org/10.1016/j.knosys.2023.110418,https://www.sciencedirect.com/science/article/pii/S0950705123001685,,"Logical fallacy, Explainability, Case-based reasoning, Knowledge injection, Data augmentation, Robustness","The spread of misinformation, propaganda, and flawed argumentation has been amplified in the Internet era. Given the volume of data and the subtlety of identifying violations of argumentation norms, supporting information analytics tasks, like content moderation, with trustworthy methods that can identify logical fallacies is essential. In this paper, we formalize prior theoretical work on logical fallacies into a comprehensive three-stage evaluation framework of detection, coarse-grained, and fine-grained classification. We adapt existing evaluation datasets for each stage of the evaluation. We employ three families of robust and explainable methods based on prototype reasoning, instance-based reasoning, and knowledge injection. The methods combine language models with background knowledge and explainable mechanisms. Moreover, we address data sparsity with strategies for data augmentation and curriculum learning. Our three-stage framework natively consolidates prior datasets and methods from existing tasks, like propaganda detection, serving as an overarching evaluation testbed. We extensively evaluate these methods on our datasets, focusing on their robustness and explainability. Our results provide insight into the strengths and weaknesses of the methods on different components and fallacy classes, indicating that fallacy identification is a challenging task that may require specialized forms of reasoning to capture various classes. We share our open-source code and data on GitHub to support further work on logical fallacy identification.",article,,,,,,,,,
Regulating the Internet: clutching at a straw?,Computer Communications,20,1519-1526,1998,0140-3664,https://doi.org/10.1016/S0140-3664(97)00161-8,https://www.sciencedirect.com/science/article/pii/S0140366497001618,Assafa Endeshaw,"Copyright, Internet regulation, ‘Undesirable’ material","The Internet has created intricate problems for the law. Established standards comfortable with traditional means of communication (print, common carriers and broadcasting) have been shaken or made redundant. This paper seeks to highlight the major changes the law has undergone and may continue to do so in the future. It focuses on the striving of the major industrial countries to regulate the Internet as a whole and to address issues of privacy and intellectual property. It concludes by pointing out the inherent limitations of national solutions to problems raised by the Internet and the appropriateness of international arrangements.",article,16,Internet: State-of-the-art,,,,,,,
PERCY: A post-hoc explanation-based score for logic rule dissemination consistency assessment in sentiment classification,Knowledge-Based Systems,275,110685,2023,0950-7051,https://doi.org/10.1016/j.knosys.2023.110685,https://www.sciencedirect.com/science/article/pii/S0950705123004355,Shashank Gupta and Mohamed Reda Bouadjenek and Antonio Robles-Kelly,"Logic rules dissemination, Sentiment classification, Explainable AI","Disseminating and incorporating logic rules into deep neural networks has been extensively explored for sentiment classification in recent years. In particular, most methods and algorithms proposed for this purpose rely on a specific component that aims to capture and model logic rules, followed by a sequence model to process the input sequence. While the authors of these methods claim that they effectively capture syntactic structures that affect sentiment classification, they only show improvement in accuracy to support their claims without further analysis. Focusing on various syntactic structures, particularly contrastive discourse relations such as the A-but-B structure, we introduce the PERCY score, a novel Post-hoc Explanation-based Rule ConsistencY Score to analyze and study the ability of several of these methods to identify these structures in a given sentence, and to make their classification decisions based on the appropriate conjunct. Specifically, we explore the use of model-agnostic post-hoc explanation frameworks to explain the predictions of any classifier in an interpretable and faithful manner. These model explainability frameworks provide feature attribution scores to estimate each word’s impact on the final classification decision. Then, they are combined to check whether the model has based its decision on the right conjunct. Our experiments show that (a) accuracy – or any other performance metric – can be misleading in assessing the ability of logic rule dissemination methods to base their decisions on the right conjunct, (b) not all analyzed methods effectively capture syntactic structures, (c) often, the underlying sequence model is what captures the structure, and (d) for the best method, less than 25% of the test examples are classified based on the appropriate conjunct, indicating that a lot of research needs to be done on this topic. Finally, we experimentally demonstrate that the PERCY scores calculated are robust and stable w.r.t. the feature-attribution frameworks used.",article,,,,,,,,,
A multi-disjunctive-graph model-based memetic algorithm for the distributed job shop scheduling problem,Advanced Engineering Informatics,60,102401,2024,1474-0346,https://doi.org/10.1016/j.aei.2024.102401,https://www.sciencedirect.com/science/article/pii/S1474034624000491,Sihan Wang and Xinyu Li and Liang Gao and Jiahang Li,"Distributed job shop scheduling problem, Multi-disjunctive-graph model, Memetic algorithm, Encoding scheme, Neighborhood structure","With the influence of the digital economy, the traditional manufacturing model is undergoing a shift towards a distributed manufacturing model. This transition involves multiple workshops across diverse geographic regions. The core of distributed manufacturing is the concept of decentralized management and execution, which includes various stages, resources, and tasks within the production process. A key technology in this domain is the distributed shop scheduling problem. Notably, the distributed job shop scheduling problem (DJSSP), considering job shops, is widespread in real distributed manufacturing processes and is difficult to solve as an NP-hard problem. Although several heuristic solvers and metaheuristic algorithms have attempted to address this problem, currently two sub-problems included in the problem, factory allocation and sequence of operations, are treated separately and the description of the problem is incomplete. This paper introduces a multi-disjunctive-graph model-based memetic algorithm (MDGMBMA) developed for DJSSP to minimize the makespan. The multi-disjunctive-graph model is proposed to fully represent the DJSSP solution space. Additionally, an innovative encoding method is proposed to achieve an adequate search of the solution space, and two decoding strategies are proposed to address the search and evaluation demands of the algorithm. Furthermore, based on the property of critical job exchange between factories, a specialized critical job exchange-based neighborhood structure is designed to enhance the efficiency of the tabu search. To evaluate the performance of the MDGMBMA, numerical results from 240 large instances (ranging from 2 to 4 factories) derived from well-known JSSP benchmarks are compared against recently published discrete metaheuristic algorithms. The experimental results indicate that the proposed algorithm performs effectively in solving DJSSP.",article,,,,,,,,,
Architecture of distributed data base systems,Journal of Systems and Software,10,77-95,1989,0164-1212,https://doi.org/10.1016/0164-1212(89)90021-6,https://www.sciencedirect.com/science/article/pii/0164121289900216,Sudha Ram and Clark L. Chastain,,"Research and development over the last twenty years has culminated in the widespread use of data base management system (DBMS) software. As usage has grown, the desire to link and integrate separate data bases has resulted in substantial effort being directed towards the design of distributed data base systems. This paper presents the major architectures which have emerged for distributed data base systems. The architectures are compared and evaluated. Sixteen distributed data base management system (DDBMS) projects have been surveyed and classified according to the architectures. The various projects represent widely differing stages of effort: academic research, industrial testbeds, and commercial prototypes. The survey reviews important features of the DDBMSs. It does not attempt a qualitative performance comparison. The focus is instead on identification of overall architectural characteristics. The usefulness of the survey lies in the summary information which it imparts on current research, and in the classification scheme for generic distributed data base architectures which it provides.",article,2,,,,,,,,
How to leverage anthropomorphism for chatbot service interfaces: The interplay of communication style and personification,Computers in Human Behavior,149,107954,2023,0747-5632,https://doi.org/10.1016/j.chb.2023.107954,https://www.sciencedirect.com/science/article/pii/S0747563223003059,Andreas Janson,"Chatbots, Conversational agents, Anthropomorphic design, Social presence, Empathy, Trust","Although chatbots are oftentimes used in customer service encounters, interactions are oftentimes perceived as not satisfactory. One key aspect for designing chatbots is the use of anthropomorphic design elements. In this experimental study, we examine the two anthropomorphic chatbot design elements of personification, which includes a human-like appearance, and social orientation of communication style, which means a more sensitive and extensive communication. We tested the influence of the two design elements on social presence, satisfaction, trust and empathy towards a chatbot. First, the results show a significant influence of both anthropomorphic design elements on social presence. Second, our findings illustrate that social presence influences trusting beliefs, empathy, and satisfaction. Third, social presence acts as a mediator for both anthropomorphic design elements for satisfaction with a chatbot. Our implications provide a better understanding of anthropomorphic chatbot design elements when designing chatbots for short-term interactions, and we offer actionable implications for practice that enable more effective chatbot implementations.",article,,,,,,,,,
Intelligent decision support systems for dementia care: A scoping review,Artificial Intelligence in Medicine,150,102815,2024,0933-3657,https://doi.org/10.1016/j.artmed.2024.102815,https://www.sciencedirect.com/science/article/pii/S0933365724000575,Amirhossein Eslami Andargoli and Nalika Ulapane and Tuan Anh Nguyen and Nadeem Shuakat and John Zelcer and Nilmini Wickramasinghe,"Artificial intelligence, Decision support systems, Analytics, Dementia, Alzheimer","In the context of dementia care, Artificial Intelligence (AI) powered clinical decision support systems have the potential to enhance diagnosis and management. However, the scope and challenges of applying these technologies remain unclear. This scoping review aims to investigate the current state of AI applications in the development of intelligent decision support systems for dementia care. We conducted a comprehensive scoping review of empirical studies that utilised AI-powered clinical decision support systems in dementia care. The results indicate that AI applications in dementia care primarily focus on diagnosis, with limited attention to other aspects outlined in the World Health Organization (WHO) Global Action Plan on the Public Health Response to Dementia 2017–2025 (GAPD). A trifecta of challenges, encompassing data availability, cost considerations, and AI algorithm performance, emerges as noteworthy barriers in adoption of AI applications in dementia care. To address these challenges and enhance AI reliability, we propose a novel approach: a digital twin-based patient journey model. Future research should address identified gaps in GAPD action areas, navigate data-related obstacles, and explore the implementation of digital twins. Additionally, it is imperative to emphasize that addressing trust and combating the stigma associated with AI in healthcare should be a central focus of future research directions.",article,,,,,,,,,
"Law versus technology: Blockchain, GDPR, and tough tradeoffs",Computer Law & Security Review,38,105454,2020,0267-3649,https://doi.org/10.1016/j.clsr.2020.105454,https://www.sciencedirect.com/science/article/pii/S0267364920300595,Unal Tatar and Yasir Gokce and Brian Nussbaum,"General data protection regulation, GPDR, Blockchain, Privacy, Personal information, Privacy by design, Privacy by default, Right to be forgotten, Data controller","Inconsistency between the way in which the law is structured, and the way in which technologies actually operate is always an interesting and useful topic to explore. When a law conflicts with a business model, the solution will often be changing the business model. However, when the law comes into conflict with the architecture of hardware and software, it is less clear how the problem will be managed. In this paper, we analyze the contradiction of blockchain technology and the requirements of GDPR. The three contradictions we examine are (i) right to be forgotten versus irreversibility/immutability of records, (ii) data protection by design versus tamper-proofness and transparency of blockchain, and (iii) data controller versus decentralized nodes. We highlight that the conflicts can be handled through focusing on commonalities of GDPR and the blockchain, developing new approaches and interpretations, and tailoring the blockchain technology according to the needs of data protection law.",article,,,,,,,,,
A dual-stream recurrence-attention network with global–local awareness for emotion recognition in textual dialog,Engineering Applications of Artificial Intelligence,128,107530,2024,0952-1976,https://doi.org/10.1016/j.engappai.2023.107530,https://www.sciencedirect.com/science/article/pii/S0952197623017141,Jiang Li and Xiaoping Wang and Zhigang Zeng,"Dialog emotion recognition, Recurrent neural network, Multi-head attention network, Dialog system, Dual-stream network","In real-world dialog systems, the ability to understand the user’s emotions and interact anthropomorphically is of great significance. Emotion Recognition in Conversation (ERC) is one of the key ways to accomplish this goal and has attracted growing attention. How to model the context in a conversation is a central aspect and a major challenge of ERC tasks. Most existing approaches struggle to adequately incorporate both global and local contextual information, and their network structures are overly sophisticated. For this reason, we propose a simple and effective Dual-stream Recurrence-Attention Network (DualRAN), which is based on Recurrent Neural Network (RNN) and Multi-head ATtention network (MAT). DualRAN eschews the complex components of current methods and focuses on combining recurrence-based methods with attention-based ones. DualRAN is a dual-stream structure mainly consisting of local- and global-aware modules, modeling a conversation simultaneously from distinct perspectives. In addition, we develop two single-stream network variants for DualRAN, i.e., SingleRANv1 and SingleRANv2. According to the experimental findings, DualRAN boosts the weighted F1 scores by 1.43% and 0.64% on the IEMOCAP and MELD datasets, respectively, in comparison to the strongest baseline. On two other datasets (i.e., EmoryNLP and DailyDialog), our method also attains competitive results.",article,,,,,,,,,
Neural networks for the classification of image texture,Engineering Applications of Artificial Intelligence,7,381-393,1994,0952-1976,https://doi.org/10.1016/0952-1976(94)90004-3,https://www.sciencedirect.com/science/article/pii/0952197694900043,Anwar K. Muhamad and Farzin Deravi,"Neural networks, texture images, co-occurrence matrices, classification, wear particles","In this paper, an extensive study of the efficiency of secondary features extracted from co-occurrence matrices and then direct use of co-occurrence matrices for the classification of texture images is carried out. First, the classification capabilities of individual, and then several combinations of, secondary features extracted from the co-occurrence matrices are examined. An adequate combination consisting of four of those features is hence established. A feed-forward artificial neural network is used as a classifier. The classification performance of the network as a function of the training data volume, its size and the training strategy are investigated. Next, an investigation is made into the direct use of co-occurrence matrix entries for classification. A simple technique, where the gray levels in the original images are grouped into a small number of equal bands, is employed in order to reduce the size of the matrix. It is shown that results obtained here compare favorably with those obtained earlier using the set of secondary features. The efficiency of the last technique is further demonstrated by means of considering a practical application. This involves the surface texture classification of wear particles found in the lubricating oil of moving machinery and engines. Again, it is shown that high recognition rates can be achieved when dealing with such classification tasks.",article,4,,,,,,,,
Editorial Board,Computer Law & Security Review,4,IFC,1988,0267-3649,https://doi.org/10.1016/0267-3649(88)90079-9,https://www.sciencedirect.com/science/article/pii/0267364988900799,,,,article,1,,,,,,,,
Bibliography,Computers & Geosciences,14,719-915,1988,0098-3004,https://doi.org/10.1016/0098-3004(88)90012-X,https://www.sciencedirect.com/science/article/pii/009830048890012X,,,,article,6,,,,,,,,
Text mining tool for translating terms of contract into technical specifications: Development and application in the railway sector,Computers in Industry,124,103357,2021,0166-3615,https://doi.org/10.1016/j.compind.2020.103357,https://www.sciencedirect.com/science/article/pii/S0166361520305911,G. Fantoni and E. Coli and F. Chiarello and R. Apreda and F. Dell’Orletta and G. Pratelli,"Contract terms, Technical requirements, Tendering, Computational science, Text mining, Natural language processing","Tenders or technical terms contain a large quantity of both technical, legal, managerial information mixed in a nested and complex net of relationships. Extracting technical and design information from a document whose aim is both legal and technical, and that is written using several specific jargons, is not a trivial task: the purpose of the research is to try to detect, extract, split and assign information from the text of a tender in an automatic way. It means being able to understand technical and legal terms and organize them in multiple ways: according to product structure, internal organisational structure, etc. The focus is in providing a handy tool that could speed up and facilitate human analysis and allow tackling also the process of transforming customer’s requirements into design specifications. The approach chosen to overcome the various issues is to support state-of-the-art Computational Linguistic tools with a wide Knowledge Base. The latter has been constructed both manually and automatically and comprises not only keywords but also concepts, relationships and regular expressions. The implementation of the methodology has been carried out during a project for AnsaldoBreda S.p.A. (now Hitachi Rail Europe). A case study about the tender for a high-speed train has been included to show the functioning and output of the entire software system.",article,,,,,,,,,
"Intellectual Property Law: by Paul Marett, 1996, soft-cover, Sweet & Maxwell, 249 pp., sl2.95, ISBN 0 42155420 7",Computer Law & Security Review,13,271,1997,0267-3649,https://doi.org/10.1016/S0267-3649(97)88859-0,https://www.sciencedirect.com/science/article/pii/S0267364997888590,,,,article,4,,,,,,,,
Editorial Board,Computer Law & Security Review,3,IFC,1987,0267-3649,https://doi.org/10.1016/0267-3649(87)90068-9,https://www.sciencedirect.com/science/article/pii/0267364987900689,,,,article,1,,,,,,,,
Can the PRC’S new anti-monopoly law stop monopolistic activities: Let the PRC’S telecommunications industry tell you the answer,Telecommunications Policy,33,360-370,2009,0308-5961,https://doi.org/10.1016/j.telpol.2009.03.005,https://www.sciencedirect.com/science/article/pii/S0308596109000275,Grace Li,"AML, PRC telecommunications reform, Regulation, Competitive market","A new PRC Anti-Monopoly Law (AML) was enacted on August 30, 2007 and took effect on August 1, 2008. The new AML is a milestone in Chinese economic policy. It will reorganise the competition paradigm in many Chinese domestic sectors. PRC's telecommunications sector had undergone a number of major reforms in the past two decades. These reforms include one fundamental regulatory restructuring and three significant market reorganisations. As a result, the PRC telecommunications industry has developed by an astonishingly fast speed. In May 2008, the PRC State Council initiated a large telecommunications reform in both regulatory domain and market restructure. This reform has created a mega-telecommunications regulator, and merged six telecommunications players into three giant operators. All these were done 3 months prior to the country's AML become effective. Against this background, this paper studies the new AML and its various provisions in relation to antimonopoly and anti-competitive conducts and attempts to validate those provisions against the recent PRC telecommunications reform. Part 1 provides an introduction of the telecommunications industry in China, including its regulatory framework and its market arrangement. Part 2 studies the 13-year long law making history of the new AML and highlights some of the major aspects of this act. Part 3 analyses the recent telecommunications reform in China. Part 4 analyses the interplay between the AML and the recent telecommunications reform. In conclusion, the paper argues that the recent telecommunications reform constitutes an administrative monopolistic conduct in the telecommunications market, which clearly breached provisions set in chapter 5 of the AML. As a result, the AML is unlikely to effectively foster competition in China's telecom industry. Moreover, if this situation is not addressed by the PRC government in a timely manner, it would undermine the sustainability of the telecommunications industry and challenge the effectiveness of the AML.",article,7,,,,,,,,
"A meta-analysis of third-person perception related to distorted information: Synthesizing the effect, antecedents, and consequences",Information Processing & Management,60,103425,2023,0306-4573,https://doi.org/10.1016/j.ipm.2023.103425,https://www.sciencedirect.com/science/article/pii/S0306457323001620,Meng Chen and Weihua Yu and Ke Liu,"Misinformation, Disinformation, Third-person perception/effect, Meta-analysis","In the long run of fighting distorted information, empowering Internet users is believed to be an economic and sustainable solution. The effectiveness of this approach relies on the assumption that Internet users pay close attention to and hold unbiased perceptions of the distorted information. To obtain a systematic examination of people's perceptions of the distorted information, we performed a two-part meta-analysis based on 24 articles with 20,777 participants across three continents. Drawing on the third-person perception/effect (TPP/TPE) framework, Part I synthesized the literature examining the perpetual gap of distorted information's influence on self and others. Based on 28 effect sizes, the results confirmed a strong third-person perception related to distorted information (d = 0.614, p <.0001). Factors identified as moderating the effect magnitude include distorted information type, TPP operationalization, and study context. Part II was a synthesis of 63 effect sizes examining the potential antecedents and consequences of distorted information TPP. The results indicated that media use, distorted information exposure, and efficacy beliefs are predictors of distorted information TPP. However, policy support, proposed as a potential consequence, was not found to be so. The implications of our findings and directions for future research are discussed.",article,5,,,,,,,,
Citation prediction by leveraging transformers and natural language processing heuristics,Information Processing & Management,61,103583,2024,0306-4573,https://doi.org/10.1016/j.ipm.2023.103583,https://www.sciencedirect.com/science/article/pii/S0306457323003205,,"Citation prediction, Transformers architecture, Mask-filling, Named entity recognition, BERT","In scientific papers, it is common practice to cite other articles to substantiate claims, provide evidence for factual assertions, reference limitations, and research gaps, and fulfill various other purposes. When authors include a citation in a given sentence, there are two considerations they need to take into account: (i) where in the sentence to place the citation and (ii) which citation to choose to support the underlying claim. In this paper, we focus on the first task as it allows multiple potential approaches that rely on the researcher’s individual style and the specific norms and conventions of the relevant scientific community. We propose two automatic methodologies that leverage transformers architecture for either solving a Mask-Filling problem or a Named Entity Recognition problem. On top of the results of the proposed methodologies, we apply ad-hoc Natural Language Processing heuristics to further improve their outcome. We also introduce s2orc-9K, an open dataset for fine-tuning models on this task. A formal evaluation demonstrates that the generative approach significantly outperforms five alternative methods when fine-tuned on the novel dataset. Furthermore, this model’s results show no statistically significant deviation from the outputs of three senior researchers.",article,1,,,,,,,,
"Data Collection, data mining and transfer of learning based on customer temperament-centered complaint handling system and one-of-a-kind complaint handling dataset",Advanced Engineering Informatics,60,102520,2024,1474-0346,https://doi.org/10.1016/j.aei.2024.102520,https://www.sciencedirect.com/science/article/pii/S147403462400168X,Ching-Hung Lee and Xuejiao Zhao,"Customer Complaint Handling System, Customer Temperament, Data Mining, Correspondence Analysis, Interactive marketing","One of the most significant sources of information from customers is customer complaints. Successful and effective complaint management can end complaint crises and ensure client loyalty, which is a sign of great service performance. In this paper, we proposed a novel customer temperament-centered and e-CCH system-based data collection and data mining method titled “3D” model for customer complaint data analysis. Three phases are (1) Development and launch of e-Customer Complaint Handling system, (2) Data collection and transfer of learning by e-Customer Complaint Handling system, and (3) Data mining by e-Customer Complaint Handling system. An advanced electronic Customer Complaint Handling System called the e-CCH system was then developed and launched. This system adapts the seasonal associations model based on Hippocrates's customer temperament theory to the whole stages of customer complaint reporting and handling. With this system, we conducted a dataset collection work from restaurant chains of two brands over four years. As a result, we collect thousands of real-world temperament-centred customer complaint cases by four years to form the one-of-a-kind CCH dataset. This one-of-a-kind CCH dataset was open-sourced with detailed customer complaint attributes and heuristic decision-making for valuable industrial handling manner. After further analysis of this dataset, we found that customers with different temperament types tend to have different types of complaints. In addition, adapting the temperament theory to the e-CCH system can classify customer types better and provide personalized solutions. To our best knowledge, this rich and the one-of-a-kind CCH dataset reported in this paper is the first comprehensive study of customer complaint handling in an industrial service management context. Meanwhile, data mining with cross analysis and correspondence analysis and an ChatGPT experiment for transfer of learning based on this yearly and one-of-a-kind industrial customer complaint dataset was analyzed and discussed. In addition, how this dataset may contribute to more realistic complaint-handling theoretic studies for better service failure recovery and interactive marketing is discussed in-depth.",article,,,,,,,,,
A self-selection distributed arbiter for a multiprocessor,Microprocessors and Microsystems,19,201-207,1995,0141-9331,https://doi.org/10.1016/0141-9331(95)91859-3,https://www.sciencedirect.com/science/article/pii/0141933195918593,,"multiprocessor, self-selection arbiter, contention","Some years ago Gespac included the well known self-selection arbitration technique in their new G-96 backplane bus standard. Along with other enhancements introduced later (such as the message passing specification) this adoption made possible the implementation of small and inexpensive bus-based multiprocessors. In this paper we describe an integrated circuit implementation of the self-selection arbiter (SSA) which is now being used in our G-96 bus-based multiprocessor system. The arbiter is based on a single semi-custom 1.5 μm CMOS ASIC which implements a complete SSA unit for one processor. Each SSA unit is composed of two main functional blocks: the self-selection network, which performs arbitration at the most basic level, and the self-selection arbiter control unit, which deals with the specific details of the bus protocol, arbitration policies and, in general, all control and synchronization required to carry out the bus allocation process. Although our implementation concerns the G-96 bus, its basic principles are quite general, so most of its features can easily be ported to other buses.",article,4,,,,,,,,
Railway accident causation prediction with improved transformer model based on lexical information and contextual relationships,Knowledge-Based Systems,296,111897,2024,0950-7051,https://doi.org/10.1016/j.knosys.2024.111897,https://www.sciencedirect.com/science/article/pii/S0950705124005318,Bin Jiang and Keming Wang,"Railway safety, Causal analysis, Natural language processing, Deep learning, Data analysis","The railway system is a prime example of a safety-critical system. Predicting the causes of railway accidents holds immense significance in enhancing railway transportation safety. Previous approaches to railway causation analysis have encountered huge challenges regarding data processing and analytical capabilities. To address this concern, this paper proposes an innovative deep model framework based on the Transformer architecture that utilizes historical data on railway equipment accidents to predict the causes behind such incidents. Firstly, this paper proposes the utilization of Convolutional Block Attention in the domain of text processing, serving as a lexical encoder to augment word semantics acquisition in accident texts. Subsequently, in order to address the deficiency of traditional Transformers that lack positional representation information, we propose incorporating a BiGRU (Bidirectional Gated Recurrent Unit) as a contextual positional information encoder to capture contextual positional information in railway accident data effectively. Finally, considering that accident data reports are discrete tabular data, this study suggests employing cue word techniques for preprocessing accident data to alleviate the model's learning burden. We applied the proposed model to the FRA (Federal Railroad Administration) dataset. The results demonstrate that our model surpasses the current state-of-the-art language models, exhibiting superior performance compared to the optimal model with a notable improvement of 3.56%, 0.42%, and 0.76% in Precision, Recall, and F1-score, respectively. Furthermore, our model accurately predicts accident categories prone to misjudgment even when trained on limited data, outperforming existing language models. The study findings will contribute to the prevention and management of railway accidents.",article,,,,,,,,,
A multi-level model-driven regime for value-added tax compliance in ERP systems,Computers in Industry,60,709-727,2009,0166-3615,https://doi.org/10.1016/j.compind.2009.05.013,https://www.sciencedirect.com/science/article/pii/S0166361509001031,Jan B.M. Goossenaerts and Alexander T.M. Zegers and Jan M. Smits,"ERP systems, Value-added tax, Information systems, Model driven, Compliance, Multi-level perspective","As the economy becomes global and ICT-reliant, approaches practiced in enterprise software product development and enterprise resource planning (ERP) system implementation must cope with increasingly complex situations induced by contemporary supply chain and regulations. Compliance with regulations in the market is one aspect of the requirements that enterprise software must meet. Recent research of KPMG IT Advisory has confirmed that where little attention is being paid to the value-added tax (VAT) issues during large ERP projects, there is a higher VAT-risk exposure. In a design-oriented approach, we first identify the stakeholders and their interests in the VAT compliance of ERP systems. Enterprise architecture (EA) and model driven engineering in a multi-level perspective serve as the source of solution patterns. The efficient solution of compliance problems builds upon stakeholders utilizing a set of interdependent models and methods that are suitably allocated to the public and proprietary domains.",article,9,,,,,,,,
Globalization and regulatory change: The interplay of laws and technologies in E-commerce in Southeast Asia,Computer Law & Security Review,35,105315,2019,0267-3649,https://doi.org/10.1016/j.clsr.2019.03.009,https://www.sciencedirect.com/science/article/pii/S0267364918304308,Heejin Kim,"Electronic commerce law, The use and recognition of electronic signatures, Technological neutrality, ASEAN law and policy, Domestic regulatory adaptation","Electronic commerce has brought about business and technological changes globally, and these global changes have given rise to major legal reforms across nations. In the fast-changing global digital economy, states need strategies to maintain competitiveness of their markets while simultaneously ensuring the secure and effective use of technologies involved in conducting electronic transactions. This paper examines how the use and recognition of electronic signatures are regulated in Southeast Asia – the region that has shown the most significant growth in global e-commerce in past few years. Based on a comparative analysis of the laws of four representative ASEAN member states – namely Singapore, Thailand, Malaysia, and Vietnam, this paper argues that there is a regional trend towards adopting more liberal and technology-neutral standards for electronic signatures. Electronic signature regulation in Southeast Asia is now built upon limited technological neutrality (or the so-called “two-tiered” approach) as a shared regulatory understanding, but this approach is operationalized differently in each state due to distinctive national contexts. Within the common legal framework, each state has developed its own system of control and management with respect to higher-level signatures (using advanced technologies). The principle of technological neutrality, a concept originally developed for the regulation of technologies in response to the liberalization of telecommunications market, has been the central theme of discussions on the e-transactions policy-making scene. As the author shows, in the process through which states localize the global standards of technological neutrality, ASEAN as a vehicle of regulatory change has played an essential role in translating this principle to the national context.",article,5,,,,,,,,
A comprehensive survey of robust deep learning in computer vision,Journal of Automation and Intelligence,2,175-195,2023,2949-8554,https://doi.org/10.1016/j.jai.2023.10.002,https://www.sciencedirect.com/science/article/pii/S294985542300045X,Jia Liu and Yaochu Jin,"Robustness, Deep learning, Computer vision, Survey, Adversarial attack, Adversarial defenses","Deep learning has presented remarkable progress in various tasks. Despite the excellent performance, deep learning models remain not robust, especially to well-designed adversarial examples, limiting deep learning models employed in security-critical applications. Therefore, how to improve the robustness of deep learning has attracted increasing attention from researchers. This paper investigates the progress on the threat of deep learning and the techniques that can enhance the model robustness in computer vision. Unlike previous relevant survey papers summarizing adversarial attacks and defense technologies, this paper also provides an overview of the general robustness of deep learning. Besides, this survey elaborates on the current robustness evaluation approaches, which require further exploration. This paper also reviews the recent literature on making deep learning models resistant to adversarial examples from an architectural perspective, which was rarely mentioned in previous surveys. Finally, interesting directions for future research are listed based on the reviewed literature. This survey is hoped to serve as the basis for future research in this topical field.",article,4,,,,,,,,
Computer vision tools for early post-disaster assessment: Enhancing generalizability,Engineering Applications of Artificial Intelligence,136,108855,2024,0952-1976,https://doi.org/10.1016/j.engappai.2024.108855,https://www.sciencedirect.com/science/article/pii/S0952197624010133,Rojiar Soleimani and Mohammad Hesam Soleimani-Babakamali and Shuochuan Meng and Onur Avci and Ertugrul Taciroglu,"Natural hazards, Data fusion, Aerial damage assessment, Ensemble learning, Disaster response, Disaster recovery","Remote sensing data, particularly satellite imagery, have made early, post-hazard aerial damage assessment possible due to its fast availability and extensive coverage. Despite breakthroughs in using deep computer vision with satellite image inputs, achieving high generalizability across diverse hazards and locations remains the main obstacle to effectively deploying early assessment tools in real-world scenarios, as the same hazard can manifest differently across various landscapes and urban textures. This challenge may be overlooked when working with curated datasets with minimal (test-to-train) shifts in urban textures and hazard damage features (e.g., due to undersized study regions), leaving models ill-prepared for real-world scenarios. The primary objective of this study was to understand non-trivial generalizability challenges by taking the 2023 Turkiye earthquake and the Maui wildfire incidents as “training” and “unseen test” events that simulated a demanding scenario. Subsequently, strategies such as augmenting image channels with damage proxy maps, data fusion, deep ensemble learning, and Test Time Augmentation were exclusively designed and implemented to address those challenges. These measures significantly improved the damage detection, with or without severity classification, with F1 scores increased from 0.71 to 0.82 and 0.40 to 0.87, respectively. Furthermore, and through data fusion, the proposed framework accommodates estimating socioeconomic loss metrics at the individual building level, supporting both response and recovery phases. This research has the potential to enhance the effectiveness of rapid aerial damage assessment models, ultimately aiding in more efficient and targeted disaster response and recovery efforts. Data, models, and codes are available at https://github.com/TRG-AI4Good/Lahaina_Generalizability.",article,,,,,,,,,
Chapter 13 - WoX: Model-Driven Development of Web of Things Applications,,,357-387,2017,,https://doi.org/10.1016/B978-0-12-809764-9.00017-2,https://www.sciencedirect.com/science/article/pii/B9780128097649000172,Adriana Caione and Alessandro Fiore and Luca Mainetti and Luigi Manco and Roberto Vergallo,"Internet of things, Internet of everything, Web of topics, Model-driven, Cloud platforms","Nodes of the Internet of Things (IoT) are heterogeneous: Bluetooth Low Energy (BLE), Radio Frequency Identification (RFID), Near Field Communication (NFC), Wireless Sensors Networks (WSN), Konnex (KNX), just to name the most popular. IoT clients are heterogeneous too: mobile apps, laptops, enterprise applications, business processes instances, not to mention that even IoT nodes can be clients for other nodes. In this many-to-many relationship scenario, developing a seamless IoT system is arduous even for a specialized developer. All the more so, enable non-technical people to autonomously define innovative IoT-based scenarios is far from being trivial. This calls for the definition of a common design model shared by all the IoT stakeholder: device manufacturers, developers, stakeholders, business entities, end users. The Web of Things (WoT) paradigm has brought the IoT a step closer to the people perception, because it allows treating a networked thing as a Web resource. Nevertheless, sharing a common application layer protocol on top of the physical “things” does not guarantee that IoT application will be fast-developed, robust and easily evolvable. REST APIs definition for the IoT objects is left to the individual developer. Technological needs may vary along the application lifecycle. Stakeholders are often interested in virtual or aggregated environment features, rather than the single networked thing. To overcome these open issues, we think that it is needed an additional abstraction level between the WoT and the application layer. This should be model-driven – in order this to be adequately agreed by all the IoT stakeholders – and topic-based – because of the event-driven nature of the IoT. In this work we propose Web of Topics (WoX), a Cloud platform for the Internet of (every)Thing (IoE). WoX APIs allows companies and organisations to realise robust and high-maintainable IoT-based services, while minimising deployment costs and the time-to-market. Its model-driven approach guarantees a great end-user experience and a seamless integration among the heterogeneous IoT entities. In this book chapter we present the WoX model and the concrete architecture supporting it. As a proof of concept, in this work we also show how we implemented an original IoT scenarios using the WoX concepts, APIs and architecture: the airport short-stay parking service.",incollection,,,Quan Z. Sheng and Yongrui Qin and Lina Yao and Boualem Benatallah,Managing the Web of Things,Morgan Kaufmann,,978-0-12-809764-9,Boston,
An investigation of the nature of parameterization for the Hough transform,Pattern Recognition,30,1009-1040,1997,0031-3203,https://doi.org/10.1016/S0031-3203(97)85267-3,https://www.sciencedirect.com/science/article/pii/S0031320397852673,Shiu Yin Yuen and Chi Ho Ma,"Hough transform, Parameterization, Fourier descriptor, Parametric form, Subdivision method, Computer graphics, Inherent bias, Search space","A novel parameterization method for the Hough transform is reported. Instead of the conventional non-parametric form, the parametric form is used and copies of the transformed shape are plotted on two-dimensional slices of the Hough space. It is shown that the corresponding parameterization has uniform precision with respect to translation, and cancels out the quantization uncertainty due to image digitization. A problem of the Hough transform is discovered which is due to non-uniform discretized voting. It is shown that the above class of parameterizations avoids the problem. Finally, a particular solution of the parameterization scheme is described which is called the Fourier parameterization. It is shown that the parameterization has uniform precision with respect to the affine transformation.",article,6,,,,,,,,
Mathematical modal logic: A view of its evolution,,7,1-98,2006,1874-5857,https://doi.org/10.1016/S1874-5857(06)80027-0,https://www.sciencedirect.com/science/article/pii/S1874585706800270,Robert Goldblatt,,"Publisher Summary
From the early 1930s, there evolved two kinds of mathematical semantics for modal logic. Algebraic semantics interprets modal connectives as operators on Boolean algebras. Relational semantics uses relational structures, often called Kripke models, whose elements are thought of variously as being possible worlds, moments of time, evidential situations, or states of a computer. This chapter reviews these developments in a way that provides some insight into how the present came to be as it is. The pervading theme is the mathematics underlying modal logic, and this has at least three dimensions. To begin with there are the new mathematical ideas: when and why they were introduced, and how they interacted and evolved. Then there is the use of methods and results from other areas of mathematical logic, algebra and topology in the analysis of modal systems. And there is the application of modal syntax and semantics to study notions of mathematical and computational interest.",incollection,,,Dov M. Gabbay and John Woods,Logic and the Modalities in the Twentieth Century,North-Holland,Handbook of the History of Logic,,,
A Machine's ethos? An inquiry into artificial ethos and trust,Computers in Human Behavior,153,108108,2024,0747-5632,https://doi.org/10.1016/j.chb.2023.108108,https://www.sciencedirect.com/science/article/pii/S0747563223004594,,"Ethos, Trust, Reliance, Rhetoric, Human-machine interaction, Human-robot-interaction","Every day we trust other individuals as we engage in social interactions in which various desirable outcomes depend on others acting the way we hope, or they have indicated. Trust extends beyond specific individuals, however, as we might trust unknown others – individuals, institutions, corporations, and governments. Some also say that we trust various artifacts, such as machines. But what is the basis of trust, and can we really trust technology? Trust is intimately connected to the notion ethos from the study of rhetoric and human persuasion, which is often used to describe various characteristics of the speaker, the audience, the relationship between the speaker and the audience, and the wider context in which communication and interaction occurs. In this article I explore to what degree machines can be considered to have ethos, and consequently whether ethos is a useful concept for understanding persuasive and credibility-related situations in HMI and by extension key aspects of human-machine trust. This allows us to draw on a long lineage of research from, for example, rhetoric, communication studies, and cognitive and social psychology to better understand the usefulness – or not – of using the notion of trust to describe our relationship with machines.",article,,,,,,,,,
An empirical study of a CDC 844-41 disk subsystem,Performance Evaluation,2,29-56,1982,0166-5316,https://doi.org/10.1016/0166-5316(82)90019-0,https://www.sciencedirect.com/science/article/pii/0166531682900190,,"Disk Subsystem Parameters, Disk Subsystem Modelling, Seek Overlap, Seek Arm Scheduling, Seek Arm Movement, File Positioning, Performance Evaluation, Simulation Models","Three methods (seek overlap, seek arm scheduling, static file repositioning) for improvement of disk subsystem performance are reviewed. Detailed measured data are reported for seek time, probability of zero-length seek, latency, and transfer time, for a 12-spindle CDC 844-41 disk subsystem shared between two CDC Cyber 170/750 central processors. The probability of zero-length seeks is shown to be high, and the spindle queue lengths are observed to be low. The transfer time data are very different from the data published by others for IBM systems. A detailed simulation model of the measured system is shown to validate. This model is then used to demonstrate that seek arm scheduling is unlikely to produce much improvement, that static file repositioning can improve performance by about 20%, and that seek overlap can almost double the interactive capacity of the system measured.",article,1,,,,,,,,
Responsible AI (RAI) in Manufacturing: A Qualitative Framework,Procedia Computer Science,232,813-822,2024,1877-0509,https://doi.org/10.1016/j.procs.2024.01.081,https://www.sciencedirect.com/science/article/pii/S1877050924000814,Philipp Besinger and Daniel Vejnoska and Fazel Ansari,"Manufacturing, Responsible Artifical Intelligence, Responsible Research and Innovation","Artificial Intelligence (AI) has profound economic influence in manufacturing, but its unmindful integration can also pose societal and environmental risks. This paper provides a quantified overview of manufacturing areas that are highly advanced in AI capability research, such as maintenance. Integrating Responsible AI (RAI) in further studies of those areas is essential to mitigate risks and deliver business benefits. To enable this, manufacturing specific RAI dimensions are defined to represent accountability, explainability, fairness, human-centricity, sustainability (Green AI) and privacy & security. Further, a qualitative RAI framework consisting of responsibility areas (human involvement, decision making, business focus, system design) is proposed. Practical considerations to align the framework with manufacturing requirements are made by discussing it within an AI systems lifecycle.",article,,5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023),,,,,,,
A comprehensive evaluation on the benefits of context based password cracking for digital forensics,Journal of Information Security and Applications,84,103809,2024,2214-2126,https://doi.org/10.1016/j.jisa.2024.103809,https://www.sciencedirect.com/science/article/pii/S2214212624001121,Aikaterini Kanta and Iwen Coisel and Mark Scanlon,"Password, Dictionary, Contextual information, Password cracking, Wordlist","Password-based authentication systems have many weaknesses, yet they remain overwhelmingly used and their announced disappearance is still undated. The system admin overcomes the imperfection by skilfully enforcing a strong password policy and sane password management on the server side. But in the end, the user behind the password is still responsible for the password’s strength. A poor choice can have dramatic consequences for the user or even for the service behind, especially considering critical infrastructure. On the other hand, law enforcement can benefit from a suspect’s weak decisions to recover digital content stored in an encrypted format. Generic password cracking procedures can support law enforcement in this matter — however, these approaches quickly demonstrate their limitations. This article proves that more targeted approaches can be used in combination with traditional strategies to increase the likelihood of success when contextual information is available and can be exploited.",article,,,,,,,,,
ChatGPT - opportunities or threats in the educational process,Procedia Computer Science,225,4551-4559,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.10.453,https://www.sciencedirect.com/science/article/pii/S1877050923016113,Agnieszka Ubowska and Tomasz Królikowski,"ChatGPT, Artificial Intelligence, survey research","The article is based on surveys carried out among students of selected technical universities in the West Pomeranian Voivodeship (Poland). It aims to determine students' knowledge of new tools such as ChatGPT, the use of which raises a discussion among the scientific community and beyond. According to some groups it can support learning, whereas others claim that it can limit problem-solving skills and creative thinking. Three hundred students of engineering and master's studies participated in the study. The results of the conducted research show the directions of the use of ChatGPT by students and their interest in this tool.",article,,27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023),,,,,,,
"This new conversational AI model can be your friend, philosopher, and guide ... and even your worst enemy",Patterns,4,100676,2023,2666-3899,https://doi.org/10.1016/j.patter.2022.100676,https://www.sciencedirect.com/science/article/pii/S2666389922003233,Joyjit Chatterjee and Nina Dethlefs,,"We explore the recently released ChatGPT model, one of the most powerful conversational AI models that has ever been developed. This opinion provides a perspective on its strengths and weaknesses and a call to action for the AI community (including academic researchers and industry) to work together on preventing potential misuse of such powerful AI models in our everyday lives.",article,1,,,,,,,,
CLSR welcomes new Report Correspondents,Computer Law & Security Review,22,341-342,2006,0267-3649,https://doi.org/10.1016/j.clsr.2006.07.007,https://www.sciencedirect.com/science/article/pii/S0267364906000732,Stephen Saxby,,,article,5,,,,,,,,
Contents,Procedia Computer Science,225,iii-xxxi,2023,1877-0509,https://doi.org/10.1016/S1877-0509(23)01659-9,https://www.sciencedirect.com/science/article/pii/S1877050923016599,,,,article,,27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023),,,,,,,
"5 - ‘Enquiring Minds’ and the role of information literacy in the design, management and assessment of student research tasks",,,87-119,2011,,https://doi.org/10.1016/B978-1-84334-610-4.50005-7,https://www.sciencedirect.com/science/article/pii/B9781843346104500057,Keith Puttick,"enquiry, student research, information literacy, IL competency standards, information specialists, EBL","Abstract:
This chapter considers the Enquiring Minds (EM) project at Staffordshire University, and the role that information literacy (IL) principles and standards can play in improving the quality of students’ research. Clearly, an essential pre-requisite is institutions’ commitment to embedding IL principles in the curriculum, while at the same time creating more (and better) opportunities to deploy research and research-related skills and putting Enquiry-Based Learning (EBL) at the heart of the student experience. However, if progress is to be made it will be essential for lecturers – working with groups like information specialists – to factor IL principles and competency standards effectively into research task design, guidance, and assessment criteria. While it is clearly essential to bring IL requirements into learning outcomes (a vital step in changing behaviour, and raising the quality of students’ research), it is also necessary to recognise the limitations of learning outcomes. Consequently, there is considerable value in ensuring that IL standards are adapted and deployed to meet the needs of particular tasks, with task designers supplementing learning outcomes with effective guidance. As EM research has suggested, some EBL schemes or types of project work may require close guidance on specifics like taking ‘preliminary steps’, using ‘a range of different enquiry methods’, or ‘communicating research results effectively’. The chapter concludes by emphasising the value of embedding IL principles in the curriculum – something that has considerable potential for improving students’ research, and empowering them to become independent learners and ‘producers of knowledge’. However, this will necessitate more effective ways of assessing IL aspects of learning outcomes, and rewarding good practice in conducting effective searches, authentication, and critical evaluation.",incollection,,,Geoff Walton and Alison Pope,Information Literacy,Chandos Publishing,Chandos Information Professional Series,978-1-84334-610-4,,
A BERT-Based Sequential POI Recommender system in Social Media,Computer Standards & Interfaces,87,103766,2024,0920-5489,https://doi.org/10.1016/j.csi.2023.103766,https://www.sciencedirect.com/science/article/pii/S0920548923000478,A. Noorian,"BERT, POI Route, Context-Aware, Deep Neural Networks, Personalization, Sequential recommendation","Route schema is challenging for tourists because they must choose Points of Interest (POIs) in unknown areas that meet their preferences and limitations. Historically, sequential methods were utilized to generate recommendations based on previous user interactions. Despite their efficacy, however, such left-to-right unidirectional models are suboptimal due to the following factors: a) user behavior sequences are restricted in their ability to utilize hidden representations in unidirectional architectures; b) a rigidly ordered sequence is frequently assumed but is not always possible. This paper proposes a novel personalized sequential recommendation model, termed BERTSeqHybrid, which utilizes Bidirectional Encoder Representations from Transformers (BERT) to circumvent these limitations. In addition to contextual data from POIs, asymmetric schemas, and topic modeling are employed to improve the user-user similarity model. Furthermore, a novel method for evaluating user preferences is proposed utilizing explicit demographic data to mitigate the cold start problem. In the experimental evaluation, the developed methodology, which was applied to two different datasets (Yelp and Flickr), produced superior root mean square error RMSE, F-Score, mean average precision (MAP), and normalized discounted cumulative gain (NDCG) indexes.",article,,,,,,,,,
Regional models: A new approach for nonlinear system identification via clustering of the self-organizing map,Neurocomputing,147,31-46,2015,0925-2312,https://doi.org/10.1016/j.neucom.2013.11.046,https://www.sciencedirect.com/science/article/pii/S0925231214007085,,"System identification, Self-Organizing Maps, Global models, Local models, Outliers, Robust regression","Global modelling consists in fitting a single regression model to the available data, using the whole set of input and output observations. On the other side of the spectrum stands the local modelling approach, in which the input space is segmented into several small partitions and a specialized regression model is fit to each partition. In this paper, we propose a novel approach, called Regional Models (RM), that stands in between the global and local modelling ones. The proposal extends the two-level clustering approach by Vesanto and Alhoniemi (2000 [1]) to regression problems, more specifically, to system identification. In this regard, we first partition the input space using the Self-Organizing Map (SOM), and then perform clustering over the prototypes of the trained SOM. Finally, regional regression models are built over the clusters (i.e. over the regions) of SOM prototypes, not over each SOM prototype as in local modelling. Under the proposed framework, we build regional linear and nonlinear regression models. For the linear case, we use autoregressive models with eXogenous (ARX) whose parameters are estimated using the ordinary least-squares (OLS) method. Regional nonlinear ARX (NARX) models are built using the Extreme Learning Machine network. Additionally, we develop robust variants of the proposed regional models through the use of M-estimation, a statistical framework for handling outliers, since the OLS is highly sensitive to them. Comprehensive performance evaluation of the proposed models on synthetic and real-world datasets is carried out and the results compared to those achieved by standard global and local models.",article,,Advances in Self-Organizing Maps Subtitle of the special issue: Selected Papers from the Workshop on Self-Organizing Maps 2012 (WSOM 2012),,,,,,,
Ethical management of human-AI interaction: Theory development review,The Journal of Strategic Information Systems,32,101772,2023,0963-8687,https://doi.org/10.1016/j.jsis.2023.101772,https://www.sciencedirect.com/science/article/pii/S0963868723000185,Teresa Heyder and Nina Passlack and Oliver Posegga,"Artificial intelligence, Ethics, Human-AI interaction, Theoretical review, Sociomateriality","AI-based technologies have changed the nature of the symbiosis between humans and AI, and so strategic management of human-AI interaction in organizations requires deeper ethical considerations. Aligning AI with human values requires a systematic understanding of the ethical management of human-AI interaction. We conduct a theoretical review, from a sociotechnical perspective, and analyze ethical management of human-AI interaction through the lens of sociomateriality. Our systematic approach helps explain and clarify the interdependencies between two ethical perspectives – duty and virtue ethics – in sociotechnical systems. We also provide a theoretical framework that leads to seven avenues for future research.",article,3,,,,,,,,
A neural network architecture for automatic segmentation of fluorescence micrographs,Neurocomputing,48,357-367,2002,0925-2312,https://doi.org/10.1016/S0925-2312(01)00642-7,https://www.sciencedirect.com/science/article/pii/S0925231201006427,Tim W. Nattkemper and Heiko Wersing and Walter Schubert and Helge Ritter,"Segmentation, Contour grouping, Fluorescence microscopy, Functional proteomics","A system for the automatic segmentation of fluorescence micrographs is presented. In the first step, positions of fluorescent cells are detected by a fast learning neural network, which acquires the visual knowledge from a set of training cell-image patches selected by the user. Guided by the detected cell positions the system extracts in the second step the contours of the cells. For contour extraction, a recurrent neural network model is used to approximate the cell shapes. Even though the micrographs are noisy and the fluorescent cells vary in shape and size, the system detects at minimum 95% of the cells.",article,1,,,,,,,,
Balancing act: Tackling organized retail fraud on e-commerce platforms with imbalanced learning text models,International Journal of Information Management Data Insights,4,100256,2024,2667-0968,https://doi.org/10.1016/j.jjimei.2024.100256,https://www.sciencedirect.com/science/article/pii/S2667096824000454,Abed Mutemi and Fernando Bacao,"Fraud detection, Machine learning, Text classification, E-commerce, Text mining, Natural language processing, Word representation learning, Text data augmentation","As online shopping expands rapidly, so does the prevalence of fraud, resulting in significant losses for retailers. According to the 2020 National Retail Federation (NRF) report, organized retail crime costs retailers nearly $800,000 per billion in sales, with an expected global annual increase of over fourteen percent. This paper introduces a text-based fraud detection framework to mitigate these losses efficiently. The framework comprises four key components: text preprocessing, representation, knowledge extraction via machine learning algorithms, and model evaluation. By integrating data augmentation techniques, the framework enhances classifier performance in detecting fraud. The proposed method, employing a combination of FastText and Random Forest classifiers, achieves an impressive F1 score of 0.833 and AUC score of 0.99 on an augmented dataset, surpassing conventional keyword-based models. Informed by best practices in fraud detection, this scalable framework promises a solution to combat the escalating fraud associated with the exponential growth of online shopping.",article,2,,,,,,,,
A vulnerability detection framework by focusing on critical execution paths,Information and Software Technology,174,107517,2024,0950-5849,https://doi.org/10.1016/j.infsof.2024.107517,https://www.sciencedirect.com/science/article/pii/S0950584924001228,Jianxin Cheng and Yizhou Chen and Yongzhi Cao and Hanpin Wang,"Vulnerability detection, Software security, Code representation, Control flow graph, Deep learning","Context:
Vulnerability detection is critical to ensure software security, and detecting vulnerabilities in smart contract code is currently gaining massive attention. Existing deep learning-based vulnerability detection methods represent the code as a code structure graph and eliminate vulnerability-irrelevant nodes. Then, they learn vulnerability-related code features from the simplified graph for vulnerability detection. However, this simplified graph struggles to represent relatively complete structural information of code, which may affect the performance of existing vulnerability detection methods.
Objective:
In this paper, we present a novel Vulnerability Detection framework based on Critical Execution Paths (VDCEP), which aims to improve smart contract vulnerability detection.
Method:
Firstly, given a code structure graph, we deconstruct it into multiple execution paths that reflect rich structural information of code. To reduce irrelevant code information, a path selection strategy is employed to identify critical execution paths that may contain vulnerable code information. Secondly, a feature extraction module is adopted to learn feature representations of critical paths. Finally, we feed all path feature representations into a classifier for vulnerability detection. Also, the feature weights of paths are provided to measure their importance in vulnerability detection.
Results:
We evaluate VDCEP on a large dataset with four types of smart contract vulnerabilities. Results show that VDCEP outperforms 14 representative vulnerability detection methods by 5.34%–60.88% in F1-score. The ablation studies analyze the effects of our path selection strategy and feature extraction module on VDCEP. Moreover, VDCEP still outperforms ChatGPT by 34.46% in F1-score.
Conclusion:
Compared to existing vulnerability detection methods, VDCEP is more effective in detecting smart contract vulnerabilities by utilizing critical execution paths. Besides, we can provide interpretable details about vulnerability detection by analyzing the path feature weights.",article,,,,,,,,,
"Meet the authors: Rita González-Márquez, Philipp Berens, and Dmitry Kobak",Patterns,5,100993,2024,2666-3899,https://doi.org/10.1016/j.patter.2024.100993,https://www.sciencedirect.com/science/article/pii/S2666389924001089,Rita González-Márquez and Philipp Berens and Dmitry Kobak,,"In their recent publication in Patterns,1 the authors present a 2D atlas of the entire English biomedical literature.",article,6,,,,,,,,
Computer-aided modelling and simulation of mechanisms and manipulators,Computer-Aided Design,21,577-583,1989,0010-4485,https://doi.org/10.1016/0010-4485(89)90019-5,https://www.sciencedirect.com/science/article/pii/0010448589900195,P. Fanghella and C. Galletti and E. Giannotti,"computer-aided design, modelling, simulation, mechanisms","Some choices related to the design and implementation of computer programs for the kinematic modelling and simulation of mechanisms and manipulators are discussed, including global versus modular modelling features, numeric versus symbolic computation, and precompiler versus interpreter implementation. Within this framework, two original applications are presented: first, a program for modelling spatial single-loop robot arms that works as a precompiler of manipulators and reaches an automatic, symbolic, closed-form solution to the inverse position analysis; and second, an approach to friendly training in computer-aided mechanism design through hypertext. Various examples of implementations are given.",article,9,,,,,,,,
Chapter Six - Emotional AI: Neuroethics and Socially aligned networks,,,101-130,2024,,https://doi.org/10.1016/B978-0-443-19096-4.00002-X,https://www.sciencedirect.com/science/article/pii/B978044319096400002X,Markus Krebsz and Divya Dwivedi,"ChatGPT, Consciousness, Diverse inputs and multistakeholder feedback, Freedom of thought, Generative AI, IoT (Internet-of-things), Neuro-rights, Neuroethics, Sentience, Socially aligned networks, Spiritual AI, Thought-related and neural data","A new term, socially aligned networks, going beyond pure social media is introduced which considers the alignment of participants' common interests and ways how users communicate, create, compete, and/or challenge each other within technological ecosystems such as online services, gaming suites, metaverse or virtual/augmented-/extended-reality spaces. The current scope and predicted growth of the world digital population is considered in light of Big Tech companies' domination of social media as well as continuing digital exclusion. The ethical studies’ landscape is mapped by looking at comparative studies in this relatively new field and with the aim of establishing a suitable ethic principles baseline for such socially aligned networks further illustrated by human–AI interface case studies. Beyond establishing a suitable ethic principles baseline for such socially aligned networks, current advances in neuroethics are considered within the context of emotional AI and human–AI interactions. Neuroethics principles are considered within the context of different philosophical schools, leading to a discussion of relatively new neurorights. An interconnection of morality, ethics, and spirituality is discussed, together with immature legal frameworks and ethical boundaries for Internet of things (IoT) devices. In conclusion, neuroethics are considered a suitable blueprint for socially aligned networks and highlighting that regulation alone is likely not going to be sufficient on its own, particularly when considering the rapid growth of generative AI.",incollection,,,Muskan Garg and Deepika Koundal,Emotional AI and Human-AI Interactions in Social Networking,Academic Press,,978-0-443-19096-4,,
"Artificial intelligence research: A review on dominant themes, methods, frameworks and future research directions",Telematics and Informatics Reports,14,100127,2024,2772-5030,https://doi.org/10.1016/j.teler.2024.100127,https://www.sciencedirect.com/science/article/pii/S2772503024000136,Kingsley Ofosu-Ampong,"Artificial intelligence, Classification, Literature review, Technological issues, Research agenda","This article presents an analysis of artificial intelligence (AI) in information systems and innovation-related journals to determine the current issues and stock of knowledge in AI literature, research methodology, frameworks, level of analysis and conceptual approaches. By doing this, the article aims to identify research gaps that can guide future investigations. A total of 85 peer-reviewed articles from 2020 to 2023 were used in the analysis. The findings show that extant literature is skewed towards the prevalence of technological issues and highlights the relatively lower focus on other themes, such as contextual knowledge co-creation issues, conceptualisation, and application domains. While there have been increasing technological issues with artificial intelligence, the three identified areas of security concern are data security, model security and network security. Furthermore, the review found that contemporary AI, which continually drives the boundaries of computational capabilities to tackle increasingly intricate decision-making challenges, distinguishes itself from earlier iterations in two primary aspects that significantly affect organisational learning in dealing with AI's potential: autonomy and learnability. This study contributes to AI research by providing insights into current issues, research methodology, level of analysis and conceptual approaches, and AI framework to help identify research gaps for future investigations.",article,,,,,,,,,
"Generative artificial intelligence in healthcare: A scoping review on benefits, challenges and applications",International Journal of Medical Informatics,188,105474,2024,1386-5056,https://doi.org/10.1016/j.ijmedinf.2024.105474,https://www.sciencedirect.com/science/article/pii/S1386505624001370,,"Generative artificial intelligence, Health, Artificial intelligence","Background
Generative artificial intelligence (GAI) is revolutionizing healthcare with solutions for complex challenges, enhancing diagnosis, treatment, and care through new data and insights. However, its integration raises questions about applications, benefits, and challenges. Our study explores these aspects, offering an overview of GAI's applications and future prospects in healthcare.
Methods
This scoping review searched Web of Science, PubMed, and Scopus . The selection of studies involved screening titles, reviewing abstracts, and examining full texts, adhering to the PRISMA-ScR guidelines throughout the process.
Results
From 1406 articles across three databases, 109 met inclusion criteria after screening and deduplication. Nine GAI models were utilized in healthcare, with ChatGPT (n = 102, 74 %), Google Bard (Gemini) (n = 16, 11 %), and Microsoft Bing AI (n = 10, 7 %) being the most frequently employed. A total of 24 different applications of GAI in healthcare were identified, with the most common being “offering insights and information on health conditions through answering questions” (n = 41) and “diagnosis and prediction of diseases” (n = 17). In total, 606 benefits and challenges were identified, which were condensed to 48 benefits and 61 challenges after consolidation. The predominant benefits included “Providing rapid access to information and valuable insights” and “Improving prediction and diagnosis accuracy”, while the primary challenges comprised “generating inaccurate or fictional content”, “unknown source of information and fake references for texts”, and “lower accuracy in answering questions”.
Conclusion
This scoping review identified the applications, benefits, and challenges of GAI in healthcare. This synthesis offers a crucial overview of GAI's potential to revolutionize healthcare, emphasizing the imperative to address its limitations.",article,,,,,,,,,
Stock movement predictive network via incorporative attention mechanisms based on tweet and historical prices,Neurocomputing,418,326-339,2020,0925-2312,https://doi.org/10.1016/j.neucom.2020.07.108,https://www.sciencedirect.com/science/article/pii/S0925231220313060,Hongfeng Xu and Lei Chai and Zhiming Luo and Shaozi Li,"Stock prediction, Incorporative attention, Local semantics, Contextual information","The recent advances usually attempt to mine the effective market information from the chaotic data and learn multilevel representations by using attention mechanisms to conduct a stock prediction task. However, such methods usually lack the full utilization of local semantic embedding which contains the abundant textual semantics information. Moreover, these models suffer from the severe noise diffusion in contextual embeddings from a sequence after passing through the RNN. The noises diffusion constrains the performance of the proposed methods. In this work, we propose a stock movement predictive network via incorporative attention mechanisms. The core innovation is that the incorporative attention combines local and contextual attention mechanisms to clean the contextual embeddings by using local semantics. As a result, the attention effectively reduce the noises in the constructed higher-level representations and enhance the performance. Moreover, the local semantics and context are merged into the constructed higher-level representations which provide more abundant local semantic and contextual information. The experimental results demonstrate the state-of-the-art performance of the proposed approach on tweet and historical price dataset.",article,,,,,,,,,
Fixed-point IDCT without multiplications based on B.G. Lee's algorithm,Digital Signal Processing,19,770-777,2009,1051-2004,https://doi.org/10.1016/j.dsp.2008.11.004,https://www.sciencedirect.com/science/article/pii/S105120040800184X,Pingping Zhu and Jianguo Liu and Shengkui Dai,"DCT, IDCT, B.G. Lee, Addition, Shift","An improved approach to compute the inverse discrete cosine transform (IDCT) for image and video coding applications for mobile devices is proposed based on B.G. Lee algorithm. We replace the multiplication operators in original B.G. Lee's algorithm with addition and shift operators to realize the fix-point computation. Due to the absence of the multiplication operators, this modified algorithm takes less time to complete the computation than the traditional B.G. Lee's.",article,4,,,,,,,,
Time pattern reconstruction for classification of irregularly sampled time series,Pattern Recognition,147,110075,2024,0031-3203,https://doi.org/10.1016/j.patcog.2023.110075,https://www.sciencedirect.com/science/article/pii/S0031320323007720,Chenxi Sun and Hongyan Li and Moxian Song and Derun Cai and Baofeng Zhang and Shenda Hong,"Classification of irregularly sampled time series, Time pattern, Deep learning, Healthcare and medical application","Irregularly Sampled Time Series (ISTS) include partially observed feature vectors caused by the lack of temporal alignment across dimensions and the presence of variable time intervals. Especially in medical applications, because patients’ examinations depend on their health status, observations in this event-based medical time series are nonuniformly distributed. When using deep learning models to classify ISTS, most work defines the problem that needs to be solved as alignment-caused data missing or nonuniformity-caused dependency change. However, they only modeled relationships between observed values, ignoring the fact that time is the independent variable for a time series. In this paper, we emphasize that irregularity is active, time-depended, and class-associated and is reflected in the Time Pattern (TP). To this end, this paper focused on the TP of ISTS for the first time, proposing a Time Pattern Reconstruction (TPR) method. It first encodes time information by the time encoding mechanism, then imputes values from time codes by the continuous-discrete Kalman network, selects key time points by the conditional masking mechanism, and finally classifies ISTS based on the reconstructed TP. Experiments on four real-world medical datasets and three other datasets show that TPR outperforms all baselines. We also show that TP can reveal biomarkers and key time points for diseases.",article,,,,,,,,,
DFRWS EU 10-year review and future directions in Digital Forensic Research,Forensic Science International: Digital Investigation,48,301685,2024,2666-2817,https://doi.org/10.1016/j.fsidi.2023.301685,https://www.sciencedirect.com/science/article/pii/S2666281723002044,Frank Breitinger and Jan-Niclas Hilgert and Christopher Hargreaves and John Sheppard and Rebekah Overdorf and Mark Scanlon,"Digital forensics research, Digital forensic science, DFRWS, Research trends, Future directions","Conducting a systematic literature review and comprehensive analysis, this paper surveys all 135 peer-reviewed articles published at the Digital Forensics Research Conference Europe (DFRWS EU) spanning the decade since its inaugural running (2014–2023). This comprehensive study of DFRWS EU articles encompasses sub-disciplines such as digital forensic science, device forensics, techniques and fundamentals, artefact forensics, multimedia forensics, memory forensics, and network forensics. Quantitative analysis of the articles’ co-authorships, geographical spread and citation metrics are outlined. The analysis presented offers insights into the evolution of digital forensic research efforts over these ten years and informs some identified future research directions.",article,,DFRWS EU 2024 - Selected Papers from the 11th Annual Digital Forensics Research Conference Europe,,,,,,,
Out of the BLEU: How should we assess quality of the Code Generation models?,Journal of Systems and Software,203,111741,2023,0164-1212,https://doi.org/10.1016/j.jss.2023.111741,https://www.sciencedirect.com/science/article/pii/S016412122300136X,Mikhail Evtikhiev and Egor Bogomolov and Yaroslav Sokolov and Timofey Bryksin,"Code generation, Metrics, Neural networks, Code similarity","In recent years, researchers have created and introduced a significant number of various code generation models. As human evaluation of every new model version is unfeasible, the community adopted automatic evaluation metrics such as BLEU to approximate the results of human judgement. These metrics originate from the machine translation domain and it is unclear whether they are applicable for the code generation tasks and how well they agree with the human evaluation on this task. There are also other metrics, CodeBLEU and RUBY, developed to estimate the similarity of code, that take into account the properties of source code. However, for these metrics there are hardly any studies on their agreement with the human evaluation. Despite all that, minimal differences in the metric scores have been used in recent papers to claim superiority of some code generation models over the others. In this paper, we present a study on the applicability of six metrics—BLEU, ROUGE-L, METEOR, ChrF, CodeBLEU, and RUBY—for evaluation of code generation models. We conduct a study on two different code generation datasets and use human annotators to assess the quality of all models run on these datasets. The results indicate that for the CoNaLa dataset of Python one-liners, none of the metrics can correctly emulate human judgement on which model is better with >95% certainty if the difference in model scores is less than 5 points. For the HearthStone dataset, which consists of classes of a particular structure, a difference in model scores of at least 2 points is enough to claim the superiority of one model over the other. Our findings suggest that the ChrF metric is a better fit for the evaluation of code generation models than the commonly used BLEU and CodeBLEU. Yet, finding a metric for code generation that closely agrees with humans requires additional work.",article,,,,,,,,,
CoProLITE: Constrained Proxy Learning for lIver and hepaTic lesion sEgmentation,Neurocomputing,598,128014,2024,0925-2312,https://doi.org/10.1016/j.neucom.2024.128014,https://www.sciencedirect.com/science/article/pii/S0925231224007859,Yuchen Fu and Song Liu and Cong Wang and Zhiwei Jiang and Juan Du and Qing Gu,"Liver tumor segmentation, Proxy learning, Deep convolutional neural network","Liver and hepatic lesion segmentation is an important task in medical image analysis, which plays a crucial role in diagnosis, treatment planning and monitoring of liver diseases. We observed an ordinal layout of the feature space that aligns with CT image characteristics will improve performance on liver and hepatic lesion segmentation task. In order to enforce the samples to conform to a specific layout of the feature space, we propose a novel liver and hepatic lesion segmentation method called CoProLITE, which learns a constrained proxy for each classes. Specifically, We replace the traditional FCN-based segmentation head by a proxy learning-based head to learn feature representations of the images, and introduces constraints during the training process to guide the learning of the proxies. We extensively evaluate CoProLITE on three public datasets and compare it to state-of-the-art methods. The experimental results demonstrate the effectiveness of the proposed method.",article,,,,,,,,,
A historical perspective of biomedical explainable AI research,Patterns,4,100830,2023,2666-3899,https://doi.org/10.1016/j.patter.2023.100830,https://www.sciencedirect.com/science/article/pii/S266638992300199X,Luca Malinverno and Vesna Barros and Francesco Ghisoni and Giovanni Visonà and Roman Kern and Philip J. Nickel and Barbara Elvira Ventura and Ilija Šimić and Sarah Stryeck and Francesca Manni and Cesar Ferri and Claire Jean-Quartier and Laura Genga and Gabriele Schweikert and Mario Lovrić and Michal Rosen-Zvi,"explainability, COVID-19, coronavirus, artificial intelligence, machine learning, meta-review, PRISMA, decision-making, trustworthiness, foundation models","Summary
The black-box nature of most artificial intelligence (AI) models encourages the development of explainability methods to engender trust into the AI decision-making process. Such methods can be broadly categorized into two main types: post hoc explanations and inherently interpretable algorithms. We aimed at analyzing the possible associations between COVID-19 and the push of explainable AI (XAI) to the forefront of biomedical research. We automatically extracted from the PubMed database biomedical XAI studies related to concepts of causality or explainability and manually labeled 1,603 papers with respect to XAI categories. To compare the trends pre- and post-COVID-19, we fit a change point detection model and evaluated significant changes in publication rates. We show that the advent of COVID-19 in the beginning of 2020 could be the driving factor behind an increased focus concerning XAI, playing a crucial role in accelerating an already evolving trend. Finally, we present a discussion with future societal use and impact of XAI technologies and potential future directions for those who pursue fostering clinical trust with interpretable machine learning models.",article,9,,,,,,,,
Prompt-based learning framework for zero-shot cross-lingual text classification,Engineering Applications of Artificial Intelligence,133,108481,2024,0952-1976,https://doi.org/10.1016/j.engappai.2024.108481,https://www.sciencedirect.com/science/article/pii/S0952197624006390,Kai Feng and Lan Huang and Kangping Wang and Wei Wei and Rui Zhang,"Cross-lingual text classification, Cross-lingual text representation, Prompt learning","Cross-lingual text classification is a challenging task that aims to train classifiers with data in one language, known as the source language, and apply the acquired knowledge to data in another language, referred to as the target language. Recent advancements in multilingual pre-trained language models (PLMs) have made significant progress in addressing cross-lingual issues, and the application of prompt-based learning has further improved task performance. However, these models still face challenges such as the gap between cross-lingual classification tasks and pre-training tasks of PLMs, as well as issues related to scarce resources and data noise, which hinder the full exploitation of the implicit knowledge in PLMs. In this paper, we propose a Prompt-based Cross-lingual Learning (PCL) framework that combines language-agnostic continuous prompt learning with self-learning process. Specifically, PCL framework leverages language-agnostic prompts and PLMs to achieve semantic transfer between source and target languages. To enhance the semantic relationship between prompts and category labels, a label attention module is introduced. Additionally, a set of self-training rules is proposed, which includes a scoring function. In a few-shot setting, noisy data is dynamically filtered through scoring and ranking of the data. During each training iteration, both the model and scoring function weights are updated, further improving the discrimination capability of the model. In summary, the proposed PCL framework builds upon cross-lingual prompt learning, effectively removing noisy data and applying it to zero-shot cross-lingual text classification, which is beneficial for engineering applications. The findings of this study have implications for prompt learning method. The PCL framework achieves state-of-the-art performance in cross-lingual text classification task, with a 14% performance improvement compared to basic soft prompt learning. This demonstrates its potential in addressing classification problems in resource-limited scenarios.",article,,,,,,,,,
Exploratory machine learning with unknown unknowns,Artificial Intelligence,327,104059,2024,0004-3702,https://doi.org/10.1016/j.artint.2023.104059,https://www.sciencedirect.com/science/article/pii/S0004370223002059,Peng Zhao and Jia-Wei Shan and Yu-Jie Zhang and Zhi-Hua Zhou,"Exploratory machine learning, Unknown unknowns, Robust AI, Robustness","In conventional supervised learning, a training dataset is given with ground-truth labels from a known label set, and the learned model will classify unseen instances to known labels. This paper studies a new problem setting in which there are unknown classes in the training data misperceived as other labels, and thus their existence appears unknown from the given supervision. We attribute the unknown unknowns to the fact that the training dataset is badly advised by the incompletely perceived label space due to the insufficient feature information. To this end, we propose the exploratory machine learning, which examines and investigates training data by actively augmenting the feature space to discover potentially hidden classes. Our method consists of three ingredients including rejection model, feature exploration, and model cascade. We provide theoretical analysis to justify its superiority, and validate the effectiveness on both synthetic and real datasets.",article,,,,,,,,,
The joint learning of multi-resolution feature for multi-class retinal vessel segmentation,Neurocomputing,584,127570,2024,0925-2312,https://doi.org/10.1016/j.neucom.2024.127570,https://www.sciencedirect.com/science/article/pii/S0925231224003412,Xiaofan Tang and Hao Chen and Xiangru Li and Sihua Yang,"Retinal vessel segmentation, Multi-resolution learning, Cross-scale fusion, Deep learning","The task of multi-class vessel segmentation on retinal images is the basis for the arteriovenous quantitative analysis, and plays an important role in the diagnosis and treatment of cerebrovascular diseases. Due to the intricate details and intertwining of the retinal vessels, traditional feature learning networks based on single-level resolution images are prone to the troubles from arteriovenous confusion and vascular edge discontinuity. To this end, we develop a paradigm of multi-level image resolution joint learning. This scheme overcomes the limitation of the methods depending on single-level image resolution on feature modeling. Specifically, we designed a cross-scale feature fusion network with a dual-branch structure that integrates global and local perspectives. This approach enables the extraction of retinal image features across multiple resolutions, effectively compensating for the vascular feature gaps inherent in single-resolution network models. This framework not only corrects the intra-segment misclassification, but also improves continuity by supplementing the details of vascular edge. Furthermore, the cross-scale fusion process of the network at multiple stages is conducive to its optimization and enhances the collaborative learning ability of dual-branch. Meanwhile, we use the generative adversarial structure as the backbone to supervise and constrain the aforementioned feature fusion results. Finally, extensive experiments are conducted on three publicly available datasets, DRIVE-AV, LES-AV, and HRF-AV. It is shown that the proposed scheme outperforms the current state-of-the-art methods significantly. The source code is available at https://github.com/Tang9867/Multi-Resolution-Learning.",article,,,,,,,,,
"Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation",Information Fusion,99,101896,2023,1566-2535,https://doi.org/10.1016/j.inffus.2023.101896,https://www.sciencedirect.com/science/article/pii/S1566253523002129,,"Trustworthy AI, AI ethics, Responsible AI systems, AI regulation, Regulatory sandbox","Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system’s entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system’s life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple perspective: What each requirement for trustworthy AI is, Why it is needed, and How each requirement can be implemented in practice. On the other hand, a practical approach to implement trustworthy AI systems allows defining the concept of responsibility of AI-based systems facing the law, through a given auditing process. Therefore, a responsible AI system is the resulting notion we introduce in this work, and a concept of utmost necessity that can be realized through auditing processes, subject to the challenges posed by the use of regulatory sandboxes. Our multidisciplinary vision of trustworthy AI culminates in a debate on the diverging views published lately about the future of AI. Our reflections in this matter conclude that regulation is a key for reaching a consensus among these views, and that trustworthy and responsible AI systems will be crucial for the present and future of our society.",article,,,,,,,,,
Chapter 17 - MOVIE — Multitasking Object-oriented Visual Interactive Environment,,,671-737,1994,,https://doi.org/10.1016/B978-0-08-051351-5.50021-X,https://www.sciencedirect.com/science/article/pii/B978008051351550021X,Geoffrey C. Fox and Roy D. Williams and Paul C. Messina,,"Publisher Summary
This chapter discusses the MOVIE System and is considered as a summary of the design and prototype development stage. It also contains a brief description of the current status and the planned near-term applications. The concentration is on one current application—Terrain Map Understanding and on one planned application area—virtual reality. MOVIE System is a network of MOVIE servers. MOVIE Server is an interpreter of MovieScript. MovieScript is a high-level object-oriented programming language derived from PostScript. PostScript is embedded in the larger language model of MovieScript. This includes new types and operators as well as syntax extension towards the C + + style object-oriented model with dynamic binding and multiple inheritance. MOVIE Server is based on the custom-made high-performance MovieScript interpreter. Some design concepts of MovieScript are inherited from the NeWS model developed by Sun. C-shell-based CASE tools are constructed for automated server language extension. The default development model for MOVIE applications is based on MovieScript programming.",incollection,,,Geoffrey C. Fox and Roy D. Williams and Paul C. Messina,Parallel Computing Works!,Morgan Kaufmann,,978-0-08-051351-5,San Francisco (CA),
Three new members of the CLSR editorial and professional boards,Computer Law & Security Review,33,419-420,2017,0267-3649,https://doi.org/10.1016/j.clsr.2017.07.001,https://www.sciencedirect.com/science/article/pii/S0267364917302327,Steve Saxby,,,article,4,,,,,,,,
Chapter 6 - Artificial intelligence in breast cancer: An opportunity for early diagnosis,,,73-89,2023,,https://doi.org/10.1016/B978-0-443-15280-1.00004-2,https://www.sciencedirect.com/science/article/pii/B9780443152801000042,Rama Rao Malla and Vedavathi Katneni,"Artificial intelligence, Breast cancer, Deep learning, Diagnosis, Machine learning","One of the serious women's cancer types across the globe is breast cancer (BC). It occurs due to complex heterogeneity as well as multiple etiological factors. Early diagnosis will increase the survival of the patients and reduce mortality to a great extent. Generally, different types of biopsy procedures, mammography, ultrasonography, PET scan, and magnetic resonance imaging (MRI) scan are used to detect breast tumors. However, for the accurate diagnosis of BC, there is a complex necessity for a reliable system. Nowadays, a combination of artificial intelligence (AI), especially a machine learning (ML) approach with digital imaging techniques, has assisted in reducing the false diagnoses of BC. This review presents the fundamentals of ML algorithms and ML models for BC prediction, model assessment, and current knowledge on ML-based approaches for BC diagnosis. Finally, it presents the challenges and scope of AI in precision medicine for BC. Therefore, AI could be useful for achieving ground-breaking progress in precision medicine for BC.",incollection,,,,Computational Methods in Drug Discovery and Repurposing for Cancer Therapy,Academic Press,,978-0-443-15280-1,,
Exploring the competence of ChatGPT for customer and patient service management,Intelligent Pharmacy,2,392-414,2024,2949-866X,https://doi.org/10.1016/j.ipha.2024.03.002,https://www.sciencedirect.com/science/article/pii/S2949866X24000480,Abid Haleem and Mohd Javaid and Ravi Pratap Singh,"Artificial intelligence (AI), ChatGPT, Applications, Healthcare, Customer, Patient","The modern language generation model ChatGPT, created by Open Artificial Intelligence (AI), is recognised for its capacity to comprehend context and produce pertinent content. This model is built on the transformer architecture, which enables it to process massive volumes of data and produce text that is both cohesive and illuminating. Service is a crucial component everywhere as it provides the basis for establishing client rapport and offering aid and support. In healthcare, the application of ChatGPT for patient service support has been one of the most significant advances in recent years. ChatGPT can help overcome language obstacles and improve patient satisfaction by facilitating communication with healthcare personnel and understanding of care. It can assist in enhancing the entire patient experience by offering personalised information and support to patients and making it more straightforward for them to communicate with healthcare professionals. Its goal can be to expedite and streamline service by promptly and accurately responding to customers. Businesses of all sizes increasingly use ChatGPT since it allows them to provide 24/7 customer support without requiring human contact. This paper briefly discusses ChatGPT and the need for better services. Various perspectives on improving customer and patient services through ChatGPT are discussed. The article also discussed the major key enablers of ChatGPT for refining customer and patient assistance. Further, the paper identifies and discusses the critical application areas of ChatGPT for customer and patient service. With its ability to handle several requests simultaneously, respond quickly and accurately to client questions, and gain knowledge from every interaction, ChatGPT is revolutionising customer and patient service. Its accessibility and compatibility with various communication channels make it a desirable solution for businesses looking to improve support. As technology advances, ChatGPT is positioned to become an essential tool for businesses wishing to provide speedy and customised service. Although ChatGPT may give convincing solutions, the chance of providing accurate and updated information poses a problem for its usage in service jobs that need accurate and up-to-date information. In future, various services will become better and more efficient due to ChatGPT and AI.",article,3,,,,,,,,
Integrated information processing for manufacturing—from CAD/CAM to CIM,Computers in Industry,5,311-318,1984,0166-3615,https://doi.org/10.1016/0166-3615(84)90054-X,https://www.sciencedirect.com/science/article/pii/016636158490054X,D. Kochan,"Computer-Aided Production Planning, Computer-Aided Manaufacturing, work division man-machine, termini, Automated Integrated Systems of data flow, decision making, CAD, CAM, CIM","The trend to integrated information processing (CAD/ CAM) leads to some new requiements. Consistent information processing in the presparation of production and in the production process demands an efficient division of labour between man and machine, the future-oriented engineer having to be especially prepared for this. In the present paper the resulting new demands on training and education are formulated. The essential features of integrated systems of automated information processing are characterized, and fundamental problems of the complex decision findings are explained from the angle of the manufacturing engineering. The paper is concluded by examples of solutions for the fully automated preparation and execution of production.",article,4,,,,,,,,
"An era of ChatGPT as a significant futuristic support tool: A study on features, abilities, and challenges","BenchCouncil Transactions on Benchmarks, Standards and Evaluations",2,100089,2022,2772-4859,https://doi.org/10.1016/j.tbench.2023.100089,https://www.sciencedirect.com/science/article/pii/S2772485923000066,Abid Haleem and Mohd Javaid and Ravi Pratap Singh,"Artificial Intelligence (AI), ChatGPT, Role, Features, Capabilities, Challenges","Open Artificial Intelligence (AI) published an AI chatbot tool called ChatGPT at the end of November 2022. Generative Pre-trained Transformer (GPT) architecture is the foundation of ChatGPT. On the internet, ChatGPT has been rapidly growing. This chatbot enables users to discuss with the AI by inputting prompts, and it is based on OpenAI’s language model. Although ChatGPT is fantastic and produces exciting results for writing tales, poetry, songs, essays, and other things, it has certain restrictions. Users may ask the bot questions, and it will reply with pertinent, convincing subjects and replies. ChatGPT has now risen to the top of several academic agendas. Administrators create task teams and hold institution-wide meetings to react to the tools, with most of the advice being to adopt this technology. This paper briefs about the ChatGPT and its need. Further, various Progressive Work Flow Processes of the ChatGPT Tool are stated diagrammatically. Specific features and capabilities of the ChatGPT Support System are studied in this paper. Finally, we identified and discussed the significant roles of ChatGPT in the current scenario. The neural language models that form the foundation of character AI have been developed from the bottom up with talks in mind. This technology implies that the programme uses deep learning methods to analyse and produce text. The model “understands” the subtleties of human-produced natural language using vast amounts of data from the internet.",article,4,,,,,,,,
Conducting research with school children and data in line with “ethical principles” lawyers at work in the ethics management of the H2020 mathisis project,Computer Law & Security Review,38,105451,2020,0267-3649,https://doi.org/10.1016/j.clsr.2020.105451,https://www.sciencedirect.com/science/article/pii/S026736492030056X,,"School children, School children with special needs, Ethical principles, Research, National schooling systems, Personal data protection law","Recent advancements in human-computer interaction, machine learning and in artificial intelligence hold the potential to influence both the curriculum and the pedagogy of school children. While the impacts of new technologies remain uncertain, ongoing research and innovation projects are already developing and testing such technologies in schools. This article builds on the experience of the authors as advisors for a Horizon 2020 (H2020) project conducting research with schoolchildren in twenty schools across the United Kingdom, Italy and Spain (the project MaTHiSiS). This contribution presents and discusses how the authors lived up to the obligation of conducting research in line with “ethical principles”.",article,,,,,,,,,
Contents,Procedia Computer Science,237,iii-ix,2024,1877-0509,https://doi.org/10.1016/S1877-0509(24)01221-3,https://www.sciencedirect.com/science/article/pii/S1877050924012213,,,,article,,International Conference on Industry Sciences and Computer Science Innovation,,,,,,,
How To Teach Artificial Intelligence To Manage Our Organizations?,Procedia Computer Science,225,4795-4804,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.10.479,https://www.sciencedirect.com/science/article/pii/S187705092301637X,Piotr Śliwa and Grzegorz Krzos,"artificial intelligence, machine learning, artificial general intelligence, intelligent agents, management, organizational model, sustainability","Undoubtedly, Artificial Intelligence (AI) is going mainstream. More and more AI agents come into existence to augment human agents in their work by synthesizing a gigantic body of knowledge in a conversational interface (e.g., ChatGPT), generating art from a provided description (e.g., Stable Diffusion), creating software code based on a provided description (e.g., Codex), just to name a few. It becomes evident that at some point an AI agent will similarly help human managers in their daily operations, and, when it reaches the level of artificial general intelligence (AGI), unlock completely new levels of performance and sustainability. The authors used the critical review method and identified a research gap concerning the development of a generalized, numerical model of an organization and its environment that could be applied in machine learning pipelines, and effectively support managers in the key management functions.",article,,27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023),,,,,,,
Editorial Board,Computer Law & Security Review,14,IFC,1998,0267-3649,https://doi.org/10.1016/S0267-3649(98)90003-6,https://www.sciencedirect.com/science/article/pii/S0267364998900036,,,,article,1,,,,,,,,
"Exploring the integration of edge computing and blockchain IoT: Principles, architectures, security, and applications",Journal of Network and Computer Applications,226,103884,2024,1084-8045,https://doi.org/10.1016/j.jnca.2024.103884,https://www.sciencedirect.com/science/article/pii/S1084804524000614,,"Edge computing, Blockchain, Internet-of-thing, Architecture, Security, Application","IoT systems are widely used in various applications, including healthcare, agriculture, manufacturing, and smart cities. However, these systems still have limitations, such as lack of security, high latency, energy inefficiency, the inefficiency of bandwidth utilization, and shortage of automaticity. The integration of edge computing and blockchain into IoT has been proposed to address these limitations. Yet, this integration is challenging and has not been deeply investigated. This paper aims to conduct a review of the integration of edge computing and blockchain into IoT systems. To the best of our knowledge, this is the first review paper that covers all aspects of system architectures and categories of blockchain-based edge deployment, complete security requirements, including confidentiality, integrity, authentication, authorization/access control, privacy, trust/confidence, transparency, availability, secure automaticity, and tolerance, and applications of blockchain-based edge potential usages with consideration of security requirements. Additionally, this review provides comprehensive discussions of challenges and insights into the future direction of blockchain-based edge IoT systems. The review aims to serve as an entry point for non-expert readers and researchers to various aspects of blockchain-based edge IoT systems.",article,,,,,,,,,
Complex business ecosystem intelligence using AI-powered visual analytics,Decision Support Systems,178,114133,2024,0167-9236,https://doi.org/10.1016/j.dss.2023.114133,https://www.sciencedirect.com/science/article/pii/S0167923623002087,Rahul C. Basole and Hyunwoo Park and C. David Seuss,"Business ecosystem, Artificial intelligence, Text mining, Complex networks, Interactive visualization","Business ecosystems are complex, dynamic systems characterized by a multitude of entities, including companies, ventures, and technologies, as well as activities and trends. Understanding the state of business ecosystems is an increasingly critical strategic imperative for many decision makers, but it is a resource-intensive activity as relevant information sources are dispersed, often highly unstructured, and not integrated or curated to deliver actionable insights. In this research, we present the design and implementation of an interactive visual analytic system that integrates artificial intelligence and graph visualization techniques to augment decision makers’ understanding of the complex public narrative associated with business ecosystems entities. Our system is driven by a real-time content engine of 100,000+ global data sources including press releases, news articles, industry reports, analyst blogs in multiple languages organized across several domain-specific repositories. Following a user-specified query, the engine extracts both domain-agnostic and domain-specific entities and concepts for each document in the result set. We then model and visualize the resulting data as a dynamic, multipartite network and implement graph pruning algorithms and interactive data controls to enable users to interactively explore and discover the underlying business ecosystem from multiple perspectives. We illustrate and discuss the value of our system using representative use cases. Our study makes multiple contributions to visual decision support theory and practice, including mining unstructured data, constructing and interacting with knowledge graphs, and designing visual analytic tools for ecosystem intelligence. We conclude the study with implications and future research opportunities.",article,,,,,,,,,
Pseudo dense counterfactual augmentation for aspect-based sentiment analysis,Neurocomputing,561,126869,2023,0925-2312,https://doi.org/10.1016/j.neucom.2023.126869,https://www.sciencedirect.com/science/article/pii/S092523122300992X,Jihong Ouyang and Shi Feng and Bing Wang and Zhiyao Yang,"Aspect-based sentiment analysis, Encoder-decoder, Data augmentation","Aspect-based sentiment analysis (ABSA) is a fine-grained text classification task, and the cutting-edge ABSA models have achieved outstanding performance. Unfortunately, the robustness of these ABSA models is neglected. ABSA models must face numerous challenges to be robust, and we concentrate on one of these challenges caused by negation words, such as “not”, “un-”. In the actual context, these negation words intuitively result in two problems: negative sensitivity and spurious correlation. First, a negation word tends to reverse the sentiment polarity of a sentence. Meanwhile, in the ABSA datasets, most sentences containing negation words express Negative polarities, which will lead the predictive model to learn the spurious correlation between negation words and polarities. To resolve these ambiguous issues, we are inspired by causal inference and propose a novel data augmentation framework, namely Pseudo Dense Counterfactual Augmentation (PDCaug) for ABSA. Specifically, we initialize a pseudo sequence and employ a multi-head multi-layer attention network to achieve counterfactual augmentation for a vanilla sentence in the hidden space. This pseudo sequence will be adversarially trained. PDCaug is a plug-and-play method for various ABSA models, so we evaluate it on discriminative models and generative prompt-based models. Our extensive experiments show that our PDCaug can significantly and consistently outperform several data augmentation methods and ABSA models.",article,,,,,,,,,
New member of the CLSR Editorial Board,Computer Law & Security Review,29,639,2013,0267-3649,https://doi.org/10.1016/j.clsr.2013.10.001,https://www.sciencedirect.com/science/article/pii/S0267364913001738,,,,article,6,,,,,,,,
The European AI liability directives – Critique of a half-hearted approach and lessons for the future,Computer Law & Security Review,51,105871,2023,0267-3649,https://doi.org/10.1016/j.clsr.2023.105871,https://www.sciencedirect.com/science/article/pii/S026736492300081X,Philipp Hacker,"Artificial intelligence, ChatGPT, Product liability, EU law, AI act, Sustainability, Innovation, Large generative AI models","The optimal liability framework for AI systems remains an unsolved problem across the globe. With ChatGPT and other large generative models taking the technology to the next level, solutions are urgently needed. In a much-anticipated move, the European Commission advanced two proposals outlining the European approach to AI liability in September 2022: a novel AI Liability Directive (AILD) and a revision of the Product Liability Directive (PLD). They constitute the final cornerstone of AI regulation in the EU. Crucially, the liability proposals and the proposed EU AI Act are inherently intertwined: the latter does not contain any individual rights of affected persons, and the former lack specific, substantive rules on AI development and deployment. Taken together, these acts may well trigger a “Brussels effect” in AI regulation, with significant consequences for the US and other countries. Against this background, this paper makes three novel contributions. First, it examines in detail the liability proposals and shows that, while making steps in the right direction, they ultimately represent a half-hearted approach: if enacted as foreseen, AI liability in the EU will primarily rest on disclosure of evidence mechanisms and a set of narrowly defined presumptions concerning fault, defectiveness and causality. Hence, second, the article suggests amendments to the proposed AI liability framework. They are collected in a concise Annex at the end of the paper. I argue, inter alia, that the dichotomy between the fault-based AILD Proposal and the supposedly strict liability PLD Proposal is fictional and should be abandoned; that an EU framework for AI liability should comprise one fully harmonizing regulation instead of two insufficiently coordinated directives; and that the current proposals unjustifiably collapse fundamental distinctions between social and individual risk by equating high-risk AI systems in the AI Act with those under the liability framework. Third, based on an analysis of the key risks AI poses, the final part of the paper maps out a road for the future of AI liability and regulation, in the EU and beyond. More specifically, I make four key proposals. Effective compensation should be ensured by combining truly strict liability for certain high-risk AI systems with general presumptions of defectiveness, fault and causality in cases involving SMEs or non-high-risk AI systems. The paper introduces a novel distinction between illegitimate- and legitimate-harm models to delineate strict liability's scope. Truly strict liability should be reserved for high-risk AI systems that, from a social perspective, should not cause harm (illegitimate-harm models, e.g., autonomous vehicles or medical AI). Models meant to cause some unavoidable harm by ranking and rejecting individuals (legitimate-harm models, e.g., credit scoring or insurance scoring) may merely face rebuttable presumptions of defectiveness and causality. General-purpose AI systems and Foundation Models should only be subjected to high-risk regulation, including liability for high-risk AI systems, in specific high-risk use cases for which they are deployed. Consumers, in turn, ought to be liable based on regular fault, in general. Furthermore, innovation and legal certainty should be fostered through a comprehensive regime of safe harbours, defined quantitatively to the best extent possible. Moreover, trustworthy AI remains an important goal for AI regulation. Hence, the liability framework must specifically extend to non-discrimination cases and provide for clear rules concerning explainability (XAI). Finally, awareness for the climate effects of AI, and digital technology more broadly, is rapidly growing in computer science. In diametrical opposition to this shift in discourse and understanding, however, EU legislators have long neglected environmental sustainability in both the draft AI Act and the proposed liability regime. To counter this, I propose to jump-start sustainable AI regulation via sustainability impact assessments in the AI Act and sustainable design defects in the liability regime. In this way, the law may help spur not only fair AI and XAI, but also sustainable AI (SAI).",article,,,,,,,,,
HydroRTC: A web-based data transfer and communication library for collaborative data processing and sharing in the hydrological domain,Environmental Modelling & Software,178,106068,2024,1364-8152,https://doi.org/10.1016/j.envsoft.2024.106068,https://www.sciencedirect.com/science/article/pii/S1364815224001294,,"Satellite data, Sensor data, Decentralized data distribution, Collaborative data exchange, Distributed data processing, Large scale peer to peer data sharing, WebRTC, Web sockets, Hydrology","The exponential growth in data generated by satellites, radars, sensors, and analysis and reanalysis from model outputs for the hydrological domain requires efficient real-time data management and distribution mechanisms. This paper introduces HydroRTC, a web-based data transfer and communication library designed to accelerate large-scale data sharing and analysis. Leveraging next-generation web technologies like WebSockets, WebRTC and Node.js, the library enables seamless peer-to-peer sharing, smart data transmission, and large dataset streaming. Three primary scenarios are presented as use cases, demonstrating the potential of HydroRTC as server-to-peer with intelligent data scheduling and large data streaming, peer-to-peer data sharing, and peer-to-server for data exchange. HydroRTC offers a promising solution for collaborative infrastructures in the hydrological and environmental domain, allowing real-time and high-throughput data sharing and transfer for enhancing research efficiency and collaboration capabilities.",article,,,,,,,,,
"The 2012 CLSR-LSPI seminar on privacy, data protection & cyber-security – Presented at the 7th international conference on Legal, Security and Privacy Issues in IT law (LSPI) October 2–4, 2012, Athens",Computer Law & Security Review,29,4-12,2013,0267-3649,https://doi.org/10.1016/j.clsr.2012.11.007,https://www.sciencedirect.com/science/article/pii/S0267364912002038,Steve Saxby,"Cyber-privacy, Data security, Data abuse, Function-creep, Privacy regulation, Surveillance society, Privacy by design, Social media privacy, Cyber-crime","This has been a big year for privacy with so much going on within the EU regarding reform of data protection. What are the implications of reform here and what are the issues that concern us about the proposed new data protection regime contained in the proposed Regulation? We hear a lot about the ‘right to be forgotten’. How is that possible in the digital age within the online world? And what can be done about the big players who stand charged with the erosion of privacy viz Facebook, Google, Skype & YouTube etc? How can the law keep up with technological change when the latter is moving so fast e.g. with RFID, Cloud and social networking? To what extent can data breach notification, net neutrality and privacy impact assessment help and how should the law approach issues of liability and criminality in relation to privacy? What is the state of play too in the relationship between privacy policy and state surveillance and, given its implications for privacy, what obligations should governments adopt in response to cybersecurity regulation and data management? Is there a place for privacy self-regulation and if so in what respects and how effective are the Information Commissioners who often complain of being under resourced? In reviewing the way privacy law has emerged do we now need a completely new approach to the whole issue? Has the law crept into its present form simply by default? Do we need some new thinking now that reflects the fact that law is only one dimension in the battle for privacy? If so what are the other factors we need to recognise?",article,1,,,,,,,,
Chapter 13 - Artificial intelligence and basic human needs: the shadow aspects of emerging technology,,,259-278,2024,,https://doi.org/10.1016/B978-0-443-18851-0.00004-4,https://www.sciencedirect.com/science/article/pii/B9780443188510000044,Tay Keong Tan,"Artificial intelligence, autonomous vehicles, facial recognition, AI writing assistant, AI image generator, human needs","While advancing artificial intelligence (AI) applications have brought ease and benefit to human life in meeting our physical needs, it is less obvious how they would impact psychological needs. This study analyzes three emerging technologies—autonomous vehicles; facial recognition systems; and AI writing or image generators—from the perspective of six fundamental human needs; certainty, variety, significance, connection, growth, and contribution. Our core human needs can greatly influence the acceptability, feasibility, and utility of these technologies. A prognosis of the human needs implications of AI can help algorithm designers, policymakers, regulators, and end users mitigate the risks and accentuate its benefits.",incollection,,,Santi Caballé and Joan Casas-Roma and Jordi Conesa,Ethics in Online AI-based Systems,Academic Press,Intelligent Data-Centric Systems,978-0-443-18851-0,,
ESA: Excitation-Switchable Attention for convolutional neural networks,Neurocomputing,557,126706,2023,0925-2312,https://doi.org/10.1016/j.neucom.2023.126706,https://www.sciencedirect.com/science/article/pii/S0925231223008299,Shanshan Zhong and Zhongzhan Huang and Wushao Wen and Zhijing Yang and Jinghui Qin,"Attention mechanism, Switchable excitation module, Neural network","Although various attention mechanisms can boost the representational power of convolutional neural networks (CNNs) and improve their performance, selecting an appropriate attention module becomes challenging when backbones or datasets change. Besides, as different CNN layers can learn distinct semantic features, applying the same attention module across all layers may not yield optimal results for enhancing the performance of a deep neural network. To address the above issues, we propose a novel excitation-switchable attention (ESA) to automatically select and integrate different excitation modules to compute attention maps, enabling a DNN to apply different attention modules in different layers for better feature learning and performance improvement. Extensive experiments on three widely-used image classification benchmarks demonstrate the superiority of our ESA over several well-known and widely-adopted attention modules.",article,,,,,,,,,
Managing cloud via Smart Cloud Engine and Knowledge Base,Future Generation Computer Systems,78,142-154,2018,0167-739X,https://doi.org/10.1016/j.future.2016.10.006,https://www.sciencedirect.com/science/article/pii/S0167739X16303867,Pierfrancesco Bellini and Ivan Bruno and Daniele Cenni and Paolo Nesi,"Knwoledge base, Smart cloud, Cloud computing, Service level agreement","Complexity of cloud infrastructures needs models and tools for process management, configuration, scaling, elastic computing and cloud resource health control. This paper presents a Smart Cloud Engine and solution based on a Knowledge Base, KB, with the aim of modeling cloud resources, Service Level Agreements and their evolutions, and enabling the reasoning on structures by implementing strategies of efficient smart cloud management and intelligence. The solution proposed provides formal verification and intelligence tools for cloud control. It can be easily integrated with a large range of cloud configuration manager, cloud orchestrator, and monitoring tools, since the connections with these tools are performed by using REST calls and XML files. The proposed solution has been validated in the context of large ICARO Cloud project and in the cloud facility of a national cloud service provider. Some data resulting from the validation phases have been reported and are referring to the dynamic management of real ECLAP social network http://www.eclap.eu.",article,,,,,,,,,
Chapter 1 - History of graph computing and graph databases,,,1-32,2024,,https://doi.org/10.1016/B978-0-443-14162-1.00004-0,https://www.sciencedirect.com/science/article/pii/B9780443141621000040,Ricky Sun,"Big data, Graph thinking, Graph database, Graph computing, Knowledge graph, Network analysis","This chapter introduces the core concept throughout this book—graph thinking. Concrete and visualized real-world examples were given in the first section to facilitate the readers to understand the depth and breadth of the concept, and how to put it to work. The first section is completed with a review of the historical development of graph theory and technologies. The second section gives an overview of how data processing technologies and frameworks have evolved from relational databases to big-data frameworks and eventually to graph databases, and insights into their differences. The final section focuses on introducing the amazing and unprecedented capabilities of graph databases, again, with real-world practical examples. This section ended with a comparison of graph computing and graph databases, hoping to clarify any potential confusion between the two topics.",incollection,,,Ricky Sun,The Essential Criteria of Graph Databases,Elsevier,,978-0-443-14162-1,,
Analyzing the potential benefits and use cases of ChatGPT as a tool for improving the efficiency and effectiveness of business operations,"BenchCouncil Transactions on Benchmarks, Standards and Evaluations",3,100140,2023,2772-4859,https://doi.org/10.1016/j.tbench.2023.100140,https://www.sciencedirect.com/science/article/pii/S2772485923000571,Rohit Raj and Arpit Singh and Vimal Kumar and Pratima Verma,"ChatGPT, benefits, business, efficiency, automation","The study addresses the potential benefits for companies of adopting ChatGPT, a popular chatbot built on a large-scale transformer-based language model known as a generative pre-trained transformer (GPT). Chatbots like ChatGPT may improve customer service, handle several client inquiries at once, and save operational costs. Moreover, ChatGPT may automate regular processes like order tracking and billing, allowing human employees to focus on more complex and strategic responsibilities. Nevertheless, before deploying ChatGPT, enterprises must carefully analyze its use cases and restrictions, as well as its strengths and disadvantages. ChatGPT, for example, requires training data that is particular to the business domain and might produce erroneous and ambiguous findings. The study identifies areas of deployment of ChatGPT's possible benefits in enterprises by drawing on the literature that is currently accessible on ChatGPT, massive language models, and artificial intelligence. Then, using the PSI (Preference Selection Index) and COPRAS (Complex Proportional Assessment) approaches, potential advantages are taken into account and prioritized. By highlighting current trends and possible advantages in the industry, this editorial seeks to provide insight into the present state of employing ChatGPT in enterprises and research. ChatGPT may also learn biases from training data and create replies that reinforce those biases. As a result, enterprises must train and fine-tune ChatGPT to specific operations, set explicit boundaries and limitations for its use, and implement appropriate security measures to avoid malicious input. The study highlights the research gap in the dearth of literature by outlining ChatGPT's potential benefits for businesses, analyzing its strengths and limits, and offering insights into how organizations might use ChatGPT's capabilities to enhance their operations.",article,3,,,,,,,,
Advances in medical image analysis with vision Transformers: A comprehensive review,Medical Image Analysis,91,103000,2024,1361-8415,https://doi.org/10.1016/j.media.2023.103000,https://www.sciencedirect.com/science/article/pii/S1361841523002608,Reza Azad and Amirhossein Kazerouni and Moein Heidari and Ehsan Khodapanah Aghdam and Amirali Molaei and Yiwei Jia and Abin Jose and Rijo Roy and Dorit Merhof,"Transformers, Medical image analysis, Vision transformers, Deep neural networks","The remarkable performance of the Transformer architecture in natural language processing has recently also triggered broad interest in Computer Vision. Among other merits, Transformers are witnessed as capable of learning long-range dependencies and spatial correlations, which is a clear advantage over convolutional neural networks (CNNs), which have been the de facto standard in Computer Vision problems so far. Thus, Transformers have become an integral part of modern medical image analysis. In this review, we provide an encyclopedic review of the applications of Transformers in medical imaging. Specifically, we present a systematic and thorough review of relevant recent Transformer literature for different medical image analysis tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation. For each of these applications, we investigate the novelty, strengths and weaknesses of the different proposed strategies and develop taxonomies highlighting key properties and contributions. Further, if applicable, we outline current benchmarks on different datasets. Finally, we summarize key challenges and discuss different future research directions. In addition, we have provided cited papers with their corresponding implementations in https://github.com/mindflow-institue/Awesome-Transformer.",article,,,,,,,,,
Customizing SVM as a base learner with AdaBoost ensemble to learn from multi-class problems: A hybrid approach AdaBoost-MSVM,Knowledge-Based Systems,217,106845,2021,0950-7051,https://doi.org/10.1016/j.knosys.2021.106845,https://www.sciencedirect.com/science/article/pii/S0950705121001088,Zafar Mehmood and Sohail Asghar,"Machine learning classifiers, Class overlapping, Imbalanced distribution of data, Imbalanced problem, Decomposition techniques","Learning from a multi-class problem has not been an easy task for most of the classifiers, because of multiple issues. In the complex multi-class scenarios, samples of different classes overlap with each other by sharing attribute, and hence the visibility of least represented samples decrease even more. Learning from imbalanced data studied extensively in the research community, however, the overlapping issues and the co-occurrence impact of overlapping with data imbalance have received comparatively less attention, even though their joint impact is more thoughtful on classifiers’ performance. In this paper, we introduce a modified SVM, MSVM to use as a base classifier with the AdaBoost ensemble classifier (MSVM-AdB) to enhance the learning capability of the ensemble classifier. To implement the proposed technique, we divide the multi-class dataset into overlapping and non-overlapping region. The overlapping region is further filter into the Critical and less Critical region depending upon their sample contribution in the overlapped region. The MSVM is designed to map the overlapped samples in a higher dimension by modifying the kernel mapping function of the standard SVM by using the mean distance of the Critical region samples. To highlight the learning enhancement of the MSVM-AdB, we use 20 real datasets with varying imbalance ratio and the overlapping degree to compare the significance of the AdaBoost-MSVM with the standard SVM, and AdaBoost with standard base classifiers. Experimental results show the superiority of the MSVM-AdB on a collection of benchmark datasets to its standard counterpart classifiers.",article,,,,,,,,,
Enhanced subgraph matching for large graphs using candidate region-based decomposition and ordering,Journal of King Saud University - Computer and Information Sciences,35,101694,2023,1319-1578,https://doi.org/10.1016/j.jksuci.2023.101694,https://www.sciencedirect.com/science/article/pii/S1319157823002483,Zubair Ali Ansari and Md. Aslam Parwez and Irfan Rashid Thoker and  Jahiruddin,"Subgraph isomorphism, Graph search, Eccentricity, Candidate region ordering, Large graph, Embedding, Straggler query","The subgraph matching problem associated with large graphs is an emerging research challenge in graph search due to the growing size of the web, social, and metabolic graphs, and the wide availability of graph databases. Such problems involve finding all instances (aka embedding) of the small-sized query graph in the associated large-sized reference graph. Many state-of-the-art algorithms, including VF3, RI, CFL-Match, and Glasgow, exist to solve subgraph matching problem. RI is one of the fastest subgraph matching algorithms focusing mainly on time efficiency performance measures. However, other performance measures, such as the number of found instances of the query graph (embedding count), the method of ordering the query graph’s vertices, and the number of recursive calls, are crucial for the efficiency and effectiveness of the subgraph matching. In this paper, the RI+ algorithm is proposed as an enhanced version of RI, which has been designed using candidate region-based decomposition and ordering. Three novel candidate region orderings have been introduced, namely vertex-count, density, and average-path-length, based on the structural properties of the candidate regions. On empirical analysis of RI+ on real-world data sets, it was observed that RI+ shows significant improvement in efficiency and effectiveness over RI on both performance evaluation measures, namely, embedding count and search time. The influence of the proposed candidate region orderings on the search time of RI+ was also analyzed, revealing that a suitable candidate region ordering has the potential to improve the search time of the proposed algorithm.",article,8,,,,,,,,
A popular topic detection method based on microblog images and short text information,Journal of Web Semantics,81,100820,2024,1570-8268,https://doi.org/10.1016/j.websem.2024.100820,https://www.sciencedirect.com/science/article/pii/S1570826824000064,Wenjun Liu and Hai Wang and Jieyang Wang and Huan Guo and Yuyan Sun and Mengshu Hou and Bao Yu and Hailan Wang and Qingcheng Peng and Chao Zhang and Cheng Liu,"Topic detection, Image description, Semantic similarity, Internet New Word Detection, Short Text","Popular topic detection is a topic identification by the information of documents posted by users in social networking platforms. In a large body of research literature, most popular topic detection methods identify the distribution of unknown topics by integrating information from documents based on social networking platforms. However, among these popular topic detection methods, most of them have a low accuracy in topic detection due to the short text content and the abundance of useless punctuation marks and emoticons. Image information in short texts has also been overlooked, while this information may contain the real topic matter of the user's posted content. In order to solve the above problems and improve the quality of topic detection, this paper proposes a popular topic detection method based on microblog images and short text information. The method uses an image description model to obtain more information about short texts, identifies hot words by a new word discovery algorithm in the preprocessing stage, and uses a PTM model to improve the quality and effectiveness of topic detection during topic detection and aggregation. The experimental results show that the topic detection method in this paper improves the values of evaluation indicators compared with the other three topic detection methods. In conclusion, the popular topic detection method proposed in this paper can improve the performance of topic detection by integrating microblog images and short text information, and outperforms other topic detection methods selected in this paper.",article,,,,,,,,,
AI in HRM: case study analysis. Preliminary research,Procedia Computer Science,225,2351-2360,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.10.226,https://www.sciencedirect.com/science/article/pii/S1877050923013844,Wiesława Gryncewicz and Ryszard Zygała and Agnieszka Pilch,"Human Resource Management, Artificial Intelligence, Alghoritms, Artificial Intelligence Applications","The article attempts to identify Artificial Intelligence (AI) algorithms in Human Resources Management (HRM) systems focusing particular attention on candidate selection, career building, and predicting employee attrition. The review examines case studies that demonstrate the benefits of AI in HRM, including enhancing employee engagement and satisfaction, improving recruitment processes, supporting decision-making and predicting employee retention. The research indicates that interpretable algorithms, such as decision trees, are frequently used in HRM solutions. The study emphasizes that AI should be viewed as a tool rather than a replacement for human judgment in HRM. Both the review and article highlight the growing trend of AI in HRM systems and the need for further research in this area to fully understand its impact on HRM practices and outcomes.",article,,27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023),,,,,,,
Higher-order GNN with Local Inflation for entity alignment,Knowledge-Based Systems,293,111634,2024,0950-7051,https://doi.org/10.1016/j.knosys.2024.111634,https://www.sciencedirect.com/science/article/pii/S0950705124002697,Jianrui Chen and Luheng Yang and Zhihui Wang and Maoguo Gong,"Graph neural network, Local inflation, Higher-order, Entity alignment","In the age of massive data, the construction of knowledge graph has increasingly become a forceful support for the downstream applications of artificial intelligence. However, the information of entities in knowledge graphs is usually incomplete, so it is urgent to supplement the relations of entities through entity alignment task. Frustratingly, the current entity alignment models are facing serious challenges. First, some models only focus on structural features and other auxiliary information (e.g., attributes, images and descriptions), but ignore the features of the entity itself can be scaled resulting in over-smoothing issue. Second, most models utilize higher-order networks to aggregate neighborhood information by stacking layers, but the training cost of these models are drastically higher. Third, most models are supervised or semi-supervised, but there are few pre-aligned seeds for alignment, which greatly limits the improvement of model performance. Hence, to address the above three issues, we propose a Higher-Order Graph Neural Network with Local Inflation for entity alignment, named HOLI-GNN. Specifically, we introduce a local inflation mechanism, which enlarges the each feature of entities to mitigate the impact of over-smoothing caused by neighborhood aggregation. Additionally, we propose a novel higher-order encoder to capture higher-order information. Furthermore, our model also employ currently popular iteration strategy to increase labeled entity pairs, which can markedly promote the performance of align task. Finally, we perform comprehensive experiments to validate the effectiveness of our model on benchmark datasets. The results strongly indicate that our model exhibits better performance than the state-of-the-art models.",article,,,,,,,,,
A comprehensive review of synthetic data generation in smart farming by using variational autoencoder and generative adversarial network,Engineering Applications of Artificial Intelligence,131,107881,2024,0952-1976,https://doi.org/10.1016/j.engappai.2024.107881,https://www.sciencedirect.com/science/article/pii/S0952197624000393,Yaganteeswarudu Akkem and Saroj Kumar Biswas and Aruna Varanasi,"Variational autoencoders, Generative adversarial networks, Smart farming","In this study, we propose the use of Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to generate synthetic data for crop recommendation (CR). CR is critical in agriculture, assisting farmers in making informed decisions about crop cultivation, considering factors like soil conditions, weather patterns etc. Unfortunately, the availability of labeled data for CR is often limited, posing a significant challenge in training accurate recommendation models. VAEs and GANs are employed to create synthetic data that closely mirrors real-world crop data. VAEs are utilized to extract latent representation from the input data, enabling the generation of new samples with similar characteristics. GANs play a crucial role in generating data by training a generator network to produce synthetic samples that closely resemble real data, while a discriminator network distinguishes between genuine and synthetic data. The generated synthetic data serves as a valuable resource to prepare datasets for CR, enhancing the performance of recommendation models. Our research explores the effectiveness of VAEs and GANs in producing high-quality synthetic CR data, facilitating improved training and evaluation of recommendation systems. This paper presents the architecture and training process of the proposed models and evaluates the quality and utility of the generated synthetic data using various experiments, including visualizations such as heatmaps, scatter plots, cumulative sum per feature plots, and distribution per feature plots. The results of this study hold the potential to make a significant contribution to the field of agriculture by providing a reliable and abundant source of training data for CR systems.",article,,,,,,,,,
Issue 112: A Note from the Editor-in-Chief,Computers & Graphics,112,A1-A4,2023,0097-8493,https://doi.org/10.1016/j.cag.2023.06.002,https://www.sciencedirect.com/science/article/pii/S0097849323000833,Joaquim Jorge,,,article,,,,,,,,,
An effective classification and numbering system for dental bitewing radiographs using teeth region and contour information,Pattern Recognition,43,1380-1392,2010,0031-3203,https://doi.org/10.1016/j.patcog.2009.10.005,https://www.sciencedirect.com/science/article/pii/S0031320309003781,P.L. Lin and Y.H. Lai and P.W. Huang,"Dental bitewing radiograph, Homomorphic filter, Homogeneity measurement, Adaptive contrast stretching, Teeth classification, Sequence alignment, Teeth numbering","We propose a dental classification and numbering system to effectively segment, classify, and number teeth in dental bitewing radiographs. An image enhancement method that combines homomorphic filtering, homogeneity-based contrast stretching, and adaptive morphological transformation is proposed to improve both contrast and illumination evenness of the radiographs simultaneously. Iterative thresholding and integral projection are adapted to isolate teeth to regions of interest (ROIs) followed by contour extraction of the tooth and the pulp (if available) from each ROI. A binary linear support vector machine using the skew-adjusted relative length/width ratios of both teeth and pulps, and crown size as features is proposed to classify each tooth to molar or premolar. Finally, a numbering scheme that combines a missing teeth detection algorithm and a simplified version of sequence alignment commonly used in bioinformatics is presented to assign each tooth a proper number. Experimental results show that our system has accuracy rates of 95.1% and 98.0% for classification and numbering, respectively, in terms of number of teeth tested, and correctly classifies and numbers the teeth in four images that were reported either misclassified or erroneously numbered, respectively.",article,4,,,,,,,,
Churn Prediction in Telecommunication using Logistic Regression and Logit Boost,Procedia Computer Science,167,101-112,2020,1877-0509,https://doi.org/10.1016/j.procs.2020.03.187,https://www.sciencedirect.com/science/article/pii/S1877050920306529,Hemlata Jain and Ajay Khunteta and Sumit Srivastava,"Machine Learning, Logistic Regression, Logit Boost","Today in every industry weather, it is ISP, IT products, social network or mobile services there is the problem of customer churn (Customers changing their services from one service provider to another). However, in telecommunication the customers churning very frequently. As the market in telecom is fiercely competitive, in that case, companies proactively have to determine the customers churn by analyzing their behavior and try to put effort and money in retaining the customers. In this proposed model, two machine-learning techniques were used for predicting customer churn Logistic regression and Logit Boost. Experiment was carried out in the WEKA Machine-learning tool, along with a real database from an American company Orange. The result were shown in different evaluation measures.",article,,International Conference on Computational Intelligence and Data Science,,,,,,,
Potentials of the Metaverse for Robotized Applications in Industry 4.0 and Industry 5.0,Procedia Computer Science,232,1829-1838,2024,1877-0509,https://doi.org/10.1016/j.procs.2024.02.005,https://www.sciencedirect.com/science/article/pii/S1877050924001820,Eric Guiffo Kaigom,"Robotics, Metaverse, Digital Twin, VR/AR, AI/ML, Foundation Model","As a digital environment of interconnected virtual ecosystems driven by measured and synthesized data, the Metaverse has so far been mostly considered from its gaming perspective that closely aligns with online edutainment. Although it is still in its infancy and more research as well as standardization efforts remain to be done, the Metaverse could provide considerable advantages for smart robotized applications in the industry. Workflow efficiency, collective decision enrichment even for executives, as well as a natural, resilient, and sustainable robotized assistance for the workforce are potential advantages. Hence, the Metaverse could consolidate the connection between Industry 4.0 and Industry 5.0. This paper identifies and puts forward potential advantages of the Metaverse for robotized applications and highlights how these advantages support goals pursued by the Industry 4.0 and Industry 5.0 visions.",article,,5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023),,,,,,,
Research on the impact of trends related to ChatGPT,Procedia Computer Science,221,1284-1291,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.08.117,https://www.sciencedirect.com/science/article/pii/S187705092300875X,Yunxi Yan and Biao Li and Jinyuan Feng and Yang Du and Zhichen Lu and Manling Huang and Youyuan Li,"ChatGPT, Artificial Intelligence","Since ChatGPT was launched, it has attracted great attention across society. Especially in non-professional fields, ChatGPT can answer follow-up questions, reject inappropriate requests, challenge erroneous assumptions, and admit mistakes from a user's experience. It has many emergent capabilities such as high-quality dialogue, complex reasoning, chains of thought (CoT), zero/low-shot learning (contextual learning), cross-task generalization, code understanding/generation, etc. The emergence of ChatGPT has brought a profound impact on the development of all aspects, and brought huge changes to the social economy and living environment.",article,,Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023),,,,,,,
DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing,Journal of Biomedical Informatics,138,104286,2023,1532-0464,https://doi.org/10.1016/j.jbi.2023.104286,https://www.sciencedirect.com/science/article/pii/S1532046423000072,Yanjun Gao and Dmitriy Dligach and Timothy Miller and John Caskey and Brihat Sharma and Matthew M. Churpek and Majid Afshar,"Natural language processing, Clinical diagnostic reasoning, Clinical diagnostic decision support, Clinical natural language processing benchmark","The meaningful use of electronic health records (EHR) continues to progress in the digital era with clinical decision support systems augmented by artificial intelligence. A priority in improving provider experience is to overcome information overload and reduce the cognitive burden so fewer medical errors and cognitive biases are introduced during patient care. One major type of medical error is diagnostic error due to systematic or predictable errors in judgement that rely on heuristics. The potential for clinical natural language processing (cNLP) to model diagnostic reasoning in humans with forward reasoning from data to diagnosis and potentially reduce cognitive burden and medical error has not been investigated. Existing tasks to advance the science in cNLP have largely focused on information extraction and named entity recognition through classification tasks. We introduce a novel suite of tasks coined as Diagnostic Reasoning Benchmarks, Dr.Bench, as a new benchmark for developing and evaluating cNLP models with clinical diagnostic reasoning ability. The suite includes six tasks from ten publicly available datasets addressing clinical text understanding, medical knowledge reasoning, and diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to be a natural language generation framework to evaluate pre-trained language models for diagnostic reasoning. The goal of DR. BENCH is to advance the science in cNLP to support downstream applications in computerized diagnostic decision support and improve the efficiency and accuracy of healthcare providers during patient care. We fine-tune and evaluate the state-of-the-art generative models on DR.BENCH. Experiments show that with domain adaptation pre-training on medical knowledge, the model demonstrated opportunities for improvement when evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab repository with a systematic approach to load and evaluate models for the cNLP community. We also discuss the carbon footprint produced during the experiments and encourage future work on DR.BENCH to report the carbon footprint.",article,,,,,,,,,
"The 2013 CLSR-LSPI seminar on electronic identity: The global challenge – Presented at the 8th International Conference on Legal, Security and Privacy issues in IT Law (LSPI) November 11–15, 2013, Tilleke & Gibbins International Ltd., Bangkok, Thailand",Computer Law & Security Review,30,112-125,2014,0267-3649,https://doi.org/10.1016/j.clsr.2014.01.007,https://www.sciencedirect.com/science/article/pii/S0267364914000247,Steve Saxby,"Digital identity, Electronic identity, Identity crime, Managing online identity, Big data, Mobile identity, Automated identification, Identity surveillance, Biometrics","We are the middle of a global identity crisis. New notions of identity are made possible in the online world where people eagerly share their personal data and leave ‘digital footprints’. Multiple, partial identities emerge distributed across cyberspace divorced from the physical person. The representation of personal characteristics in data sets, together with developing technologies and systems for identity management, in turn change how we are identified. Trustworthy means of electronic identification is now a key issue for business, governments and individuals in the fight against online identity crime. Yet, along with the increasing economic value of digital identity, there are also risks of identity misuse by organisations that mine large data sets for commercial purposes and in some cases by governments. Data proliferation and the non-transparency of processing practices make it impossible for the individual to track and police their use. Potential risks encompass not only threats to our privacy, but also knowledge-engineering that can falsify digital profiles attributed to us with harmful consequences. This panel session will address some of the big challenges around identity in the digital age and what they mean for policy and law (its regulation and protection). Questions for discussion include: What does identity mean today? What types of legal solutions are fit for purpose to protect modern identity interests? What rights, obligations and responsibilities should be associated with our digital identities? Should identity management be regulated and who should be held liable and for what? What should be the role of private and public sectors in identity assurance schemes? What are the global drivers of identity policies? How can due process be ensured where automated technologies affect the rights and concerns of citizens? How can individuals be more empowered to control their identity data and give informed consent to its use? How are biometrics and location-tracking devices used in body surveillance changing the identity landscape?",article,2,,,,,,,,
Trust aware energy management system for smart homes appliances,Computers & Electrical Engineering,97,107641,2022,0045-7906,https://doi.org/10.1016/j.compeleceng.2021.107641,https://www.sciencedirect.com/science/article/pii/S004579062100567X,Kashif Naseer Qureshi and Adi Alhudhaif and Adil Hussain and Saleem Iqbal and Gwanggil Jeon,"Smart homes, Energy consumption, Technologies, Scheduling, System, Cost, Appliances, Trust",Smart grids have gained popularity to manage energy resources on the consumption side. Energy management on the home side is still under consideration where the system controls the home appliances intelligently with cost-effective and manageable processes. This paper presents a Trust-aware Energy Management System for Smart Homes (TEMSH) by using smart scheduling and time management based on controllable and uncontrollable appliances management. This system is based on advanced communication technologies and security mechanisms. The trust mechanism provides the authentication services at the edge level to secure the user data from any type of unauthorized access and data leakage. The proposed system is deployed on houses to analyze the overall energy consumption and appliances. The results indicate the proposed system is more feasible for home appliances and able to manage and reduce the energy cost by around 55% cumulative Cost in the shape of bills and best for a green environment. The proposed system will be feasible to control the energy crises all over the world and reduce energy utilization and cost at the home level.,article,,,,,,,,,
Chapter Eight - Irregular situations in real-world intelligent systems,,134,253-283,2024,0065-2458,https://doi.org/10.1016/bs.adcom.2023.04.006,https://www.sciencedirect.com/science/article/pii/S0065245823000438,Ashutosh Mishra and Shiho Kim,"Artificial intelligence, Future mobility, Intelligent systems, Irregular situations, Privacy and safety, SOTIF","Real-world is full of uncertainty. This uncertainty introduces examples of irregular situations (situations that are contrary to the normal routine). Artificial intelligence (AI) has greatly benefited society through automation and intelligent systems. However, real-world situations often involve elements of unpredictability and irregularity, which can pose challenges for AI systems. To address these challenges, researchers have developed various techniques to improve the robustness and adaptability of AI systems. These include methods for handling uncertainty, data pre-processing, explainable AI, safety-critical AI, etc. However, despite these efforts, many open questions and challenges must be addressed to make AI systems more robust and adaptable to real-world situations. In this work, we have defined the possible irregular situations (IS) and introduced the potential solutions to countermeasure such situations. Here, we have surveyed the IS in image, audio, olfactory, and motion intelligence. Further, we have investigated a few of the way-outs and solutions. In addition, we have demonstrated the IS in automated driving depending upon the level of autonomy in autonomous vehicles (AVs) and discussed the safety and privacy issues with a consideration of the safety of the intended functionality (SOTIF) standard. These findings will undoubtedly facilitate research in the direction of future mobility.",incollection,,,Shiho Kim and Ganesh Chandra Deka,Artificial Intelligence and Machine Learning for Open-world Novelty,Elsevier,Advances in Computers,,,
European AI and EO convergence via a novel community-driven framework for data-intensive innovation,Future Generation Computer Systems,160,505-521,2024,0167-739X,https://doi.org/10.1016/j.future.2024.06.013,https://www.sciencedirect.com/science/article/pii/S0167739X24003133,Antonis Troumpoukis and Iraklis Klampanos and Despina-Athanasia Pantazi and Mohanad Albughdadi and Vasileios Baousis and Omar Barrilero and Alexandra Bojor and Pedro Branco and Lorenzo Bruzzone and Andreina Chietera and Philippe Fournand and Richard Hall and Michele Lazzarini and Adrian Luna and Alexandros Nousias and Christos Perentis and George Petrakis and Dharmen Punjani and David Röbl and George Stamoulis and Eleni Tsalapati and Indrė Urbanavičiūtė and Giulio Weikmann and Xenia Ziouvelou and Marcin Ziółkowski and Manolis Koubarakis and Vangelis Karkaletsis,"Artificial Intelligence, Earth observation, DIAS, Applications, Case-study, Methodology, Platform","Artificial Intelligence (AI) represents a collection of tools and methodologies that have the potential to revolutionise various aspects of human activity. Earth observation (EO) data, including satellite and in-situ, are essential in a number of high impact applications, ranging from security and energy to agriculture and health. In this paper, we present the AI4Copernicus framework for bridging the two domains within the European context to enable data-centred innovation. In order to achieve this goal, AI4Copernicus has developed and enriches the European AI-on-demand platform with a number of application bootstrapping services and tools to accelerate uptake and innovation, whilst it provides integration over AI-on-Demand services and the Copernicus ecosystem, targeting the highly successful Data and Information Access Service (DIAS) Cloud platforms. More specifically, by employing procedures for onboarding and validating models and tools, and by utilising a host of meticulously reviewed and supervised open calls-enabled projects, and containerisation best-practices, AI4Copernicus deployed and made available several products on DIAS platforms. Moreover, these products and resources have been made available on the AI-on-Demand platform catalogue for discovery, use and further development. The AI4Copernicus framework is being used by a number of business-driven projects and SMEs spanning several application domains. This article provides an overview of the European AI and EO context as well as the AI4Copernicus technological framework and tools offered. Further, we present real world use-cases as well as a community-centred evaluation of our framework based on usage and feedback received from several projects.",article,,,,,,,,,
EnhancedBERT: A feature-rich ensemble model for Arabic word sense disambiguation with statistical analysis and optimized data collection,Journal of King Saud University - Computer and Information Sciences,36,101911,2024,1319-1578,https://doi.org/10.1016/j.jksuci.2023.101911,https://www.sciencedirect.com/science/article/pii/S1319157823004652,Sanaa Kaddoura and Reem Nassar,"Arabic natural language processing, Word sense disambiguation, Machine learning, Knowledge-based, BERT, Performance evaluation","Accurate assignment of meaning to a word based on its context, known as Word Sense Disambiguation (WSD), remains challenging across languages. Extensive research aims to develop automated methods for determining word senses in different contexts. However, the literature lacks the presence of datasets generated for the Arabic language WSD. This paper presents a dataset comprising a hundred polysemous Arabic words. Each word in the dataset encompasses 3–8 distinct senses, with ten example sentences per sense. Some statistical operations are conducted to gain insights into the dataset, enlightening its characteristics and properties. Subsequently, a novel WSD approach is proposed to utilize similarity measures and find the overlap between contextual information and dictionary definitions. The proposed method uses the power of BERT, a pre-trained language model, to enable effective Arabic word disambiguation. In training, new features are integrated to improve the model's ability to differentiate between various senses of words. The proposed BERT models are combined to compose an ensemble model architecture to improve the classification performances. The performance of the WSD system outperforms state-of-the-art systems, achieving an approximate F1-score of 96 %. Statistical analyses are performed to evaluate the overall performance of the WSD approach by providing additional information on model predictions. A case study was implemented to test the effectiveness of WSD in sentiment analysis, a downstream task.",article,1,,,,,,,,
Integrating learners’ knowledge background to improve course recommendation fairness: A multi-graph recommendation method based on contrastive learning,Information Processing & Management,61,103750,2024,0306-4573,https://doi.org/10.1016/j.ipm.2024.103750,https://www.sciencedirect.com/science/article/pii/S0306457324001109,Wenjun Ma and Wen Chen and Liuxing Lu and Xiaomao Fan,"Course recommendation, Algorithmic fairness, Contrastive learning, Knowledge graph, MOOC","Massive Open Online Course (MOOC) recommendations that fail to align with the learners’ prior knowledge have the potential to adversely affect educational outcomes. Despite the advancements in deep learning-based course recommendation (CR) methods, there remains a lack of comprehensive examination concerning the biases associated with the diverse knowledge backgrounds of learners. Furthermore, the phenomenon of popularity bias exists in current CR systems. In light of the above issues, this study proposes a model called Contrastive Learning and Graph Convolution Network-based Attentive Decay Network (CLGADN), which aims to improve fairness in CR by taking into account the learners’ knowledge backgrounds. Specifically, (1) CLGADN employs contrastive learning to recognize the diverse knowledge backgrounds of learners and to address the challenge of popularity bias within CR, and (2) A monotonic attention decay mechanism is incorporated into the CLGADN to account for the knowledge forgetting curve, acknowledging that the knowledge learners have recently acquired shapes their understanding of the new course, more than the knowledge obtained in the past. Real-world XuetangX data are used to evaluate the proposed method. Experimental results reveal that (1) the CLGADN outperforms other recent CR methods regarding accuracy and fairness, achieving 74.73% on HR@10, 48.35% on NDCG@10, 41.45% on MRR, and 3.9% on TotalScore, a metric for evaluating whether the recommendations align with the learner’s knowledge background, and (2) Multi-graph contrastive learning can improve fairness by dealing with the issues of sparse data and popularity bias. This study provides insights for MOOC platforms enhancing the fairness of CR algorithms by considering the varied knowledge backgrounds of different users. It can potentially mitigate the negative effects on learners’ educational outcomes by recommending courses aligned with their knowledge backgrounds.",article,4,,,,,,,,
Prompt-based event relation identification with Constrained Prefix ATTention mechanism,Knowledge-Based Systems,281,111072,2023,0950-7051,https://doi.org/10.1016/j.knosys.2023.111072,https://www.sciencedirect.com/science/article/pii/S0950705123008225,Hang Zhang and Wenjun Ke and Jianwei Zhang and Zhizhao Luo and Hewen Ma and Zhen Luan and Peng Wang,"Event relation identification, Prompt tuning, Pre-trained language model, Template generation, Information extraction","Event Relation Identification (ERI) aims at mining the inter-event dependencies expressed in event-mentioned sentences. The main challenge of this task lies in recognizing the implicit clue for utterances without context words indicating the relation definitely. When confronting a lack of training samples, mainstream techniques fail to efficiently capture the subtle relations between events because the parameters of neural networks cannot be adequately fitted. Although there is a rising trend of using prompt learning to alleviate such issues, existing methods lack optimization of the prompt and prompts-tuning process. These deficiencies lead to two weaknesses: co-occurrence interference and amphibolous prompting. To this end, this paper proposes a Constrained Prefix ATTention mechanism (CPATT) and incorporates it into the traditional prompt-tuning process. In this fashion, our approach integrates context semantic features into dynamic prompts to mitigate co-occurrence interference. Moreover, CPATT supervises the guide effect of prompts via incorporating mutual exclusivity between categories into the loss function. The experimental results on two widely used datasets demonstrate that our method outperforms all state-of-the-art baselines, including GPT3.5-turbo, in terms of intra- and inter-sentence event relation identification tasks.",article,,,,,,,,,
Exploring perceptions of decision-makers and specialists in defensive machine learning cybersecurity applications: The need for a standardised approach,Computers & Security,139,103694,2024,0167-4048,https://doi.org/10.1016/j.cose.2023.103694,https://www.sciencedirect.com/science/article/pii/S0167404823006041,Omar Alshaikh and Simon Parkinson and Saad Khan,"Machine learning, Cybersecurity, Capabilities, ML application, Perception, Cybercrime, Thematic analysis, Themes","Machine learning (ML) utilisation has achieved a vast global impact. This is evident in the cybersecurity sector, where ML has wide-ranging applications, such as identifying and blocking threats, uncovering unusual software and user behaviour, and many others. However, the increase in successful cyberattacks demonstrates that the effectiveness of ML in cybersecurity applications can be questioned. Although the attacks may be new, ML is often adopted due to its ability to handle diverse and often unforeseen situations – a capability that is not possible using traditional rule-based security mechanisms. As both the rate of attacks and adoption of ML solutions are increasing, there is a need to determine whether ML-based security solutions are meeting the expectations of businesses and whether businesses are genuinely aware of the ML capabilities and limitations. Moreover, current literature shows a significant variation in how ML solutions are evaluated in cybersecurity applications, which might result in a poor understanding of ML capabilities. This paper explores the common perceptions and observations of decision-makers and specialists using ML for cybersecurity regarding its capabilities, implementation, evaluation, and communication. A semi-structured interview is conducted with individuals in various managerial positions to perform this investigation. The finding of this study reveals a pressing need for a standard to manifest ML capabilities. As significant variation in the understanding of Machine Learning Cyber Security (MLCS) capabilities is observed, a standard could help better communicate MLCS capabilities. It is observed that external influences heavily impact ML adoption decisions, potentially leading to misinterpretation of ML capabilities.",article,,,,,,,,,
Legalization of live game streaming through statutory licence in China,Computer Law & Security Review,46,105714,2022,0267-3649,https://doi.org/10.1016/j.clsr.2022.105714,https://www.sciencedirect.com/science/article/pii/S0267364922000619,Zhaoxia Deng and Jyh-An Lee,"Live game streaming, Copyright infringement, Performer, Right of performance, Statutory licensing mechanism","Live game streaming depends on the use of audiovisual images of the games play, which may infringe the copyright of game developers. In recent years, there has been a surge in copyright litigation initiated by game developers against players/streamers in China. The courts needed to resolve several fundamental copyright issues in these cases, such as those pertaining to copyright subject matter, economic right, and copyright limitation. This article provides an in-depth exploration on copyright doctrines relevant to live game streaming industry, following by a proposal of new statutory licensing mechanism specifically for live game streaming. We argue that the proposal can properly balance various interests of different stakeholders in the game industry, such as incentive and proper reasonable compensation for developers, platforms’ dissemination of entertaining information to the audiences, and gamers’ development of professional skills. Under this mechanism, streamers would be allowed to stream games without permission and streaming platform operators would be automatically licensed for game streaming and obliged to remunerate game developers. Compared to the traditional “one-to-one” licensing, the proposed licensing scheme would effectively reduce transaction costs and improve licensing efficiency.",article,,,,,,,,,
"The AI ethics of digital COVID-19 diagnosis and their legal, medical, technological, and operational managerial implications",Artificial Intelligence in Medicine,152,102873,2024,0933-3657,https://doi.org/10.1016/j.artmed.2024.102873,https://www.sciencedirect.com/science/article/pii/S0933365724001155,Christina C. Bartenschlager and Ulrich M. Gassner and Christoph Römmele and Jens O. Brunner and Kerstin Schlögl-Flierl and Paula Ziethmann,"AI ethics, Digital diagnosis of COVID-19, Interdisciplinary stakeholders","The COVID-19 pandemic has given rise to a broad range of research from fields alongside and beyond the core concerns of infectiology, epidemiology, and immunology. One significant subset of this work centers on machine learning-based approaches to supporting medical decision-making around COVID-19 diagnosis. To date, various challenges, including IT issues, have meant that, notwithstanding this strand of research on digital diagnosis of COVID-19, the actual use of these methods in medical facilities remains incipient at best, despite their potential to relieve pressure on scarce medical resources, prevent instances of infection, and help manage the difficulties and unpredictabilities surrounding the emergence of new mutations. The reasons behind this research-application gap are manifold and may imply an interdisciplinary dimension. We argue that the discipline of AI ethics can provide a framework for interdisciplinary discussion and create a roadmap for the application of digital COVID-19 diagnosis, taking into account all disciplinary stakeholders involved. This article proposes such an ethical framework for the practical use of digital COVID-19 diagnosis, considering legal, medical, operational managerial, and technological aspects of the issue in accordance with our diverse research backgrounds and noting the potential of the approach we set out here to guide future research.",article,,,,,,,,,
Contents,Procedia Computer Science,232,iii-xxi,2024,1877-0509,https://doi.org/10.1016/S1877-0509(24)00329-6,https://www.sciencedirect.com/science/article/pii/S1877050924003296,,,,article,,5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023),,,,,,,
Static and adaptive subspace information fusion for indefinite heterogeneous proximity data,Neurocomputing,555,126635,2023,0925-2312,https://doi.org/10.1016/j.neucom.2023.126635,https://www.sciencedirect.com/science/article/pii/S0925231223007580,Maximilian Münch and Manuel Röder and Simon Heilig and Christoph Raab and Frank-Michael Schleif,"Indefinite learning, Multi-modal data, Heterogeneous data analysis, Multiple kernel learning, Kernel fusion, Proximity learning","Heterogeneous data is common in many real-world machine learning applications, such as healthcare, market analysis, environmental sciences, and social media analysis. In these domains, data is often represented in different modalities and, most of the time, in non-vectorial formats, like text, images, and video. Traditional machine learning algorithms are often limited in their ability to effectively analyze and learn from such diverse data types. In this paper, we propose two approaches for such heterogeneous data analysis: static and adaptive subspace kernel fusion. The first approach is a kernel-based method extracting the essential parts of the subspace of each input modality and creating one single fused representation of the data. The second approach utilizes an adaptation step by integrating the weighting of spectral properties into the fusion process in order to improve the data’s representation with respect to a given classification task. Our proposed methods are evaluated on several multi-modal, heterogeneous data sets and demonstrate significant performance improvement compared to other methods in the field. Our results highlight the importance of fusing the underlying subspace information of heterogeneous data for achieving superior performance in machine learning tasks.",article,,,,,,,,,
In the matter of application no 9204959. 2 by Fujitsu Ltd,Computer Law & Security Review,13,269-271,1997,0267-3649,https://doi.org/10.1016/S0267-3649(97)88858-9,https://www.sciencedirect.com/science/article/pii/S0267364997888589,Jackie Wilson,,This case note examines the recent Court of Appeal decision on the patentability of computer program-related inventions.,article,4,,,,,,,,
Fake news detection in low-resource languages: A novel hybrid summarization approach,Knowledge-Based Systems,296,111884,2024,0950-7051,https://doi.org/10.1016/j.knosys.2024.111884,https://www.sciencedirect.com/science/article/pii/S0950705124005185,Jawaher Alghamdi and Yuqing Lin and Suhuai Luo,"Fake news detection, Multilingual NLP, Pre-trained language models, Content summarization","The proliferation of fake news across languages and domains on social media platforms poses a significant societal threat. Current automatic detection methods for low-resource languages (e.g., Swahili, Indonesian and other low-resource languages) face limitations due to two factors: sequential length restrictions in pre-trained language models (PLMs) like multilingual bidirectional encoder representation from transformers (mBERT), and the presence of noisy training data. This work proposes a novel and efficient multilingual fake news detection (MFND) approach that addresses these challenges. Our solution leverages a hybrid extractive and abstractive summarization strategy to extract only the most relevant content from news articles. This significantly reduces data length while preserving crucial information for fake news classification. The pre-processed data is then fed into mBERT for classification. Extensive evaluations on a publicly available multilingual dataset demonstrate the superiority of our approach compared to state-of-the-art (SOTA) methods. Our analysis, both quantitative and qualitative, highlights the strengths of this method, achieving new performance benchmarks and emphasizing the impact of content condensation on model accuracy and efficiency. This framework paves the way for faster, more accurate MFND, fostering more robust information ecosystems.",article,,,,,,,,,
Estimation of realized volatility of cryptocurrencies using CEEMDAN-RF-LSTM,Future Generation Computer Systems,158,219-229,2024,0167-739X,https://doi.org/10.1016/j.future.2024.04.043,https://www.sciencedirect.com/science/article/pii/S0167739X24001729,Huiqing Wang and Yongrong Huang and Zhide Chen and Xu Yang and Xun Yi and Hai Dong and Xuechao Yang,"CEEMDAN, Random forest, LSTM, Cryptocurrency, Realized volatility","Predicting cryptocurrency volatility is crucial for investors, traders, and decision-makers but is complicated by the market’s high non-linearity, volatility, and noise. This paper presents a novel approach, the CEEMDAN-RF-LSTM hybrid model, which is the first to combine the strengths of Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN), Random Forest (RF), and Long Short-Term Memory Network (LSTM) to predict the Realized Volatility (RV) of mainstream cryptocurrencies. The model exploits CEEMDAN’s proficiency in processing non-linear and non-stationary signals, RF’s exceptional feature selection capabilities, and LSTM’s distinctive advantages in dealing with time-series problems. Applied to actual transaction data for Bitcoin (BTC), Ethereum (ETH), and Binance Coin (BNB), empirical results show the superior performance of our model in predicting actual cryptocurrency volatility. These findings contribute to the academic understanding of cryptocurrency volatility and provide practical guidance for quantitative trading strategy development, offering fresh insights and methodologies for related research fields.",article,,,,,,,,,
FABSA: An aspect-based sentiment analysis dataset of user reviews,Neurocomputing,562,126867,2023,0925-2312,https://doi.org/10.1016/j.neucom.2023.126867,https://www.sciencedirect.com/science/article/pii/S0925231223009906,Georgios Kontonatsios and Jordan Clive and Georgia Harrison and Thomas Metcalfe and Patrycja Sliwiak and Hassan Tahir and Aji Ghose,"ABSA, Multi-domain dataset, Deep learning","Aspect-based sentiment analysis (ABSA) aims at automatically extracting aspects of entities and classifying the polarity of each extracted aspect. The majority of available ABSA systems heavily rely on manually annotated datasets to train supervised machine learning models. However, the development of such manually curated datasets is a labour-intensive process and therefore existing ABSA datasets cover only a few domains and they are limited in size. In response, we present FABSA (Feedback ABSA), a new large-scale and multi-domain ABSA dataset of feedback reviews. FABSA consists of approximately 10,500 reviews which span across 10 domains. We conduct a number of experiments to evaluate the performance of state-of-the-art deep learning models when applied to the FABSA dataset. Our results demonstrate that ABSA models can generalise across different domains when trained on our FABSA dataset while the performance of the models is enhanced when using a larger training dataset. Our FABSA dataset is publicly available.11https://github.com/kontonag86/fabsa-dataset.",article,,,,,,,,,
Players’ rights to game mods: Towards a more balanced copyright regime,Computer Law & Security Review,43,105634,2021,0267-3649,https://doi.org/10.1016/j.clsr.2021.105634,https://www.sciencedirect.com/science/article/pii/S0267364921001072,Zhaoxia Deng and Yahong Li,"Player contributed content, Game mods, Terms of service, Social benefits/harm, Right of modding, Community-based approach","In the context of video game, there is a notable convergence between the users and producers of content. There is also a tension between control over created content and innovative uses of that content, which arises from the gap existed between copyright law and the emerging practices of online communities. This paper examines a distinct form of player-contributed content, namely game Mods, through the perspective of social welfare rather than that of content creators. It argues that law is not the only factor affecting copyright owners’ decision-making behavior; social and economic factors also play an essential role. These factors explain why game developers may tolerate or even encourage minor alterations to their works but prohibit total conversion of the Mods. Given that the existing law and terms of service cannot serve as “effective cure” for regulating game Mods, this paper explores the social and economic factors that impact how game corporations address modding, framing these factors in a four-quadrant model according to the relative benefits and harm of Mods to game developers and users/modders. The inconsistency between the letter of the law and its practical application in the modding context suggests a need for law reform. Based on the findings of the above examinations, this paper proposes a two-pronged solution to the modding problem. The first prong concerns the social benefit of game Mods, aiming at changing the copyright regime from being exclusive to non-exclusive, which confers on gamers the legal right to modify video games without permission but obliges them to remunerate the original developers for commercial use of those Mods. The second prong concerns the potential social harm of game Mods and proposes a community-based approach, under which game operators are imposed a common law duty to monitor infringement and to ensure the fair implementation of game developers’ terms of service.",article,,,,,,,,,
Guided evolutionary neural architecture search with efficient performance estimation,Neurocomputing,584,127509,2024,0925-2312,https://doi.org/10.1016/j.neucom.2024.127509,https://www.sciencedirect.com/science/article/pii/S0925231224002807,Vasco Lopes and Miguel Santos and Bruno Degardin and Luís A. Alexandre,"Neural Architecture Search, Convolutional Neural Networks, Evolution, Guided search, AutoML, Zero-proxy estimator","Neural Architecture Search (NAS) methods have been successfully applied to image tasks with excellent results. However, NAS methods are often complex and tend to converge to local minima as soon as generated architectures yield good results. This paper proposes GEA, a novel approach for guided NAS. GEA guides the evolution by exploring the search space by generating and evaluating several architectures in each generation at initialization stage using a zero-proxy estimator, where only the highest-scoring architecture is trained and kept for the next generation. Subsequently, GEA continuously extracts knowledge about the search space without increased complexity by generating several off-springs from an existing architecture at each generation. Moreover, GEA forces exploitation of the most performant architectures by descendant generation while simultaneously driving exploration through parent mutation and favouring younger architectures to the detriment of older ones. Experimental results demonstrate the effectiveness of the proposed method, and extensive ablation studies evaluate the importance of different parameters. Results show that GEA achieves competitive results on all data sets of NAS-Bench-101, NAS-Bench-201 and TransNAS-Bench-101 benchmarks, as well as in the DARTS search space.",article,,,,,,,,,
DTL-IDS: An optimized Intrusion Detection Framework using Deep Transfer Learning and Genetic Algorithm,Journal of Network and Computer Applications,221,103784,2024,1084-8045,https://doi.org/10.1016/j.jnca.2023.103784,https://www.sciencedirect.com/science/article/pii/S1084804523002035,Shahid Latif and Wadii Boulila and Anis Koubaa and Zhuo Zou and Jawad Ahmad,"Cybersecurity, Genetic Algorithm, IIoT, Intrusion Detection, Transfer learning","In the dynamic field of the Industrial Internet of Things (IIoT), the networks are increasingly vulnerable to a diverse range of cyberattacks. This vulnerability necessitates the development of advanced intrusion detection systems (IDSs). Addressing this need, our research contributes to the existing cybersecurity literature by introducing an optimized Intrusion Detection System based on Deep Transfer Learning (DTL), specifically tailored for heterogeneous IIoT networks. Our framework employs a tri-layer architectural approach that synergistically integrates Convolutional Neural Networks (CNNs), Genetic Algorithms (GA), and bootstrap aggregation ensemble techniques. The methodology is executed in three critical stages: First, we convert a state-of-the-art cybersecurity dataset, Edge_IIoTset, into image data, thereby facilitating CNN-based analytics. Second, GA is utilized to fine-tune the hyperparameters of each base learning model, enhancing the model’s adaptability and performance. Finally, the outputs of the top-performing models are amalgamated using ensemble techniques, bolstering the robustness of the IDS. Through rigorous evaluation protocols, our framework demonstrated exceptional performance, reliably achieving a 100% attack detection accuracy rate. This result establishes our framework as highly effective against 14 distinct types of cyberattacks. The findings bear significant implications for the ongoing development of secure, efficient, and adaptive IDS solutions in the complex landscape of IIoT networks.",article,,,,,,,,,
Contents,Procedia Computer Science,231,iii-x,2024,1877-0509,https://doi.org/10.1016/S1877-0509(23)02274-3,https://www.sciencedirect.com/science/article/pii/S1877050923022743,,,,article,,14th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 13th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (EUSPN/ICTH 2023),,,,,,,
EarthVQANet: Multi-task visual question answering for remote sensing image understanding,ISPRS Journal of Photogrammetry and Remote Sensing,212,422-439,2024,0924-2716,https://doi.org/10.1016/j.isprsjprs.2024.05.001,https://www.sciencedirect.com/science/article/pii/S0924271624001990,Junjue Wang and Ailong Ma and Zihang Chen and Zhuo Zheng and Yuting Wan and Liangpei Zhang and Yanfei Zhong,"Visual question answering, Semantic segmentation, Multi-modal fusion, Multi-task learning, Knowledge reasoning","Monitoring and managing Earth’s surface resources is critical to human settlements, encompassing essential tasks such as city planning, disaster assessment, etc. To accurately recognize the categories and locations of geographical objects and reason about their spatial or semantic relations , we propose a multi-task framework named EarthVQANet, which jointly addresses segmentation and visual question answering (VQA) tasks. EarthVQANet contains a hierarchical pyramid network for segmentation and semantic-guided attention for VQA, in which the segmentation network aims to generate pixel-level visual features and high-level object semantics, and semantic-guided attention performs effective interactions between visual features and language features for relational modeling. For accurate relational reasoning, we design an adaptive numerical loss that incorporates distance sensitivity for counting questions and mines hard-easy samples for classification questions, balancing the optimization. Experimental results on the EarthVQA dataset (city planning for Wuhan, Changzhou, and Nanjing in China), RSVQA dataset (basic statistics for general objects), and FloodNet dataset (disaster assessment for Texas in America attacked by Hurricane Harvey) show that EarthVQANet surpasses 11 general and remote sensing VQA methods. EarthVQANet simultaneously achieves segmentation and reasoning, providing a solid benchmark for various remote sensing applications. Data is available at http://rsidea.whu.edu.cn/EarthVQA.htm",article,,,,,,,,,
"Understanding insiders in cloud adopted organizations: A survey on taxonomies, incident analysis, defensive solutions, challenges",Future Generation Computer Systems,158,427-446,2024,0167-739X,https://doi.org/10.1016/j.future.2024.04.033,https://www.sciencedirect.com/science/article/pii/S0167739X24001614,Asha S. and Shanmugapriya D.,"Insider threat, Malicious insider threat, Masqueraders, Traitors, Unintentional insider threat","In cybersecurity, one of the most significant challenges is an insider threat, in which existing researchers must provide an extensive solution aiming at an enhanced security network. This study proposes a comprehensive taxonomy as well as a state-of-the-art research categorization according to the contribution of insider threat incidents and the defensive mechanism utilized against such insiders. The major objective of a proposed categorization is to provide structural information in the field of insider threat based on past research theories for analyzing literature review. The proposed categorization is classified into four groups: (i) dataset analysis, (ii) incident analysis, (iii) defensive solution, and (iv) encountered challenges. However, the respective taxonomies and annotations are included for complete insight into insiders. i.e., existing studies on systematic taxonomy based on incidents of insider threats are presented. The major contribution of this study in the area of insider threat is to deliver the following knowledge to upcoming domain specific researchers: (i) taxonomy in an innovative systematic approach concerning the categories of incidents and determine the possible defensive mechanism against insiders. (ii) a study on available benchmark datasets used by existing research for evaluating the defensive mechanisms. (iii) a brief description of past solutions and frameworks to model insider behavior with the aim of studying existing defensive mechanisms, and (iv) a short discussion of challenges encountered by defensive solutions based on existing research in the area of insider threat.",article,,,,,,,,,
Hybrid Approach To Unsupervised Keyphrase Extraction,Procedia Computer Science,235,1498-1511,2024,1877-0509,https://doi.org/10.1016/j.procs.2024.04.141,https://www.sciencedirect.com/science/article/pii/S1877050924008172,Vijender Singh and Bharat Kumar Bolla,"Keyphrase Extraction, Textual Approach, Graph Based Approach, Hybrid based Approach, F1 Score, Benchmarking, RAKE, FRAKE, BERT, Information Retrieval, TFIDF, Hybrid Ranking, Perfomance Score, Exploratory Data Analysis","The exponential growth of textual data poses a monumental challenge for extracting meaningful knowledge. Manually identifying descriptive keywords or keyphrases for each document is infeasible given the massive daily generated text. Automatic keyphrase extraction is, therefore, essential. However, current techniques struggle with learning the most salient semantic features from lengthy documents. This hybrid keyphrase extraction framework uniquely combines the complementary strengths of graph-based and textual feature methods. Our approach demonstrates improved performance over relying solely on statistical or graphical. Graph-based systems leverage word co- occurrence networks to score importance. Textual methods extract keyphrases using linguistic properties. Together, these complementary techniques overcome the limitations of relying on any strategy. The hybrid approach is evaluated on standard SemEval 2017 Task 10 and SemEval 2010 Task 5 benchmark datasets for scientific paper keyphrase extraction. Performance is quantified using the F1 score relative to human-annotated ground truth keyphrase. Results will quantify effectiveness on long documents with thousands of terms where only a few keywords represent salient concepts. Results show our technique effectively identifies the most salient semantic keywords, overcoming limitations of current techniques that struggle to mix features of graphical or statistical methods. Our experiments demonstrate that the proposed hybrid approach achieves superior F1 scores compared to current state-of-the-art methods on benchmark datasets. These results validate that synergistically combining graph and textual features enables more accurate keyphrase extraction, especially for long documents laden with extraneous terms.",article,,International Conference on Machine Learning and Data Engineering (ICMLDE 2023),,,,,,,
A PORTABLE PROGRAM TO PRESENT COURSEWARE ON MICROCOMPUTERS,,,39-44,1981,,https://doi.org/10.1016/B978-0-08-028111-7.50010-6,https://www.sciencedirect.com/science/article/pii/B9780080281117500106,,,"In computer assisted instruction, certain main functions which have to be carried out by the computer can be distinguished. The performance of these functions will have different software and hardware requirements in different educational environments and both the requirements and the technology to implement them will change over time. It is therefore advantageous to make CAI systems as adaptable as possible. One approach to this is to divide the systems into independent modules each designed to achieve good portability both for software and for hardware. This paper describes such a module which is part of the Modular CAI System Delft. The program makes it possible to present on different types of microcomputers courseware designed using other modules of the system. The program is implemented in Pascal to yield maximum portability on modern microcomputers. Its future and portability are discussed.",incollection,,,PR SMITH,Computer Assisted Learning,Pergamon,Computers and Education,978-0-08-028111-7,Amsterdam,
Chapter Eleven - Designing meaningful metrics to demonstrate ethical supervision of autonomous systems: How do you measure that?,,,189-208,2024,,https://doi.org/10.1016/B978-0-44-315991-6.00017-0,https://www.sciencedirect.com/science/article/pii/B9780443159916000170,Don Brutzman and Curtis Blais,"Ethics, ethical AI, autonomy, metrics, negligence, human–machine teams, Autonomous Vehicle Command Language (AVCL), Mission Execution Ontology (MEO), Dimensions of Autonomous Decision Making (DADM), TestDevOps, virtual environments, trust","Design and testing of meaningful metrics for artificial intelligence (AI) guiding ethical robots holds fundamental importance for useful progress and trustable operations. Moral responsibility and authority for ethical behaviors by remote autonomous systems ultimately lies with the humans responsible for unleashing individual robots. Lines of success or failure are sharply defined when delegating tasks to robots which have the capacity for life-saving or lethal force. Goals, constraints, and metrics that are commonly defined and shared by humans and robots can be mutually understood, formally verifiable as consistent, and further testable in repeatable ways. Metrics for AI are essential, as illustrated by the diverse topics explored throughout this book. It is interesting that commonplace gaps in applied AI often derive from “Here are the measurements we know how to take” which are too easily over-extrapolated or over-simplified into conclusions matching prior preconceptions. In other words, legacy metrics are appealing but might not broadly apply to general situations. We assert that necessary subsequent questions are “How do we define meaningful objectives and outcomes for a current autonomous system?” and “How do we measure those characteristics that indicate expected success/failure?” Since testing drives system evolution, such questions then become “Once we can measure meaningful results, how do we assemble exemplars into test suites that confirm successful completion across ongoing system lifecycles?” This chapter explores potential design principles for metrics that test ethical AI systems, in both real and virtual system frameworks. Such comprehensive test frameworks are essential for achieving meaningful human authority over autonomous robots. Final success is measurable when trust is achieved.",incollection,,,Peggy Wu and Michael Salpukas and Hsin-Fu Wu and Shannon Ellsworth,Trolley Crash,Academic Press,,978-0-443-15991-6,,
Attention-based multimodal sentiment analysis and emotion recognition using deep neural networks,Applied Soft Computing,144,110494,2023,1568-4946,https://doi.org/10.1016/j.asoc.2023.110494,https://www.sciencedirect.com/science/article/pii/S1568494623005124,Ajwa Aslam and Allah Bux Sargano and Zulfiqar Habib,"Sentiment analysis, Emotion recognition, Multimodal attention, Deep neural networks","There has been a growing interest in multimodal sentiment analysis and emotion recognition in recent years due to its wide range of practical applications. Multiple modalities allow for the integration of complementary information, improving the accuracy and precision of sentiment and emotion recognition tasks. However, working with multiple modalities presents several challenges, including handling data source heterogeneity, fusing information, aligning and synchronizing modalities, and designing effective feature extraction techniques that capture discriminative information from each modality. This paper introduces a novel framework called “Attention-based Multimodal Sentiment Analysis and Emotion Recognition (AMSAER)” to address these challenges. This framework leverages intra-modality discriminative features and inter-modality correlations in visual, audio, and textual modalities. It incorporates an attention mechanism to facilitate sentiment and emotion classification based on visual, textual, and acoustic inputs by emphasizing relevant aspects of the task. The proposed approach employs separate models for each modality to automatically extract discriminative semantic words, image regions, and audio features. A deep hierarchical model is then developed, incorporating intermediate fusion to learn hierarchical correlations between the modalities at bimodal and trimodal levels. Finally, the framework combines four distinct models through decision-level fusion to enable multimodal sentiment analysis and emotion recognition. The effectiveness of the proposed framework is demonstrated through extensive experiments conducted on the publicly available Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. The results confirm a notable performance improvement compared to state-of-the-art methods, attaining 85% and 93% accuracy for sentiment analysis and emotion classification, respectively. Additionally, when considering class-wise accuracy, the results indicate that the “angry” emotion and “positive” sentiment are classified more effectively than the other emotions and sentiments, achieving 96.80% and 93.14% accuracy, respectively.",article,,,,,,,,,
Anonymisation of personal data – A missed opportunity for the European Commission,Computer Law & Security Review,30,403-418,2014,0267-3649,https://doi.org/10.1016/j.clsr.2014.05.008,https://www.sciencedirect.com/science/article/pii/S0267364914000946,Francis Aldhouse,"Data protection, Privacy, Anonymisation, Identification, Reidentification, Directive 95/46/EC, Proposed EU Regulation","As early as the 1970's, privacy studies recognised that ‘anonymisation’ needed to be approached with caution. This caution has since been vindicated by the increasing sophistication of techniques for reidentification. Yet the courts in the UK have so far only hesitatingly grappled with the issues involved, while European courts have produced no guidance. Reviewing the limited case law, the author finds the concepts of both ‘personal data’ (which must be protected) and ‘anonymisation’ (which removes this requirement) misleadingly simplistic. A more practical approach would recognise that identifiability sits on a continuum so that regulation needs to be risk-based and proportional. He proposes some consequential changes to the proposed EU Regulation, albeit with modest hopes for success. This paper is a shortened and slightly revised version of a dissertation submitted in April 2013 to Staffordshire University for the award of the degree of LLM.",article,4,,,,,,,,
Multi-turn dialogue comprehension from a topic-aware perspective,Neurocomputing,578,127385,2024,0925-2312,https://doi.org/10.1016/j.neucom.2024.127385,https://www.sciencedirect.com/science/article/pii/S0925231224001565,Xinbei Ma and Yi Xu and Hai Zhao and Zhuosheng Zhang,"Multi-turn dialogue modeling, Topic-aware, Segmentation, Clustering, Response selection","Dialogue Machine Reading Comprehension requires language models to effectively decouple and model multi-turn dialogue passages. As a dialogue development goes after the intentions of participants, its topic may not remain constant throughout the whole passage. Hence, it is non-trivial to detect and leverage the topic shift in dialogue modeling. Topic modeling, although has been widely studied in plain text, deserves far more utilization in dialogue reading comprehension. This paper proposes to model multi-turn dialogues from a topic-aware perspective. This paper starts with a dialogue segmentation algorithm to split a dialogue passage into topic-concentrated fragments in an unsupervised way. Then these fragments are used as topic-aware language processing units in further dialogue comprehension. On one hand, the split segments indict specific topics rather than mixed intentions, thus showing convenience on in-domain topic detection and location. For this task, this paper designs a clustering system with a self-training auto-encoder, and two constructed datasets are built for evaluation. On the other hand, the split segments are an appropriate element of multi-turn dialogue response selection. For this purpose, this paper further presents a novel model, Topic-Aware Dual-Attention Matching (TADAM) Network, which takes topic segments as processing elements and matches response candidates with a dual cross-attention. Empirical studies on three public benchmarks show great improvements over baselines. Our work continues the previous studies on document topic, and brings the dialogue modeling to a novel topic-aware perspective with exhaustive experiments and analyses.",article,,,,,,,,,
Connecting the indispensable roles of IoT and artificial intelligence in smart cities: A survey,Journal of Information and Intelligence,2,261-285,2024,2949-7159,https://doi.org/10.1016/j.jiixd.2024.01.003,https://www.sciencedirect.com/science/article/pii/S2949715924000039,Hoang Nguyen and Dina Nawara and Rasha Kashef,"Smart city, Internet of Things, Artificial intelligence, Machine learning, Deep learning","The pace of society development is faster than ever before, and the smart city paradigm has also emerged, which aims to enable citizens to live in more sustainable cities that guarantee well-being and a comfortable living environment. This has been done by a network of new technologies hosted in real time to track the activities and provide smart solutions for the incoming requests or problems of the citizens. One of the most often used methodologies for creating a smart city is the Internet of Things (IoT). Therefore, the IoT-enabled smart city research topic, which consists of many different domains such as transportation, healthcare, and agriculture, has recently attracted increasing attention in the research community. Further, advances in artificial intelligence (AI) significantly contribute to the growth of IoT. In this paper, we first present the smart city concept, the background of smart city development and the components of the IoT-based smart city. This is followed up by a literature review of the research literature on the most recent IoT-enabled smart cities developments and breakthroughs empowered by AI techniques to highlight the current stage, major trends and unsolved challenges of adopting AI-driven IoT technologies for the establishment of desirable smart cities. Finally, we summarize the paper with a discussion of future research to provide recommendations for research direction in the smart city domain.",article,3,,,,,,,,
"The linguistic summarization and the interpretability, scalability of fuzzy representations of multilevel semantic structures of word-domains",Microprocessors and Microsystems,81,103641,2021,0141-9331,https://doi.org/10.1016/j.micpro.2020.103641,https://www.sciencedirect.com/science/article/pii/S0141933120307882,Cat Ho Nguyen and Thi Lan Pham and Tu N. Nguyen and Cam Ha Ho and Thu Anh Nguyen,"Fuzzy sets, Hedge algebras, Human-computer interaction, Knowledge discovery, Linguistic data summarization, Natural languages","ABSTRACT
The effect of the linguistic (L-) summarization mined from a given dataset D by a human-made method M strongly depends on the fuzzy sets constructed to represent the L-words of dataset attributes. One can observe that the semantics of words is objective (commonly understood the same between human experts,) and word-domains of dataset attributes have their inherent semantic structures. It suggests that to limit the intuitive human influences on such construction, in this study, it requires that the constructed fuzzy set (fs-) representations of the declared word-sets should be the isomorphic images of their words. Such fs-representations of the word-domains are called, in this study, interpretable based on the concept of interpretability in the math-logical theories of A. Tarski et al. It requires the interpretability of the inherent semantic structures of the declared word-sets in their fs-representations structures. With this new feature, the study proposes a data-summarization method that can reveal L-distributions of fuzzy groups of objects represented by a given dataset to the desired dataset L-attribute. The set of all such mined LSs satisfies essential specific human usual L-knowledge, the scalability of its current attributes word-sets, and the current knowledge itself. An experimental study using the Bank Marketing dataset taken from the UCI dataset repository is performed to show the specific advantages of the proposed method.",article,,,,,,,,,
Utilization of generative AI for the characterization and identification of visual unknowns,Natural Language Processing Journal,7,100064,2024,2949-7191,https://doi.org/10.1016/j.nlp.2024.100064,https://www.sciencedirect.com/science/article/pii/S2949719124000128,Kara Combs and Trevor J. Bihl and Subhashini Ganapathy,"Generative AI, Computer vision, Natural language processing, Analogical reasoning","Current state-of-the-art artificial intelligence (AI) struggles with accurate interpretation of out-of-library objects. One method proposed remedy is analogical reasoning (AR), which utilizes abductive reasoning to draw inferences on an unfamiliar scenario given knowledge about a similar familiar scenario. Currently, applications of visual AR gravitate toward analogy-formatted image problems rather than real-world computer vision data sets. This paper proposes the Image Recognition Through Analogical Reasoning Algorithm (IRTARA) and its “generative AI” version called “GIRTARA” which describes and predicts out-of-library visual objects. IRTARA characterizes the out-of-library object through a list of words called the “term frequency list”. GIRTARA uses the term frequency list to predict what the out-of-library object is. To evaluate the quality of the results of IRTARA, both quantitative and qualitative assessments are used, including a baseline to compare the automated methods with human-generated results. The accuracy of GIRTARA’s predictions is calculated through a cosine similarity analysis. This study observed that IRTARA had consistent results in the term frequency list based on the three evaluation methods for the high-quality results and GIRTARA was able to obtain up to 65% match in terms of cosine similarity when compared to the out-of-library object’s true labels.",article,,,,,,,,,
Towards unbalanced multiclass intrusion detection with hybrid sampling methods and ensemble classification,Applied Soft Computing,157,111517,2024,1568-4946,https://doi.org/10.1016/j.asoc.2024.111517,https://www.sciencedirect.com/science/article/pii/S1568494624002916,Thi-Thu-Huong Le and Yeongjae Shin and Myeongkil Kim and Howon Kim,"Intrusion detection, Unbalanced data, Ensemble classification, Undersampling, Oversampling, Hybrid sampling","Intrusion Detection Systems (IDS) play a crucial role in securing computer networks against malicious activities. However, their efficacy is consistently hindered by the persistent challenge of class imbalance in real-world datasets. While various methods, such as resampling techniques, ensemble methods, cost-sensitive learning, data augmentation, and so on, have individually addressed imbalance classification issues, there exists a notable gap in the literature for effective hybrid methodologies aimed at enhancing IDS performance. To bridge this gap, our research introduces an innovative methodology that integrates hybrid undersampling and oversampling strategies within an ensemble classification framework. This novel approach is designed to harmonize dataset distributions and optimize IDS performance, particularly in intricate multi-class scenarios. In-depth evaluations were conducted using well-established intrusion detection datasets, including the Car Hacking: Attack and Defense Challenge 2020 (CHADC2020) and IoTID20. Our results showcase the remarkable efficacy of the proposed methodology, revealing significant improvements in precision, recall, and F1-score metrics. Notably, the hybrid-ensemble method demonstrated an exemplary average F1 score exceeding 98% for both datasets, underscoring its exceptional capability to substantially enhance intrusion detection accuracy. In summary, this research represents a significant contribution to the field of IDS, providing a robust solution to the pervasive challenge of class imbalance. The hybrid framework not only strengthens IDS efficacy but also illuminates the seamless integration of undersampling and oversampling within ensemble classifiers, paving the way for fortified network defenses.",article,,,,,,,,,
Opportunities for incorporating intersectionality into biomedical informatics,Journal of Biomedical Informatics,154,104653,2024,1532-0464,https://doi.org/10.1016/j.jbi.2024.104653,https://www.sciencedirect.com/science/article/pii/S1532046424000716,,"Biomedical Informatics, Intersectionality, Qualitative research, Quantitative research, Mixed-methods research, Systems of privilege and oppression","Many approaches in biomedical informatics (BMI) rely on the ability to define, gather, and manipulate biomedical data to support health through a cyclical research-practice lifecycle. Researchers within this field are often fortunate to work closely with healthcare and public health systems to influence data generation and capture and have access to a vast amount of biomedical data. Many informaticists also have the expertise to engage with stakeholders, develop new methods and applications, and influence policy. However, research and policy that explicitly seeks to address the systemic drivers of health would more effectively support health. Intersectionality is a theoretical framework that can facilitate such research. It holds that individual human experiences reflect larger socio-structural level systems of privilege and oppression, and cannot be truly understood if these systems are examined in isolation. Intersectionality explicitly accounts for the interrelated nature of systems of privilege and oppression, providing a lens through which to examine and challenge inequities. In this paper, we propose intersectionality as an intervention into how we conduct BMI research. We begin by discussing intersectionality’s history and core principles as they apply to BMI. We then elaborate on the potential for intersectionality to stimulate BMI research. Specifically, we posit that our efforts in BMI to improve health should address intersectionality’s five key considerations: (1) systems of privilege and oppression that shape health; (2) the interrelated nature of upstream health drivers; (3) the nuances of health outcomes within groups; (4) the problematic and power-laden nature of categories that we assign to people in research and in society; and (5) research to inform and support social change.",article,,,,,,,,,
Transformer-based text similarity and second language proficiency: A case of written production by learners of Korean,Natural Language Processing Journal,6,100060,2024,2949-7191,https://doi.org/10.1016/j.nlp.2024.100060,https://www.sciencedirect.com/science/article/pii/S2949719124000086,Gyu-Ho Shin and Boo Kyung Jung and Seongmin Mun,"Second language writing, Transformer, Similarity score, Proficiency, Human rating","The present study applies two transformer models (BERT; GPT-2) to analyse argumentative essays produced by two first-language groups (Czech; English) of second-language learners of Korean and investigates how informative similarity scores of learner writing obtained by these models explain general language proficiency in Korean. Results show three major aspects on model performance. First, the relationships between the similarity scores and the proficiency scores differ from the tendencies between the human rating scores and the proficiency scores. Second, the degree to which the similarity scores obtained by each model explain the proficiency scores is asymmetric and idiosyncratic. Third, the performance of the two models is affected by learners’ native language and essay topic. These findings invite the need for researchers and educators to pay attention to how computational algorithms operate, together with learner language characteristics and language-specific properties of the target language, in utilising Natural Language Processing methods and techniques for their research or instructional purposes.",article,,,,,,,,,
Towards a modular architecture for science factories††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d3dd00142c,Digital Discovery,2,1980-1998,2023,2635-098X,https://doi.org/10.1039/d3dd00142c,https://www.sciencedirect.com/science/article/pii/S2635098X2300133X,Rafael Vescovi and Tobias Ginsburg and Kyle Hippe and Doga Ozgulbas and Casey Stone and Abraham Stroka and Rory Butler and Ben Blaiszik and Tom Brettin and Kyle Chard and Mark Hereld and Arvind Ramanathan and Rick Stevens and Aikaterini Vriza and Jie Xu and Qingteng Zhang and Ian Foster,,"ABSTRACT
Advances in robotic automation, high-performance computing (HPC), and artificial intelligence (AI) encourage us to conceive of science factories: large, general-purpose computation- and AI-enabled self-driving laboratories (SDLs) with the generality and scale needed both to tackle large discovery problems and to support thousands of scientists. Science factories require modular hardware and software that can be replicated for scale and (re)configured to support many applications. To this end, we propose a prototype modular science factory architecture in which reconfigurable modules encapsulating scientific instruments are linked with manipulators to form workcells, that can themselves be combined to form larger assemblages, and linked with distributed computing for simulation, AI model training and inference, and related tasks. Workflows that perform sets of actions on modules can be specified, and various applications, comprising workflows plus associated computational and data manipulation steps, can be run concurrently. We report on our experiences prototyping this architecture and applying it in experiments involving 15 different robotic apparatus, five applications (one in education, two in biology, two in materials), and a variety of workflows, across four laboratories. We describe the reuse of modules, workcells, and workflows in different applications, the migration of applications between workcells, and the use of digital twins, and suggest directions for future work aimed at yet more generality and scalability. Code and data are available at https://ad-sdl.github.io/wei2023 and in the ESI.",article,6,,,,,,,,
Usability Analysis of Text Generation by ChatGPT OpenAI Using System Usability Scale Method,Procedia Computer Science,227,381-388,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.10.537,https://www.sciencedirect.com/science/article/pii/S1877050923017040,Angelina Patience Mulia and Pirelli Rahelya Piri and Cuk Tho,"Text Generation, Artificial Intelligence, ChatGPT, System Usability Scale, Questionnaire, SUS, Usability Analysis, OpenAI","The development of artificial intelligence systems has resulted in various AI products including ChatGPT, which is a new product classified as a chatbot. This research aims to ensure that text generation systems such as ChatGPT open AI have the best level of quality and usability and are able to provide a satisfying experience for users. To measure and evaluate the effectiveness, efficiency and user satisfaction of the ChatGPT platform, researchers used the System Usability Scale (SUS) method. This data collection was carried out using an online questionnaire. After the collected data has been tested for validity and reliability, the researchers then analyzed the data results. From the results of the research conducted, the SUS value of the ChatGPT platform is 67.44. This score is included in the marginal high category of class D, with a reasonable or sufficient interpretation. With the results of the analysis per question item, it shows that users tend to agree that the system runs quite effectively, efficiently, well and is easy to understand. Although ChatGPT is able to perform tasks or commands well. However, it should be noted that not all information loaded by ChatGPT is presented in a complete, current and correct manner. This is because the information presented by ChatGPT is only limited to 2021. Because ChatGPT is a new technology and is still under development, further researchers are expected to test other features or ChatGPT to ensure the stability and reliability of the entire ChatGPT system using other research methods.",article,,8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023),,,,,,,
Continuous agile cyber–physical systems architectures based on digital twins,Future Generation Computer Systems,153,350-359,2024,0167-739X,https://doi.org/10.1016/j.future.2023.11.024,https://www.sciencedirect.com/science/article/pii/S0167739X23004326,Alexander Vodyaho and Nataly Zhukova and Radhakrishnan Delhibabu and Alexey Subbotin,"Compute continuum, Digital twin, Digital twin networks, Digital threads, Model synthesis, Continuous architecture, Agile architecture, cyber-physical system","Modern cyber-physical systems, for the most part, are large-scale multilevel heterogeneous distributed systems that integrate subsystems of different kinds and are built on the Internet of Things platforms, where system structure and behavior are not constant. Managing such systems and keeping them in working condition throughout their lifetime is a difficult task. The proposed article discusses one of the possible approaches to solving this problem, based on the use of well-known continuous and agile architecture paradigms. However, there are currently no effective mechanisms for implementing these paradigms. The proposed article suggests a new approach to implementing continuous agile architectures by utilizing digital twins and proposes a reference architecture for a run-time dynamic digital twin. This method is unique because it builds a series of dynamic digital twins that model the system in real time, utilizing data about system events. Build the first models using the models used in earlier stages of the system lifecycle. This gives the following opportunities: i) a way to use dynamic digital twins to implement the continuous agile architecture paradigm; ii) a generalized three-level model of the life cycle of the continuous agile architecture; iii) a reference architecture for dynamic digital twins; and iv) a set of models that are all about using dynamic digital twins. The suggested approach enables the management of heterogeneous multilevel cyber-physical systems with variable structure and behavior variability.",article,,,,,,,,,
SDRNet: Camouflaged object detection with independent reconstruction of structure and detail,Knowledge-Based Systems,299,112051,2024,0950-7051,https://doi.org/10.1016/j.knosys.2024.112051,https://www.sciencedirect.com/science/article/pii/S0950705124006853,Juwei Guan and Xiaolin Fang and Tongxin Zhu and Weiqi Qian,"Camouflaged object detection, Structure and detail reconstruction, Feature enhancement, Feature fusion, Feature decomposition","The simultaneous reconstruction of structure and detail is a prevalent strategy in camouflaged object detection. However, the reconstruction features required for structure and detail exhibit disparities, a facet overlooked in existing methods. Therefore, we present a novel methodology, termed SDRNet, which employs a dual-branch approach for the independent reconstruction of structure and detail, aiming to discern camouflaged targets and their edges. Specifically, we propose a decomposition block to segregate encoded features into distinct structure and detail components. Furthermore, structure enhancement block and detail enhancement block are proposed as feature enhancement methods to boost the capacity of structure and detail information. Subsequently, the introduced structure fusion block and detail fusion block progressively amalgamate the enhanced features. Additionally, the shared feature block is designed to serve as a bridge for the interaction between structure and detail information. Experimental results demonstrate that SDRNet outperforms existing state-of-the-art methods significantly on benchmark datasets. Our code is available at https://github.com/whyandbecause/SDRNet/.",article,,,,,,,,,
BMSE: Blockchain-based multi-keyword searchable encryption for electronic medical records,Computer Standards & Interfaces,89,103824,2024,0920-5489,https://doi.org/10.1016/j.csi.2023.103824,https://www.sciencedirect.com/science/article/pii/S0920548923001058,Fanfan Shen and Lin Shi and Jun Zhang and Chao Xu and Yong Chen and Yanxiang He,"Blockchain, Multiple keyword, Searchable encryption, K-means","The storage of electronic medical records (EMRs) is an area of extensive research, and healthcare systems often delegate this task to cloud service providers (CSP). Typically, CSP transmits the encrypted EMRs to a cloud server with a searchable encryption scheme for easy retrieval. However, the enormous power held by centralized CSP poses a potential threat to patients’ personal privacy, as it can lead to unauthorized access and misuse of medical data by both CSP and data users, such as doctors. This paper proposes a blockchain-based multi-keyword searchable encryption (BMSE) electronic medical record solution. The scheme consists of two parts. On the one hand, our solution involves the integration of blockchain technology and the utilization of advanced encryption standard (AES) for symmetric data encryption. Additionally, we employ attribute-based encryption (ABE) to encrypt the search index. This approach aims to address the issue of excessive power held by centralized CSP, which can potentially result in the compromise of patients’ privacy. On the other hand, we use the K-means algorithm to cluster the documents, and use the relevance score of keywords and documents as the search index to solve the problem of low efficiency of the existing multi-keyword searchable encryption schemes. Finally, we verify the safety of BMSE through safety analysis, and the experimental analysis shows that BMSE improves the search efficiency.",article,,,,,,,,,
A/B testing: A systematic literature review,Journal of Systems and Software,211,112011,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112011,https://www.sciencedirect.com/science/article/pii/S0164121224000542,Federico Quin and Danny Weyns and Matthias Galster and Camila Costa Silva,"A/B testing, Systematic literature review, A/B test engineering","A/B testing, also referred to as online controlled experimentation or continuous experimentation, is a form of hypothesis testing where two variants of a piece of software are compared in the field from an end user’s point of view. A/B testing is widely used in practice to enable data-driven decision making for software development. While a few studies have explored different facets of research on A/B testing, no comprehensive study has been conducted on the state-of-the-art in A/B testing. Such a study is crucial to provide a systematic overview of the field of A/B testing driving future research forward. To address this gap and provide an overview of the state-of-the-art in A/B testing, this paper reports the results of a systematic literature review that analyzed primary studies. The research questions focused on the subject of A/B testing, how A/B tests are designed and executed, what roles stakeholders have in this process, and the open challenges in the area. Analysis of the extracted data shows that the main targets of A/B testing are algorithms, visual elements, and workflow and processes. Single classic A/B tests are the dominating type of tests, primarily based in hypothesis tests. Stakeholders have three main roles in the design of A/B tests: concept designer, experiment architect, and setup technician. The primary types of data collected during the execution of A/B tests are product/system data, user-centric data, and spatio-temporal data. The dominating use of the test results are feature selection, feature rollout, continued feature development, and subsequent A/B test design. Stakeholders have two main roles during A/B test execution: experiment coordinator and experiment assessor. The main reported open problems are related to the enhancement of proposed approaches and their usability. From our study we derived three interesting lines for future research: strengthen the adoption of statistical methods in A/B testing, improving the process of A/B testing, and enhancing the automation of A/B testing.",article,,,,,,,,,
MTLink: Adaptive multi-task learning based pre-trained language model for traceability link recovery between issues and commits,Journal of King Saud University - Computer and Information Sciences,36,101958,2024,1319-1578,https://doi.org/10.1016/j.jksuci.2024.101958,https://www.sciencedirect.com/science/article/pii/S1319157824000478,Yang Deng and Bangchao Wang and Qiang Zhu and Junping Liu and Jiewen Kuang and Xingfu Li,"Issue-commit link recovery, Multi-teacher knowledge distillation, Adaptive multi-task","Traceability links between issues and commits (issue-commit links recovery (ILR)) play a significant role in software maintenance tasks by enhancing developers’ observability in practice. Recent advancements in large language models, particularly pre-trained models, have improved the effectiveness of automated ILR. However, these models’ large parameter sizes and extended training time pose challenges in large software projects. Besides, existing methods often overlook the association and distinction among artifacts, leading to the generation of erroneous links. To mitigate these problems, this paper proposes a novel link recovery method called MTLink. It utilizes multi-teacher knowledge distillation (MTKD) to compress the model and employs an adaptive multi-task strategy to reduce information loss and improve link accuracy. Experiments are conducted on four open-source projects. The results show that (i) MTLink outperforms state-of-the-art methods; (ii) The multi-teacher knowledge distillation maintains accuracy despite model size reduction; (iii) The adaptive multi-task tracing method effectively handles confusion caused by similar artifacts and balances each task. In conclusion, MTLink offers an efficient solution for ILR in software traceability. The code is available at https://zenodo.org/records/10321150.",article,2,,,,,,,,
"The Third BenchCouncil International Symposium on Intelligent Computers, Algorithms, and Applications (IC 2023) Call for Papers","BenchCouncil Transactions on Benchmarks, Standards and Evaluations",3,100123,2023,2772-4859,https://doi.org/10.1016/j.tbench.2023.100123,https://www.sciencedirect.com/science/article/pii/S2772485923000406,,"IC 2023, IC23, Call for papers","Sponsored and organized by the International Open Benchmark Council (BenchCouncil), the IC conference is to provide a pioneering technology map through searching and advancing state-of-the-art and state-of-the-practice in processors, systems, algorithms, and applications for machine learning, deep learning, spiking neural network and other AI techniques across multidisciplinary and interdisciplinary areas. IC 2023 invites manuscripts describing original work in the above areas and topics. All accepted papers will be presented at the IC 2023 conference and published by Springer CCIS (Indexed by EI). The IC conferences have been successfully held for two series from 2019 to 2022 and attracted plenty of paper submissions and participants. IC 2023 will be held on December 4-6, 2023 in Sanya and invites manuscripts describing original work in processors, systems, algorithms, and applications for AI techniques across multidisciplinary and interdisciplinary areas. The conference website is https://www.benchcouncil.org/ic2023/. Important Dates: Paper Submission: July 31, 2023, at 11:59 PM AoE Notification: September 30, 2023, at 11:59 PM AoE Final Papers Due: October 31, 2023, at 11:59 PM AoE Conference Date: December 4-6, 2023 Submission Site: https://ic2023.hotcrp.com/",article,2,,,,,,,,
Fostering security-related citizenship through the employee-supervisor relationship: An examination of supervisor security embodiment,Computers & Security,142,103896,2024,0167-4048,https://doi.org/10.1016/j.cose.2024.103896,https://www.sciencedirect.com/science/article/pii/S0167404824001986,Joshua M. Davis and Deepti Agrawal and Rebekah Austin,"Supervisor security embodiment, Social identity theory of leadership, Leader-member exchange, Behavioral information security","Organizational information security performance is increasingly dependent on employees’ security-related citizenship behaviors that stretch beyond the scope of formal organizational prescription and control. Unfortunately, cultivating enactment of these valued behaviors has proven challenging for many companies. The literature has recognized workplace relationships as important determinants of behavioral security outcomes and extra-role security behaviors (ERBs) in particular. Taken further, an employee's relationship with the immediate supervisor is recognized as one of the most influential relational factors shaping a variety of workplace behaviors, including those related to security. Consistent with these notions, scholars have called for making the employee-supervisor relationship a more central component of behavioral security research and practice. Currently however, beyond recognition of this relationship's importance, the knowledge base is unclear about how it shapes ERB enactment. Because employees view supervisors as both organizational agents and as individuals in their own rights, this relationship has the potential to drive productive or counterproductive security behaviors, depending on how aligned the supervisor's security values are with those of the organization. Yet, the security literature has given surprisingly little consideration to the notion that employees can differ in the extent to which they perceive supervisors as embodying organizational information security values. Responding to this gap, the current study examines how employee-supervisor relations and perceived security-related value alignment between supervisors and the broader organization shape employees’ commitment to organizational information security and ultimately, ERB enactment. Grounded in the social identity theory of leadership (SITL), a research model is developed that positions high-quality employee-supervisor exchange as a direct antecedent of affective commitment to organizational information security, which then serves as a central intrinsic motivational mechanism driving ERB enactment. Further, rooted in SITL's principles on leader prototypicality and supervisor organizational embodiment, employee-perceived value alignment between the immediate supervisor and the organization as a whole—referred to here as supervisor security embodiment (SSE)—is introduced as a critical boundary condition influencing the extent to which employee-supervisor relations drive commitment. Results from model testing empirically demonstrate the value of SSE in explicating how this important relationship shapes workplace ERB enactment, through its influence on affective commitment to organizational information security performance.",article,,,,,,,,,
Using machine learning for continuous updating of meta-analysis in educational context,Computers in Human Behavior,156,108215,2024,0747-5632,https://doi.org/10.1016/j.chb.2024.108215,https://www.sciencedirect.com/science/article/pii/S0747563224000839,Olga Chernikova and Matthias Stadler and Ivan Melev and Frank Fischer,"Machine learning, Abstract screening, Systematic literature review, Meta-analysis","Machine learning and learning analytics are powerful tools that not only support researchers in the detailed measurement and enhancement of learning processes in various learning environments, but also enable the aggregation and synthesis of evidence regarding effective educational practices. This paper describes the development and application of machine learning algorithms aimed at semi-automatic selection of abstracts for a meta-analysis on the effects of simulation-based learning in higher education. The goal was to reduce the workload while also maintaining the transparency and objectivity of the selection process. The algorithms were trained, validated, and tested on a set of 3187 studies on simulation-based learning found in medical and educational databases collected before April 2018. Subsequently, they were utilized to classify abstracts for a follow-up meta-analysis consisting of 2373 studies (published between 2018 and 2020). The aim of training the algorithms was to predict studies’ abstract eligibility based on words and combinations of words used in these abstracts. The application of the algorithms reduced the number of studies that had to be manually screened from 2373 to 711. A total of 458 studies from automatically selected abstracts were included in the full-text screening, indicating the high precision of the algorithms (also compared to the performance of human raters). We conclude that machine learning algorithms can be trained and used to classify abstracts for their eligibility, significantly reducing the workload for the researchers without diminishing objectivity and quality when updating systematic literature reviews with or without a meta-analysis.",article,,,,,,,,,
What is missing in autonomous discovery: open challenges for the community,Digital Discovery,2,1644-1659,2023,2635-098X,https://doi.org/10.1039/d3dd00143a,https://www.sciencedirect.com/science/article/pii/S2635098X23001171,Phillip M. Maffettone and Pascal Friederich and Sterling G. Baird and Ben Blaiszik and Keith A. Brown and Stuart I. Campbell and Orion A. Cohen and Rebecca L. Davis and Ian T. Foster and Navid Haghmoradi and Mark Hereld and Howie Joress and Nicole Jung and Ha-Kyung Kwon and Gabriella Pizzuto and Jacob Rintamaki and Casper Steinmann and Luca Torresi and Shijing Sun,,"ABSTRACT
Self-driving labs (SDLs) leverage combinations of artificial intelligence, automation, and advanced computing to accelerate scientific discovery. The promise of this field has given rise to a rich community of passionate scientists, engineers, and social scientists, as evidenced by the development of the Acceleration Consortium and recent Accelerate Conference. Despite its strengths, this rapidly developing field presents numerous opportunities for growth, challenges to overcome, and potential risks of which to remain aware. This community perspective builds on a discourse instantiated during the first Accelerate Conference, and looks to the future of self-driving labs with a tempered optimism. Incorporating input from academia, government, and industry, we briefly describe the current status of self-driving labs, then turn our attention to barriers, opportunities, and a vision for what is possible. Our field is delivering solutions in technology and infrastructure, artificial intelligence and knowledge generation, and education and workforce development. In the spirit of community, we intend for this work to foster discussion and drive best practices as our field grows.",article,6,,,,,,,,
The landscape of biomedical research,Patterns,5,100968,2024,2666-3899,https://doi.org/10.1016/j.patter.2024.100968,https://www.sciencedirect.com/science/article/pii/S266638992400076X,Rita González-Márquez and Luca Schmidt and Benjamin M. Schmidt and Philipp Berens and Dmitry Kobak,"metascience, publications, PubMed, language models, embeddings, visualization, machine learning, gender bias, retractions","Summary
The number of publications in biomedicine and life sciences has grown so much that it is difficult to keep track of new scientific works and to have an overview of the evolution of the field as a whole. Here, we present a two-dimensional (2D) map of the entire corpus of biomedical literature, based on the abstract texts of 21 million English articles from the PubMed database. To embed the abstracts into 2D, we used the large language model PubMedBERT, combined with t-SNE tailored to handle samples of this size. We used our map to study the emergence of the COVID-19 literature, the evolution of the neuroscience discipline, the uptake of machine learning, the distribution of gender imbalance in academic authorship, and the distribution of retracted paper mill articles. Furthermore, we present an interactive website that allows easy exploration and will enable further insights and facilitate future research.",article,6,,,,,,,,
The prominent and heterogeneous gender disparities in scientific novelty: Evidence from biomedical doctoral theses,Information Processing & Management,61,103743,2024,0306-4573,https://doi.org/10.1016/j.ipm.2024.103743,https://www.sciencedirect.com/science/article/pii/S0306457324001031,Meijun Liu and Zihan Xie and Alex Jie Yang and Chao Yu and Jian Xu and Ying Ding and Yi Bu,"Scientific novelty, Ph.D. graduates, Gender disparities, Doctoral theses, Quantile regression analyses, Mentorship","Scientific novelty is the essential driving force for research breakthroughs and innovation. However, little is known about how early-career scientists pursue novel research paths, and the gender disparities in this process. To address this research gap, this study investigates a comprehensive dataset of 277,288 doctoral theses in the biomedical sciences authored by US Ph.D. graduates. Spanning from 1980 to 2016, the data originates from the ProQuest Dissertations & Theses Database. This study aims to shed light on Ph.D. students’ pursuit of scientific novelty in their doctoral theses and assess gender-related differences in this process. Using a combinatorial approach and a pre-trained Bio-BERT model, we quantify the scientific novelty of doctoral theses based on bio-entities. Applying fractional logistic and quantile regression models, this study reveals a decreasing trend in scientific novelty over time and heterogeneous gender disparities in doctoral theses. Specifically, female students consistently exhibit lower scientific novelty levels than their male peers. Under the supervision of female advisors, students tend to produce doctoral theses that exhibit lower levels of novelty compared to those supervised by male advisors. The significant interaction effect of female students and female advisors suggests that female advisors may amplify gender disparities in scientific novelty. Moreover, heterogeneous gender disparities in scientific novelty are identified, with non-top-tier universities displaying more pronounced disparities, while the gender differences at higher percentile ranges of scientific novelty scores were comparatively more minor. These findings indicate a potential underrepresentation of early-career female scientists pursuing novel research. Notably, the outcomes of this study hold significant policy implications for advancing the careers of female scientists.",article,4,,,,,,,,
Strategic Trends in Artificial Intelligence Through Impact of Computational Science: What Young Scientists Should Expect,Procedia Computer Science,229,1-7,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.12.001,https://www.sciencedirect.com/science/article/pii/S1877050923019919,Alexandra Klimova and Denis Nasonov and Alexander Hvatov and Nikolay O. Nikitin and Sergey V. Ivanov and Anna V. Kalyuzhnaya and Alexander Boukhanovsky,"Artificial Intelligence, Computational Science, Trends, Impact, Young Scientists","This volume presents selected papers of the 12th Young Scientists Conference in Computational Science (YSC'2023). ITMO University annually organises the event with various academic partners to disseminate current trends in Artificial Intelligence and Computational science among young researchers. In this paper, we present our view on major trends and challenges today in front of scientific and industrial society in this promising area.",article,,"12th International Young Scientists Conference in Computational Science, YSC2023",,,,,,,
ChatGPT as an innovative tool for increasing sales in online stores,Procedia Computer Science,225,3450-3459,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.10.340,https://www.sciencedirect.com/science/article/pii/S1877050923014989,Michał Orzoł and Katarzyna Szopik-Depczyńska,"ChatGPT, electronic commerce, e-commerce, online stores","The development of e-commerce is determined by several factors, including digital transformation, the COVID-19 pandemic, changing consumer behavior and product innovations that appear on the market, including ChatGPT which is one of the latest innovations in the field of artificial intelligence and which offers many opportunities for the e-commerce industry. Thus, the main aim of the paper is answering to the research question how ChatGPT can help e-commerce stores improve their customer communication, increase sales conversions, customer service, and build loyalty? In the article, a simple case study of a conversation between authors and an artificial intelligence-based chatbot ChatGPT was introduced. Several questions were asked related to e-commerce sphere.",article,,27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023),,,,,,,
Good machine learning practices: Learnings from the modern pharmaceutical discovery enterprise,Computers in Biology and Medicine,177,108632,2024,0010-4825,https://doi.org/10.1016/j.compbiomed.2024.108632,https://www.sciencedirect.com/science/article/pii/S0010482524007170,Vladimir Makarov and Christophe Chabbert and Elina Koletou and Fotis Psomopoulos and Natalja Kurbatova and Samuel Ramirez and Chas Nelson and Prashant Natarajan and Bikalpa Neupane,"Artificial intelligence, Machine learning, Pharmaceutical, Drug discovery, Best practice, Life sciences","Machine Learning (ML) and Artificial Intelligence (AI) have become an integral part of the drug discovery and development value chain. Many teams in the pharmaceutical industry nevertheless report the challenges associated with the timely, cost effective and meaningful delivery of ML and AI powered solutions for their scientists. We sought to better understand what these challenges were and how to overcome them by performing an industry wide assessment of the practices in AI and Machine Learning. Here we report results of the systematic business analysis of the personas in the modern pharmaceutical discovery enterprise in relation to their work with the AI and ML technologies. We identify 23 common business problems that individuals in these roles face when they encounter AI and ML technologies at work, and describe best practices (Good Machine Learning Practices) that address these issues.",article,,,,,,,,,
Index,,,285-293,2024,,https://doi.org/10.1016/B978-0-44-313943-7.00018-1,https://www.sciencedirect.com/science/article/pii/B9780443139437000181,,,,incollection,,,Shufei Li and Pai Zheng and Lihui Wang,Proactive Human-Robot Collaboration Toward Human-Centric Smart Manufacturing,Elsevier,,978-0-443-13943-7,,
The fusion of fuzzy theories and natural language processing: A state-of-the-art survey,Applied Soft Computing,162,111818,2024,1568-4946,https://doi.org/10.1016/j.asoc.2024.111818,https://www.sciencedirect.com/science/article/pii/S1568494624005921,Ming Liu and Hongjun Zhang and Zeshui Xu and Kun Ding,"Fuzzy theory, Natural language processing, Fusion, Artificial intelligence","Recent years have witnessed a drastic surge in natural language processing (NLP), which is a popular research orientation in artificial intelligence. In contrast to precise numbers, human language is very complex and diverse, with millions of expressions, both spoken and written. It is due to this ambiguity and imprecision that most of the problems in NLP relating to cognition, translation, and understanding are non-trivial. Fuzzy theory, which accepts the fact that ambiguity exists, aims to address and actively quantify conceptual vagueness into messages that can be processed by computers. Following the thread of recent studies, we systematically review the fusion of fuzzy theory and NLP technologies from the aspects of commonly used fuzzy theories in NLP, the NLP tasks fuzzy theories are applied to, the application fields of fusion and the basic paradigms of fusion. Towards the end of this paper, we delineate the constraints and obstacles encountered in current researches, while also endeavoring to suggest avenues for enhancement that may serve as a reference for subsequent scholarly inquiry.",article,,,,,,,,,
A parametric characterization and an ε-approximation scheme for the minimization of a quasiconcave program,Discrete Applied Mathematics,17,39-66,1987,0166-218X,https://doi.org/10.1016/0166-218X(87)90006-0,https://www.sciencedirect.com/science/article/pii/0166218X87900060,Naoki Katoh and Toshihide Ibaraki,,"This paper deals with the problem P of minimizing a quasiconcave function over a given feasible region. We first introduce an auxiliary problem P(λ) with a parametric vector λ such that, for an appropriate λ, its optimal solution is also optimal to the original problem. Based on this, an approximation scheme for P is developed. If P(λ) is polynomially solvable, this becomes a polynomial time approximation scheme. In particular, we show that fully polynomial time approximation schemes can be developed for a large class of stochastic programming problems with 0–1 variables in which cost coefficients are subject to independent normal distributions, if their deterministic versions obtained by replacing cost coefficients by constants have polynomial time algorithms or fully polynomial time approximation schemes (e.g., problems of shortest path, assignment, minimum cut, 0–1 knapsack and minimum directed spanning tree).",article,1,SPECIAL ISSUE,,,,,,,
Technical risk model of machine learning based software project development - A multinational empirical study using modified Delphi-AHP method,Information and Software Technology,171,107449,2024,0950-5849,https://doi.org/10.1016/j.infsof.2024.107449,https://www.sciencedirect.com/science/article/pii/S0950584924000545,Ching-Te Lin and Sun-Jen Huang,"Technical risk assessment, Machine learning, Software project development, Modified Delphi, AHP","Context
The development of machine learning (ML) based software projects has increased significantly over the past decade, introducing new technical risks that rarely or never appear in traditional software development projects.
Objective
This research aims to identify and prioritize the technical risk factors that may lead to the failure of ML-based software development projects.
Method
First, a literature review was conducted to compile a preliminary list of technical risk factors for ML-based software project development. Then, two rounds of the modified Delphi process were conducted with 17 ML experts to review and verify the completeness and appropriateness of the preliminary technical risk factors. A hierarchy of five technical risk categories with 22 technical risk factors was concluded for the analytic hierarchy process (AHP). Then, three rounds of online AHP questionnaires were administered. The consistency ratio (CR) was used to check the respondents’ answers, and the quartile deviation (QD) was applied to assess the consensus on all 96 questions. Finally, we prioritized the technical risk categories and associated technical risk factors.
Results
We found that ",article,,,,,,,,,
BeeFlow: Behavior tree-based Serverless workflow modeling and scheduling for resource-constrained edge clusters,Journal of Systems Architecture,143,102968,2023,1383-7621,https://doi.org/10.1016/j.sysarc.2023.102968,https://www.sciencedirect.com/science/article/pii/S1383762123001479,Ke Luo and Tao Ouyang and Zhi Zhou and Xu Chen,"Edge computing, Serverless computing, Serverless workflow, Behavior tree, Workflow modeling, Workflow scheduling","Serverless computing has gained popularity in edge computing due to its flexible features, including the pay-per-use pricing model, auto-scaling capabilities, and multi-tenancy support. Complex Serverless-based applications typically rely on Serverless workflows (also known as Serverless function orchestration) to express task execution logic, and numerous application- and system-level optimization techniques have been developed for Serverless workflow scheduling. However, there has been limited exploration of optimizing Serverless workflow scheduling in edge computing systems, particularly in high-density, resource-constrained environments such as system-on-chip clusters and single-board-computer clusters. In this work, we discover that existing Serverless workflow scheduling techniques typically assume models with limited expressiveness and cause significant resource contention. To address these issues, we propose modeling Serverless workflows using behavior trees, a novel and fundamentally different approach from existing directed-acyclic-graph- and state machine-based models. Behavior tree-based modeling allows for easy analysis without compromising workflow expressiveness. We further present observations derived from the inherent tree structure of behavior trees for contention-free function collections and awareness of exact and empirical concurrent function invocations. Based on these observations, we introduce BeeFlow, a behavior tree-based Serverless workflow system tailored for resource-constrained edge clusters. Experimental results demonstrate that BeeFlow achieves up to 3.2× speedup in a high-density, resource-constrained edge testbed and 2.5× speedup in a high-profile cloud testbed, compared with the state-of-the-art. BeeFlow also demonstrates superior robustness in scenarios with heavy system workloads.",article,,,,,,,,,
Linguistic-based Mild Cognitive Impairment detection using Informative Loss,Computers in Biology and Medicine,176,108606,2024,0010-4825,https://doi.org/10.1016/j.compbiomed.2024.108606,https://www.sciencedirect.com/science/article/pii/S0010482524006917,,"Mild Cognitive Impairment classification, Informative Loss function, Natural Language Processing, Transformers, Linguistic features detection, I-CONECT dataset","This paper presents a deep learning method using Natural Language Processing (NLP) techniques, to distinguish between Mild Cognitive Impairment (MCI) and Normal Cognitive (NC) conditions in older adults. We propose a framework that analyzes transcripts generated from video interviews collected within the I-CONECT study project, a randomized controlled trial aimed at improving cognitive functions through video chats. Our proposed NLP framework consists of two Transformer-based modules, namely Sentence Embedding (SE) and Sentence Cross Attention (SCA). First, the SE module captures contextual relationships between words within each sentence. Subsequently, the SCA module extracts temporal features from a sequence of sentences. This feature is then used by a Multi-Layer Perceptron (MLP) for the classification of subjects into MCI or NC. To build a robust model, we propose a novel loss function, called InfoLoss, that considers the reduction in entropy by observing each sequence of sentences to ultimately enhance the classification accuracy. The results of our comprehensive model evaluation using the I-CONECT dataset show that our framework can distinguish between MCI and NC with an average area under the curve of 84.75%.",article,,,,,,,,,
Have we found a solution for health misinformation? A ten-year systematic review of health misinformation literature 2013–2022,International Journal of Medical Informatics,188,105478,2024,1386-5056,https://doi.org/10.1016/j.ijmedinf.2024.105478,https://www.sciencedirect.com/science/article/pii/S1386505624001412,Shiyi Zhang and Huiyu Zhou and Yimei Zhu,"Misinformation, Health misinformation, Trust, Solutions to health misinformation","Background
Health misinformation (HM) has emerged as a prominent social issue in recent years, driven by declining public trust, popularisation of digital media platforms and escalating public health crisis. Since the Covid-19 pandemic, HM has raised critical concerns due to its significant impacts on both individuals and society as a whole. A comprehensive understanding of HM and HM-related studies would be instrumental in identifying possible solutions to address HM and the associated challenges.
Methods
Following the PRISMA procedure, 11,739 papers published from January 2013 to December 2022 were retrieved from five electronic databases, and 813 papers matching the inclusion criteria were retained for further analysis. This article critically reviewed HM-related studies, detailing the factors facilitating HM creation and dissemination, negative impacts of HM, solutions to HM, and research methods employed in those studies.
Results
A growing number of studies have focused on HM since 2013. Results of this study highlight that trust plays a significant while latent role in the circuits of HM, facilitating the creation and dissemination of HM, exacerbating the negative impacts of HM and amplifying the difficulty in addressing HM.
Conclusion
For health authorities and governmental institutions, it is essential to systematically build public trust in order to reduce the probability of individuals acceptation of HM and to improve the effectiveness of misinformation correction. Future studies should pay more attention to the role of trust in how to address HM.",article,,,,,,,,,
A multi-graph representation for event extraction,Artificial Intelligence,332,104144,2024,0004-3702,https://doi.org/10.1016/j.artint.2024.104144,https://www.sciencedirect.com/science/article/pii/S0004370224000808,Hui Huang and Yanping Chen and Chuan Lin and Ruizhang Huang and Qinghua Zheng and Yongbin Qin,"Event extraction, Multigraph, Argument multiplexing, Event representation","Event extraction has a trend in identifying event triggers and arguments in a unified framework, which has the advantage of avoiding the cascading failure in pipeline methods. The main problem is that joint models usually assume a one-to-one relationship between event triggers and arguments. It leads to the argument multiplexing problem, in which an argument mention can serve different roles in an event or shared by different events. To address this problem, we propose a multigraph-based event extraction framework. It allows parallel edges between any nodes, which is effective to represent semantic structures of an event. The framework enables the neural network to map a sentence(s) into a structurized semantic representation, which encodes multi-overlapped events. After evaluated on four public datasets, our method achieves the state-of-the-art performance, outperforming all compared models. Analytical experiments show that the multigraph representation is effective to address the argument multiplexing problem and helpful to advance the discriminability of the neural network for event extraction.",article,,,,,,,,,
5 - Framework part II: artificial intelligence + political economics,,,133-184,2024,,https://doi.org/10.1016/B978-0-443-21597-1.00005-6,https://www.sciencedirect.com/science/article/pii/B9780443215971000056,Dominique J. Monlezun,"Political economics, determinants of health, managed strategic competition, medical diplomacy, supply chain resilience, large language models, clean energy, de-risking, diversification, multilateralism, deterrence, defense, development","This chapter considers the political economic or meta-determinants of health for the global public health ecosystem, critical for the scale, scope, and speed of coordinated actions (including in consensus-based governance and financing) to generate equitable and effective global health solutions to urgent shared challenges. Rising international separation and tensions between democracies and autocracies in addition to the Global North and the Global South undermines the health of these regimes and regions and that of humanity. This chapter thus considers global health and artificial intelligence (AI) in their political economic context in the strategic competition of dominant power players, particularly with the governments, militaries, and corporations of the United States and China which account for most of the global health financing and programs along with that of AI’s development and deployment. Failures in managed strategic competition can not only undermine the cooperation required for the AI-driven global public health ecosystem, but they may even imperil it through accelerated and even catastrophic conflicts. This chapter therefore considers the history and foreseeable future of the global public health ecosystem from the structural perspective of political economics, including the underlying values that may provide a durable foundation for coordinated health action. It additionally considers emergent solutions and advances for the health ecosystem toward this including with human security and data sovereignty within Political Liberalism articulating a bridge between the above blocs, while addressing health determinants integrally and globally: social determinants of health, political determinants of health, economic determinants of health, commercial determinants of health, and digital determinants of health. Specific advances include shared global governance, affordable clean energy transition, and affordable AI digital transformation for sustainable development (with deference and deterrence guardrails maximizing cooperation, managing strategic competition, and minimizing conflict). The chapter additionally considers medical diplomacy, multilateral development, deep medicine, large language models (including ChatGPT), commercial fusion, and digital supply chain resilience (with diversification and de-risking), in the context of moving away from an imperial ideological values-driven ruler-based world order to a more sovereign integral values-driven rules-based world order.",incollection,,,Dominique J. Monlezun,Responsible Artificial Intelligence Re-engineering the Global Public Health Ecosystem,Morgan Kaufmann,,978-0-443-21597-1,,
Deception detection using machine learning (ML) and deep learning (DL) techniques: A systematic review,Natural Language Processing Journal,6,100057,2024,2949-7191,https://doi.org/10.1016/j.nlp.2024.100057,https://www.sciencedirect.com/science/article/pii/S2949719124000050,Shanjita Akter Prome and Neethiahnanthan Ari Ragavan and Md Rafiqul Islam and David Asirvatham and Anasuya Jegathevi Jegathesan,"Deception detection, Lie detection, Artificial intelligence, Deep learning, Machine learning, Facial expression","Deception detection is a crucial concern in our daily lives, with its effect on social interactions. The human face is a rich source of data that offers trustworthy markers of deception. The deception detection systems are non-intrusive, cost-effective, and mobile by identifying face expressions. Over the last decade, numerous studies have been conducted on deception/lie detection using several advanced techniques. Researchers have given their attention to inventing more effective and efficient solutions for deception detection. However, there are still a lot of opportunities for innovative deception detection methods. Thus, in this literature review, we conduct the statistical analysis by following the PRISMA protocol and extract various articles from five e-databases. The main objectives of this paper are (i) to explain the overview of machine learning (ML) and deep learning (DL) techniques for deception detection, (ii) to outline the existing literature, and (iii) to address the current challenges and its research prospects for further study. While significant issues in deception detection methods are acknowledged, the review highlights key conclusions and offers a systematic analysis of state-of-the-art techniques, emphasizing contributions and opportunities. The findings illuminate current trends and future research prospects, fostering ongoing development in the field.",article,,,,,,,,,
Thai-language chatbot security: Detecting instruction attacks with XLM-RoBERTa and Bi-GRU,Computers and Electrical Engineering,116,109186,2024,0045-7906,https://doi.org/10.1016/j.compeleceng.2024.109186,https://www.sciencedirect.com/science/article/pii/S0045790624001149,Vajratiya Vajrobol and Brij B. Gupta and Akshat Gaurav,"Instruction attack, Chatbot, Thai language, Bi-GRU, XLM-roBERTa","Instruction attack is a malicious attempt to manipulate a chatbot by providing misleading or harmful prompts to achieve unintended outcomes. Detecting instruction attacks is crucial to protect the integrity and safety of chatbot interactions. In this study, we focus on identifying different types of instruction attacks which includes Goal Hijacking, Prompt Leaking, Reverse Exposure, Role Play Instruction and Unsafe Instruction Topic. Given the widening threat scope and the lack of research thus far in this field in a Thai language-oriented context, our intentions are to develop an effective defence system. We suggest an innovative approach: combining XLM-RoBERTa, a state-of-the art language model, with a Bidirectional Gated Recurrent Unit (Bi-GRU). By combining rigorous experimentation and comprehensive evaluation, our method provides outstanding accuracy of 96.52% , precision 96.50% , Recall and F1-score 96.41%. This research contributes to creating a safer and more trustworthy environment for chatbot-mediated interactions in the Thai language context.",article,,,,,,,,,
"Exploring the Determinants of Artificial Intelligence (AI) Literacy: Digital Divide, Computational Thinking, Cognitive Absorption",Telematics and Informatics,83,102026,2023,0736-5853,https://doi.org/10.1016/j.tele.2023.102026,https://www.sciencedirect.com/science/article/pii/S0736585323000904,Ismail Celik,"AI Literacy, Digital Divide, Computational Thinking, Cognitive Absorption, ChatGPT","To effectively utilize artificial intelligence (AI)-based technologies such as ChatGPT and realize their novel ethical issues, individuals must have a variety of knowledge and skills about AI. Such knowledge and skills have led to the emergence of AI literacy. Despite the importance of AI literacy in everyday life, little is known about its determinants. To better understand the determinants of AI literacy, we attempted to build a research model relying on previous research and different theoretical frameworks. The model incorporated digital divide, cognitive absorption, and computational thinking. As a major finding from the current study, computational thinking was found to be a significant determinant of AI literacy, which facilitate using, recognizing, and evaluating AI-based technologies. Moreover, we found out that individuals with physical access to information and communication technologies (ICTs) are more expected to use and recognize AI. Also, motivation and skills in using ICTs enable individuals to better evaluate the outcomes of AI-based technologies. The findings also showed that convenient access to ICTs contributes to a deep involvement with AI-based technologies in the use. Further, individuals with higher motivation and skills to use AI technologies are likely to have a pleasant experience after using these technologies.",article,,,,,,,,,
Sentence salience contrastive learning for abstractive text summarization,Neurocomputing,593,127808,2024,0925-2312,https://doi.org/10.1016/j.neucom.2024.127808,https://www.sciencedirect.com/science/article/pii/S0925231224005794,Ying Huang and Zhixin Li and Zhenbin Chen and Canlong Zhang and Huifang Ma,"Contrastive learning, Abstractive text summarization, Semantic similarity, Sentence salience","Abstractive Text summarization aims to generate a short summary for a document while preserving salient information. Recently, contrastive learning has been extended from visual representation to summarization tasks. At present, the methods of contrastive learning summarization focus on modeling the global semantics of source documents, targets and candidate summaries to maximize their similarities. However, they ignore the influence of sentence semantics in the document. In this paper, we propose a sentence-level salience contrastive learning method to help the model capture salient information and denoise. The model expresses the sentence salience according to the semantic similarity between the summaries and sentences of the source document, and integrates the similarity distance into the contrastive loss in the form of soft weights. Therefore, our model maximize the similarity between summaries and salient information, while minimizing the similarity between summaries and potential noise. We have verified our method in three widely used datasets, CNN/Daily Mail, XSum and PubMed. The experimental results show that the proposed method can significantly improve the baseline performance and achieve competitive performance in the existing contrastive learning methods.",article,,,,,,,,,
EduCross: Dual adversarial bipartite hypergraph learning for cross-modal retrieval in multimodal educational slides,Information Fusion,109,102428,2024,1566-2535,https://doi.org/10.1016/j.inffus.2024.102428,https://www.sciencedirect.com/science/article/pii/S1566253524002069,Ming Li and Siwei Zhou and Yuting Chen and Changqin Huang and Yunliang Jiang,"Dual generative adversarial network, Cross-modal retrieval, Deep bipartite hypergraph learning, Framelet transform, Multimodal educational slides","In the digital education landscape, cross-modal retrieval (CMR) from multimodal educational slides represents a significant challenge, particularly because of the complex nature of academic content, which includes images, diagrams, equations, and tables across various subjects such as mathematics and biology. Current CMR systems are primarily designed for “(natural) image to text” interactions (or vice versa) and inadequately address real-world educational scenarios. This study presents EduCross, a novel framework devised to enhance CMR within multimodal educational slides, which is a domain in which traditional retrieval systems fall short. Recognizing the imperative for a system that is tailored to the educational context, EduCross integrates dual adversarial bipartite hypergraph learning, harnessing the capabilities of generative adversarial networks with figure-text dual channels. This powerful combination facilitates robust bidirectional mapping, allowing for the precise association of figures with their descriptive spoken language segments and ensuring a comprehensive CMR experience. Specifically, we develop framelet-based deep bipartite hypergraph neural networks that effectively manage the high-order relationships between diverse educational content types and various types of slide figures. Our experimental results underscore the superior performance of EduCross, demonstrating its effectiveness through the use of the real Multimodal Lecture Presentations dataset that mirrors authentic educational settings. These outcomes highlight the significant advancements of EduCross over existing methods, marking a leap forward in the accurate retrieval of multimodal educational content.",article,,,,,,,,,
An intelligent conversational agent for educating the general public about HIV,Neurocomputing,563,126902,2024,0925-2312,https://doi.org/10.1016/j.neucom.2023.126902,https://www.sciencedirect.com/science/article/pii/S0925231223010251,Joan C. Moreno and Victor Sánchez-Anguix and Juan M. Alberola and Vicente Julián and Vicent Botti,"Conversational agent, Natural language understanding, HIV, Health informatics, Human–computer interaction, Empirical study","The article presents a Spanish conversational agent that focuses on raising awareness about HIV. The agent aims to provide natural communication, personalized information based on user requests, and a centralized source of information about HIV. The core of the agent’s logic is formed by a natural language understanding conversational model, supported by a knowledge base of medical responses and real conversations with users. An empirical study was conducted with 71 users to evaluate the agent’s effectiveness as a sexual health educational tool. The results show that HIV knowledge raised by 18.44% after using the agent. That, and the positive user experience support the agent’s role as a tool for raising HIV prevention and awareness.",article,,,,,,,,,
Index,,,301-306,2024,,https://doi.org/10.1016/B978-0-443-19096-4.20001-1,https://www.sciencedirect.com/science/article/pii/B9780443190964200011,,,,incollection,,,Muskan Garg and Deepika Koundal,Emotional AI and Human-AI Interactions in Social Networking,Academic Press,,978-0-443-19096-4,,
Cutting through the noise to motivate people: A comprehensive analysis of COVID-19 social media posts de/motivating vaccination,Natural Language Processing Journal,,100085,2024,2949-7191,https://doi.org/10.1016/j.nlp.2024.100085,https://www.sciencedirect.com/science/article/pii/S2949719124000335,Ashiqur Rahman and Ehsan Mohammadi and Hamed Alhoori,"Misinformation, Motivation, Vaccine hesitancy, Science communication, Social media, Social psychology","The COVID-19 pandemic exposed significant weaknesses in the healthcare information system. The overwhelming volume of misinformation on social media and other socioeconomic factors created extraordinary challenges to motivate people to take proper precautions and get vaccinated. In this context, our work explored a novel direction by analyzing an extensive dataset collected over two years, identifying the topics de/motivating the public about COVID-19 vaccination. We analyzed these topics based on time, geographic location, and political orientation. We noticed that while the motivating topics remain the same over time and geographic location, the demotivating topics rapidly. We also identified that intrinsic motivation, rather than external mandate, is more advantageous to inspire the public. This study addresses scientific communication and public motivation in social media. It can help public health officials, policymakers, and social media platforms develop more effective messaging strategies to cut through the noise of misinformation and educate the public about scientific findings.",article,,,,,,,,,
Deep transfer learning for automatic speech recognition: Towards better generalization,Knowledge-Based Systems,277,110851,2023,0950-7051,https://doi.org/10.1016/j.knosys.2023.110851,https://www.sciencedirect.com/science/article/pii/S0950705123006019,Hamza Kheddar and Yassine Himeur and Somaya Al-Maadeed and Abbes Amira and Faycal Bensaali,"Automatic speech recognition, Deep transfer learning, Fine-tuning, Domain adaptation, Models fusion, Large language model","Automatic speech recognition (ASR) has recently become an important challenge when using deep learning (DL). It requires large-scale training datasets and high computational and storage resources. Moreover, DL techniques and machine learning (ML) approaches in general, hypothesize that training and testing data come from the same domain, with the same input feature space and data distribution characteristics. This assumption, however, is not applicable in some real-world artificial intelligence (AI) applications. Moreover, there are situations where gathering real data is challenging, expensive, or rarely occurring, which cannot meet the data requirements of DL models. deep transfer learning (DTL) has been introduced to overcome these issues, which helps develop high-performing models using real datasets that are small or slightly different but related to the training data. This paper presents a comprehensive survey of DTL-based ASR frameworks to shed light on the latest developments and helps academics and professionals understand current challenges. Specifically, after presenting the DTL background, a well-designed taxonomy is adopted to inform the state-of-the-art. A critical analysis is then conducted to identify the limitations and advantages of each framework. Moving on, a comparative study is introduced to highlight the current challenges before deriving opportunities for future research.",article,,,,,,,,,
Harnessing customized AI to create voice of customer via GPT3.5,Advanced Engineering Informatics,61,102462,2024,1474-0346,https://doi.org/10.1016/j.aei.2024.102462,https://www.sciencedirect.com/science/article/pii/S1474034624001101,Mohammad Shahin and F. Frank Chen and Ali Hosseinzadeh,"ChatGPT, VoC, Lean Six Sigma, Industry 5.0, Artificial General Intelligence","The integration of customer feedback is universally acknowledged as crucial in the product development process. Yet, traditional feedback collection methods employed by companies, such as interviews and surveys, have remained mainly unchanged and come with limitations. Interviews often fail to accurately capture customers' needs due to communication barriers, while surveys prompt only incremental changes instead of inspiring innovation. This challenge is compounded in the service industry, where feedback is intangible and more difficult to quantify. Text analysis presents a promising solution to delve into customer preferences more deeply, providing insights that can guide the development of new products and services. Our research advances the use of generative AI, specifically the GPT engine, beyond its conventional role as a chatbot. We innovate by adapting it to extract actionable insights from customer-service interactions, offering real-time, valuable data for decision-making and representing a significant leap forward in Voice of the Customer (VoC) analysis.",article,,,,,,,,,
Application of machine learning in affordable and accessible insulin management for type 1 and 2 diabetes: A comprehensive review,Artificial Intelligence in Medicine,151,102868,2024,0933-3657,https://doi.org/10.1016/j.artmed.2024.102868,https://www.sciencedirect.com/science/article/pii/S0933365724001106,Maryam Eghbali-Zarch and Sara Masoud,"Insulin treatment, Diabetes management, Insulin affordability, Machine learning, Supervised learning","Proper insulin management is vital for maintaining stable blood sugar levels and preventing complications associated with diabetes. However, the soaring costs of insulin present significant challenges to ensuring affordable management. This paper conducts a comprehensive review of current literature on the application of machine learning (ML) in insulin management for diabetes patients, particularly focusing on enhancing affordability and accessibility within the United States. The review encompasses various facets of insulin management, including dosage calculation and response, prediction of blood glucose and insulin sensitivity, initial insulin estimation, resistance prediction, treatment adherence, complications, hypoglycemia prediction, and lifestyle modifications. Additionally, the study identifies key limitations in the utilization of ML within the insulin management literature and suggests future research directions aimed at furthering accessible and affordable insulin treatments. These proposed directions include exploring insurance coverage, optimizing insulin type selection, assessing the impact of biosimilar insulin and market competition, considering mental health factors, evaluating insulin delivery options, addressing cost-related issues affecting insulin usage and adherence, and selecting appropriate patient cost-sharing programs. By examining the potential of ML in addressing insulin management affordability and accessibility, this work aims to envision improved and cost-effective insulin management practices. It not only highlights existing research gaps but also offers insights into future directions, guiding the development of innovative solutions that have the potential to revolutionize insulin management and benefit patients reliant on this life-saving treatment.",article,,,,,,,,,
Extracting goal models from natural language requirement specifications,Journal of Systems and Software,211,111981,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.111981,https://www.sciencedirect.com/science/article/pii/S0164121224000244,Souvick Das and Novarun Deb and Agostino Cortesi and Nabendu Chaki,"Natural language requirements, Natural language processing, Transformer model, Entity type recognition, Contextual vector, Synonymy vector","Unstructured (or, semi-structured) natural language is mostly used to capture the requirement specifications both for legacy software systems and for modern day software systems. The adoption of a formal approach to the specification of the requirements, using goal models, enables rigorous and formal inspections while analyzing the requirements for satisfiability, consistency, completeness, conflicts and ambiguities. However, such a formal approach is often considered burdening for the analysts’ activity as it requires additional skills, and is therefore, discarded a priori. This works aims to bridge the gap between natural language requirement specifications and efficient goal model analysis techniques. We propose a framework that uses extensive natural language processing techniques to transform a set of unstructured natural language requirement specifications to the corresponding goal model. We combine techniques such as parts-of-speech tagging, dependency parsing, contextual and synonymy vector generation with the FiBER transformer model. An extensive unbiased crowd-sourced evaluation of the proposed framework has been performed, showing an acceptability rate (total and partial combined) of 95%. Time and space analyses of our framework also demonstrate the scalability of the proposed solution.",article,,,,,,,,,
Automating modern code review processes with code similarity measurement,Information and Software Technology,173,107490,2024,0950-5849,https://doi.org/10.1016/j.infsof.2024.107490,https://www.sciencedirect.com/science/article/pii/S0950584924000958,Yusuf Kartal and E. Kaan Akdeniz and Kemal Özkan,"Modern code review, Vectorization, Code similarity, Information retrieval","Context:
Modern code review is a critical component in software development processes, as it ensures security, detects errors early and improves code quality. However, manual reviews can be time-consuming and unreliable. Automated code review can address these issues. Although deep-learning methods have been used to recommend code review comments, they are expensive to train and employ. Instead, information retrieval (IR)-based methods for automatic code review are showing promising results in efficiency, effectiveness, and flexibility.
Objective:
Our main objective is to determine the optimal combination of the vectorization method and similarity to measure what gives the best results in an automatic code review, thereby improving the performance of IR-based methods.
Method:
Specifically, we investigate different vectorization methods (Word2Vec, Doc2Vec, Code2Vec, and Transformer) that differ from previous research (TF-IDF and Bag-of-Words), and similarity measures (Cosine, Euclidean, and Manhattan) to capture the semantic similarities between code texts. We evaluate the performance of these methods using standard metrics, such as Blue, Meteor, and Rouge-L, and include the run-time of the models in our results.
Results:
Our results demonstrate that the Transformer model outperforms the state-of-the-art method in all standard metrics and similarity measurements, achieving a 19.1% improvement in providing exact matches and a 6.2% improvement in recommending reviews closer to human reviews.
Conclusion:
Our findings suggest that the Transformer model is a highly effective and efficient approach for recommending code review comments that closely resemble those written by humans, providing valuable insight for developing more efficient and effective automated code review systems.",article,,,,,,,,,
Security provision for protecting intelligent sensors and zero touch devices by using blockchain method for the smart cities,Microprocessors and Microsystems,90,104503,2022,0141-9331,https://doi.org/10.1016/j.micpro.2022.104503,https://www.sciencedirect.com/science/article/pii/S0141933122000631,,"Authentication, Botnet, Ddos, Brute force, Port scanning, Corda virtual machine, Mirai, Manufacturer usage description, Owasp, Vulnerability","Internet of Things (IoT) networks has gained popularity due to their amazing and cost-effective services and one of the main areas in smart cities. The stability of these networks is based on stable and secure data transmission without any vulnerabilities present used devices. Distributed Denial of Services (DDoS) attacks have brought critical interruptions in IoT services and significantly damage the network. In DDoS attacks, attackers utilize botnets, with the capability of frequently exploiting the millions of IoT devices around the globe. After the source code of Mirai malware is loaded on GitHub, the threats are significantly increased. Manufacturer Usage Description (MUD) is an embedded software standard for IoT device makers to advertise device specifications, including the intended communication patterns when it connects to the network. Even though the MUD mechanism is promising exertion, still there is a need for evaluating its viability, recognize its limits, and upgrade its architecture to reduce shortcomings in its architecture as well as to increase its effectiveness. This standard neither identifies the vulnerability path before the creation of the MUD profile. Thus, it is possible to exploit an IoT device even after the MUD profile is issued to the device by manipulating the vulnerabilities in the device. By keeping in mind this situation, this paper discusses the limitations of MUD in detail and proposed a framework to identify the patch and default vulnerabilities by using blockchain method before the generation/creation of MUD profiles. The proposed framework can also mitigate open ports, DDoS attacks, and Brute force attacks. The experiment results show the identification, elimination, and sharing of vulnerability report with vendors and significantly minimized the risk of IoT device exploitation.",article,,,,,,,,,
A review of green artificial intelligence: Towards a more sustainable future,Neurocomputing,,128096,2024,0925-2312,https://doi.org/10.1016/j.neucom.2024.128096,https://www.sciencedirect.com/science/article/pii/S0925231224008671,Verónica Bolón-Canedo and Laura Morán-Fernández and Brais Cancela and Amparo Alonso-Betanzos,"Green machine learning, Sustainability, Green-by AI, Green-in AI","Green artificial intelligence (AI) is more environmentally friendly and inclusive than conventional AI, as it not only produces accurate results without increasing the computational cost but also ensures that any researcher with a laptop can perform high-quality research without the need for costly cloud servers. This paper discusses green AI as a pivotal approach to enhancing the environmental sustainability of AI systems. Described are AI solutions for eco-friendly practices in other fields (green-by AI), strategies for designing energy-efficient machine learning (ML) algorithms and models (green-in AI), and tools for accurately measuring and optimizing energy consumption. Also examined are the role of regulations in promoting green AI and future directions for sustainable ML. Underscored is the importance of aligning AI practices with environmental considerations, fostering a more eco-conscious and energy-efficient future for AI systems.",article,,,,,,,,,
"Sentiment analysis methods, applications, and challenges: A systematic literature review",Journal of King Saud University - Computer and Information Sciences,36,102048,2024,1319-1578,https://doi.org/10.1016/j.jksuci.2024.102048,https://www.sciencedirect.com/science/article/pii/S131915782400137X,Yanying Mao and Qun Liu and Yu Zhang,"Sentiment analysis, Methods, Applications, Large language models, Challenges","With the expansion of Internet-based applications, the number of comments shows explosive growth. Analyzing the attitudes and emotions behind comments provides powerful assistance for businesses, governments, and scholars. However, it is hard to effectively extract user’s attitude from the massive amounts of comments. Sentiment analysis (SA) provides an automatic, fast and efficient tool to identify reviewers’ opinions and sentiments. However, the existing literature reviews cover a limited number of studies or have a narrow field of studies for sentiment analysis. This paper provided a systematic literature review of sentiment analysis methods, applications, and challenges. This systematic literature review gives insights into the goal of the sentiment analysis task, offers comparisons of different techniques, investigates the application domains of sentiment analysis, highlights the challenges and limitations encountered by scholars, provides suggestions on possible solutions and explores the future research directions. The study’s findings emphasize the significant role of artificial intelligence technologies in automatic text sentiment analysis and highlight the importance of sentiment analysis in people’s production and life. This research not only contributes to the existing sentiment analysis knowledge body but also provides references to scholars and practitioners in choosing a suitable methodology and good practices to perform sentiment analysis.",article,4,,,,,,,,
"Evidence, my Dear Watson: Abstractive dialogue summarization on learnable relevant utterances",Neurocomputing,572,127132,2024,0925-2312,https://doi.org/10.1016/j.neucom.2023.127132,https://www.sciencedirect.com/science/article/pii/S0925231223012559,Paolo Italiani and Giacomo Frisoni and Gianluca Moro and Antonella Carbonaro and Claudio Sartori,"Abstractive dialogue summarization, Input augmentation, Text classification, Gumbel-softmax trick, Interpretable natural language processing","Abstractive dialogue summarization requires distilling and rephrasing key information from noisy multi-speaker documents. Combining pre-trained language models with input augmentation techniques has recently led to significant research progress. However, existing solutions still struggle to select relevant chat segments, primarily relying on open-domain and unsupervised annotators not tailored to the actual needs of the summarization task. In this paper, we propose DearWatson, a task-aware utterance-level annotation framework for improving the effectiveness and interpretability of pre-trained dialogue summarization models. Precisely, we learn relevant utterances in the source document and mark them with special tags, that then act as supporting evidence for the generated summary. Quantitative experiments are conducted on two datasets made up of real-life messenger conversations. The results show that DearWatson allows model attention to focus on salient tokens, achieving new state-of-the-art results in three evaluation metrics, including semantic and factuality measures. Human evaluation proves the superiority of our solution in semantic consistency and recall. Finally, extensive ablation studies confirm each module’s importance, also exploring different annotation strategies and parameter-efficient fine-tuning of large generative language models.",article,,,,,,,,,
"Exploring the impact of ChatGPT on art creation and collaboration: Benefits, challenges and ethical implications",Telematics and Informatics Reports,14,100138,2024,2772-5030,https://doi.org/10.1016/j.teler.2024.100138,https://www.sciencedirect.com/science/article/pii/S2772503024000240,Sijin Zhu and Zheng Wang and Yuan Zhuang and Yuyang Jiang and Mengyao Guo and Xiaolin Zhang and Ze Gao,"Creative AI, HumanAI collaboration, Language models, Interactive AI literacy","This paper examines the chaos caused by introducing advanced language models, specifically ChatGPT, to art. Our focus is on the potential impact of ChatGPT on art creation and collaboration. We explore how it has been utilized to generate art and assist in creative writing and how it facilitates collaboration between artists. This exploration includes an investigation into the use of AI in creating art, music, and literature, emphasizing ChatGPT’s role in generating poetry and prose and its ability to provide valuable suggestions for sentence structure and word choice in creative writing. We conduct case studies and interviews with diverse artists and AI experts to understand the benefits and challenges of using ChatGPT in the creative process. Our findings reveal that artists find ChatGPT helpful in generating new ideas, overcoming creative blocks, and improving the quality of their work. It enables remote collaboration between artists by providing a real-time communication and idea-sharing platform. However, ethical concerns relating to authorship ownership and authenticity have emerged. Artists fear using ChatGPT may lead to losing their artistic identity and ownership of their work. While our data suggests that ChatGPT holds the potential to transform the art world, careful consideration must be given to the ethical implications of AI in art. We recommend future research to focus on developing guidelines for the responsible use of AI in art, safeguarding artists’ rights, and preserving artistic authenticity.",article,,,,,,,,,
Pre-trained language models in medicine: A survey,Artificial Intelligence in Medicine,154,102904,2024,0933-3657,https://doi.org/10.1016/j.artmed.2024.102904,https://www.sciencedirect.com/science/article/pii/S0933365724001465,Xudong Luo and Zhiqi Deng and Binxia Yang and Michael Y. Luo,"Natural language processing, Medical science, Healthcare, Pre-trained language model, BERT, GPT","With the rapid progress in Natural Language Processing (NLP), Pre-trained Language Models (PLM) such as BERT, BioBERT, and ChatGPT have shown great potential in various medical NLP tasks. This paper surveys the cutting-edge achievements in applying PLMs to various medical NLP tasks. Specifically, we first brief PLMS and outline the research of PLMs in medicine. Next, we categorise and discuss the types of tasks in medical NLP, covering text summarisation, question-answering, machine translation, sentiment analysis, named entity recognition, information extraction, medical education, relation extraction, and text mining. For each type of task, we first provide an overview of the basic concepts, the main methodologies, the advantages of applying PLMs, the basic steps of applying PLMs application, the datasets for training and testing, and the metrics for task evaluation. Subsequently, a summary of recent important research findings is presented, analysing their motivations, strengths vs weaknesses, similarities vs differences, and discussing potential limitations. Also, we assess the quality and influence of the research reviewed in this paper by comparing the citation count of the papers reviewed and the reputation and impact of the conferences and journals where they are published. Through these indicators, we further identify the most concerned research topics currently. Finally, we look forward to future research directions, including enhancing models’ reliability, explainability, and fairness, to promote the application of PLMs in clinical practice. In addition, this survey also collect some download links of some model codes and the relevant datasets, which are valuable references for researchers applying NLP techniques in medicine and medical professionals seeking to enhance their expertise and healthcare service through AI technology.",article,,,,,,,,,
MATOP: An interactive FORTRAN 77 program for solving problems in geometrical crystallography,Computers & Geosciences,14,37-53,1988,0098-3004,https://doi.org/10.1016/0098-3004(88)90051-9,https://www.sciencedirect.com/science/article/pii/0098300488900519,M.B. Boisen and G.V. Gibbs,"Geometrical crystallography, Crystal structure drawing, Symmetry, Isometrics, Matrix algebra","The interactive FORTRAN 77 program MATOP is designed to help students and researchers in crystallography calculate the geometrical properties of crystals. The applications of MATOP include calculating lengths and angles involved face poles, zones and bonds, determining normals to planes of atoms and transforming atomic coordinates for viewing a crystal structure down any line or perpendicular to any plane, and determining symmetries. The program assists in the solution of these problems by determining metrical matrices, change of basis matrices, matrix representations of isometrics, inverses of matrices, inner products, and cross products with respect to any given basis. The program can store up to 15 3 x 3 matrices and 12 3-dimensional vectors. The program was written to apply the theory of matrix and vector algebra to these crystallographic problems as discussed in our book Mathematical Crystallography.",article,1,,,,,,,,
"Can ChatGPT Challenge the Scientific Impact of Published Research, Particularly in the Context of Industry 4.0 and Smart Manufacturing?",Procedia Computer Science,232,2540-2550,2024,1877-0509,https://doi.org/10.1016/j.procs.2024.02.072,https://www.sciencedirect.com/science/article/pii/S1877050924002497,Vagan Terziyan and Olena Kaikova and Mariia Golovianko and Oleksandra Vitko,"Artificial Intelligence, ChatGPT, Industry 4.0, Smart Manufacturing, academic impact","The released ChatGPT as a powerful language model is capable of assisting with a wide range of tasks, including answering questions, summarizing, paraphrasing, proofreading, classifying, and integrating texts. In this study, we tested ChatGPT capability to assist researchers in evaluating the academic articles’ contribution. We suggest a dialogue schema in which ChatGPT is asked to answer research questions from the target article and then to compare its own answers with the answers from the article. Finally, ChatGPT is asked to integrate both solutions coherently. We experimented with Proceedings of ISM-2022 Conference on Industry 4.0 and Smart Manufacturing, utilizing explicit research questions. The chat context enabled assessing studied articles’ contributions to Industry 4.0, uncovering advancements beyond the state-of-the-art. However, ChatGPT demonstrates limitations in content understanding and contribution evaluation. We conclude that while it collaborates with humans on academic tasks, human guidance remains essential, while ChatGPT's assistance efficiently complements traditional academic processes.",article,,5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023),,,,,,,
Excitements and concerns in the post-ChatGPT era: Deciphering public perception of AI through social media analysis,Telematics and Informatics,92,102158,2024,0736-5853,https://doi.org/10.1016/j.tele.2024.102158,https://www.sciencedirect.com/science/article/pii/S0736585324000625,Weihong Qi and Jinsheng Pan and Hanjia Lyu and Jiebo Luo,"Generative AI, Public opinion, Social media","As AI systems become increasingly prevalent in various aspects of daily life, gaining a comprehensive understanding of public perception towards these AI systems has become increasingly essential for several reasons such as ethical considerations, user experience, fear, disinformation, regulation, collaboration, and co-creation. In this study, we investigate how mass social media users perceive the recent rise of AI frameworks such as ChatGPT. We collect a total of 33,912 comments in 388 unique subreddits spanning from November 30, 2022 to June 8, 2023 using a list of AI-related keywords. We employ a combination of thematic and sentiment analysis, using advanced natural language processing techniques. Specifically, we use BERTopic to uncover the major themes regarding AI on Reddit. Our findings indicate that technology-focused subreddits primarily discuss the technical dimensions of AI, while non-technical subreddits more often address societal impacts, such as job displacement concerns. The disparity in focus between subreddits suggests a gap in the public understanding of AI. We leverage GPT-3.5 with zero-shot prompting and LIWC to analyze the sentiment and perception of AI among individual users. Through a comprehensive sentiment and emotion analysis, we discover that tech-centric communities exhibit greater polarization compared to non-tech communities when discussing AI topics. This suggests that individuals with a deeper understanding or familiarity with AI technologies might have more divided opinions, possibly reflecting a mix of optimism about technological advancements and skepticism about potential impacts. This research contributes to our broader understanding of public opinion surrounding artificial intelligence.",article,,,,,,,,,
Chapter 1 - Adverse effects of intelligent support of CSCL—the ethics of conversational agents,,,3-23,2024,,https://doi.org/10.1016/B978-0-443-18851-0.00015-9,https://www.sciencedirect.com/science/article/pii/B9780443188510000159,Birk Thierfelder and Pantelis M. Papadopoulos and Armin Weinberger and Stavros Demetriadis and Stergios Tegos,"CSCL, education, chatbots, ethical AI, Artificial Intelligence, AI framework, conversational agent","The requirement for scaffolded guidance in computer-supported collaborative learning (CSCL) has prompted researchers to investigate the potential of using AI-supported conversational agents (CAs) or chatbots as a means of supporting learners in CSCL environments. Recent advances in the field have shown promise for the development of adaptive systems that can effectively guide learners throughout the CSCL process. However, CSCL research has problems communicating how such technologies are helping or hindering constructive, ethical collaborations. AI ethics, being a fractured field, with many parallel high-level frameworks, demands to be broken down each time by designers in order to arrive at domain- and discipline-specific ethical AI guidelines and/or measurable standards for practical use. The abstract nature of AI ethics may generate dilemmas when coming into contact with the field of pedagogical ethics in an educational setting such as CSCL. We present points of friction using practical examples (edge cases) and highlight considerations for educators that may give an out, taking both angles into account.",incollection,,,Santi Caballé and Joan Casas-Roma and Jordi Conesa,Ethics in Online AI-based Systems,Academic Press,Intelligent Data-Centric Systems,978-0-443-18851-0,,
Generative pretrained transformer 4: an innovative approach to facilitate value-based healthcare,Intelligent Medicine,4,10-15,2024,2667-1026,https://doi.org/10.1016/j.imed.2023.09.001,https://www.sciencedirect.com/science/article/pii/S2667102623000608,Han Lyu and Zhixiang Wang and Jia Li and Jing Sun and Xinghao Wang and Pengling Ren and Linkun Cai and Zhenchang Wang and Max Wintermark,"Generative pretrained transformer 4 model, Natural language processing, Medical imaging, Appropriateness","Objective
Appropriate medical imaging is important for value-based care. We aim to evaluate the performance of generative pretrained transformer 4 (GPT-4), an innovative natural language processing model, providing appropriate medical imaging automatically in different clinical scenarios.
Methods
Institutional Review Boards (IRB) approval was not required due to the use of nonidentifiable data. Instead, we used 112 questions from the American College of Radiology (ACR) Radiology-TEACHES Program as prompts, which is an open-sourced question and answer program to guide appropriate medical imaging. We included 69 free-text case vignettes and 43 simplified cases. For the performance evaluation of GPT-4 and GPT-3.5, we considered the recommendations of ACR guidelines as the gold standard, and then three radiologists analyzed the consistency of the responses from the GPT models with those of the ACR. We set a five-score criterion for the evaluation of the consistency. A paired t-test was applied to assess the statistical significance of the findings.
Results
For the performance of the GPT models in free-text case vignettes, the accuracy of GPT-4 was 92.9%, whereas the accuracy of GPT-3.5 was just 78.3%. GPT-4 can provide more appropriate suggestions to reduce the overutilization of medical imaging than GPT-3.5 (t = 3.429, P = 0.001). For the performance of the GPT models in simplified scenarios, the accuracy of GPT-4 and GPT-3.5 was 66.5% and 60.0%, respectively. The differences were not statistically significant (t = 1.858, P = 0.070). GPT-4 was characterized by longer reaction times (27.1 s in average) and extensive responses (137.1 words on average) than GPT-3.5.
Conclusion
As an advanced tool for improving value-based healthcare in clinics, GPT-4 may guide appropriate medical imaging accurately and efficiently.",article,1,,,,,,,,
DCTM: Dual Contrastive Topic Model for identifiable topic extraction,Information Processing & Management,61,103785,2024,0306-4573,https://doi.org/10.1016/j.ipm.2024.103785,https://www.sciencedirect.com/science/article/pii/S0306457324001456,Rui Wang and Peng Ren and Xing Liu and Shuyu Chang and Haiping Huang,"Contrastive learning, Neural-based topic model, Topic modeling","The recent advanced Contrastive Neural Topic Model (CNTM) was proposed to tackle topic collapse through document-level contrastive learning. However, limited by its usage of the Logistic-Normal prior in topic space and document level contrastive learning, it is less capable of disentangling semantically similar topics. To address the limitation, we propose a novel Dual Contrastive Topic Model (DCTM) that utilizes the Dirichlet prior to capture interpretable patterns. Besides, it incorporates dual (document-level and topic-level) contrastive learning on the topic distribution matrix which helps generate discriminative topic representations and mine identifiable topics. Our proposed DCTM outperforms the state-of-the-art neural topic models in terms of topic coherence and diversity, which is verified by extensive experimentation on three publicly available text corpora. In detail, the proposed DCTM surpasses baselines on almost all the used topic coherence metrics (CP, CA, NPMI for 20Newsgroups, CP, CA, NPMI and UCI for Grolier and DBPedia), and it also obtains higher topic diversity with 1 datasets respectively. Moreover, when performing text clustering, DCTM also achieves significant improvements, with observed increases of more than 1% (20Newsgroups) and 6% (DBPedia) in accuracy.",article,5,,,,,,,,
Curriculum Compositional Continual Learning for Neural Machine Translation,Procedia Computer Science,222,167-176,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.08.154,https://www.sciencedirect.com/science/article/pii/S1877050923009195,Kehinde Owoeye,"Continual learning, Compositionality, Curriculum learning, Neural Machine Translation","Current trends in language modelling leverage large language models pre-trained on a huge corpus of data to achieve state of the art results on several NLP tasks. On the other hand, humans acquire language from small amount of data using cognitive principles. Recently, a continual learning approach using compositionality to disentangle the syntax and semantics of an input sentence for downstream sequence to sequence tasks was proposed. In this work, we show how curriculum learning can be incorporated with this framework to improve performance. More specifically, first, we show that using the model of interest with reduced hidden size as the auxiliary model to generate curriculum is not necessarily optimal and second, we propose a novel variant of the one best score approach for curriculum learning where, a sequence to sequence model is used as the auxiliary model to generate the conditional probabilities of word predictions (proxy for difficulty) and consequently used this to generate a curriculum. Results on a variety of translation tasks, demonstrate the superiority of the proposed approach compared to several baselines, enabling the improvement of sentence accuracy with respect to knowledge transfer and catastrophic-forgetting both by at least a significant margin of 35% with respect to the best performing baseline on the English-French translation task.",article,,International Neural Network Society Workshop on Deep Learning Innovations and Applications (INNS DLIA 2023),,,,,,,
"ChatGPT: Jack of all trades, master of none",Information Fusion,99,101861,2023,1566-2535,https://doi.org/10.1016/j.inffus.2023.101861,https://www.sciencedirect.com/science/article/pii/S156625352300177X,Jan Kocoń and Igor Cichecki and Oliwier Kaszyca and Mateusz Kochanek and Dominika Szydło and Joanna Baran and Julita Bielaniewicz and Marcin Gruza and Arkadiusz Janz and Kamil Kanclerz and Anna Kocoń and Bartłomiej Koptyra and Wiktoria Mieleszczenko-Kowszewicz and Piotr Miłkowski and Marcin Oleksy and Maciej Piasecki and Łukasz Radliński and Konrad Wojtasik and Stanisław Woźniak and Przemysław Kazienko,"ChatGPT, GPT-4, Natural language processing (NLP), Semantic NLP tasks, Pragmatic NLP tasks, Subjective NLP tasks, Natural language inference (NLI), Sentiment analysis, Offensive content, Emotion recognition, Humor detection, Stance detection, Word sense disambiguation (WSD), Question answering (QA), Model personalization, Text classification, SOTA analysis, Large language model, Prompting","OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. The first contact with the chatbot reveals its ability to provide detailed and precise answers in various areas. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT’s capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed that the more difficult the task (lower SOTA performance), the higher the ChatGPT loss. It especially refers to pragmatic NLP problems like emotion recognition. We also tested the ability to personalize ChatGPT responses for selected subjective tasks via Random Contextual Few-Shot Personalization, and we obtained significantly better user-based predictions. Additional qualitative analysis revealed a ChatGPT bias, most likely due to the rules imposed on human trainers by OpenAI. Our results provide the basis for a fundamental discussion of whether the high quality of recent predictive NLP models can indicate a tool’s usefulness to society and how the learning and validation procedures for such systems should be established.",article,,,,,,,,,
Assessing growth potential of careers with occupational mobility network and ensemble framework,Engineering Applications of Artificial Intelligence,127,107306,2024,0952-1976,https://doi.org/10.1016/j.engappai.2023.107306,https://www.sciencedirect.com/science/article/pii/S0952197623014902,Jiamin Liu and Tao Wang and Feng Yao and Witold Pedrycz and Yanjie Song and Renjie He,"Human capital, Occupational mobility network, Growth potential of career, Machine learning","The growth potential of a career reflects its future prospects and is an important consideration for individuals and organizations when career planning. There is still a lack of quantitative assessment tools for growth potential of careers. In this study, considering the key role of human capital in human resource management, as well as the excellent performance of complex network and machine learning in big data analysis and prediction, a career growth potential assessment model with human capital ensemble is proposed through human capital-based occupational mobility network and ensemble learning. First, an occupational mobility network is constructed based on online professional dataset to associate occupations with each other. Then, five dimensions of human capital measurements are designed to quantify human capital in terms of education, experience, social capital, occupational size, and concentration. These are then combined with the occupational mobility network to create a new network that depicts human capital flows among occupations. Finally, an ensemble framework for assessing career growth potential is constructed to integrate multidimensional human capital information in the network and obtain quantitative scores of growth potential. This study is the original attempt to adopt a data-driven idea and an intelligent approach to understand career growth potential. The experimental results show that it also makes a useful exploration for modeling human capital flows and intelligent assessment of career prospects.",article,,,,,,,,,
Utilizing cognitive signals generated during human reading to enhance keyphrase extraction from microblogs,Information Processing & Management,61,103614,2024,0306-4573,https://doi.org/10.1016/j.ipm.2023.103614,https://www.sciencedirect.com/science/article/pii/S0306457323003515,Xinyi Yan and Yingyi Zhang and Chengzhi Zhang,"Automatic Keyphrase Extraction (AKE), Human readings behavior, EEG, Eye-tracking, Cognitive signal","Microblogging platforms have seen exponential growth, leading to an abundance of user-generated content. The challenge now is to efficiently extract crucial information from this vast and dispersed text data. It also serves as the goal of our research on Automatic Keyphrase Extraction (AKE) for microblog. Eye-tracking signals, that reflect users' tendency to prioritize certain words while reading, have been employed to enhance AKE performance from microblogs. However, relying solely on eye-tracking has its limitations owing to constraints in physiological mechanism support, acquisition techniques, and feature decoding. Consequently, we propose the integration of electroencephalogram (EEG) signals with eye-tracking signals to improve microblogs-based AKE, thereby overcoming the aforementioned limitations. Our first step is identifying specific features present in cognitive signals generated during human reading. We selected EEG signals (8 features) and eye-tracking signals (17 features) from the cognitive language processing corpus ZUCO, to examine the efficacy when they are combined with the microblogs-based AKE. To avoid cognitive signal distortion by certain model structures, we introduced these signals at the inputs of the soft attention layer and at the query vectors of the self-attention layer. For evaluation, we performed several AKE tests on microblogs with various combinations of cognitive signals. The results demonstrate a consistent enhancement in the performance of AKE due to cognitive signals generated during human reading, regardless of different feature combinations and models. Specifically, EEG signals exhibited the most significant improvement. However, combining EEG signals with eye-tracking signals yielded results that fell between the performance levels of the two signal types, indicating that their integration might have some synergistic effects. Further investigation is needed to understand the underlying mechanisms responsible for this outcome. The code and dataset for this paper can be accessed at https://github.com/yan-xinyi/AKE.",article,2,,,,,,,,
Review of low-cost self-driving laboratories in chemistry and materials science: the “frugal twin” concept,Digital Discovery,3,842-868,2024,2635-098X,https://doi.org/10.1039/d3dd00223c,https://www.sciencedirect.com/science/article/pii/S2635098X24000846,Stanley Lo and Sterling G. Baird and Joshua Schrier and Ben Blaiszik and Nessa Carson and Ian Foster and Andrés Aguilar-Granda and Sergei V. Kalinin and Benji Maruyama and Maria Politi and Helen Tran and Taylor D. Sparks and Alán Aspuru-Guzik,,"This review proposes the concept of a “frugal twin,” similar to a digital twin, but for physical experiments. Frugal twins range from simple toy examples to low-cost surrogates of high-cost research systems. For example, a color-mixing self-driving laboratory (SDL) can serve as a low-cost version of a costly multi-step chemical discovery SDL. Frugal twins already provide hands-on experience for SDLs with low costs and low risks. They can also offer as test beds for software prototyping (e.g., optimization, data infrastructure), and a low barrier to entry for democratizing SDLs. However, there is room for improvement. The true value of frugal twins can be realized in three core areas. Firstly, hardware and software modularity; secondly, purpose-built design (human-inspired vs. hardware-centric vs. human-in-the-loop); and thirdly state-of-the-art (SOTA) software (e.g., multi-fidelity optimization). We also describe the ethical benefits and risks that come with the democratization of science through frugal twins. For future work, we suggest ideas for new frugal twins, SDL educational course outcomes, and a classification scheme for autonomy levels.",article,5,,,,,,,,
Enhancing empirical software performance engineering research with kernel-level events: A comprehensive system tracing approach,Journal of Systems and Software,216,112117,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112117,https://www.sciencedirect.com/science/article/pii/S0164121224001626,Morteza Noferesti and Naser Ezzati-Jivan,"Empirical software engineering, Software performance engineering, Kernel-level tracing, Software reliability, Performance monitoring","Performance engineering is a proactive and systematic approach aimed at designing, building, and enhancing software systems to ensure their efficient and reliable operation. It involves observing and measuring the operational behavior of a software system without interference, assessing performance metrics like response times, throughput, and resource utilization. This entails delving into kernel-level events related to performance monitoring, which play a significant role in understanding system behavior and diagnosing performance-related issues. Kernel-level events offer insights into how both the operating system and hardware resources are utilized. This information empowers system administrators, developers, and performance analysts to optimize and troubleshoot the system effectively. A critical aspect of performance analysis is root cause analysis, which involves delving deep into kernel-level events connected to performance monitoring. These events provide valuable insights into the utilization of operating system and hardware resources, equipping system administrators, developers, and performance analysts with tools to effectively troubleshoot and optimize the system. Our study introduces an innovative artifact that captures kernel-level events using Elasticsearch and Kibana, facilitating comprehensive performance analysis under diverse scenarios. By defining both Light-load and Heavy-load scenarios and simulating CPU, I/O, Network, and Memory noise, we offer researchers a realistic environment to explore innovative approaches to system performance enhancement. The artifact comprises both kernel events and system calls, resulting in a cumulative count of 24,263,691 events. The proposed artifact can serve three distinct applications. The first application emphasizes performance analysis by utilizing kernel events for monitoring. The second application targets noise detection and root cause analysis, again using kernel events. Finally, the third application investigates software phase detection through monitoring at the kernel level. These applications demonstrate that through our artifact, researchers can effectively analyze performance, detect and address performance noise, and identify software phases, contributing to the advancement of performance engineering methodologies. All the system configurations, scripts, and traces can be found in the artifact GitHub repository.11URL: https://github.com/mnoferestibrocku/dataset-repo.",article,,,,,,,,,
Atomist or holist? A diagnosis and vision for more productive interdisciplinary AI ethics dialogue,Patterns,4,100652,2023,2666-3899,https://doi.org/10.1016/j.patter.2022.100652,https://www.sciencedirect.com/science/article/pii/S2666389922002926,Travis Greene and Amit Dhurandhar and Galit Shmueli,,"Summary
In response to growing recognition of the social impacts of new artificial intelligence (AI)-based technologies, major AI and machine learning (ML) conferences and journals now encourage or require papers to include ethics impact statements and undergo ethics reviews. This move has sparked heated debate concerning the role of ethics in AI research, at times devolving into name calling and threats of “cancellation.” We diagnose this conflict as one between “atomist” and “holist” ideologies. Among other things, atomists believe facts are and should be kept separate from values, while holists believe facts and values are and should be inextricable from one another. With the goal of reducing disciplinary polarization, we draw on numerous philosophical and historical sources to describe each ideology’s core beliefs and assumptions. Finally, we call on atomists and holists within the ever-expanding data science community to exhibit greater empathy during ethical disagreements and propose four targeted strategies to ensure AI research benefits society.",article,1,,,,,,,,
Learning entity-oriented representation for biomedical relation extraction,Journal of Biomedical Informatics,147,104527,2023,1532-0464,https://doi.org/10.1016/j.jbi.2023.104527,https://www.sciencedirect.com/science/article/pii/S1532046423002484,Ying Hu and Yanping Chen and Yongbin Qin and Ruizhang Huang,"Biomedical natural language processing, Overlapping semantics, Information extraction","Biomedical Relation Extraction (BioRE) aims to automatically extract semantic relations for given entity pairs and is of great significance in biomedical research. Current popular methods often utilize pretrained language models to extract semantic features from individual input instances, which frequently suffer from overlapping semantics. Overlapping semantics refers to the situation in which a sentence contains multiple entity pairs that share the same context, leading to highly similar information between these entity pairs. In this study, we propose a model for learning Entity-oriented Representation (EoR) that aims to improve the performance of the model by enhancing the discriminability between entity pairs. It contains three modules: sentence representation, entity-oriented representation, and output. The first module learns the global semantic information of the input instance; the second module focuses on extracting the semantic information of the sentence from the target entities; and the third module enhances distinguishability among entity pairs and classifies the relation type. We evaluated our approach on four BioRE tasks with eight datasets, and the experiments showed that our EoR achieved state-of-the-art performance for PPI, DDI, CPI, and DPI tasks. Further analysis demonstrated the benefits of entity-oriented semantic information in handling multiple entity pairs in the BioRE task.",article,,,,,,,,,
"A survey on detecting mental disorders with natural language processing: Literature review, trends and challenges",Computer Science Review,53,100654,2024,1574-0137,https://doi.org/10.1016/j.cosrev.2024.100654,https://www.sciencedirect.com/science/article/pii/S1574013724000388,Arturo Montejo-Ráez and M. Dolores Molina-González and Salud María Jiménez-Zafra and Miguel Ángel García-Cumbreras and Luis Joaquín García-López,"Mental disorders detection, Natural language processing, Machine learning, Survey","For years, the scientific community has researched monitoring approaches for the detection of certain mental disorders and risky behaviors, like depression, eating disorders, gambling, and suicidal ideation among others, in order to activate prevention or mitigation strategies and, in severe cases, clinical treatment. Natural Language Processing is one of the most active disciplines dealing with the automatic detection of mental disorders. This paper offers a comprehensive and extensive review of research works on Natural Language Processing applied to the identification of some mental disorders. To this end, we have identified from a literature review, which are the main types of features used to represent the texts, the machine learning algorithms that are preferred or the most targeted social media platforms, among other aspects. Besides, the paper reports on scientific forums and projects focused on the automatic detection of these problems over the most popular social networks. Thus, this compilation provides a broad view of the matter, summarizing main strategies, and significant findings, but, also, recognizing some of the weaknesses in the research works published so far, serving as clues for future research.",article,,,,,,,,,
The End is the Beginning is the End: The closed-loop learning analytics framework,Computers in Human Behavior,158,108305,2024,0747-5632,https://doi.org/10.1016/j.chb.2024.108305,https://www.sciencedirect.com/science/article/pii/S0747563224001730,Michael Sailer and Manuel Ninaus and Stefan E. Huber and Elisabeth Bauer and Samuel Greiff,"Learning analytics, Multimodal, Artificial intelligence, Education, Adaptivity, Personalization","This article provides a comprehensive review of current practices and methodologies within the field of learning analytics, structured around a dedicated closed-loop framework. This framework effectively integrates various aspects of learning analytics into a cohesive framework, emphasizing the interplay between data collection, processing and analysis, as well as adaptivity and personalization, all connected by the learners involved and underpinned by educational and psychological theory. In reviewing each step of the closed loop, the article delves into the advancements in data collection, exploring how technological progress has expanded data collection methods, particularly focusing on the potential of multimodal data acquisition and how theory can inform this step. The processing and analysis step is thoroughly reviewed, highlighting a range of methods including machine learning and AI, and discussing the critical balance between prediction accuracy and interpretability. The adaptivity and personalization step examines the current state of research, underscoring significant gaps and the necessity for theory-informed, personalized learning interventions. Overall, the article underscores the importance of interdisciplinarity in learning analytics, advocating for the integration of insights from various fields to address challenges such as ethical data usage and the creation of quality learning experiences. This framework and review aim to guide future research and practice in learning analytics, promoting the development of effective, learner-centric educational environments driven by balancing data-driven insights and theoretical understanding.",article,,,,,,,,,
Investigating ChatGPT and cybersecurity: A perspective on topic modeling and sentiment analysis,Computers & Security,135,103476,2023,0167-4048,https://doi.org/10.1016/j.cose.2023.103476,https://www.sciencedirect.com/science/article/pii/S0167404823003863,Ogobuchi Daniel Okey and Ekikere Umoren Udo and Renata Lopes Rosa and Demostenes Zegarra Rodríguez and João Henrique Kleinschmidt,"ChatGPT, Cybersecurity, Sentiment analysis, Generative pre-trained transformers, Artificial intelligence, Data security","In early 2023, the Artificial Intelligence (AI) industry experienced a significant advancement with the emergence of OpenAI's ChatGPT, a research product that demonstrated remarkable capabilities and garnered widespread attention. ChatGPT is an advanced chatbot powered by the Generative Pretrained Transformers (GPT) architecture, designed to generate human-like conversations encompassing a wide range of knowledge domains. Many AI researchers are currently engaging with the new technology to understand its functionality and limitations. Various expressions across a range of social media platforms, including Twitter, YouTube, Facebook, and numerous others, are currently under investigation. This research seeks to analyze the opinions of ChatGPT users as it regards cybersecurity. This research is important due to its contribution towards gaining enhanced understanding and devising intricate improvements for the chatbot. The Latent Dirichlet Allocation (LDA) algorithm is utilized to extract relevant topics from the texts. Additionally, to analyze user opinions and decipher the sentiments as either positive, negative, or neutral, we use the Natural language tool kit Valence Aware Dictionary for sEntiment Reasoning (NLTK's VADER) and Robustly Optimized BERT Pretraining Approach (roBERTa) libraries. The data used is obtained from Twitter via the SNScrape library, which aided in the retrieval of over 700,000 tweets via the search terms #chatgptsecurity, #chatgpthackers, #chatgptcybersecurity, and #chatgptcyberthreats. The analysis of the results by the VADER model shows 43.8% positive, 36.3% neutral, and 19.9% negative sentiments. Similarly, the roBERTa model shows 14.1% positive, 53.2% neutral, and 32.7% negative. These results show that there is an ongoing concern about ChatGPT and cybersecurity, especially in malware code generation, hacking, intelligence gathering, and phishing attacks.",article,,,,,,,,,
Harnessing GPT-4 for generation of cybersecurity GRC policies: A focus on ransomware attack mitigation,Computers & Security,134,103424,2023,0167-4048,https://doi.org/10.1016/j.cose.2023.103424,https://www.sciencedirect.com/science/article/pii/S0167404823003346,Timothy McIntosh and Tong Liu and Teo Susnjak and Hooman Alavizadeh and Alex Ng and Raza Nowrozy and Paul Watters,"GPT, Cybersecurity policies, Ransomware, Policy generation, GRC","This study investigated the potential of Generative Pre-trained Transformers (GPTs), a state-of-the-art large language model, in generating cybersecurity policies to deter and mitigate ransomware attacks that perform data exfiltration. We compared the effectiveness, efficiency, completeness, and ethical compliance of GPT-generated Governance, Risk and Compliance (GRC) policies, with those from established security vendors and government cybersecurity agencies, using game theory, cost-benefit analysis, coverage ratio, and multi-objective optimization. Our findings demonstrated that GPT-generated policies could outperform human-generated policies in certain contexts, particularly when provided with tailored input prompts. To address the limitations of our study, we conducted our analysis with thorough human moderation, tailored input prompts, and the inclusion of legal and ethical experts. Based on these results, we made recommendations for corporates considering the incorporation of GPT in their GRC policy making.",article,,,,,,,,,
Technology-Assisted Motivational Interviewing: Developing a Scalable Framework for Promoting Engagement with Tobacco Cessation Using NLP and Machine Learning,Procedia Computer Science,206,121-131,2022,1877-0509,https://doi.org/10.1016/j.procs.2022.09.091,https://www.sciencedirect.com/science/article/pii/S1877050922009644,Ahson Saiyed and John Layton and Brian Borsari and Jing Cheng and Tatyana Kanzaveli and Maksim Tsvetovat and Jason Satterfield,"smoking cessation, motivational interviewing, digital health, chatbot, machine learning models","Motivational interviewing (MI) improves readiness for smoking cessation but can be time-intensive, require substantial expertise, and patients must still be linked with evidence-based cessation programs sensitive to local resources and patient preferences. Technology-assisted MI may provide a more efficient way to promote readiness and facilitate behavior change. This study developed the Technology Assisted Motivational Interviewing Coach (TAMI), a digital conversational agent that incorporates machine learning models to deliver MI for tobacco cessation and create tailored quit plans. This manuscript describes and evaluates the architecture and nested machine learning models within TAMI leveraged during the pilot clinical trial.",article,,International Society for Research on Internet Interventions 11th Scientific Meeting,,,,,,,
A knowledge-sharing platform for space resources,Data & Knowledge Engineering,151,102286,2024,0169-023X,https://doi.org/10.1016/j.datak.2024.102286,https://www.sciencedirect.com/science/article/pii/S0169023X24000107,,"Knowledge engineering, Knowledge graph, Ontology, Space resources","The ever-increasing interest of academia, industry, and government institutions in space resource information highlights the difficulty of finding, accessing, integrating, and reusing this information. Although information is regularly published on the internet, it is disseminated on many different websites and in different formats, including scientific publications, patents, news, and reports. We are currently developing a knowledge management and sharing platform for space resources. This tool, which relies on the combined use of knowledge graphs and ontologies, formalises the domain knowledge contained in the above-mentioned documents and makes it more readily available to the community. In this article, we describe the concepts and techniques of knowledge extraction and management adopted during the design and implementation of the platform.",article,,,,,,,,,
Contributor Biographies,,,xix-xxiv,2015,,https://doi.org/10.1016/B978-0-12-801379-3.09988-5,https://www.sciencedirect.com/science/article/pii/B9780128013793099885,,,,incollection,,,Mohammad Dastbaz and Colin Pattinson and Babak Akhgar,Green Information Technology,Morgan Kaufmann,,978-0-12-801379-3,Boston,
"ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope",Internet of Things and Cyber-Physical Systems,3,121-154,2023,2667-3452,https://doi.org/10.1016/j.iotcps.2023.04.003,https://www.sciencedirect.com/science/article/pii/S266734522300024X,Partha Pratim Ray,"ChatGPT, Language model, GPT-3.5, Generative AI, Conversational AI, Context understanding, Natural language processing","In recent years, artificial intelligence (AI) and machine learning have been transforming the landscape of scientific research. Out of which, the chatbot technology has experienced tremendous advancements in recent years, especially with ChatGPT emerging as a notable AI language model. This comprehensive review delves into the background, applications, key challenges, and future directions of ChatGPT. We begin by exploring its origins, development, and underlying technology, before examining its wide-ranging applications across industries such as customer service, healthcare, and education. We also highlight the critical challenges that ChatGPT faces, including ethical concerns, data biases, and safety issues, while discussing potential mitigation strategies. Finally, we envision the future of ChatGPT by exploring areas of further research and development, focusing on its integration with other technologies, improved human-AI interaction, and addressing the digital divide. This review offers valuable insights for researchers, developers, and stakeholders interested in the ever-evolving landscape of AI-driven conversational agents. This study explores the various ways ChatGPT has been revolutionizing scientific research, spanning from data processing and hypothesis generation to collaboration and public outreach. Furthermore, the paper examines the potential challenges and ethical concerns surrounding the use of ChatGPT in research, while highlighting the importance of striking a balance between AI-assisted innovation and human expertise. The paper presents several ethical issues in existing computing domain and how ChatGPT can invoke challenges to such notion. This work also includes some biases and limitations of ChatGPT. It is worth to note that despite of several controversies and ethical concerns, ChatGPT has attracted remarkable attentions from academia, research, and industries in a very short span of time.",article,,,,,,,,,
GraphRec-based Korean expert recommendation using author contribution index and the paper abstracts in marine,Engineering Applications of Artificial Intelligence,133,108219,2024,0952-1976,https://doi.org/10.1016/j.engappai.2024.108219,https://www.sciencedirect.com/science/article/pii/S0952197624003774,Jeong-Wook Lee and Jae-Hoon Kim,"Recommendation system, Marine expert recommendation system, Graph neural networks","Expert recommendation systems recommend specialized experts in a particular field to users based on the knowledge of those experts. However, these systems are limited by the number of experts available and the potential for subjective evaluation, which may result in inappropriate recommendations. Furthermore, we explore the evolution from traditional to deep learning-based recommendation systems, emphasizing graph-based recommendation systems. Nonetheless, deep learning-based systems require large amounts of data, and marine expert recommendation training data are scarce. To address these issues, we constructed and utilized marine expert data in this study. The dataset contains abstracts of marine-related papers and information on their authors. Graphs were generated by assessing the similarity among the abstracts, representing them in a graph format indicative of this similarity, and using the author contribution index to depict the relationship between the abstracts and their respective authors. Various similarity methods and abstract embedding techniques were experimentally explored to realize performance optimization. In the experiments, the optimized model achieved a mean absolute error of 0.7556 and a root-mean-squared error of 1.0421. Notably, this study highlights the limitations of traditional evaluation metrics and proposes the averaged mean reciprocal rank as a suitable alternative. This metric facilitates the quantitative evaluation of model performance on newly created data, obviating a comparison model. Finally, applying the newly constructed data to the GraphRec model by using their graphical representation significantly improves the system performance.",article,,,,,,,,,
MDM: Meta diffusion model for hard-constrained text generation,Knowledge-Based Systems,283,111147,2024,0950-7051,https://doi.org/10.1016/j.knosys.2023.111147,https://www.sciencedirect.com/science/article/pii/S0950705123008973,Wenjun Ke and Yikai Guo and Qi Liu and Wanyi Chen and Peng Wang and Haoran Luo and Zhizhao Luo,"Hard-constrained text generation, Diffusion model, Meta-learning","Hard-constrained text generation (Hard-CTG) task aims to generate text with given keywords, which is helpful for summarization, data augmentation, story writing, etc. Existing Hard-CTG models face two significant challenges. Firstly, hard-CTG models based on the editing method suffer from error propagation, resulting in low generation quality. Secondly, Hard-CTG models utilizing the prompt method cannot guarantee high keyword coverage. To tackle these challenges, we propose Meta Diffusion Model (MDM), a non-autoregressive diffusion model with novel meta-learning module. Specifically, we fix the embedding of keywords in the generation process, while all non-keyword tokens evolve simultaneously and contribute to each other towards the target sentence under given keywords, addressing the above issues. Moreover, existing diffusion models for the text domain have an inconsistency in the training and inference stages. To unify the training and inference processes, we present an adaptively denoising method derived from meta-learning, and further improves generation quality. Experiments on three representative datasets demonstrate that our method achieves state-of-the-art performance evaluated on empirical metrics. Especially, compared with strong baselines, MDM significantly improves the BLEU-4, CIDEr, and ROUGE by 0.48%—11.56%, 17.33%—82.87%, and 23.15%–29.78%, respectively. In terms of keyword coverage, our MDM outperforms ChatGPT by 2.93%–7.88%.",article,,,,,,,,,
Index,,,383-397,2024,,https://doi.org/10.1016/B978-0-443-18851-0.00027-5,https://www.sciencedirect.com/science/article/pii/B9780443188510000275,,,,incollection,,,Santi Caballé and Joan Casas-Roma and Jordi Conesa,Ethics in Online AI-based Systems,Academic Press,Intelligent Data-Centric Systems,978-0-443-18851-0,,
Do background characteristics matter in Children's mastery of digital literacy? A cognitive diagnosis model analysis,Computers in Human Behavior,122,106850,2021,0747-5632,https://doi.org/10.1016/j.chb.2021.106850,https://www.sciencedirect.com/science/article/pii/S0747563221001734,,"Digital literacy, Background characteristics, Cognitive diagnosis models, Three-step analysis approach, Latent logistic regression","This study aims to investigate the mastery profiles of digital literacy skills of Hong Kong primary students using a general cognitive diagnosis model (CDM) framework. In particular, the relationship between the mastery of each digital skill and a number of students' background characteristics is explored using a three-step approach. The current study analyzes data collected from 642 Grade 3 students in Hong Kong using a newly developed digital literacy assessment (DLA). CDMs are fitted to the data to determine students' mastery profiles of five digital skills, as well as test properties; subsequently latent logistic regression analyses were implemented to determine the relationship between skill mastery and the covariates. Results indicate that CDM analysis is an appropriate method to analyze the DLA performance data, which exhibited measurement invariance across gender and socioeconomic status (SES). Despite low mastery proportions for all digital skills, students' skill mastery can be accurately classified. Finally, the latent logistic regression results indicate that children's background characteristics (i.e., gender, educational aspiration, home language, SES, and access to digital devices) are differentially related to their mastery of each digital skill.",article,,,,,,,,,
Informatics on a social view and need of ethical interventions for wellbeing via interference of artificial intelligence,Telematics and Informatics Reports,11,100065,2023,2772-5030,https://doi.org/10.1016/j.teler.2023.100065,https://www.sciencedirect.com/science/article/pii/S2772503023000257,Kabita Das and Manaswini Pattanaik and Smitimayee Basantia and Radhashyam Mishra and Debashreemayee Das and Kanhucharan Sahoo and Biswaranjan Paital,"Artificial intelligence, Ethical enquiry, Ethics in technology, Human conduct, Moral value, Social cognition, Human intelligence","The main focus of this paper was to discuss and appraise the attribution of intelligence and value judgement on Artificial Intelligence (AI) and its regulated use in society. Humans are tool-making creatures and AI is used for civilization via tools. During the time of pre-civilization, tools were simple in the form of crude construction, using hand skills but at present, the achievements are the substitution of machinery to relieve/replace human intellect. AI is the scientific technique of bringing learning, adaptation, and self-organization of machines. It encompasses various concepts and methods, deployed by researchers in many diverse fields of computation and cognition. This is the computational mode of a brain, based on artificial neural networks. The usefulness of AI ethically, initiates a big question i.e. if the human mind is not self-sufficient for any work without harming the moral sentiment of others then, how can people believe in a computational model of the mind, is a machine, morally responsible for any good or bad action. We highlight issues on the use of AI in the replacement of the human mind asking what is the value of humans in this age of AI? Can AI reciprocate and respect human values better than human beings? Can AI replace human intelligence? In the case of ethical enquiry, it is rather a herculean task to consider a machine's action to be moral or immoral, after all, it is just a machinery action devoid of any moral quality.",article,,,,,,,,,
"Explainable AI for Operational Research: A defining framework, methods, applications, and a research agenda",European Journal of Operational Research,317,249-272,2024,0377-2217,https://doi.org/10.1016/j.ejor.2023.09.026,https://www.sciencedirect.com/science/article/pii/S0377221723007294,,"Decision analysis, XAI, Explainable artificial intelligence, Interpretable machine learning, XAIOR","The ability to understand and explain the outcomes of data analysis methods, with regard to aiding decision-making, has become a critical requirement for many applications. For example, in operational research domains, data analytics have long been promoted as a way to enhance decision-making. This study proposes a comprehensive, normative framework to define explainable artificial intelligence (XAI) for operational research (XAIOR) as a reconciliation of three subdimensions that constitute its requirements: performance, attributable, and responsible analytics. In turn, this article offers in-depth overviews of how XAIOR can be deployed through various methods with respect to distinct domains and applications. Finally, an agenda for future XAIOR research is defined.",article,2,,,,,,,,
XR technologies to enhance the emotional skills of people with autism spectrum disorder: A systematic review,Computers & Graphics,121,103942,2024,0097-8493,https://doi.org/10.1016/j.cag.2024.103942,https://www.sciencedirect.com/science/article/pii/S0097849324000773,Christian Poglitsch and Saeed Safikhani and Erin List and Johanna Pirker,"Extended reality (XR), Virtual reality (VR), Augmented reality (AR), Autism spectrum disorder, Emotional skills, Emotion recognition","In this paper, we present a systematic review of the applications of (1) Extended Reality (XR), (2) Augmented Reality (AR), and (3) Virtual Reality (VR) technologies to enhance emotion recognition and emotion expression in people with Autism Spectrum Disorder (ASD). ASD can affect various abilities, and poses challenges to the recognition of emotions in others, which is often referred to as “social blindness”. Treating this condition typically requires intensive one-on-one or small-group therapy sessions, which can be costly and limited in terms of availability. With the growing number of diagnoses of ASD, concerns have risen regarding a potential “lost generation” that may face difficulties in fulfilling its potential. Through this comprehensive review, we aim to provide an overview of innovative approaches that use XR technologies to improve the learning experience of individuals with ASD.",article,,,,,,,,,
Computer analysis of atomic absorption spectrophotometer generated data: basic and fortran 77 programs,Computers & Geosciences,14,151-180,1988,0098-3004,https://doi.org/10.1016/0098-3004(88)90003-9,https://www.sciencedirect.com/science/article/pii/0098300488900039,Uwe Brand and Edward G. Lorek,"Geochemistry, Chemical data (AAS) management, BASIC and FORTRAN 77 programs, Weight and insoluble residue manipulation, Error recovery, Publishable printout, Precision computation, Data transfer capabilities","The atomic absorption spectrophotometer (AAS) programs (FORTRAN 77, v.2.0 and BASIC, v.2.1) allow for the efficient computation of chemical data generated by atomic absorption spectrophotometer. In the BASIC program, additional data, such as weight and insoluble residue, are entered directly into the program from a digital balance linked by RS-232C line to the minicomputer. In the FORTRAN 77 program weight, insoluble residue, and AAS are entered manually. Both programs have full editing facilities for easy error recovery, and the calculated data is printed in publishable table form. Also a terminal emulator program, in the BASIC version, is included to handle the efficient and automatic transfer of data to mainframe computers.",article,2,,,,,,,,
Educational models for cognition: Methodology of modeling intellectual skills for intelligent tutoring systems,Cognitive Systems Research,,101261,2024,1389-0417,https://doi.org/10.1016/j.cogsys.2024.101261,https://www.sciencedirect.com/science/article/pii/S138904172400055X,Oleg Sychev,"Reasoning modeling, Constraint-based modeling, Intelligent tutoring systems","Automation of teaching people new skills requires modeling of human reasoning because human cognition involves active reasoning over the new subject domain to acquire skills that will later become automatic. The article presents Thought Process Trees — a language for modeling human reasoning that was created to facilitate the development of intelligent tutoring systems, which can perform the same reasoning that is expected of a student and find deficiencies in their line of thinking, providing explanatory messages and allowing them to learn from performance errors. The methodology of building trees which better reflect human learning is discussed, with examples of design choices during the modeling process and their consequences. The characteristics of educational modeling that impact building subject-domain models for intelligent tutoring systems are discussed. The trees were formalized and served as a basis for developing a framework for constructing intelligent tutoring systems. This significantly lowered the time required to build and debug a constraint-based subject-domain model. The framework has already been used to develop five intelligent tutoring systems and their prototypes and is being used to develop more of them.",article,,,,,,,,,
Defending novice user privacy: An evaluation of default web browser configurations,Computers & Security,140,103784,2024,0167-4048,https://doi.org/10.1016/j.cose.2024.103784,https://www.sciencedirect.com/science/article/pii/S0167404824000853,Kristina Radivojevic and Nicholas Clark and Anna Klempay and Paul Brenner,"Desktop browsers, Privacy, Novice users","Cyber novices often enter sensitive data into web browsers for routine activities such as online shopping and bill payments, making them targets for malicious entities, including cybercriminals and oppressive governments. The proliferation of online advertising technologies further exacerbates privacy concerns by exploiting user data for marketing or surveillance, frequently without explicit consent. It is crucial to regularly ensure the latest features of default configurations, which are most relevant for novice users, adequately address growing privacy demands given the centrality of web browsers to internet usage. Our work scrutinizes the privacy claims of 14 desktop browsers and their default configurations, from mainstream options like Chrome to those prioritizing privacy, such as Brave. We validate these claims through a suite of privacy tests on two operating systems commonly used by cyber novices. Based on our findings, we categorize browsers into three tiers of privacy protection. We conclude by outlining future browser design principles and offering privacy-centric recommendations tailored for novice users.",article,,,,,,,,,
Batched sparse and mixed-precision linear algebra interface for efficient use of GPU hardware accelerators in scientific applications,Future Generation Computer Systems,160,359-374,2024,0167-739X,https://doi.org/10.1016/j.future.2024.06.004,https://www.sciencedirect.com/science/article/pii/S0167739X24003017,Piotr Luszczek and Ahmad Abdelfattah and Hartwig Anzt and Atsushi Suzuki and Stanimire Tomov,"Batched computations, Numerical linear algebra, BLAS, Hardware accelerators, Mixed-precision computing","Batched Sparse Linear Algebra has become an emergent processing mode on modern hardware accelerators based on Graphics Processing Units (GPUs) developed over the years to serve as the main compute devices in the largest computing clusters and supercomputers. We propose a set solver interface designs for batched sparse numerical solvers on these hardware accelerators. We motivate our specific designs by both their use in scientific applications of national importance and also by the possibility of implementing them in an efficient and portable manner with multiple options for vendor-specific optimizations. We present the C language interface calls for the linker-agnostic interchange of functional entry points. We also show how using C++ for the batched solvers simplifies the interface design while giving the user much broader set of opportunities for customization, testing, and debugging. We also cover in our proposals the option of exploiting multiple floating-point arithmetic precisions to directly match the application needs in terms of accuracy. Finally, a selected sample of performance experiments show how our proposed interface can be efficiently implemented to outperform the available alternatives many times over. In the end, we plan for an ongoing evolution of our newly proposed interface standard to keep up with the updates in programming languages, accelerator hardware, and application needs.",article,,,,,,,,,
Developing a deep learning natural language processing algorithm for automated reporting of adverse drug reactions,Journal of Biomedical Informatics,137,104265,2023,1532-0464,https://doi.org/10.1016/j.jbi.2022.104265,https://www.sciencedirect.com/science/article/pii/S1532046422002702,Christopher McMaster and Julia Chan and David F.L. Liew and Elizabeth Su and Albert G. Frauman and Wendy W. Chapman and Douglas E.V. Pires,"Natural language processing, Machine learning, Adverse drug reactions, Transfer learning","The detection of adverse drug reactions (ADRs) is critical to our understanding of the safety and risk-benefit profile of medications. With an incidence that has not changed over the last 30 years, ADRs are a significant source of patient morbidity, responsible for 5%–10% of acute care hospital admissions worldwide. Spontaneous reporting of ADRs has long been the standard method of reporting, however this approach is known to have high rates of under-reporting, a problem that limits pharmacovigilance efforts. Automated ADR reporting presents an alternative pathway to increase reporting rates, although this may be limited by over-reporting of other drug-related adverse events. We developed a deep learning natural language processing algorithm to identify ADRs in discharge summaries at a single academic hospital centre. Our model was developed in two stages: first, a pre-trained model (DeBERTa) was further pre-trained on 1.1 million unlabelled clinical documents; secondly, this model was fine-tuned to detect ADR mentions in a corpus of 861 annotated discharge summaries. This model was compared to a version without the pre-training step, and a previously published RoBERTa model pretrained on MIMIC III, which has demonstrated strong performance on other pharmacovigilance tasks. To ensure that our algorithm could differentiate ADRs from other drug-related adverse events, the annotated corpus was enriched for both validated ADR reports and confounding drug-related adverse events using. The final model demonstrated good performance with a ROC–AUC of 0.955 (95% CI 0.933 - 0.978) for the task of identifying discharge summaries containing ADR mentions, significantly outperforming the two comparator models.",article,,,,,,,,,
A call for introducing LegalTech in the classroom,Computer Law & Security Review,36,105399,2020,0267-3649,https://doi.org/10.1016/j.clsr.2020.105399,https://www.sciencedirect.com/science/article/pii/S0267364920300042,Chris Ireland and Ryan Hockley,"LawTech, LegalTech, Tech, Innovation, LLB, Classroom","Change is coming to the way the business of law is conducted. It is an unavoidable reality that the delivery of professional legal services is on the cusp of major disruption. The way law firms and in-house legal teams operate is predicted to change dramatically. It is theorised that a majority of the aforementioned change will come from the adoption of more sophisticated technology by law firms and the courts. Technological change has already made some lawyer hours obsolete, and this trend is only expected to continue. Given this incoming wave of change, there exists a strong justification for the inclusion of legaltech in the undergraduate LLB curriculum. This article asses the feasibility of such an inclusion and provides suggestions for what institutions could be doing to support their users.",article,,,,,,,,,
Beyond code: Is there a difference between comments in visual and textual languages?,Journal of Systems and Software,215,112087,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112087,https://www.sciencedirect.com/science/article/pii/S0164121224001328,Alexander Boll and Pooja Rani and Alexander Schultheiß and Timo Kehrer,"Documentation, Graphical, Diagram, Knowledge-transfer, Simulink, Model-driven engineering, Comment clones, Taxonomy","Code comments are crucial for program comprehension and maintenance. To better understand the nature and content of comments, previous work proposed taxonomies of comment information for textual languages, notably classical programming languages. However, paradigms such as model-driven or model-based engineering often promote the use of visual languages, to which existing taxonomies are not directly applicable. Taking MATLAB/Simulink as a representative of a sophisticated and widely used modeling environment, we extend a multi-language comment taxonomy onto new (visual) comment types and two new languages: Simulink and MATLAB. Furthermore, we outline Simulink commenting practices and compare them to textual languages. We analyze 259,267 comments from 9095 Simulink models and 17,792 MATLAB scripts. We identify the comment types, their usage frequency, classify comment information, and analyze their correlations with model metrics. We manually analyze 757 comments to extend the taxonomy. We also analyze commenting guidelines and developer adherence to them. Our extended taxonomy, SCoT (Simulink Comment Taxonomy), contains 25 categories. We find that Simulink comments, although often duplicated, are used at all model hierarchy levels. Of all comment types, Annotations are used most often; Notes scarcely. Our results indicate that Simulink developers, instead of extending comments, add new ones, and rarely follow commenting guidelines. Overall, we find Simulink comment information comparable to textual languages, which highlights commenting practice similarity across languages.",article,,,,,,,,,
Challenges and strategies for wide-scale artificial intelligence (AI) deployment in healthcare practices: A perspective for healthcare organizations,Artificial Intelligence in Medicine,151,102861,2024,0933-3657,https://doi.org/10.1016/j.artmed.2024.102861,https://www.sciencedirect.com/science/article/pii/S0933365724001039,Pouyan Esmaeilzadeh,"Artificial intelligence, AI, Deployment challenges, Healthcare, Data, Ethics, Law","Healthcare organizations have realized that Artificial intelligence (AI) can provide a competitive edge through personalized patient experiences, improved patient outcomes, early diagnosis, augmented clinician capabilities, enhanced operational efficiencies, or improved medical service accessibility. However, deploying AI-driven tools in the healthcare ecosystem could be challenging. This paper categorizes AI applications in healthcare and comprehensively examines the challenges associated with deploying AI in medical practices at scale. As AI continues to make strides in healthcare, its integration presents various challenges, including production timelines, trust generation, privacy concerns, algorithmic biases, and data scarcity. The paper highlights that flawed business models and wrong workflows in healthcare practices cannot be rectified merely by deploying AI-driven tools. Healthcare organizations should re-evaluate root problems such as misaligned financial incentives (e.g., fee-for-service models), dysfunctional medical workflows (e.g., high rates of patient readmissions), poor care coordination between different providers, fragmented electronic health records systems, and inadequate patient education and engagement models in tandem with AI adoption. This study also explores the need for a cultural shift in viewing AI not as a threat but as an enabler that can enhance healthcare delivery and create new employment opportunities while emphasizing the importance of addressing underlying operational issues. The necessity of investments beyond finance is discussed, emphasizing the importance of human capital, continuous learning, and a supportive environment for AI integration. The paper also highlights the crucial role of clear regulations in building trust, ensuring safety, and guiding the ethical use of AI, calling for coherent frameworks addressing transparency, model accuracy, data quality control, liability, and ethics. Furthermore, this paper underscores the importance of advancing AI literacy within academia to prepare future healthcare professionals for an AI-driven landscape. Through careful navigation and proactive measures addressing these challenges, the healthcare community can harness AI's transformative power responsibly and effectively, revolutionizing healthcare delivery and patient care. The paper concludes with a vision and strategic suggestions for the future of healthcare with AI, emphasizing thoughtful, responsible, and innovative engagement as the pathway to realizing its full potential to unlock immense benefits for healthcare organizations, physicians, nurses, and patients while proactively mitigating risks.",article,,,,,,,,,
SigCo: Eliminate the inter-class competition via sigmoid for learning with noisy labels,Knowledge-Based Systems,294,111651,2024,0950-7051,https://doi.org/10.1016/j.knosys.2024.111651,https://www.sciencedirect.com/science/article/pii/S0950705124002867,Ang Chen and Feng Xu and Tao Zeng and Xin Lyu and Xin Li,"Noisy label, Inter-class competition problem, Sigmoid classifier, Noise-adaptive learning strategy, Co-training mechanism","Accurate predictions from deep neural networks are crucial for distinguishing clean data and correcting noisy labels in current label noise learning methods. However, the conventional Softmax classifier used in most relevant works is highly sensitive to label noise due to its inherent competition-prompting mechanism, i.e., similar categories are encouraged to compete for limited confidence scores during class activation, especially between the noisy classes and the ground-truth, which can inevitably lead to suboptimal predictions and eventually hamper model performance. To address this inter-class competition problem, we propose a novel Sigmoid-based Sample Selection and Correction method named SigCo for learning with noisy labels. Different from previous works, we develop a Sigmoid-based network in which each Sigmoid classifier independently predicts its respective class, improving the reliability of the selection and correction process through more accurate predictions. Besides, in order to mitigate the negative impact of noisy labels, we design a noise-adaptive learning strategy by imposing stringent class masking constraints on clean samples to enhance the learning of discriminative features, while adopting a loose masking strategy for noisy data to improve the robustness to label noise. Additionally, we introduce a co-training strategy between our Sigmoid-based network and the conventional Softmax-based network to implicitly boost the generalization capability of the model. Extensive experiments on synthetic and real-world benchmarks show that SigCo consistently outperforms state-of-the-art methods. Especially on CIFAR-100N with 80% and 90% symmetric noise ratios, it improves test accuracy by 5.10% and 18.44%, respectively.",article,,,,,,,,,
Walkthrough phishing detection techniques,Computers and Electrical Engineering,118,109374,2024,0045-7906,https://doi.org/10.1016/j.compeleceng.2024.109374,https://www.sciencedirect.com/science/article/pii/S0045790624003021,Tejveer Singh and Manoj Kumar and Santosh Kumar,"Phishing, Machine learning, Deep learning, Features, Cyber threat, Social engineering, Internet users, Cyber vulnerabilities, Phishing countermeasures, User authentication","Phishing has emerged as a significant cyber threat, resulting in huge financial frauds for internet users annually. This malicious activity uses social engineering and upgraded methodologies (like file archiver in the browser, content injection, calendar phishing, more convincing fake websites or emails, voice manipulation, or other tools designed to deceive and exploit the target’s confidence) to extract sensitive information from unsuspected victims. In order to mitigate these attacks, several methods and tools have been devised; various detection techniques and block phishing websites, and browser extensions that notify users about suspicious websites. Our work elaborates on meticulous analysis of the detection of phishing attacks by classifying them into four broader categories based on the adopted methodologies like List-Based Detection, Heuristic-Based Detection, machine learning (ML)-based, and deep learning (DL)-based. Additionally, it summarizes the popular devised schemes, highlighting their advantages and limitations, and how these are suitable for the different types of deployments.",article,,,,,,,,,
Detecting ChatGPT in published documents: Chatbot catchphrases and buzzwords,Informatics in Medicine Unlocked,,101516,2024,2352-9148,https://doi.org/10.1016/j.imu.2024.101516,https://www.sciencedirect.com/science/article/pii/S2352914824000728,Edward J. Ciaccio,"artificial intelligence, chatbot, ChatGPT, detection, GPTZero","Background
Nowadays, chatbot-written text can be present in academic documents, even without attribution. Development of an accurate manual screening paradigm would be helpful.
Method
In a series of four test manuscripts suspected of containing chatbot-written text, N=93 peculiar catchphrases were highlighted, and Google Search was used to find articles with each catchphrase. Paragraphs with the catchphrase in recent documents were checked for chatbot origin using the GPTZero detector. For paragraphs confirmed by GPTZero as likely to be chatbot-associated, the following statistics were recorded (N=50): the number of articles published with each catchphrase paragraph for time periods 2012-2014, 2015-2017, 2020-2022 (after GPT introduction), and 2023-March 2024 (after ChatGPT introduction), the citations per article, the publishing journal Impact Factor, and the document section in which the chatbot phrase appeared.
Results
N=86/93 suspected peculiar phrasings had paragraphs with chatbot association by GPTZero (92.5%). The mean number of published articles containing a chatbot-associated paragraph was 21.7 for 2012-2014, 25.6 for 2015-2017, and 43.2 for 2020-2022 versus 67.2 for 2023- March 2024 (p = 0.004). 75% of chatbot-containing articles studied were published in Impact Factor journals. The mean journal Impact Factor was 4.99, with some articles published in Impact Factor 10+ journals. Chatbot phrasing was commonly found in Abstracts and Introductions, but also in Methods, Results/Discussion, Limitations, and Conclusions.
Conclusions
Chatbot content often has peculiar phrasing that typically appears in other chatbot-associated documents as well. It is possible to manually detect odd chatbot phrasings. Chatbot content is increasing, and is present in top journals.",article,,,,,,,,,
Chapter 7 - Deployment roadmap of proactive human–robot collaboration,,,149-192,2024,,https://doi.org/10.1016/B978-0-44-313943-7.00014-4,https://www.sciencedirect.com/science/article/pii/B9780443139437000144,Shufei Li and Pai Zheng and Lihui Wang,"Deployment roadmap of proactive human–robot collaboration, Scene perception, Knowledge representation, Decision making, Collaborative control","This chapter presents a stepwise procedure for the development of Proactive HRC systems comprising four key modules: scene perception, knowledge representation, decision making, and collaborative control. For each module, we provide a comprehensive research roadmap of related technologies and offer an advanced algorithm as a feasible solution. The perception module is dedicated to perceiving the human–robot–workspace environment, as detailed in Section 7.1. Meanwhile, knowledge representation focuses on acquiring semantic knowledge of manufacturing tasks and transferring human expertise to robots for cognitive inference, as illustrated in Section 7.2. In Section 7.3, we delve into the decision-making module, which empowers the HRC system to make intelligent decisions for optimized trajectory planning and human information support, adapting to changing environmental conditions. Additionally, Section 7.4 provides an overview of various algorithms for robot collaborative control at the operational level. These four aspects have witnessed the widespread adoption of cutting-edge cognitive computing techniques such as deep learning, reinforcement learning, transfer learning, large language model, etc., resulting in significant enhancements to Proactive HRC system performance.",incollection,,,Shufei Li and Pai Zheng and Lihui Wang,Proactive Human-Robot Collaboration Toward Human-Centric Smart Manufacturing,Elsevier,,978-0-443-13943-7,,
Fusing semantic information for syntax-guided paraphrase generation,Neurocomputing,597,128009,2024,0925-2312,https://doi.org/10.1016/j.neucom.2024.128009,https://www.sciencedirect.com/science/article/pii/S092523122400780X,Haoran Zhang and Li Li,"Paraphrase generation, Contrastive learning, Semantics and syntax, Transformer","Syntax-guided paraphrase generation (SGPG) refers to generating a paraphrase sentence that satisfies the given syntactic structure without changing the source sentences’ semantics. The commonly utilized syntactic structures are part-of-speech (POS) sequence, constituency parse tree, and masked template, with constituency parse tree achieving State-of-The-Art (SOTA) performance because of its rich syntactic information. As a result, further mining of syntactic information in parse trees has grown popular, yet fewer works pay attention to investigating semantic information in source sentences. A sentence is made up of two parts: syntax and semantics. Multiple studies have shown that improving the model’s ability to learn semantic information is critical for paraphrase construction as well as syntax learning. In this paper, we propose Fusing Semantic Information for Syntax-guided Paraphrase Generation (FS-SPG). Specifically, we propose a transformer-based semantic encoder to obtain detailed semantics from source sentences. This encoder contains a Semantics-Aware Attention mechanism for mining semantic information. In addition, we apply contrastive learning to improve the accuracy of parse tree nodes’ guidance to semantic sentences. Experiments on ParaNMT and QQP-Pos show that our model beats the SOTA model SI-SCP by 4.92% in syntactic metrics and 1.35% in semantic metrics.",article,,,,,,,,,
"Health informatics to enhance the healthcare industry's culture: An extensive analysis of its features, contributions, applications and limitations",Informatics and Health,,,2024,2949-9534,https://doi.org/10.1016/j.infoh.2024.05.001,https://www.sciencedirect.com/science/article/pii/S2949953424000092,Mohd Javaid and Abid Haleem and Ravi Pratap Singh,"Healthcare Informatics, Artificial Intelligence, Technologies, Applications, Medical, Patient","Background
Health informatics is a fast-growing area in the healthcare sector. It concerns the technologies, tools, equipment, and procedures required to gather, store, retrieve, and use health data and medical data. Healthcare informatics provides patients, nurses, hospital administrators, physicians, insurance providers, and other stakeholders with electronic access to medical records through health information technologies (HIT). Health informatics combines nursing science with data science and analytical disciplines to gather, handle, interpret, and convey data, bringing together specialists and making health information accessible and meaningful.
Methods
This research is an outcome of an extensive scopic review, which has been conducted by identifying research and development through search keywords such as “Health informatics,” “Technologies,” and “Healthcare” from databases of Scopus, PubMed, Google Scholar, ResearchGate, and other research platforms. Further, the most relevant papers are identified and studied.
Findings
This paper explores health informatics, its technologies, and their need in the present healthcare domain. It also identifies vital aspects, characteristics, and versatile contributions of health informatics to the healthcare sector. Further, the paper identifies and discusses significant health informatics applications in the healthcare field. Patients' health information can be effectively analysed individually or in groups using health informatics technologies to meet diverse requirements.
Interpretation
Effective use of health informatics improves practice management as information is quickly shared among healthcare professionals, patients and other stakeholders. Healthcare informatics specialists' knowledge of utilising data to assist choice-making and creating best practices. It enables healthcare organisations to identify specific data offering the appropriate information for the given therapy, procedure, or training. Informatics in healthcare also addresses issues at the macro level of the organisation and also at the personal level of patient care via innovative technologies and best practices.",article,,,,,,,,,
Def-DReL: Towards a sustainable serverless functions deployment strategy for fog-cloud environments using deep reinforcement learning,Applied Soft Computing,152,111179,2024,1568-4946,https://doi.org/10.1016/j.asoc.2023.111179,https://www.sciencedirect.com/science/article/pii/S1568494623011973,Chinmaya Kumar Dehury and Shivananda Poojara and Satish Narayana Srirama,"Serverless computing, Fog computing, Cloud computing, Deep reinforcement learning, Serverless function deployment, Function offloading","Modern cloud applications are composed of tens of thousands of environment-agnostic serverless functions that can be deployed in either a fog or cloud environment. The key to sustaining fog computing is to offload the maximum amounts of computation to the cloud, and accommodate as many users as possible without compromising quality of service (QoS). However, recent research mainly focuses on assigning maximum resources to serverless applications from the fog node and not taking full advantage of the cloud environment, leading to a lack of sustainability in fog computing. As a way to fill this research gap, we explored what percentage of a user’s request should be handled by fog and cloud. As a result, we proposed Def-DReL, a Systematic Deployment of Serverless Functions in Fog and Cloud environments using Deep Reinforcement Learning, by taking into account several real-life parameters, including distance from a nearby fog node and latency, priority of the user, priority of serverless applications, and resource usage. Def-DReL’s performance is further compared with that of recent related algorithms. Simulation and comparison results clearly demonstrate a lesser number of serverless functions from each user (with approximately 10% improvement) being deployed in the fog node, resulting in accommodating limited fog resources to more number of users. The other simulation results show its superiority over other algorithms as well as its applicability to real-life scenarios.",article,,,,,,,,,
Artificial intelligence in perinatal mental health research: A scoping review,Computers in Biology and Medicine,177,108685,2024,0010-4825,https://doi.org/10.1016/j.compbiomed.2024.108685,https://www.sciencedirect.com/science/article/pii/S0010482524007704,Wai Hang Kwok and Yuanpeng Zhang and Guanjin Wang,"Perinatal, Mental health, Artificial intelligence, ML, Natural language processing, Review","The intersection of Artificial Intelligence (AI) and perinatal mental health research presents promising avenues, yet uncovers significant challenges for innovation. This review explicitly focuses on this multidisciplinary field and undertakes a comprehensive exploration of existing research therein. Through a scoping review guided by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework, we searched relevant literature spanning a decade (2013–2023) and selected fourteen studies for our analysis. We first provide an overview of the main AI techniques and their development, including traditional methods across different categories, as well as recent emerging methods in the field. Then, through our analysis of the literature, we summarize the predominant AI and ML techniques adopted and their applications in perinatal mental health studies, such as identifying risk factors, predicting perinatal mental health disorders, voice assistants, and Q&A chatbots. We also discuss existing limitations and potential challenges that hinder AI technologies from improving perinatal mental health outcomes, and suggest several promising directions for future research to meet real needs in the field and facilitate the translation of research into clinical settings.",article,,,,,,,,,
Applying model-driven engineering to the domain of chatbots: The Xatkit experience,Science of Computer Programming,232,103032,2024,0167-6423,https://doi.org/10.1016/j.scico.2023.103032,https://www.sciencedirect.com/science/article/pii/S0167642323001144,Gwendal Daniel and Jordi Cabot,"Chatbots, Commercial, Lessons learned, DSL","Chatbots are becoming a common component of many types of software systems. But they are typically developed as a side feature using ad-hoc tools and custom integrations. Moreover, current frameworks are efficient only when designing simple chatbot applications while they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs. In addition, the deployment of a chatbot application usually requires a deep understanding of the targeted platforms, especially back-end connections, increasing the development and maintenance costs. In this paper, we discuss our experiences building, evolving and distributing the Xatkit framework. Xatkit is a model-based framework built around a Domain-Specific Language to define chatbots (and voicebots and bots in general) in a platform-independent way. Xatkit also comes with a runtime engine that automatically deploys the chatbot application and manages the defined conversation logic over the platforms of choice. Xatkit has significantly evolved since its initial release. This paper focuses on describing the evolution and the reasons (technical and non-technical) that triggered them. We believe our lessons learned can be useful to any other initiative trying to build a successful industrial-level chatbot platform, and in general, any type of model-based solution.",article,,,,,,,,,
GPT-4V with emotion: A zero-shot benchmark for Generalized Emotion Recognition,Information Fusion,108,102367,2024,1566-2535,https://doi.org/10.1016/j.inffus.2024.102367,https://www.sciencedirect.com/science/article/pii/S1566253524001453,Zheng Lian and Licai Sun and Haiyang Sun and Kang Chen and Zhuofan Wen and Hao Gu and Bin Liu and Jianhua Tao,"Generalized Emotion Recognition (GER), GPT-4 with Vision (GPT-4V), Zero-shot benchmark, Multimodal fusion, Temporal modeling","Recently, GPT-4 with Vision (GPT-4V) has demonstrated remarkable visual capabilities across various tasks, but its performance in emotion recognition has not been fully evaluated. To bridge this gap, we present the quantitative evaluation results of GPT-4V on 21 benchmark datasets covering 6 tasks: visual sentiment analysis, tweet sentiment analysis, micro-expression recognition, facial emotion recognition, dynamic facial emotion recognition, and multimodal emotion recognition. This paper collectively refers to these tasks as “Generalized Emotion Recognition (GER)”. Through experimental analysis, we observe that GPT-4V exhibits strong visual understanding capabilities in GER tasks. Meanwhile, GPT-4V shows the ability to integrate multimodal clues and exploit temporal information, which is also critical for emotion recognition. However, it is worth noting that GPT-4V is primarily designed for general domains and cannot recognize micro-expressions that require specialized knowledge. To the best of our knowledge, this paper provides the first quantitative assessment of GPT-4V for GER tasks. We have open-sourced the code and encourage subsequent researchers to broaden the evaluation scope by including more tasks and datasets. Our code and evaluation results are available at: https://github.com/zeroQiaoba/gpt4v-emotion.",article,,,,,,,,,
Trust and reliance on AI — An experimental study on the extent and costs of overreliance on AI,Computers in Human Behavior,,108352,2024,0747-5632,https://doi.org/10.1016/j.chb.2024.108352,https://www.sciencedirect.com/science/article/pii/S0747563224002206,Artur Klingbeil and Cassandra Grützner and Philipp Schreck,"Human-computer interaction, Behavioral experiment, Reliance behavior, Trust attitude, Overreliance, Algorithm appreciation","Decision-making is undergoing rapid changes due to the introduction of artificial intelligence (AI), as AI recommender systems can help mitigate human flaws and increase decision accuracy and efficiency. However, AI can also commit errors or suffer from algorithmic bias. Hence, blind trust in technologies carries risks, as users may follow detrimental advice resulting in undesired consequences. Building upon research on algorithm appreciation and trust in AI, the current study investigates whether users who receive AI advice in an uncertain situation overrely on this advice — to their own detriment and that of other parties. In a domain-independent, incentivized, and interactive behavioral experiment, we find that the mere knowledge of advice being generated by an AI causes people to overrely on it, that is, to follow AI advice even when it contradicts available contextual information as well as their own assessment. Frequently, this overreliance leads not only to inefficient outcomes for the advisee, but also to undesired effects regarding third parties. The results call into question how AI is being used in assisted decision making, emphasizing the importance of AI literacy and effective trust calibration for productive deployment of such systems.",article,,,,,,,,,
Improving domain-specific neural code generation with few-shot meta-learning,Information and Software Technology,166,107365,2024,0950-5849,https://doi.org/10.1016/j.infsof.2023.107365,https://www.sciencedirect.com/science/article/pii/S0950584923002203,Zhen Yang and Jacky Wai Keung and Zeyu Sun and Yunfei Zhao and Ge Li and Zhi Jin and Shuo Liu and Yishu Li,"Code generation, Few-shot learning, Meta-learning, Transfer learning","Context:
Neural code generation aims to automatically generate code snippets guided by Natural Language Descriptions (NLDs). In recent years, various neural code generation models for mainstream Programming Languages (PLs), such as Java and Python, have been proposed and demonostrated significant success in prior studies. Nonetheless, due to the scarcity of available training examples for some domain-specific PLs, such as Solidity, Bash, and Clojure, simply adopting previous neural models may lead to overfitting and inadequate learning.
Objective:
To overcome this challenge, we propose MetaCoder, a novel meta-learning code generation approach that efficiently extracts general-purpose knowledge from a large-scale source language and rapidly adapts to domain-specific scenarios, even with relatively few samples.
Method:
MetaCoder employs MAML, a powerful few-shot meta-learning method, to construct a transfer learning framework. This framework learns general-purpose knowledge from large-scale source languages and applies it in domain-specific target languages. To acquire more general-purpose knowledge, heterogeneous sub-tasks are constructed from the source language during the pre-training phase of MAML. As such, combining with CodeBERT and K-means, we design an unsupervised category assignment method for code generation samples, thereby exploiting the n-way k-shot rule to construct the heterogeneous sub-tasks. Consequently, MetaCoder can be applied to the code generation field.
Results:
We evaluate MetaCoder with both tree-based (e.g., TreeGen) and sequence-based (e.g., CodeGPT) backbones on two domain-specific PLs, including Solidity and Bash. Extensive experiments demonstrate the superior performance of our approach compared to baselines and verified its capability of code generation visually in practice.
Conclusion:
MetaCoder effectively extracts general-purpose knowledge from large-scale source languages, thereby enhancing model performance. Therefore, we highly recommend MetaCoder as a code generation approach for domain-specific PLs.",article,,,,,,,,,
11 - From synapses to ephapsis: Embodied cognition and wearable personal assistants,,,205-222,2024,,https://doi.org/10.1016/B978-0-323-96104-2.00005-1,https://www.sciencedirect.com/science/article/pii/B9780323961042000051,Roman Ormandy,"Embodied cognition, Ephapsis, Motor action, Neural populations, Neural dynamics, Resonance, Synapses, Wearable assistants","Despite their significant successes, neural networks typically represent relatively static memory structures and solve static classification problems. The next step in the evolution of AI systems will be the capture of the dynamic aspects of cognition. The dynamics are embodied in the ephaptic fields of the neocortex and limbic system, formed by the vast populations of resonating electric dipoles comprised of a multitude of ion channels present on the surface of each neuron. These ion-based e-fields form dynamic brainwaves, which synchronize distant areas of the cortex at the speed of light (orders of magnitude faster than axonal pulse speed) via resonance in the beta, theta, and gamma range. They are quite important for the understanding of the working of the brain. Ephaptic fields are also a perfect bridge to the motor behavior of organisms. This chapter shows that it is not only walking and grasping which is motor based, but also that vision, speech, and in fact, all perception and even memory are grounded in motor action. This has deep implications for the design of AI-based personal assistants. This chapter argues that field approach, ephapsis, and motor action are indispensable if the goal for the future generations of wearable sensor-based personal assistants is the real-time capture of user intent. Multimodal correlation of motor sensors of user daily activities is the essential ingredient, which so far eluded AI researchers and precluded wearable assistants from a wider user adoption. It turns out that thinking is embodied indeed. We discuss how recent developments in making movies from chat and merging large language models AI and 3D, can lead to a new generation of personal assistants, where metaverse, AI, and AR are coming together in the most surprising ways.",incollection,,,Robert Kozma and Cesare Alippi and Yoonsuck Choe and Francesco Carlo Morabito,Artificial Intelligence in the Age of Neural Networks and Brain Computing (Second Edition),Academic Press,,978-0-323-96104-2,,Second Edition
Contents,,,v-xiii,2024,,https://doi.org/10.1016/B978-0-443-18851-0.00025-1,https://www.sciencedirect.com/science/article/pii/B9780443188510000251,,,,incollection,,,Santi Caballé and Joan Casas-Roma and Jordi Conesa,Ethics in Online AI-based Systems,Academic Press,Intelligent Data-Centric Systems,978-0-443-18851-0,,
The defeat of the Winograd Schema Challenge,Artificial Intelligence,325,103971,2023,0004-3702,https://doi.org/10.1016/j.artint.2023.103971,https://www.sciencedirect.com/science/article/pii/S0004370223001170,Vid Kocijan and Ernest Davis and Thomas Lukasiewicz and Gary Marcus and Leora Morgenstern,"Commonsense reasoning, Winograd Schema Challenge","The Winograd Schema Challenge—a set of twin sentences involving pronoun reference disambiguation that seem to require the use of commonsense knowledge—was proposed by Hector Levesque in 2011. By 2019, a number of AI systems, based on large pre-trained transformer-based language models and fine-tuned on these kinds of problems, achieved better than 90% accuracy. In this paper, we review the history of the Winograd Schema Challenge and discuss the lasting contributions of the flurry of research that has taken place on the WSC in the last decade. We discuss the significance of various datasets developed for WSC, and the research community's deeper understanding of the role of surrogate tasks in assessing the intelligence of an AI system.",article,,,,,,,,,
iCORPP: Interleaved commonsense reasoning and probabilistic planning on robots,Robotics and Autonomous Systems,174,104613,2024,0921-8890,https://doi.org/10.1016/j.robot.2023.104613,https://www.sciencedirect.com/science/article/pii/S092188902300252X,Shiqi Zhang and Piyush Khandelwal and Peter Stone,"Integrated Reasoning and Planning, Commonsense reasoning, Planning under uncertainty, Autonomous Robots, Markov Decision Processes, POMDPs","Robot sequential decision-making in the real world is a challenge because it requires the robots to simultaneously reason about the current world state and dynamics, while planning actions to accomplish complex tasks. On the one hand, declarative languages and reasoning algorithms support representing and reasoning with commonsense knowledge. But these algorithms are not good at planning actions toward maximizing cumulative reward over a long, unspecified horizon. On the other hand, probabilistic planning frameworks, such as Markov decision processes (MDPs) and partially observable MDPs (POMDPs), support planning to achieve long-term goals under uncertainty. But they are ill-equipped to represent or reason about knowledge that is not directly related to actions. In this article, we present an algorithm, called iCORPP, to simultaneously estimate the current world state, reason about world dynamics, and construct task-oriented controllers. In this process, robot decision-making problems are decomposed into two interdependent (smaller) subproblems that focus on reasoning to “understand the world” and planning to “achieve the goal” respectively. The developed algorithm has been implemented and evaluated both in simulation and on real robots using everyday service tasks, such as indoor navigation, and dialog management. Results show significant improvements in scalability, efficiency, and adaptiveness, compared to competitive baselines including handcrafted action policies.",article,,,,,,,,,
Dialogue summarization enhanced response generation for multi-domain task-oriented dialogue systems,Information Processing & Management,61,103668,2024,0306-4573,https://doi.org/10.1016/j.ipm.2024.103668,https://www.sciencedirect.com/science/article/pii/S0306457324000281,Lifang Wang and Meng Zhao and Hongru Ji and Zejun Jiang and Ronghan Li and Zhongtian Hu and Xinyu Lu,"Task-oriented dialogue system, Dialogue summarization, Response generation, Pre-trained language model","Task-oriented dialogue systems (TOD) are blossoming with the advances in pre-trained language models (PrLM). Recently, research on PrLM-based multi-domain TOD has arisen with many outstanding outcomes. However, three challenges still need to be thoroughly studied. First, most current works regard dialogue state tracking as a generative problem supervised by concatenated slot-value sequences, impairing the models’ domain adaption because of the discrepancy between PrLM’s natural text inputs and spliced slot-value spans. Second, most existing works seldom specifically consider how to deal with long and involved dialogue history caused by multiple task domains. Third, few studies are concerned with enhancing the model’s reasoning ability to handle intricate contexts. To alleviate these issues, we propose a dialogue summarization enhanced response generation framework for multi-domain TOD. Specifically, we offer a novel summarization model that employs the query and the generated summarization from the previous turn to obtain beneficial information for the current turn, which is then combined with the entire dialogue history to produce the final summary. Then, the generated dialogue summarization is fed to the response decoder as dialogue states and key dialogue histories through the designed dynamic fusion mechanism to yield responses. Experimental results indicate that the proposed model for response generation task outperforms the baseline models in both automatic and human evaluations on two public datasets.",article,3,,,,,,,,
Memorization and generalization in neural code intelligence models,Information and Software Technology,153,107066,2023,0950-5849,https://doi.org/10.1016/j.infsof.2022.107066,https://www.sciencedirect.com/science/article/pii/S0950584922001756,Md Rafiqul Islam Rabin and Aftab Hussain and Mohammad Amin Alipour and Vincent J. Hellendoorn,"Machine learning, Software engineering, Memorization and generalization, Empirical results, Models of code","Context:
Deep Neural Networks (DNNs) are increasingly being used in software engineering and code intelligence tasks. These are powerful tools that are capable of learning highly generalizable patterns from large datasets through millions of parameters. At the same time, their large capacity can render them prone to memorizing data points. Recent work suggests that the memorization risk manifests especially strongly when the training dataset is noisy, involving many ambiguous or questionable samples, and memorization is the only recourse.
Objective:
The goal of this paper is to evaluate and compare the extent of memorization and generalization in neural code intelligence models. It aims to provide insights on how memorization may impact the learning behavior of neural models in code intelligence systems.
Method:
To observe the extent of memorization in models, we add random noise to the original training dataset and use various metrics to quantify the impact of noise on various aspects of training and testing. We evaluate several state-of-the-art neural code intelligence models and benchmarks based on Java, Python, and Ruby codebases.
Results:
Our results highlight important risks: millions of trainable parameters allow the neural networks to memorize anything, including noisy data, and provide a false sense of generalization. We observed all models manifest some forms of memorization. This can be potentially troublesome in most code intelligence tasks where they rely on rather noise-prone and repetitive data sources, such as code from GitHub.
Conclusion:
To the best of our knowledge, we provide the first study to quantify memorization effects in the domain of software engineering and code intelligence systems. This work raises awareness and provides new insights into important issues of training neural models in code intelligence systems that are usually overlooked by software engineering researchers.",article,,,,,,,,,
Semantic interoperability for an AI-based applications platform for smart hospitals using HL7 FHIR,Journal of Systems and Software,215,112093,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112093,https://www.sciencedirect.com/science/article/pii/S0164121224001389,Emmanouil S. Rigas and Paris Lagakis and Makis Karadimas and Evangelos Logaras and Dimitra Latsou and Magda Hatzikou and Athanasios Poulakidas and Antonis Billis and Panagiotis D. Bamidis,"Semantic, Interoperability, FHIR, Smart hospital, Digital healthcare, Artificial intelligence, Application platform","The digitization of the healthcare domain has the potential to drastically improve healthcare services, reduce the time to diagnosis, and lower costs. However, digital applications for the healthcare domain need to be interoperable to maximize their potential. Additionally, with the rapid expansion of Artificial Intelligence (AI) and, specifically, Machine Learning (ML), large amounts of diverse types of data are being utilized. Thus, to achieve interoperability in such applications, the adoption of common semantic data models becomes imperative. In this paper, we describe the adoption of such a common semantic data model, using the well-known Health Level Seven Fast Health Interoperability Resources (HL7 FHIR) standard, in a platform that assists in the creation and storage of a plethora of AI-based applications for several medical conditions. The FHIR server’s efficiency is being showcased by using it in an application predicting coronary artery stenosis as well as for managing the platform’s key performance indicators.",article,,,,,,,,,
Efficient intent classification and entity recognition for university administrative services employing deep learning models,Intelligent Systems with Applications,19,200247,2023,2667-3053,https://doi.org/10.1016/j.iswa.2023.200247,https://www.sciencedirect.com/science/article/pii/S2667305323000728,S. Rizou and A. Theofilatos and A. Paflioti and E. Pissari and I. Varlamis and G. Sarigiannidis and K.Ch. Chatzisavvas,"Named Entity Recognition, Intent Extraction, Natural Language Understanding, Deep Learning, LSTM networks, Transformer networks, Conversational Agents","The design and implementation of a domain specific conversational agent requires efficient Natural Language Understanding (NLU). The task is harder when multiple languages have to be supported, and training datasets can be beneficial. This work focuses on the development of an intelligent system, an automated multilingual customer service conversational agent (chatbot) for university students, which supports both Greek and English and combines Intent Classification or Intent Extraction (IE) and Named Entity Recognition (NER) to understand the content (i.e. type of actions conveyed and respective entities) of users' messages. We focus on the development of the fundamental tasks required by a conversational agent to provide customer services in the education industry and manage requests with instant responses and increased customer satisfaction. Instead of handling IE and NER separately, as it is common in the related work, we develop a joint model that combines Bidirectional Long Short-Term Memory (BiLSTM) and Conditional Random Fields (CRF) layers and generates outputs both for IE and NER. We introduce a novel, open access dataset for customer services in education industry, the UniWay dataset, that has been used for training and evaluating our model, comprises students' questions in English and Greek about essential information related to their studies. A comparative evaluation of the proposed model versus state-of-the-art standalone and joint model solutions in UniWay and xSID datasets, results in improvement of the performance for the IE task up to 1.4% and it is on par with the state-of-the-art for the NER task. These results justify the intuition that closed domains can benefit from less sophisticated architectures, but less costly in terms of computational and memory resources, that jointly resolve multiple NLU tasks.",article,,,,,,,,,
"Exploring the limitations in how ChatGPT introduces environmental justice issues in the United States: A case study of 3,108 counties",Telematics and Informatics,86,102085,2024,0736-5853,https://doi.org/10.1016/j.tele.2023.102085,https://www.sciencedirect.com/science/article/pii/S0736585323001491,Junghwan Kim and Jinhyung Lee and Kee Moon Jang and Ismini Lourentzou,"ChatGPT, Disparities, Environmental justice, Generative AI, Geographic bias","The potential of Generative AI, such as ChatGPT, has sparked discussions among researchers and the public. This study empirically explores the capabilities and limitations of ChatGPT, specifically its portrayal of environmental justice issues. Using OpenAI’s ChatGPT API, we asked ChatGPT (GPT-4) to answer questions about environmental justice issues in 3,108 counties in the contiguous United States. Our findings suggest that ChatGPT provides a general overview of environmental justice issues. Consistent with research, ChatGPT appears to acknowledge the disproportionate distribution of environmental pollutants and toxic materials in low-income communities and those inhabited by people of color. However, our results also highlighted ChatGPT’s shortcomings in detailing specific local environmental justice issues, particularly in disadvantaged (e.g., rural and low-income) counties. For instance, ChatGPT could not provide information on local-specific environmental justice issues for 2,593 of 3,108 counties (83%). The results of the binary logistic regression model revealed that counties with lower population densities, higher percentages of white population, and lower incomes are less likely to receive local-specific responses from the ChatGPT. This could indicate a potential regional disparity in the volume and quality of training data, hinting at geographical biases. Our findings offer insights and implications for educators, researchers, and AI developers.",article,,,,,,,,,
Exploring the psychology of LLMs’ moral and legal reasoning,Artificial Intelligence,333,104145,2024,0004-3702,https://doi.org/10.1016/j.artint.2024.104145,https://www.sciencedirect.com/science/article/pii/S000437022400081X,Guilherme F.C.F. Almeida and José Luiz Nunes and Neele Engelmann and Alex Wiegmann and Marcelo de Araújo,"AI Ethics, Experimental jurisprudence, Ethics of artificial intelligence, Machine Behavior, Moral psychology, Machine psychology, Large language models","Large language models (LLMs) exhibit expert-level performance in tasks across a wide range of different domains. Ethical issues raised by LLMs and the need to align future versions makes it important to know how state of the art models reason about moral and legal issues. In this paper, we employ the methods of experimental psychology to probe into this question. We replicate eight studies from the experimental literature with instances of Google's Gemini Pro, Anthropic's Claude 2.1, OpenAI's GPT-4, and Meta's Llama 2 Chat 70b. We find that alignment with human responses shifts from one experiment to another, and that models differ amongst themselves as to their overall alignment, with GPT-4 taking a clear lead over all other models we tested. Nonetheless, even when LLM-generated responses are highly correlated to human responses, there are still systematic differences, with a tendency for models to exaggerate effects that are present among humans, in part by reducing variance. This recommends caution with regards to proposals of replacing human participants with current state-of-the-art LLMs in psychological research and highlights the need for further research about the distinctive aspects of machine psychology.",article,,,,,,,,,
Benchmarking ChatGPT for prototyping theories: Experimental studies using the technology acceptance model,"BenchCouncil Transactions on Benchmarks, Standards and Evaluations",3,100153,2023,2772-4859,https://doi.org/10.1016/j.tbench.2024.100153,https://www.sciencedirect.com/science/article/pii/S277248592400005X,Tiong-Thye Goh and Xin Dai and Yanwu Yang,"ChatGPT, Large language model, Technology acceptance model, Prototyping Theory","This paper explores the paradigm of leveraging ChatGPT as a benchmark tool for theory prototyping in conceptual research. Specifically, we conducted two experimental studies using the classical technology acceptance model (TAM) to demonstrate and evaluate ChatGPT's capability of comprehending theoretical concepts, discriminating between constructs, and generating meaningful responses. Results of the two studies indicate that ChatGPT can generate responses aligned with the TAM theory and constructs. Key metrics including the factors loading, internal consistency reliability, and convergence reliability of the measurement model surpass the minimum threshold, thus confirming the validity of TAM constructs. Moreover, supported hypotheses provide an evidence for the nomological validity of TAM constructs. However, both of the two studies show a high Heterotrait–Monotrait ratio of correlations (HTMT) among TAM constructs, suggesting a concern about discriminant validity. Furthermore, high duplicated response rates were identified and potential biases regarding gender, usage experiences, perceived usefulness, and behavioural intention were revealed in ChatGPT-generated samples. Therefore, it calls for additional efforts in LLM to address performance metrics related to duplicated responses, the strength of discriminant validity, the impact of prompt design, and the generalizability of findings across contexts.",article,4,,,,,,,,
ChatGPT and the rise of generative AI: Threat to academic integrity?,Journal of Responsible Technology,13,100060,2023,2666-6596,https://doi.org/10.1016/j.jrt.2023.100060,https://www.sciencedirect.com/science/article/pii/S2666659623000033,Damian Okaibedi Eke,"ChatGPT, Large language models, OpenAI, Academic integrity, Generative AI","The emergence of OpenAI's ChatGPT has put intense spotlight on Generative AI (Gen-AI) systems and their possible impacts on Academic integrity. This paper provides an overview of the current arguments around ChatGPT and Academic integrity and concludes that although these technologies are capable of revolutionising academia, the way ChatGPT and other generative AI systems are used could surely undermine academic integrity. However, to ensure that the risks to academic integrity are mitigated for greater maximisation, institutional and multi-stakeholder efforts are required.",article,,,,,,,,,
GPT-aided diagnosis on agricultural image based on a new light YOLOPC,Computers and Electronics in Agriculture,213,108168,2023,0168-1699,https://doi.org/10.1016/j.compag.2023.108168,https://www.sciencedirect.com/science/article/pii/S0168169923005562,Jiajun Qing and Xiaoling Deng and Yubin Lan and Zhikai Li,"Citrus pests and diseases, Lightweight, Large language models","Large Language Models (LLM) have been extensively studied for their ability to engage in textual dialogue and have shown promising results in various fields. However, the agricultural industry has yet to fully integrate LLM into its practice due to the dominance of visual images in agricultural data that cannot be effectively processed by LLM designed for text. Additionally, traditional image classification networks have limitations in understanding crop etiology and disease, hindering accurate diagnosis. Furthermore, the mixture of diseases can also interfere with the network's prediction. Therefore, accurately analyzing pests and diseases in agricultural scenarios and providing diagnostic reports remains a challenge. To address this issue, a novel approach that combines the deep logical reasoning capabilities of GPT-4 with the visual understanding capabilities of the YOLO (You Only Look Once) network was proposed in this study. Additionally, a new lightweight variant of YOLO, called YOLOPC, and a novel image-to-text mapping method for adapting YOLO and GPT were introduced. The experimental results demonstrate that YOLOPC, with approximately 75% fewer parameters than YOLOv5-nano, achieves a 94.5% accuracy rate. The GPT induction and reasoning module demonstrates 90% reasoning accuracy in generating agricultural diagnostic reports with text assistance. In the future, it is likely that a higher-performance GPT model will be released. The combination of GPT with agricultural scenarios will become the cornerstone of large-scale agricultural diagnostic models. The proposed method will benefit the development of large-scale models in the agricultural field.",article,,,,,,,,,
Students' Use of the Artificial Intelligence Language Model in their Learning Process,Procedia Computer Science,225,3059-3066,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.10.299,https://www.sciencedirect.com/science/article/pii/S1877050923014576,Rafał Niedbał and Adam Sokołowski and Artur Wrzalik,"ChatGPT, Generative Artificial Intelligence, chatbot, Large Language Models, modern IT in education, innovative education","Generative Artificial Intelligence (GAI), of which ChatGPT is an exemplary tool, is beginning to revolutionize the way people search for information and use the information they acquire in their personal and professional lives. ChatGPT is showing a strong track record in a variety of tasks, such as generating text, summarizing text and answering questions during a conversation. It has the potential to revolutionize a wide range of fields - including education. The purpose of this article is to evaluate the extent to which the ChatGPT language model can be applied in the learning process for two types of students: full-time and part-time. Additionally, this article assesses the level of students' familiarity with intelligent chat functionality and their ability to construct queries directed to it. The study found that the use of an advanced language model based on artificial intelligence is more beneficial for full-time students in the learning process. However, there was no statistically significant difference in the knowledge of intelligent chat functionality and the ability to construct queries directed to it between full-time and part-time students.",article,,27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023),,,,,,,
Exploring the adoption of the metaverse and chat generative pre-trained transformer: A single-valued neutrosophic Dombi Bonferroni-based method for the selection of software development strategies,Engineering Applications of Artificial Intelligence,133,108378,2024,0952-1976,https://doi.org/10.1016/j.engappai.2024.108378,https://www.sciencedirect.com/science/article/pii/S0952197624005360,Abdullah Önden and Karahan Kara and İsmail Önden and Galip Cihan Yalçın and Vladimir Simic and Dragan Pamucar,"Virtual reality, Metaverse, Natural language processing, Single-valued neutrosophic sets, Alternative ranking order method accounting for two-step normalization","The contemporary era has witnessed remarkable developments that seek to transform and reshape traditional software development methodologies. Notably, artificial intelligence (AI) supported software development as well as software development in virtual reality environments have gained considerable prominence. This article introduces software development strategies to examine how software developers and companies respond to this transformation. Also, an advanced decision model is developed using the alternative ranking order method accounting for two-step normalization (AROMAN) method and further analyzed with the single-valued neutrosophic set-based AROMAN technique. The single-valued neutrosophic weighted Dombi Bonferroni operator is employed in the analysis process. This research offers two case studies investigating the preferences of developers and managers in software development strategies. The first case study examines the preferences of developers, while the second focuses on the preferences of managers. In both case studies, three fundamental software development methods are presented. These include the “traditional developers approach”, “AI-supported developers approach”, and “mixed reality and AI-supported developers approach”. These methods are ranked based on expert opinions concerning 10 criteria that influence the software development process. In both case studies, “output quality” is identified as the most influential criterion. From the perspective of software development methods, in both case studies, the “mixed reality and AI-supported developers approach” is identified as the most effective. Recommendations are provided for developers and managers. The findings also have significant implications for guiding developers and managers in making informed decisions and optimizing software development practices to align with the evolving AI and virtual reality landscape.",article,,,,,,,,,
The impact of ChatGPT and LLMs on medical imaging stakeholders: Perspectives and use cases,Meta-Radiology,1,100007,2023,2950-1628,https://doi.org/10.1016/j.metrad.2023.100007,https://www.sciencedirect.com/science/article/pii/S2950162823000073,Jiancheng Yang and Hongwei Bran Li and Donglai Wei,"ChatGPT, LLM, Foundation models, Medical imaging","This study investigates the transformative potential of Large Language Models (LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public data, these models, which possess remarkable language understanding and generation capabilities, are augmenting the interpretive skills of radiologists, enhancing patient-physician communication, and streamlining clinical workflows. The paper introduces an analytic framework for presenting the complex interactions between LLMs and the broader ecosystem of medical imaging stakeholders, including businesses, insurance entities, governments, research institutions, and hospitals (nicknamed BIGR-H). Through detailed analyses, illustrative use cases, and discussions on the broader implications and future directions, this perspective seeks to raise discussion in strategic planning and decision-making in the era of AI-enabled healthcare.",article,1,,,,,,,,
Retrieval augmentation of large language models for lay language generation,Journal of Biomedical Informatics,149,104580,2024,1532-0464,https://doi.org/10.1016/j.jbi.2023.104580,https://www.sciencedirect.com/science/article/pii/S1532046423003015,Yue Guo and Wei Qiu and Gondy Leroy and Sheng Wang and Trevor Cohen,"Large language models, Retrieval-augmented model, Lay language summary, Background explanation, Text generation","The complex linguistic structures and specialized terminology of expert-authored content limit the accessibility of biomedical literature to the general public. Automated methods have the potential to render this literature more interpretable to readers with different educational backgrounds. Prior work has framed such lay language generation as a summarization or simplification task. However, adapting biomedical text for the lay public includes the additional and distinct task of background explanation: adding external content in the form of definitions, motivation, or examples to enhance comprehensibility. This task is especially challenging because the source document may not include the required background knowledge. Furthermore, background explanation capabilities have yet to be formally evaluated, and little is known about how best to enhance them. To address this problem, we introduce Retrieval-Augmented Lay Language (RALL) generation, which intuitively fits the need for external knowledge beyond that in expert-authored source documents. In addition, we introduce CELLS, the largest (63k pairs) and broadest-ranging (12 journals) parallel corpus for lay language generation. To evaluate RALL, we augmented state-of-the-art text generation models with information retrieval of either term definitions from the UMLS and Wikipedia, or embeddings of explanations from Wikipedia documents. Of these, embedding-based RALL models improved summary quality and simplicity while maintaining factual correctness, suggesting that Wikipedia is a helpful source for background explanation in this context. We also evaluated the ability of both an open-source Large Language Model (Llama 2) and a closed-source Large Language Model (GPT-4) in background explanation, with and without retrieval augmentation. Results indicate that these LLMs can generate simplified content, but that the summary quality is not ideal. Taken together, this work presents the first comprehensive study of background explanation for lay language generation, paving the path for disseminating scientific knowledge to a broader audience. Our code and data are publicly available at: https://github.com/LinguisticAnomalies/pls_retrieval.",article,,,,,,,,,
Exploratory prompting of large language models to act as co-pilots for augmenting business process work in document classification,Procedia Computer Science,237,420-425,2024,1877-0509,https://doi.org/10.1016/j.procs.2024.05.123,https://www.sciencedirect.com/science/article/pii/S1877050924011396,Jose Ramon Ilagan and Joseph Benjamin Ilagan and Claire Louisse Basallo and Zachary Matthew Alabastro,"LLM, Document classification, GPT, RPA, Generative AI, AI, Natural language processing, NLP","Businesses deal with different types of documents containing unstructured documents. The data in these documents must be converted into digital forms other automated systems could only process. One generic use case is document classification, which usually involves manual transformation due to human understanding needed in the process. These documents go beyond those generated through regular business transactions and operations and also include web-based content such as online news, blogs, e-mails, and various digital libraries. Recent developments in robotic process automation (RPA) and artificial intelligence (AI) aim to automate the otherwise expensive, time-consuming, and repetitive manual steps. Through more powerful natural language processing (NLP) and natural language understanding (NLU) capabilities, large language models (LLMs) may come as a big boost in applying AI to RPA initiatives. This study proposes a general approach to using LLMs as document classifier co-pilots for knowledge workers in charge of classifying documents to be useful. The manner of prompt engineering and refinement involving labeled health insurance documents to achieve better results is discussed and evaluated through early, iterative classification attempts. However, early tests with a complex sample use case show unsatisfactory results. The study ends with recommendations for future work to improve precision and recall performance.",article,,International Conference on Industry Sciences and Computer Science Innovation,,,,,,,
ChatGPT or Bard: Who is a better Certified Ethical Hacker?,Computers & Security,140,103804,2024,0167-4048,https://doi.org/10.1016/j.cose.2024.103804,https://www.sciencedirect.com/science/article/pii/S0167404824001056,Raghu Raman and Prasad Calyam and Krishnashree Achuthan,"Ethical hacking, Policy, Social behavior, Readability, Similarity analysis, Cybersecurity generative ai","In this study, we compare two leading Generative AI (GAI) tools, ChatGPT and Bard, specifically in Cybersecurity, using a robust set of standardized questions from a validated Certified Ethical Hacking (CEH) dataset. In the rapidly evolving domain of Generative AI (GAI) and large language models (LLM), a comparative analysis of tools becomes essential to measure their performance. We determine the Comprehensiveness, Clarity, and Conciseness of the AI-generated responses through a detailed questioning-based framework. The study revealed an overall accuracy rate of 80.8 % for ChatGPT and 82.6 % for Bard, indicating comparable capabilities and specific differences. Bard slightly outperformed ChatGPT in accuracy, while ChatGPT exhibited superiority in Comprehensiveness, Clarity, and Conciseness of responses. Introducing a confirmation query like “Are you sure?” increased accuracy for both generative AI tools, illustrating the potential of iterative query processing in enhancing GAI tools' effectiveness. The readability evaluation placed both tools at a college reading level, with Bard marginally more accessible. While evaluating certain questions, a distinct pattern emerged where Bard provided generic denials of assistance while ChatGPT referenced “ethics.” This discrepancy illustrates the contrasting philosophies of the developers of these tools, with Bard possibly following stricter guidelines, especially in sensitive topics like Cybersecurity. We explore the implications and identify key areas for future research that become increasingly relevant as GAI tools see broader adoption.",article,,,,,,,,,
Offline prompt polishing for low quality instructions,Neurocomputing,598,128046,2024,0925-2312,https://doi.org/10.1016/j.neucom.2024.128046,https://www.sciencedirect.com/science/article/pii/S0925231224008178,Jia Yu and Zhanchao Zhou and Long Li and Ling Li and Yuming Yan and Renjun Xu and Zhenzhong Lan,"LLM, Dataset, User scenario, Offline prompt polishing","Instruction-tuning is an effective avenue for making large language models (LLMs) better at following real users’ instructions. However, it is challenging in aligning to human preference in user scenario since the instructions model received are usually not well-formatted. In this paper, we introduce offline prompt polishing and inserting specific delimiters before inputting them to the models to cope with these bad instructions. To better understand the user behavior in proposing instructions and how language models align to them, we introduce User-based Instructional Dataset (UID), a dataset comprises over 96,000 instruction–response pairs which contains over 3k human-revised free-form instructions collected from real-world scenarios. Within UID, we kept both original and revised instructions to improve model robustness. We obtained various IOPTs checkpoints, a range of OPT models (125M to 13B) trained with UID, through offline prompt polishing and delimiter insertion. The results demonstrate that IOPT-2.7B trained on 6,000 instances can achieve comparable performance to a 175B InstructGPT. Besides, we rigorously measure the impact of various factors including data volume, model size, and instruction format on aligning to real users’ instructions. We summarize several findings to shed a light on instruction-tuning under user scenario. Our dataset will be made public upon acceptance.",article,,,,,,,,,
"The general intelligence of GPT–4, its knowledge diffusive and societal influences, and its governance",Meta-Radiology,2,100078,2024,2950-1628,https://doi.org/10.1016/j.metrad.2024.100078,https://www.sciencedirect.com/science/article/pii/S2950162824000316,,"GPT–4, Artificial general intelligence, Knowledge diffusion, Interpretability and explainability, Societal influences, Governance","Recent breakthroughs in artificial intelligence (AI) research include advancements in natural language processing (NLP) achieved by large language models (LLMs), and; in particular, generative pre–trained transformer (GPT) architectures. The latest GPT developed by OpenAI, GPT–4, has shown remarkable intelligence across various domains and tasks. It exhibits capabilities in abstraction, comprehension, vision, computer coding, mathematics, and more, suggesting it to be a significant step towards artificial general intelligence (AGI), a level of AI that possesses capabilities similar to human intelligence. This paper explores this AGI, its knowledge diffusive and societal influences, and its governance. In addition to coverage of the major associated topics studied in the literature, and making up for their loopholes, we scrutinize how GPT-4 can facilitate the diffusion of knowledge across different areas of science by promoting their interpretability and explainability (IE) to inexperts. Where applicable, the topics are also accompanied by their specific potential implications on medical imaging.",article,2,,,,,,,,
A ChatGPT-MATLAB framework for numerical modeling in geotechnical engineering applications,Computers and Geotechnics,169,106237,2024,0266-352X,https://doi.org/10.1016/j.compgeo.2024.106237,https://www.sciencedirect.com/science/article/pii/S0266352X24001733,Daehyun Kim and Taegu Kim and Yejin Kim and Yong-Hoon Byun and Tae Sup Yun,"ChatGPT, Numerical modeling, Automated Programming, Artificial Intelligence (AI), Large Language Model (LLM)","ChatGPT has recently emerged as a representative of Large Language Models (LLMs) that have brought evolutionary changes to our society, and the effectiveness of ChatGPT in various applications has been increasingly reported. This study aimed to explore the potential of employing programming performance driven by ChatGPT responses to conversational prompts in the field of geotechnical engineering. The tested examples included the analysis of seepage flow and slope stability, and the image processing of X-ray computed tomographic image for partially saturated sand. For each case, the prompt was initially fed by a narrative explanation of the problem attributes such as geometry, initial conditions, and boundary conditions to generate the MATLAB code that was in turn executed to evaluate the correctness and functionality. Any errors and unanticipated results were further refined by additional prompts until the correct outcome was achieved. ChatGPT was able to generate the numerical code at a considerable level, demonstrating creditable awareness of the refining process, when meticulous prompts were provided based on a comprehensive understanding of given problems. While ChatGPT may not be able to replace the entire process of programming, it can help minimize sloppy syntax errors and assist in designing a basic framework for logical programming.",article,,,,,,,,,
When brain-inspired AI meets AGI,Meta-Radiology,1,100005,2023,2950-1628,https://doi.org/10.1016/j.metrad.2023.100005,https://www.sciencedirect.com/science/article/pii/S295016282300005X,Lin Zhao and Lu Zhang and Zihao Wu and Yuzhong Chen and Haixing Dai and Xiaowei Yu and Zhengliang Liu and Tuo Zhang and Xintao Hu and Xi Jiang and Xiang Li and Dajiang Zhu and Dinggang Shen and Tianming Liu,,"Artificial General Intelligence (AGI) has been a long-standing goal of humanity, with the aim of creating machines capable of performing any intellectual task that humans can do. To achieve this, AGI researchers draw inspiration from the human brain and seek to replicate its principles in intelligent machines. Brain-inspired artificial intelligence is a field that has emerged from this endeavor, combining insights from neuroscience, psychology, and computer science to develop more efficient and powerful AI systems. In this article, we provide a comprehensive overview of brain-inspired AI from the perspective of AGI. We begin with the current progress in brain-inspired AI and its extensive connection with AGI. We then cover the important characteristics for both human intelligence and AGI (e.g., scaling, multimodality, and reasoning). We discuss important technologies toward achieving AGI in current AI systems, such as in-context learning and prompt tuning. We also investigate the evolution of AGI systems from both algorithmic and infrastructural perspectives. Finally, we explore the limitations and future of AGI.",article,1,,,,,,,,
"Infrastructural justice for responsible software engineering,",Journal of Responsible Technology,19,100087,2024,2666-6596,https://doi.org/10.1016/j.jrt.2024.100087,https://www.sciencedirect.com/science/article/pii/S2666659624000131,Sarah Robinson and Jim Buckley and Luigina Ciolfi and Conor Linehan and Clare McInerney and Bashar Nuseibeh and John Twomey and Irum Rauf and John McCarthy,"Responsible software engineering, Infrastructure, Social connection model of responsibility, Installed base, Deepfake technology","In recent years, we have seen many examples of software products unintentionally causing demonstrable harm. Many guidelines for ethical and responsible computing have been developed in response. Dominant approaches typically attribute liability and blame to individual companies or actors, rather than understanding how the working practices, norms, and cultural understandings in the software industry contribute to such outcomes. In this paper, we propose an understanding of responsibility that is infrastructural, relational, and cultural; thus, providing a foundation to better enable responsible software engineering into the future. Our approach draws on Young's (2006) social connection model of responsibility and Star and Ruhleder's (1994) concept of infrastructure. By bringing these theories together we introduce a concept called infrastructural injustice, which offers a new way for software engineers to consider their opportunities for responsible action with respect to society and the planet. We illustrate the utility of this approach by applying it to an Open-Source software communities’ development of Deepfake technology, to find key leverage points of responsibility that are relevant to both Deepfake technology and software engineering more broadly.",article,,,,,,,,,
The emergence of compositionality in a brain-inspired cognitive architecture,Cognitive Systems Research,86,101215,2024,1389-0417,https://doi.org/10.1016/j.cogsys.2024.101215,https://www.sciencedirect.com/science/article/pii/S1389041724000081,Howard Schneider,"Compositionality, Brain-Inspired Cognitive Architecture (BICA), Artificial Intelligence (AI), Language evolution, Large Language Model (LLM), Neurosymbolic computing","Compositionality can be considered as finding (or creating) the correct meaning of the constituents of a non-simple language expression or visual image. The Causal Cognitive Architecture is a brain-inspired cognitive architecture (BICA). It is not a traditional artificial neural network architecture, nor a traditional symbolic AI system but instead uses spatial navigation maps as its fundamental circuits. In previously described versions of the architecture, sensory inputs are compared in each existing sensory system against previous stored navigation maps for that sensory system, and the best navigation map is chosen and then updated with the new sensory inputs and a best multisensory navigation map is similarly created and used as the working navigation map. Instinctive and learned small procedures are triggered by input sensory inputs as well as matched navigation maps, and in the Navigation Module operate on the working navigation map and produce an output signal. By feeding back intermediate results in the Navigation Module it has been shown previously how causal and analogical behaviors emerge from the architecture. In new work, the Navigation Module is duplicated in a biologically plausible manner. It becomes possible to compositionally process information in the duplicated Navigation Module, and as a result compositional language comprehension and behavior readily emerge. A formalization and simulation of the architecture is presented. A demonstration example, and its negation, are explored of solving a compositional problem requiring the placement of an object in a specific location with regard to other objects. Future work is discussed using large language models to create navigation maps. Given the mammalian brain inspiration of the architecture, it suggests that it is indeed feasible for modest genetic changes to have allowed the emergence of compositional language in humans.",article,,,,,,,,,
Promoting open science in test-driven software experiments,Journal of Systems and Software,212,111971,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.111971,https://www.sciencedirect.com/science/article/pii/S0164121224000141,Marcus Kessel and Colin Atkinson,"Software, Engineering, Empirical, Experimentation, Observation, Behavior, Reproducibility, Replication, Data structures, Open science, Large language models, Machine learning, Generative artificial intelligence, Benchmark, Language-to-code, HumanEval, Automation, Measurement","A core principle of open science is the clear, concise and accessible publication of empirical data, including “raw” observational data as well as processed results. However, in empirical software engineering there are no established standards (de jure or de facto) for representing and “opening” observations collected in test-driven software experiments — that is, experiments involving the execution of software subjects in controlled scenarios. Execution data is therefore usually represented in ad hoc ways, often making it abstruse and difficult to access without significant manual effort. In this paper we present new data structures designed to address this problem by clearly defining, correlating and representing the stimuli and responses used to execute software subjects in test-driven experiments. To demonstrate their utility, we show how they can be used to promote the repetition, replication and reproduction of experimental evaluations of AI-based code completion tools. We also show how the proposed data structures facilitate the incremental expansion of execution data sets, and thus promote their repurposing for new experiments addressing new research questions.",article,,,,,,,,,
Recent advancements and challenges of NLP-based sentiment analysis: A state-of-the-art review,Natural Language Processing Journal,6,100059,2024,2949-7191,https://doi.org/10.1016/j.nlp.2024.100059,https://www.sciencedirect.com/science/article/pii/S2949719124000074,Jamin Rahman Jim and Md Apon Riaz Talukder and Partha Malakar and Md Mohsin Kabir and Kamruddin Nur and M.F. Mridha,"Sentiment classification, Text classification, Natural language processing, Emotion detection, Sentiment analysis","Sentiment analysis is a method within natural language processing that evaluates and identifies the emotional tone or mood conveyed in textual data. Scrutinizing words and phrases categorizes them into positive, negative, or neutral sentiments. The significance of sentiment analysis lies in its capacity to derive valuable insights from extensive textual data, empowering businesses to grasp customer sentiments, make informed choices, and enhance their offerings. For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative. Therefore, in this extensive survey, we began exploring the vast array of application domains for sentiment analysis, scrutinizing them within the context of existing research. We then delved into prevalent pre-processing techniques, datasets, and evaluation metrics to enhance comprehension. We also explored Machine Learning, Deep Learning, Large Language Models and Pre-trained models in sentiment analysis, providing insights into their advantages and drawbacks. Subsequently, we precisely reviewed the experimental results and limitations of recent state-of-the-art articles. Finally, we discussed the diverse challenges encountered in sentiment analysis and proposed future research directions to mitigate these concerns. This extensive review provides a complete understanding of sentiment analysis, covering its models, application domains, results analysis, challenges, and research directions.",article,,,,,,,,,
Adoption and impacts of generative artificial intelligence: Theoretical underpinnings and research agenda,International Journal of Information Management Data Insights,4,100232,2024,2667-0968,https://doi.org/10.1016/j.jjimei.2024.100232,https://www.sciencedirect.com/science/article/pii/S2667096824000211,Ruchi Gupta and Kiran Nair and Mahima Mishra and Blend Ibrahim and Seema Bhardwaj,"ChatGPT, Adoption, Generative AI, Chatbots","Large language models (LLMs) have received considerable interest in the field of natural language processing (NLP) owing to their remarkable ability to generate clear, consistent, and contextually relevant materials. Among the numerous LLMs, ChatGPT (Generative Pre-trained Transformer for Chatbots) is emerging as a prominent prospective tool for developing conversational agents such as chatbots. However, there is a need for a clear conceptual understanding of ChatGPT's potential implications for the industry and its role in marketing. This study explores the adoption of ChatGPT in marketing and examines theories that may influence its adoption by marketers and consumers, as well as its implications for marketers. This study discusses how ChatGPT may allow for more personalized and engaging content, better customer experience, and improved ROI. However, adoption also brings challenges, including ethical considerations and the need for new skill development. This study also discusses future research opportunities for the adoption of ChatGPT and other generative artificial intelligence technologies in marketing. The goal is to provide insights for organizations that consider implementing these technologies, and to contribute to the literature on the adoption of Artificial Intelligence (AI) and the use of Generative AI in marketing.",article,1,,,,,,,,
Claude 2.0 large language model: Tackling a real-world classification problem with a new iterative prompt engineering approach,Intelligent Systems with Applications,21,200336,2024,2667-3053,https://doi.org/10.1016/j.iswa.2024.200336,https://www.sciencedirect.com/science/article/pii/S2667305324000127,Loredana Caruccio and Stefano Cirillo and Giuseppe Polese and Giandomenico Solimando and Shanmugam Sundaramurthy and Genoveffa Tortora,"Claude 2.0, Large language model, Online learning, Machine learning, Massive online analytics, Forest cover-type","In the last year, Large Language Models (LLMs) have transformed the way of tackling problems, opening up new perspectives in various works and research fields, due to their ability to generate and understand human languages. In this regard, the recent release of Claude 2.0 has contributed to the processing of more complex prompts. In this scenario, the goal of this paper is to evaluate the effectiveness of Claude 2.0 in a specific classification task. In particular, we considered the Forest cover-type problem, concerning the prediction of a cover-type value according to the geospatial characterization of target worldwide areas. To this end, we propose a novel iterative prompt template engineering approach, which integrates files by exploiting prompts and evaluates the quality of responses provided by the LLM. Moreover, we conducted several comparative analyses to evaluate the effectiveness of Claude 2.0 with respect to online and batch learning models. The results demonstrated that, although some online and batch models performed better than Claude 2.0, the new iterative prompt engineering approach improved the quality of responses, leading to better performance with increases ranging from 14% to 32% in terms of accuracy, precision, recall, and F1-score.",article,,,,,,,,,
Large language models: Expectations for semantics-driven systems engineering,Data & Knowledge Engineering,152,102324,2024,0169-023X,https://doi.org/10.1016/j.datak.2024.102324,https://www.sciencedirect.com/science/article/pii/S0169023X2400048X,Robert Buchmann and Johann Eder and Hans-Georg Fill and Ulrich Frank and Dimitris Karagiannis and Emanuele Laurenzi and John Mylopoulos and Dimitris Plexousakis and Maribel Yasmina Santos,"Large language models, Systems engineering, Conceptual modeling, Knowledge engineering","The hype of Large Language Models manifests in disruptions, expectations or concerns in scientific communities that have focused for a long time on design-oriented research. The current experiences with Large Language Models and associated products (e.g. ChatGPT) lead to diverse positions regarding the foreseeable evolution of such products from the point of view of scholars who have been working with designed abstractions for most of their careers - typically relying on deterministic design decisions to ensure systems and automation reliability. Such expectations are collected in this paper in relation to a flavor of systems engineering that relies on explicit knowledge structures, introduced here as “semantics-driven systems engineering”. The paper was motivated by the panel discussion that took place at CAiSE 2023 in Zaragoza, Spain, during the workshop on Knowledge Graphs for Semantics-driven Systems Engineering (KG4SDSE). The workshop brought together Conceptual Modeling researchers with an interest in specific applications of Knowledge Graphs and the semantic enrichment benefits they can bring to systems engineering. The panel context and consensus are summarized at the end of the paper, preceded by a proposed research agenda considering the expressed positions.",article,,,,,,,,,
To prompt or not to prompt: Navigating the use of Large Language Models for integrating and modeling heterogeneous data,Data & Knowledge Engineering,152,102313,2024,0169-023X,https://doi.org/10.1016/j.datak.2024.102313,https://www.sciencedirect.com/science/article/pii/S0169023X24000375,,"Data engineering, Large language models, Conceptual schema modeling, Entity resolution, Data integration, Property graph models","Manually integrating data of diverse formats and languages is vital to many artificial intelligence applications. However, the task itself remains challenging and time-consuming. This paper highlights the potential of Large Language Models (LLMs) to streamline data extraction and resolution processes. Our approach aims to address the ongoing challenge of integrating heterogeneous data sources, encouraging advancements in the field of data engineering. Applied on the specific use case of learning disorders in higher education, our research demonstrates LLMs’ capability to effectively extract data from unstructured sources. It is then further highlighted that LLMs can enhance data integration by providing the ability to resolve entities originating from multiple data sources. Crucially, the paper underscores the necessity of preliminary data modeling decisions to ensure the success of such technological applications. By merging human expertise with LLM-driven automation, this study advocates for the further exploration of semi-autonomous data engineering pipelines.",article,,,,,,,,,
BB-GeoGPT: A framework for learning a large language model for geographic information science,Information Processing & Management,61,103808,2024,0306-4573,https://doi.org/10.1016/j.ipm.2024.103808,https://www.sciencedirect.com/science/article/pii/S0306457324001675,Yifan Zhang and Zhiyun Wang and Zhengting He and Jingxuan Li and Gengchen Mai and Jianfeng Lin and Cheng Wei and Wenhao Yu,"Large language model, GIS knowledge corpus, Domain adaptation, Self-instruct instructions","Large language models (LLMs) exhibit impressive capabilities across diverse tasks in natural language processing. Nevertheless, challenges arise such as large model parameter size and limited model accessibility through APIs such as ChatGPT and GPT-4, which prohibits the model deployment on mobile devices and domain adaptation or fine-tuning. Moreover, while LLMs excel in general domains, their performance in specialized fields such as GIS may not always align with the expectations of domain experts. This is primarily attributed to the diverse disciplinary origins of the training data, which often lack comprehensive coverage and treatment of knowledge specific to individual disciplines (e.g., GIS). Therefore, there is a crucial need to train and adapt LLMs specifically designed for different professional fields. In this paper, our focus is on the GIS domain, where we introduce BB(BaBy)-GeoGPT, a large language model with GIS-specific knowledge. To achieve this goal, we curated a comprehensive set of resources, comprising model pretraining data (BB-GeoPT, 26,907 documents), supervised fine-tuning data (BB-GeoSFT, 35,876 instructions), and evaluation data (BB-GeoEval, 600 objective questions and 150 subjective questions). BB-GeoGPT is developed by first adapting an open-source general-domain LLM, the LLaMA-2-7B model, to our pretraining data. Subsequently, we use instruction tuning to further fine-tune the model on our BB-GeoSFT. Through extensive experiments on the evaluation dataset, BB-GeoGPT demonstrates improvements ranging from 10.55% to 47.57% for objective questions and from 7.87% to 27.73% for subjective questions, when compared to general LLMs of similar size in terms of accuracy. Moreover, our data collection strategy and the amassed data can serve as a foundation for advancing LLM research in the GIS domain, fostering further development.",article,5,,,,,,,,
Lessons learned from applying model-driven engineering in 5 domains: The success story of the MontiGem generator framework,Science of Computer Programming,232,103033,2024,0167-6423,https://doi.org/10.1016/j.scico.2023.103033,https://www.sciencedirect.com/science/article/pii/S0167642323001156,Constantin Buschhaus and Arkadii Gerasimov and Jörg Christian Kirchhof and Judith Michael and Lukas Netz and Bernhard Rumpe and Sebastian Stüber,"Model-driven software engineering, Code synthesis, Domain-specific languages, Data management, Web applications","We report on our success stories in developing and using Model-Driven Engineering (MDE) tools for information systems on real-world projects within different application domains. It is necessary that we ensure the extensibility and adaptability of code generators if we want to reuse them for different domains. Up to now, research on reusing software has been mainly conducted in the software product line community but rarely discussed in the context of code generators. This paper introduces the generation framework MontiGem and shows how it has been used and evolved within five different research and industry projects in the domains of financial management, IoT, energy management, privacy policy, and wind turbine engineering. We have developed the code generator within the first project and further refined it with each of the following projects. This paper describes the projects, shows how MDE helped us in the software engineering process, and discusses the lessons we learned. These examples show how MDE techniques can be successfully applied to the development of information systems in practice, although further requirements have been met over time.",article,,,,,,,,,
Generative AI for visualization: State of the art and future directions,Visual Informatics,8,43-66,2024,2468-502X,https://doi.org/10.1016/j.visinf.2024.04.003,https://www.sciencedirect.com/science/article/pii/S2468502X24000160,Yilin Ye and Jianing Hao and Yihan Hou and Zhan Wang and Shishi Xiao and Yuyu Luo and Wei Zeng,"Visualization, Generative AI","Generative AI (GenAI) has witnessed remarkable progress in recent years and demonstrated impressive performance in various generation tasks in different domains such as computer vision and computational design. Many researchers have attempted to integrate GenAI into visualization framework, leveraging the superior generative capacity for different operations. Concurrently, recent major breakthroughs in GenAI like diffusion models and large language models have also drastically increased the potential of GenAI4VIS. From a technical perspective, this paper looks back on previous visualization studies leveraging GenAI and discusses the challenges and opportunities for future research. Specifically, we cover the applications of different types of GenAI methods including sequence, tabular, spatial and graph generation techniques for different tasks of visualization which we summarize into four major stages: data enhancement, visual mapping generation, stylization and interaction. For each specific visualization sub-task, we illustrate the typical data and concrete GenAI algorithms, aiming to provide in-depth understanding of the state-of-the-art GenAI4VIS techniques and their limitations. Furthermore, based on the survey, we discuss three major aspects of challenges and research opportunities including evaluation, dataset, and the gap between end-to-end GenAI methods and visualizations. By summarizing different generation algorithms, their current applications and limitations, this paper endeavors to provide useful insights for future GenAI4VIS research.",article,2,,,,,,,,
"Language, common sense, and the Winograd schema challenge",Artificial Intelligence,325,104031,2023,0004-3702,https://doi.org/10.1016/j.artint.2023.104031,https://www.sciencedirect.com/science/article/pii/S0004370223001777,Jacob Browning and Yann LeCun,"Winograd schema challenge, Artificial intelligence, Common-sense, Disambiguation, Symbolic AI, Large language models","Since the 1950s, philosophers and AI researchers have held that disambiguating natural language sentences depended on common sense. In 2012, the Winograd Schema Challenge was established to evaluate the common-sense reasoning abilities of a machine by testing its ability to disambiguate sentences. The designers argued only a system capable of “thinking in the full-bodied sense” would be able to pass the test. However, by 2023, the original authors concede the test has been soundly defeated by large language models which still seem to lack common sense of full-bodied thinking. In this paper, we argue that disambiguating sentences only seemed like a good test of common-sense based on a certain picture of the relationship between linguistic comprehension and semantic knowledge—one typically associated with the early computational theory of mind and Symbolic AI. If this picture is rejected, as it is by most LLM researchers, then disambiguation ceases to look like a comprehensive test of common-sense and instead appear only to test linguistic competence. The upshot is that any linguistic test, not just disambiguation, is unlikely to tell us much about common sense or genuine intelligence.",article,,,,,,,,,
Summary of ChatGPT-Related research and perspective towards the future of large language models,Meta-Radiology,1,100017,2023,2950-1628,https://doi.org/10.1016/j.metrad.2023.100017,https://www.sciencedirect.com/science/article/pii/S2950162823000176,Yiheng Liu and Tianle Han and Siyuan Ma and Jiayue Zhang and Yuanyuan Yang and Jiaming Tian and Hao He and Antong Li and Mengshen He and Zhengliang Liu and Zihao Wu and Lin Zhao and Dajiang Zhu and Xiang Li and Ning Qiang and Dingang Shen and Tianming Liu and Bao Ge,,"This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and GPT-4) research, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT-related research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.",article,2,,,,,,,,
Data preparation for Deep Learning based Code Smell Detection: A systematic literature review,Journal of Systems and Software,216,112131,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112131,https://www.sciencedirect.com/science/article/pii/S0164121224001766,Fengji Zhang and Zexian Zhang and Jacky Wai Keung and Xiangru Tang and Zhen Yang and Xiao Yu and Wenhua Hu,"Code smell detection, Deep learning, Data preparation, Systematic literature review","Code Smell Detection (CSD) plays a crucial role in improving software quality and maintainability. And Deep Learning (DL) techniques have emerged as a promising approach for CSD due to their superior performance. However, the effectiveness of DL-based CSD methods heavily relies on the quality of the training data. Despite its importance, little attention has been paid to analyzing the data preparation process. This systematic literature review analyzes the data preparation techniques used in DL-based CSD methods. We identify 36 relevant papers published by December 2023 and provide a thorough analysis of the critical considerations in constructing CSD datasets, including data requirements, collection, labeling, and cleaning. We also summarize seven primary challenges and corresponding solutions in the literature. Finally, we offer actionable recommendations for preparing and accessing high-quality CSD data, emphasizing the importance of data diversity, standardization, and accessibility. This survey provides valuable insights for researchers and practitioners to harness the full potential of DL techniques in CSD.",article,,,,,,,,,
Artificial general intelligence for radiation oncology,Meta-Radiology,1,100045,2023,2950-1628,https://doi.org/10.1016/j.metrad.2023.100045,https://www.sciencedirect.com/science/article/pii/S2950162823000450,Chenbin Liu and Zhengliang Liu and Jason Holmes and Lu Zhang and Lian Zhang and Yuzhen Ding and Peng Shu and Zihao Wu and Haixing Dai and Yiwei Li and Dinggang Shen and Ninghao Liu and Quanzheng Li and Xiang Li and Dajiang Zhu and Tianming Liu and Wei Liu,"Large foundation model, AGI, SAM, Radiation oncology, Medical imaging","The emergence of artificial general intelligence (AGI) is transforming radiation oncology. As prominent vanguards of AGI, large language models (LLMs) such as GPT-4 and PaLM 2 can process extensive texts and large vision models (LVMs) such as the Segment Anything Model (SAM) can process extensive imaging data to enhance the efficiency and precision of radiation therapy. This paper explores full-spectrum applications of AGI across radiation oncology including initial consultation, simulation, treatment planning, treatment delivery, treatment verification, and patient follow-up. The fusion of vision data with LLMs also creates powerful multimodal models that elucidate nuanced clinical patterns. Together, AGI promises to catalyze a shift towards data-driven, personalized radiation therapy. However, these models should complement human expertise and care. This paper provides an overview of how AGI can transform radiation oncology to elevate the standard of patient care in radiation oncology, with the key insight being AGI's ability to exploit multimodal clinical data at scale.",article,3,,,,,,,,
Predicting student dropout in subscription-based online learning environments: The beneficial impact of the logit leaf model,Decision Support Systems,135,113325,2020,0167-9236,https://doi.org/10.1016/j.dss.2020.113325,https://www.sciencedirect.com/science/article/pii/S0167923620300804,,"Learning analytics, Proactive student management, Subscription-based online learning, Student dropout, Logit leaf model, Machine learning","Online learning has been adopted rapidly by educational institutions and organizations. Despite its many advantages, including 24/7 access, high flexibility, rich content, and low cost, online learning suffers from high dropout rates that hamper pedagogical and economic goal outcomes. Enhanced student dropout prediction tools would help providers proactively detect students at risk of leaving and identify factors that they might address to help students continue their learning experience. Therefore, this study seeks to improve student dropout predictions, with three main contributions. First, it benchmarks a recently proposed logit leaf model (LLM) algorithm against eight other algorithms, using a real-life data set of 10,554 students of a global subscription-based online learning provider. The LLM outperforms all other methods in finding a balance between predictive performance and comprehensibility. Second, a new multilevel informative visualization of the LLM adds novel benefits, relative to a standard LLM visualization. Third, this research specifies the impacts of student demographics; classroom characteristics; and academic, cognitive, and behavioral engagement variables on student dropout. In reviewing LLM segments, these results show that different insights emerge for various student segments with different learning patterns. This notable result can be used to personalize student retention campaigns.",article,,,,,,,,,
A survey of energy concerns for software engineering,Journal of Systems and Software,210,111944,2024,0164-1212,https://doi.org/10.1016/j.jss.2023.111944,https://www.sciencedirect.com/science/article/pii/S0164121223003394,Sung Une Lee and Niroshinie Fernando and Kevin Lee and Jean-Guy Schneider,"Software engineering, Energy, Green, Sustainability","There is growing attention to energy efficiency in the software engineering field. This has been driven by modern technologies, for example, Internet of Things (IoT), Social Networking Services (SNS) and quantum computing. In addition to this, recent trends and concerns such as Environment, Social, and Governance (ESG) and human/societal/environmental well-being for responsible Artificial Intelligence (AI) have accelerated the use of energy efficient software. Despite this, energy concerns in this field have been less explored and studied. This limitation results in falling short to address and overcome greenability issues at the software level, and leaving critical challenges to be solved in this space. This study aims to address this limitation and fill the gap between previous studies. We survey green in software engineering framed by the ten knowledge areas of software engineering to not only cover the entire development life-cycle but also widen the scope of discussion to software process, method, and model management. Based on our comprehensive investigation, we discuss open challenges, trade-offs and implications of this study for both researchers and practitioners.",article,,,,,,,,,
Socratic Video Understanding on Unmanned Aerial Vehicles,Procedia Computer Science,225,144-154,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.09.101,https://www.sciencedirect.com/science/article/pii/S1877050923011560,,"Socratic Models, UAV, Scene Understanding, Large Language Models, BLIP-2, GPT-3","In this work, we propose a system for video understanding through zero-shot reading comprehension using Socratic Models. Specifically, we create a language-based world-state history of events and objects present in a scene captured by an Unmanned Aerial Vehicle (UAV). To achieve this, video footage from RYZE Tello microdrones is transmitted to a ground computer for further processing. The semantically rich information offered by Large Language Models (LLMs) enables open-ended reasoning, such as event forecasting with minimal human intervention, in a cost-effective robotic system. BLIP-2 is employed to answer a given set of instructional prompts, creating a log-state of objects, humans, and hazards that can be searched. Simultaneously, it suggests probable actions in the scene and can assist the human controller with an estimated best command. The BLIP-2 instructional prompts are then combined with OpenAI's da-vinci-003/gpt-3.5-turbo to generate comprehensive video descriptions and summarize likely actions. The LLM-enhanced generated texts achieve a GUNNING Fog median grade level in the range of 7-12.",article,,27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023),,,,,,,
A software engineering approach for medical workstations development,International Journal of Bio-Medical Computing,34,249-260,1994,0020-7101,https://doi.org/10.1016/0020-7101(94)90025-6,https://www.sciencedirect.com/science/article/pii/0020710194900256,François-Christophe Jean and Marion Lavril and David Lemaitre and Dominique Sauquet and Patrice Degoulet,"Medical workstations, Software engineering, Integration, Object orientation","Multimedia medical workstations represent the natural tool for accessing the hospital information system environment. They are complex medical systems that have to gather, in a single framework, a large collection of components dealing with multimedia medical objects. To remain current with both medical practice and with advances in the computer science field, they have to allow the iterative addition of new functions to the set of existing ones. In this paper, after a survey of commonly required medical workstation functional components, we shall try to discuss how a software engineering approach can streamline the development of a medical workstation. Different software engineering tools needed to build the functional components of a workstation are described. Their integration in a single dedicated environment is considered through four perspectives: data, presentation, communication and control. Benefits and limitations of an object-oriented approach are discussed.",article,1,The Health Care Professional Workstation,,,,,,,
"Assessment of chemistry knowledge in large language models that generate code††Electronic supplementary information (ESI) available: Supporting figures, tables, and text. Accuracy data are available as comma separated value files. Contexts are available as a markup file. The responses from the model (completions) which were the basis for expert evaluators are available in HTML format at https://doi.org/10.5281/zenodo.6800475. See DOI: https://doi.org/10.1039/d2dd00087c",Digital Discovery,2,368-376,2023,2635-098X,https://doi.org/10.1039/d2dd00087c,https://www.sciencedirect.com/science/article/pii/S2635098X23000359,,,"ABSTRACT
In this work, we investigate the question: do code-generating large language models know chemistry? Our results indicate, mostly yes. To evaluate this, we introduce an expandable framework for evaluating chemistry knowledge in these models, through prompting models to solve chemistry problems posed as coding tasks. To do so, we produce a benchmark set of problems, and evaluate these models based on correctness of code by automated testing and evaluation by experts. We find that recent LLMs are able to write correct code across a variety of topics in chemistry and their accuracy can be increased by 30 percentage points via prompt engineering strategies, like putting copyright notices at the top of files. Our dataset and evaluation tools are open source which can be contributed to or built upon by future researchers, and will serve as a community resource for evaluating the performance of new models as they emerge. We also describe some good practices for employing LLMs in chemistry. The general success of these models demonstrates that their impact on chemistry teaching and research is poised to be enormous.",article,2,,,,,,,,
The ALTAI checklist as a tool to assess ethical and legal implications for a trustworthy AI development in education,Computer Law & Security Review,53,105986,2024,0267-3649,https://doi.org/10.1016/j.clsr.2024.105986,https://www.sciencedirect.com/science/article/pii/S0267364924000530,Andrea Fedele and Clara Punzi and Stefano Tramacere,"Trustworthy AI, Education, Vulnerability, AI regulation, AI accountability, eXplainable AI","The rapid proliferation of Artificial Intelligence (AI) applications in various domains of our lives has prompted a need for a shift towards a human-centered and trustworthy approach to AI. In this study we employ the Assessment List for Trustworthy Artificial Intelligence (ALTAI) checklist to evaluate the trustworthiness of Artificial Intelligence for Student Performance Prediction (AI4SPP), an AI-powered system designed to detect students at risk of school failure. We strongly support the ethical and legal development of AI and propose an implementation design where the user can choose to have access to each level of a three-tier outcome bundle: the AI prediction alone, the prediction along with its confidence level, and, lastly, local explanations for each grade prediction together with the previous two information. AI4SPP aims to raise awareness among educators and students regarding the factors contributing to low school performance, thereby facilitating the implementation of interventions not only to help students, but also to address biases within the school community. However, we also emphasize the ethical and legal concerns that could arise from a misuse of the AI4SPP tool. First of all, the collection and analysis of data, which is essential for the development of AI models, may lead to breaches of privacy, thus causing particularly adverse consequences in the case of vulnerable individuals. Furthermore, the system’s predictions may be influenced by unacceptable discrimination based on gender, ethnicity, or socio-economic background, leading to unfair actions. The ALTAI checklist serves as a valuable self-assessment tool during the design phase of AI systems, by means of which commonly overlooked weaknesses can be highlighted and addressed. In addition, the same checklist plays a crucial role throughout the AI system life cycle. Continuous monitoring of sensitive features within the dataset, alongside survey assessments to gauge users’ responses to the systems, is essential for gathering insights and intervening accordingly. We argue that adopting a critical approach to AI development is essential for societal progress, believing that it can evolve and accelerate over time without impeding openness to new technologies. By aligning with ethical principles and legal requirements, AI systems can make significant contributions to education while mitigating potential risks and ensuring a fair and inclusive learning environment.",article,,,,,,,,,
"A contemporary review on chatbots, AI-powered virtual conversational agents, ChatGPT: Applications, open challenges and future research directions",Computer Science Review,52,100632,2024,1574-0137,https://doi.org/10.1016/j.cosrev.2024.100632,https://www.sciencedirect.com/science/article/pii/S1574013724000169,Avyay Casheekar and Archit Lahiri and Kanishk Rath and Kaushik Sanjay Prabhakar and Kathiravan Srinivasan,"Computational intelligence, Artificial intelligence, Chatbots, Conversational agents, ChatGPT","This review paper offers an in-depth analysis of AI-powered virtual conversational agents, specifically focusing on OpenAI’s ChatGPT. The main contributions of this paper are threefold: (i) an exhaustive review of prior literature on chatbots, (ii) a background of chatbots including existing chatbots/conversational agents like ChatGPT, and (iii) a UI/UX design analysis of prominent chatbots. Another contribution of this review is the comprehensive exploration of ChatGPT’s applications across a multitude of sectors, including education, business, public health, and more. This review highlights the transformative potential of ChatGPT, despite the challenges it faces such as hallucination, biases in training data, jailbreaks, and anonymous data collection. The review paper then presents a comprehensive survey of prior literature reviews on chatbots, identifying gaps in the prior work and highlighting the need for further research in areas such as chatbot evaluation, user experience, and ethical considerations. The paper also provides a detailed analysis of the UI/UX design of prominent chatbots, including their conversational flow, visual design, and user engagement. The paper also identifies key future research directions, including mitigating language bias, enhancing ethical decision-making capabilities, improving user interaction and personalization, and developing robust governance frameworks. By solving these issues, we can ensure that AI chatbots like ChatGPT are used responsibly and effectively across a broad variety of applications. This review will be a valuable resource for researchers and practitioners in understanding the current state and future potential of AI chatbots like ChatGPT.",article,,,,,,,,,
Are LLMs good at structured outputs? A benchmark for evaluating structured output capabilities in LLMs,Information Processing & Management,61,103809,2024,0306-4573,https://doi.org/10.1016/j.ipm.2024.103809,https://www.sciencedirect.com/science/article/pii/S0306457324001687,Yu Liu and Duantengchuan Li and Kaili Wang and Zhuoran Xiong and Fobo Shi and Jian Wang and Bing Li and Bo Hang,"Large language model, Structured output capability, Benchmark dataset, Q&A interaction, Causal graph","Existing benchmarks for Large Language Models (LLMs) mostly focus on general or specific domain capabilities, overlooking structured output capabilities. We introduce SoEval, a benchmark for assessing LLMs’ ability to generate structured outputs like JSON, XML, and lists. SoEval contains 3.7K entries in Chinese and English, covering 13 types of structured output tasks across 20 subjects. In experiments, we found that while current mainstream LLMs have deficiencies in structured output, GPT-4 outperforms them in this aspect. GPT-4 achieved an average score of 0.4 on SoEval, representing a 24% enhancement over the next best-performing model. At the same time, the performance of current mainstream models on English tasks is also better than on Chinese tasks. We also report the performance of mainstream large models on different structured output types and task subjects. The benchmark construction code and SoEval dataset are open-sourced at https://github.com/MoranCoder95/SoEval.",article,5,,,,,,,,
Confix: Combining node-level fix templates and masked language model for automatic program repair,Journal of Systems and Software,216,112116,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112116,https://www.sciencedirect.com/science/article/pii/S0164121224001614,Jianmao Xiao and Zhipeng Xu and Shiping Chen and Gang Lei and Guodong Fan and Yuanlong Cao and Shuiguang Deng and Zhiyong Feng,"Automatic program repair, Fix templates, Masked language model","Automatic program repair (APR) is a promising technique to fix program defects by generating patches. In the current APR techniques, template-based and learning-based techniques have demonstrated different advantages. Template-based APR techniques rely on pre-defined fix templates, providing higher controllability but limited by the variety of templates and edit expressiveness. In contrast, learning-based APR techniques treat repair as a neural machine translation task, improving the edit expressiveness through training neural networks. However, this technique also faces the influence of quality and variety of training data, leading to numerous errors and redundant code generation. To overcome their limitations, this paper proposes an innovative APR technique called Confix. Confix first constructs a code information tree to assist in mining edit changes during historical repair. It then further enriches the types of fix templates using node information in the tree. Afterward, Confix defines masked lines based on node-level fix templates to control the scope of patch generation, avoiding redundant semantic code generation. Finally, Confix leverages the powerful edit expressiveness of the masked language model and combines it with fix strategies to generate correct patches more efficiently and accurately. Experimental results show that Confix exhibits state-of-the-art performance on the Defects4J 1.2 and QuixBugs benchmarks.",article,,,,,,,,,
Collaborative Work Alternatives with ChatGPT Based on Evaluation Criteria for its Use in Higher Education: Application of the PROMETHEE-SAPEVO-M1 Method,Procedia Computer Science,221,177-184,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.07.025,https://www.sciencedirect.com/science/article/pii/S1877050923007214,Luis Hernan Contreras Pinochet and Miguel Ângelo Lellis Moreira and Luiz Paulo Fávero and Marcos dos Santos and Vanessa Itacaramby Pardim,"Artificial intelligence, ChatGPT, collaborative work, Higher Education, PROMETHEE-SAPEVO-M1 method","The objective of this article is to adopt the integration of two methods of Multicriteria Decision Support, based on the axiomatic models PROMETHEE and SAPEVO-M1, aggregating data of a qualitative nature through ordinal entries to analyze collaborative work alternatives with ChatGPT from evaluation criteria for its use in higher education. It is highlighted that the alternative with the best performance is ‘Support for Autonomous Learning,’ presenting the highest positive flow and the lowest negative flow, exposing a natural preference over the set. In this study, ‘Emotional Support’ was the worst alternative. It occurs because the tool is still under discussion when addressing issues such as the lack of human interaction, reduced critical thinking, and less empathy.",article,,Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023),,,,,,,
MaScQA: investigating materials science knowledge of large language models††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d3dd00188a,Digital Discovery,3,313-327,2024,2635-098X,https://doi.org/10.1039/d3dd00188a,https://www.sciencedirect.com/science/article/pii/S2635098X24000159,Mohd Zaki and  Jayadeva and  Mausam and N. M. Anoop Krishnan,,"Information extraction and textual comprehension from materials literature are vital for developing an exhaustive knowledge base that enables accelerated materials discovery. Language models have demonstrated their capability to answer domain-specific questions and retrieve information from knowledge bases. However, there are no benchmark datasets in the materials science domain that can be used to evaluate the understanding of the key concepts by these language models. In this work, we curate a dataset of 650 challenging questions from the materials domain that require the knowledge and skills of a materials science student who has cleared their undergraduate degree. We classify these questions based on their structure and the materials science domain-based subcategories. Further, we evaluate the performance of LLaMA-2-70B, GPT-3.5, and GPT-4 models on solving these questions via zero-shot and chain of thought prompting. It is observed that GPT-4 gives the best performance (∼62% accuracy) as compared to other models. Interestingly, in contrast to the general observation, no significant improvement in accuracy is observed with the chain of thought prompting. To evaluate the limitations, we performed an error analysis, which revealed conceptual errors (∼72%) as the major contributor compared to computational errors (∼28%) towards the reduced performance of the LLMs. We also compared GPT-4 with human performance and observed that GPT-4 is better than an average student and comes close to passing the exam. We also show applications of the best performing model (GPT-4) on composition–extraction from tables of materials science research papers and code writing tasks. While GPT-4 performs poorly on composition extraction, it outperforms all other models on the code writing task. We hope that the dataset, analysis, and applications discussed in this work will promote further research in developing better materials science domain-specific LLMs and strategies for information extraction.",article,2,,,,,,,,
Mining for cost awareness in the infrastructure as code artifacts of cloud-based applications: An exploratory study,Journal of Systems and Software,215,112112,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112112,https://www.sciencedirect.com/science/article/pii/S0164121224001572,Daniel Feitosa and Matei-Tudor Penca and Massimiliano Berardi and Rares-Dorian Boza and Vasilios Andrikopoulos,"Cloud computing, Cost awareness, Mining software repositories, Cloud orchestration","Context:
Cloud computing’s rise as the primary platform for software development and delivery is largely driven by the potential cost savings. However, it is surprising that no empirical evidence has been collected to determine whether cost awareness permeates the development process and how it manifests in practice.
Objective:
This study aims to provide empirical evidence of cost awareness by mining open source repositories of cloud-based applications. The focus is on Infrastructure-as-Code artifacts that automate software (re)deployment on the cloud.
Methods:
A systematic examination of 152735 repositories yielded 2010 relevant hits. We then analyzed 538 relevant commits and 208 relevant issues using inductive and deductive coding and corroborated findings with discussions from Stack Overflow.
Results:
The findings indicate that developers are not only concerned with the cost of their application deployments but also take actions to reduce these costs beyond selecting cheaper cloud services. We also identify research areas for future consideration.
Conclusion:
Although we focus on a particular Infrastructure-as-Code technology (Terraform), the findings can be applicable to cloud-based application development in general. The provided empirical grounding can serve developers seeking to reduce costs through service selection, resource allocation, deployment optimization, and other techniques.",article,,,,,,,,,
Evaluating password strength based on information spread on social networks: A combined approach relying on data reconstruction and generative models,Online Social Networks and Media,42,100278,2024,2468-6964,https://doi.org/10.1016/j.osnem.2024.100278,https://www.sciencedirect.com/science/article/pii/S246869642400003X,Maurizio Atzori and Eleonora Calò and Loredana Caruccio and Stefano Cirillo and Giuseppe Polese and Giandomenico Solimando,"Privacy-preserving, Password-disclosure, Data wrapping, Data reconstruction, Social network","Ensuring the security of personal accounts has become a key concern due to the widespread password attack techniques. Although passwords are the primary defense against unauthorized access, the practice of reusing easy-to-remember passwords increases security risks for people. Traditional methods for evaluating password strength are often insufficient since they overlook the public personal information that users frequently share on social networks. In addition, while users tend to limit access to their data on single profiles, personal data is often unintentionally shared across multiple profiles, exposing users to password threats. In this paper, we present an extension of a data reconstruction tool, namely soda advance, which incorporates a new module to evaluate password strength based on publicly available data across multiple social networks. It relies on a new metric to provide a comprehensive evaluation of password strength. Moreover, we investigate the capabilities and risks associated with emerging Large Language Models (LLMs) in evaluating and generating passwords, respectively. Specifically, by exploiting the proliferation of LLMs, it has been possible to interact with many LLMs through Automated Template Learning methodologies. Experimental evaluations, performed with 100 real users, demonstrate the effectiveness of LLMs in generating strong passwords with respect to data associated with users’ profiles. Furthermore, LLMs have proved to be effective also in evaluation tasks, but the combined usage of LLMs and soda advance guaranteed better classifications up to more than 10% in terms of F1-score.",article,,,,,,,,,
Programming with ChatGPT: How far can we go?,Machine Learning with Applications,15,100526,2024,2666-8270,https://doi.org/10.1016/j.mlwa.2024.100526,https://www.sciencedirect.com/science/article/pii/S2666827024000021,Alessio Bucaioni and Hampus Ekedahl and Vilma Helander and Phuong T. Nguyen,"ChatGPT, Large language models, Programming","Artificial intelligence (AI) has made remarkable strides, giving rise to the development of large language models such as ChatGPT. The chatbot has garnered significant attention from academia, industry, and the general public, marking the beginning of a new era in AI applications. This work explores how well ChatGPT can write source code. To this end, we performed a series of experiments to assess the extent to which ChatGPT is capable of solving general programming problems. Our objective is to assess ChatGPT’s capabilities in two different programming languages, namely C++ and Java, by providing it with a set of programming problem, encompassing various types and difficulty levels. We focus on evaluating ChatGPT’s performance in terms of code correctness, run-time efficiency, and memory usage. The experimental results show that, while ChatGPT is good at solving easy and medium programming problems written in C++ and Java, it encounters some difficulties with more complicated tasks in the two languages. Compared to code written by humans, the one generated by ChatGPT is of lower quality, with respect to runtime and memory usage.",article,,,,,,,,,
The automated model of comprehension version 4.0 – Validation studies and integration of ChatGPT,Computers in Human Behavior,154,108154,2024,0747-5632,https://doi.org/10.1016/j.chb.2024.108154,https://www.sciencedirect.com/science/article/pii/S0747563224000219,Dragos-Georgian Corlatescu and Micah Watanabe and Stefan Ruseti and Mihai Dascalu and Danielle S. McNamara,"Natural language processing, Reading comprehension, Automated model of comprehension, ChatGPT, Large language models","Modeling reading comprehension processes is a critical task for Learning Analytics, as accurate models of the reading process can be used to match students to texts, identify appropriate interventions, and predict learning outcomes. This paper introduces an improved version of the Automated Model of Comprehension, namely version 4.0. AMoC has its roots in two theoretical models of the comprehension process (i.e., the Construction-Integration model and the Landscape model), and the new version leverages state-of-the-art Large Language models, more specifically ChatGPT, to have a better contextualization of the text and a simplified construction of the underlying graph model. Besides showcasing the usage of the model, the study introduces three in-depth psychological validations that argue for the model's adequacy in modeling reading comprehension. In these studies, we demonstrated that AMoC is in line with the theoretical background proposed by the Construction-Integration and Landscape models, and it is better at replicating results from previous human psychological experiments than its predecessor. Thus, AMoC v4.0 can be further used as an educational tool to, for example, help teachers design better learning materials personalized for student profiles. Additionally, we release the code from AMoC v4.0 as open source in a Google Collab Notebook and a GitHub repository.",article,,,,,,,,,
API comparison knowledge extraction via prompt-tuned language model,Journal of Computer Languages,75,101200,2023,2590-1184,https://doi.org/10.1016/j.cola.2023.101200,https://www.sciencedirect.com/science/article/pii/S2590118423000102,Yangrui Yang and Yaping Zhu and Sisi Chen and Pengpeng Jian,"Knowledge extraction, API entity, Semantic relation, Joint extraction","Application Programming Interfaces (APIs) are frequent in software engineering domain texts, such as API references and Stack Overflow. These APIs and the comparison knowledge between them are not only important for solving programming issues (e.g., question answering), but they are also organized into structured knowledge to support many software engineering tasks (e.g., API misuse detection). As a result, extracting API comparison knowledge (API entities and semantic relations) from texts is essential. Existing rule-based and sequence labeling-based approaches must manually enumerate all linguistic patterns or label a large amount of data. Therefore, they involve a significant labor overhead and are exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, we formulates heterogeneous API extraction and API relation extraction tasks as a sequence-to-sequence generation task. It proposes APICKnow, an API entity-relation joint extraction model based on the large language model. To improve our model’s performance and quick learning ability, we adopt the prompt learning method to stimulate APICKnow to recognize API entities and relations. We systematically evaluate APICKnow on a set of sentences from Stack Overflow. The experimental results show that APICKnow can outperform the state-of-the-art baselines, and APICKnow has a quick learning ability and strong generalization ability.",article,,,,,,,,,
Research artifacts in software engineering publications: Status and trends,Journal of Systems and Software,213,112032,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112032,https://www.sciencedirect.com/science/article/pii/S016412122400075X,Mugeng Liu and Xiaolong Huang and Wei He and Yibing Xie and Jie M. Zhang and Xiang Jing and Zhenpeng Chen and Yun Ma,"Research artifact, Empirical study, Software engineering, Code smell","The Software Engineering (SE) community has been embracing the open science policy and encouraging researchers to disclose artifacts in their publications. However, the status and trends of artifact practice and quality remain unclear, lacking insights on further improvement. In this paper, we present an empirical study to characterize the research artifacts in SE publications. Specifically, we manually collect 1,487 artifacts from all 2,196 papers published in top-tier SE conferences (ASE, FSE, ICSE, and ISSTA) from 2017 to 2022. We investigate the common practices (e.g., URL location and format, storage websites), maintenance activities (e.g., last update time and URL validity), popularity (e.g., the number of stars on GitHub and characteristics), and quality (e.g., documentation and code smell) of these artifacts. Based on our analysis, we reveal a rise in publications providing artifacts. The usage of Zenodo for sharing artifacts has significantly increased. However, artifacts stored in GitHub tend to receive few stars, indicating a limited influence on real-world SE applications. We summarize the results and provide suggestions to different stakeholders in conjunction with current guidelines.",article,,,,,,,,,
"From COBIT to ISO 42001: Evaluating cybersecurity frameworks for opportunities, risks, and regulatory compliance in commercializing large language models",Computers & Security,,103964,2024,0167-4048,https://doi.org/10.1016/j.cose.2024.103964,https://www.sciencedirect.com/science/article/pii/S0167404824002694,Timothy R. McIntosh and Teo Susnjak and Tong Liu and Paul Watters and Dan Xu and Dongwei Liu and Raza Nowrozy and Malka N. Halgamuge,"Cybersecurity frameworks, Large language models, Risk management, AI governance, Cyber resilience, Information security","This study investigated the integration readiness of four predominant cybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0, COBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the opportunities, risks, and regulatory compliance when adopting Large Language Models (LLMs), using qualitative content analysis and expert validation. Our analysis, with both LLMs and human experts in the loop, uncovered potential for LLM integration together with inadequacies in LLM risk oversight of those frameworks. Comparative gap analysis has highlighted that the new ISO 42001:2023, specifically designed for Artificial Intelligence (AI) management systems, provided most comprehensive facilitation for LLM opportunities, whereas COBIT 2019 aligned most closely with the European Union AI Act. Nonetheless, our findings suggested that all evaluated frameworks would benefit from enhancements to more effectively and more comprehensively address the multifaceted risks associated with LLMs, indicating a critical and time-sensitive need for their continuous evolution. We propose integrating human-expert-in-the-loop validation processes as crucial for enhancing cybersecurity frameworks to support secure and compliant LLM integration, and discuss implications for the continuous evolution of cybersecurity GRC frameworks to support the secure integration of LLMs.",article,,,,,,,,,
“Will I be replaced?” Assessing ChatGPT's effect on software development and programmer perceptions of AI tools,Science of Computer Programming,235,103111,2024,0167-6423,https://doi.org/10.1016/j.scico.2024.103111,https://www.sciencedirect.com/science/article/pii/S0167642324000340,Mohammad Amin Kuhail and Sujith Samuel Mathew and Ashraf Khalil and Jose Berengueres and Syed Jawad Hussain Shah,"ChatGPT, Programmer assistant tools, AI tools, Chatbot","ChatGPT is a language model with artificial intelligence (AI) capabilities that has found utility across various sectors. Given its impact, we conducted two empirical studies to assess the potential and limitations of ChatGPT and other AI tools in software development. In the first study, we evaluated ChatGPT 3.5′s effectiveness in generating code for 180 coding problems from LeetCode, an online coding interview preparation platform. Our findings suggest that ChatGPT 3.5 is more effective in solving easy and medium coding problems but less reliable for harder problems. Further, ChatGPT 3.5 is somewhat more effective at coding problems with higher popularity scores. In the second study, we administered a questionnaire (N = 99) to programmers to gain insights into their views on ChatGPT and other AI tools. Our findings indicate that programmers use AI tools for various tasks, such as generating boilerplate code, explaining complex code, and conducting research. AI tools also help programmers to become more productive by creating better-performing, shorter, and more readable code, among other benefits. However, AI tools can sometimes misunderstand requirements and generate erroneous code. While most programmers are not currently concerned about AI tools replacing them, they are apprehensive about what the future may hold. Our research has also revealed associations between AI tool usage, trust, perceived productivity, and job security threats caused by the tools.",article,,,,,,,,,
Natural language processing models that automate programming will transform chemistry research and teaching††Electronic supplementary information (ESI) available. See DOI: 10.1039/d1dd00009h,Digital Discovery,1,79-83,2022,2635-098X,https://doi.org/10.1039/d1dd00009h,https://www.sciencedirect.com/science/article/pii/S2635098X23000694,Glen M. Hocky and Andrew D. White,,"ABSTRACT
Natural language processing models have emerged that can generate useable software and automate a number of programming tasks with high fidelity. These tools have yet to have an impact on the chemistry community. Yet, our initial testing demonstrates that this form of artificial intelligence is poised to transform chemistry and chemical engineering research. Here, we review developments that brought us to this point, examine applications in chemistry, and give our perspective on how this may fundamentally alter research and teaching.",article,2,,,,,,,,
A survey of GPT-3 family large language models including ChatGPT and GPT-4,Natural Language Processing Journal,6,100048,2024,2949-7191,https://doi.org/10.1016/j.nlp.2023.100048,https://www.sciencedirect.com/science/article/pii/S2949719123000456,Katikapalli Subramanyam Kalyan,"Large language models, LLMs, GPT-3, ChatGPT, GPT-4, Transformers, LLM survey","Large language models (LLMs) are a special class of pretrained language models (PLMs) obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI’s GPT-3 model, and the popularity of LLMs has increased exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GLLMs.",article,,,,,,,,,
Considerations for adapting higher education technology courses for AI large language models: A critical review of the impact of ChatGPT,Machine Learning with Applications,15,100513,2024,2666-8270,https://doi.org/10.1016/j.mlwa.2023.100513,https://www.sciencedirect.com/science/article/pii/S266682702300066X,Omar Tayan and Ali Hassan and Khaled Khankan and Sanaa Askool,"Artificial intelligence, ChatGPT, Higher education, Machine learning","Following the very recent launch of the ChatGPT chatbot, numerous comments and speculations were posted concerning the potential aspects of society that are expected to benefit from this AI revolution. In particular, the education sector is considered as one of the primary domains affected by this application, the impact of which remains yet to be fully understood. Furthermore, many Higher Education institutions are required to get to terms with its impact on teaching and learning, and to clarify their stances on the use of ChatGPT software. This study was developed to investigate some critical case studies considered as relevant to the inevitable re-evaluation of educational aspects needed, ranging from academic missions to student and course learning outcomes and its ethical uses. Following a review of some of the pros and cons of ChatGPT in the higher educational sector, this paper shall demonstrate several case studies of early trials in teaching and learning assessments related to various specializations. Next, the ability of some well-known AI detector software and analyzed in terms of their capacity to successfully detect AI-generated content. Analysis shall be made of the foreseen impact on important aspects including challenges and benefits related to its use in course assessments as well as academic integrity and ethical use. The study concludes with a set of recommendations made from our findings and benchmarks obtained from top universities in order to assist faculty members and decision makers at Higher Education institutions concerning their response strategy and use of ChatGPT.",article,,,,,,,,,
"A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly",High-Confidence Computing,4,100211,2024,2667-2952,https://doi.org/10.1016/j.hcc.2024.100211,https://www.sciencedirect.com/science/article/pii/S266729522400014X,Yifan Yao and Jinhao Duan and Kaidi Xu and Yuanfang Cai and Zhibo Sun and Yue Zhang,"Large Language Model (LLM), LLM security, LLM privacy, ChatGPT, LLM attacks, LLM vulnerabilities","Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.",article,2,,,,,,,,
A Large and Diverse Arabic Corpus for Language Modeling,Procedia Computer Science,225,12-21,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.09.086,https://www.sciencedirect.com/science/article/pii/S1877050923011419,Abbas Raza Ali and Muhammad Ajmal Siddiqui and Rema Algunaibet and Hasan Raza Ali,"Arabic Corpus, GPT-3, Language Model, Transformers, NLP","Large Language Models (LLMs) have ushered in a major paradigm shift in Natural Language Processing (NLP), where large pre-trained Language models (LMs) have become a fundamental component of most NLP tasks. These models are intelligent enough to find relevant and meaningful representations of a language without any supervision. They are used to fine-tune typical NLP tasks with substantially higher precision than conventional shallow learning techniques. However, training these models requires a massively large corpus that adequately represents a language. Due to the availability of enormous corpora, English LLMs typically perform better than their counterparts. This effort focuses on the design and development of a large Arabic corpus. The corpus comprises over 500 GB of Arabic cleaned text, intended to improve cross-domain knowledge and downstream generalization capability of LLMs. The corpus was employed in the training of a large Arabic LLM. In order to assess the efficacy of the LLM, a variety of typical NLP tasks were fine-tuned. The fine-tuned tasks exhibited a significant boost in accuracy ranging between 4.5 and 8.5%, when compared to those downstreamed from multi-lingual BERT (mBERT). To the best of our knowledge, this is currently the largest clean and diverse Arabic corpus ever assembled.",article,,27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023),,,,,,,
Modeling Speech Emotion Recognition via ImageBind representations,Procedia Computer Science,236,428-435,2024,1877-0509,https://doi.org/10.1016/j.procs.2024.05.050,https://www.sciencedirect.com/science/article/pii/S1877050924010664,Adil CHAKHTOUNA and Sara SEKKATE and Abdellah ADIB,"ImageBind, Speech Emotion Recognition, Embedding representations, IEMOCAP, Nu-SVM","Speech Emotion Recognition (SER) refers to the ability of Machine Learning (ML) and Deep Learning (DL) techniques to accurately predict people's emotional states from speech signals. significant progress has been achieved in the SER domain involving the incorporation of DL models to introduce novel features extraction processes. This paper introduces the use of deep representations learned from the multi-modal Large Language Model (LLM) called ImageBind. These representations were subsequently provided as input to the Nu-Support Vector Machine (Nu-SVM) with RBF kernel for the classification task. The experiments were executed using the IEMOCAP database within the context of a Speaker-Dependent (SD) scenario. The method achieved a noteworthy overall accuracy rate of 80.58% for the four emotions of IEMOCAP, representing a substantial improvement over well-established methods in the existing body of literature. Thus, affirming that the proposed methodology, founded upon ImageBind representations, introduces a novel perspective to the field of SER.",article,,International Symposium on Green Technologies and Applications (ISGTA’2023),,,,,,,
An environment for teaching “program design” by exercises,Microprocessing and Microprogramming,28,259-264,1990,0165-6074,https://doi.org/10.1016/0165-6074(90)90185-C,https://www.sciencedirect.com/science/article/pii/016560749090185C,C. Comar and P. Pintelas,"program design, problem-specific commands, pedagogical environment","Teaching program design in a correct pedagogical context, consists of helping beginners to develop problem formulation abilities by which they produce a computer processable representation modelling a solution to a problem's requirements. One easy way of introducing students to program design is by teaching programming by exercises as advocated in this paper. The method is based on providing the students with a set of exercises of increasing complexity for which the students are called to design solutions. The paper presents a typical set of exercises especially designed for introducing programming concepts. An environment designed for helping teachers and exercises' designers to develop their “problem-specific” exercises, as well as helping the students to develop, test and run their programs, is presented.",article,1,,,,,,,,
Application of ChatGPT in multilingual medical education: How does ChatGPT fare in 2023's Iranian residency entrance examination,Informatics in Medicine Unlocked,41,101314,2023,2352-9148,https://doi.org/10.1016/j.imu.2023.101314,https://www.sciencedirect.com/science/article/pii/S2352914823001600,,"ChatGPT, USMLE, Medical education, Language model","Background
ChatGPT is a large language model (LLM) artificial intelligence instrument trained on massive amounts of text data extracted from the internet and/or user input. In the present article, we aim to apply the latest version of ChatGPT to the Iranian Medical Residency Examination.
Methods
The Iranian Medical Residency Examination is composed of 200 multichoice questions covering all domains of medicine. We used ChatGPT to translate questions into English, French, and Spanish. We fed the questions as multiple-choice questions and allowed ChatGPT to provide comprehensive answers and justifications for its choices.
Results
ChatGPT was able to answer 161 (81.3% = 161/198) questions correctly when the Persian language was used. When the questions were translated into English, French, and Spanish, ChatGPT answered six, one, and five additional questions correctly, respectively. When comparing the different languages, there was no significant difference in the functioning of ChatGPT in different languages using either the McNemar test or the Binomial test.
Conclusion
ChatGPT can deliver above-average performance in the Iranian Medical Residency Examination, demonstrating its potential for using language models in medicine.",article,,,,,,,,,
"Performance and exploration of ChatGPT in medical examination, records and education in Chinese: Pave the way for medical AI",International Journal of Medical Informatics,177,105173,2023,1386-5056,https://doi.org/10.1016/j.ijmedinf.2023.105173,https://www.sciencedirect.com/science/article/pii/S1386505623001910,Hongyan Wang and WeiZhen Wu and Zhi Dou and Liangliang He and Liqiang Yang,,"Background
Although chat generative pre-trained transformer (ChatGPT) has made several successful attempts in the medical field, most notably in answering medical questions in English, no studies have evaluated ChatGPT's performance in a Chinese context for a medical task.
Objective
The aim of this study was to evaluate ChatGPT’s ability to understand medical knowledge in Chinese, as well as its potential to serve as an electronic health infrastructure for medical development, by evaluating its performance in medical examinations, records, and education.
Method
The Chinese (CNMLE) and English (ENMLE) datasets of the China National Medical Licensing Examination and the Chinese dataset (NEEPM) of the China National Entrance Examination for Postgraduate Clinical Medicine Comprehensive Ability were used to evaluate the performance of ChatGPT (GPT-3.5 and GPT-4). We assessed answer accuracy, verbal fluency, and the classification of incorrect responses owing to hallucinations on multiple occasions. In addition, we tested ChatGPT's performance on discharge summaries and group learning in a Chinese context on a small scale.
Results
The accuracy of GPT-3.5 in CNMLE, ENMLE, and NEEPM was 56% (56/100), 76% (76/100), and 62% (62/100), respectively, compared to that of GPT-4, which was of 84% (84/100), 86% (86/100), and 82% (82/100). The verbal fluency of all the ChatGPT responses exceeded 95%. Among the GPT-3.5 incorrect responses, the proportions of open-domain hallucinations were 66 % (29/44), 54 % (14/24), and 63 % (24/38), whereas close-domain hallucinations accounted for 34 % (15/44), 46 % (14/24), and 37 % (14/38), respectively. By contrast, GPT-4 open-domain hallucinations accounted for 56% (9/16), 43% (6/14), and 83% (15/18), while close-domain hallucinations accounted for 44% (7/16), 57% (8/14), and 17% (3/18), respectively. In the discharge summary, ChatGPT demonstrated logical coherence, however GPT-3.5 could not fulfill the quality requirements, while GPT-4 met the qualification of 60% (6/10). In group learning, the verbal fluency and interaction satisfaction with ChatGPT were 100% (10/10).
Conclusion
ChatGPT based on GPT-4 is at par with Chinese medical practitioners who passed the CNMLE and at the standard required for admission to clinical medical graduate programs in China. The GPT-4 shows promising potential for discharge summarization and group learning. Additionally, it shows high verbal fluency, resulting in a positive human–computer interaction experience. GPT-4 significantly improves multiple capabilities and reduces hallucinations compared to the previous GPT-3.5 model, with a particular leap forward in the Chinese comprehension capability of medical tasks. Artificial intelligence (AI) systems face the challenges of hallucinations, legal risks, and ethical issues. However, we discovered ChatGPT's potential to promote medical development as an electronic health infrastructure, paving the way for Medical AI to become necessary.",article,,,,,,,,,
Evaluation of Large language model performance on the Multi-Specialty Recruitment Assessment (MSRA) exam,Computers in Biology and Medicine,168,107794,2024,0010-4825,https://doi.org/10.1016/j.compbiomed.2023.107794,https://www.sciencedirect.com/science/article/pii/S0010482523012593,Panagiotis Tsoutsanis and Aristotelis Tsoutsanis,"Large language models, Artificial intelligence, Medical education, Medical exam","Introduction
AI-powered platforms have gained prominence in medical education and training, offering diverse applications from surgical performance assessment to exam preparation. This research paper examines the capabilities of Large Language Models (LLMs), including Llama 2, Google Bard, Bing Chat, and ChatGPT-3.5, in answering multiple-choice questions of the Clinical Problem Solving (CPS) paper of the Multi-Specialty Recruitment Assessment (MSRA) exam.
Methods
Using a dataset of 100 CPS questions from ten subject categories, we assessed the LLMs' performance against medical doctors preparing for the exam.
Results
Results showed that Bing Chat outperformed all other LLMs and even surpassed human users from the Qbank question bank. Conversely, Llama 2's performance was inferior to human users. Google Bard and ChatGPT 3.5 did not exhibit statistically significant differences in correct response rates compared to human candidates. Pairwise comparisons demonstrated Bing Chat's significant superiority over Llama 2, Google Bard, and ChatGPT 3.5. However, no significant differences were found between Llama 2 and Google Bard, Llama 2, and ChatGPT-3.5, and Google Bard and ChatGPT-3.5.
Discussion
Freely available LLMs have already demonstrated that they can perform as well or even outperform human users in answering MSRA exam questions. Bing Chat emerged as a particularly strong performer. The study also highlights the potential for enhancing LLMs' medical knowledge acquisition through tailored fine-tuning. Medical knowledge tailored LLMs such as Med-PaLM, have already shown promising results.
Conclusion
We provided valuable insights into LLMs' competence in answering medical MCQs and their potential integration into medical education and assessment processes.",article,,,,,,,,,
Generation of Radiology Findings in Chest X-Ray by Leveraging Collaborative Knowledge,Procedia Computer Science,221,1102-1109,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.08.094,https://www.sciencedirect.com/science/article/pii/S1877050923008529,Manuela Daniela Danu and George Marica and Sanjeev Kumar Karn and Bogdan Georgescu and Awais Mansoor and Florin Ghesu and Lucian Mihai Itu and Constantin Suciu and Sasa Grbic and Oladimeji Farri and Dorin Comaniciu,"abnormalities detection, Findings generation, chest X-ray, radiology report, generative large language model, collaborative knowledge","Among all the sub-sections in a typical radiology report, the Clinical Indications, Findings, and Impression often reflect important details about the health status of a patient. The information included in Impression is also often covered in Findings. While Findings and Impression can be deduced by inspecting the image, Clinical Indications often require additional context. The cognitive task of interpreting medical images remains the most critical and often time-consuming step in the radiology workflow. Instead of generating an end-to-end radiology report, in this paper, we focus on generating the Findings from automated interpretation of medical images, specifically chest X-rays (CXRs). Thus, this work focuses on reducing the workload of radiologists who spend most of their time either writing or narrating the Findings. Unlike past research, which addresses radiology report generation as a single-step image captioning task, we have further taken into consideration the complexity of interpreting CXR images and propose a two-step approach: (a) detecting the regions with abnormalities in the image, and (b) generating relevant text for regions with abnormalities by employing a generative large language model (LLM). This two-step approach introduces a layer of interpretability and aligns the framework with the systematic reasoning that radiologists use when reviewing a CXR.",article,,Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023),,,,,,,
Unlocking the opportunities through ChatGPT Tool towards ameliorating the education system,"BenchCouncil Transactions on Benchmarks, Standards and Evaluations",3,100115,2023,2772-4859,https://doi.org/10.1016/j.tbench.2023.100115,https://www.sciencedirect.com/science/article/pii/S2772485923000327,Mohd Javaid and Abid Haleem and Ravi Pratap Singh and Shahbaz Khan and Ibrahim Haleem Khan,"Artificial Intelligence, ChatGPT, Education, Students, Teaching, Learning","Artificial Intelligence (AI)-based ChatGPT developed by OpenAI is now widely accepted in several fields, including education. Students can learn about ideas and theories by using this technology while generating content with it. ChatGPT is built on State of the Art (SOA), like Deep Learning (DL), Natural Language Processing (NLP), and Machine Learning (ML), an extrapolation of a class of ML-NLP models known as Large Language Model (LLMs). It may be used to automate test and assignment grading, giving instructors more time to concentrate on instruction. This technology can be utilised to customise learning for kids, enabling them to focus more intently on the subject matter and critical thinking ChatGPT is an excellent tool for language lessons since it can translate text from one language to another. It may provide lists of vocabulary terms and meanings, assisting students in developing their language proficiency with resources. Personalised learning opportunities are one of ChatGPT’s significant applications in the classroom. This might include creating educational resources and content tailored to a student’s unique interests, skills, and learning goals. This paper discusses the need for ChatGPT and the significant features of ChatGPT in the education system. Further, it identifies and discusses the significant applications of ChatGPT in education. Using ChatGPT, educators may design lessons and instructional materials specific to each student’s requirements and skills based on current trends. Students may work at their speed and concentrate on the areas where they need the most support, resulting in a more effective and efficient learning environment. Both instructors and students may profit significantly from using ChatGPT in the classroom. Instructors may save time on numerous duties by using this technology. In future, ChatGPT will become a powerful tool for enhancing students’ and teachers’ experience.",article,2,,,,,,,,
Developing and validating a knowledge-based AI assessment system for learning clinical core medical knowledge in otolaryngology,Computers in Biology and Medicine,178,108765,2024,0010-4825,https://doi.org/10.1016/j.compbiomed.2024.108765,https://www.sciencedirect.com/science/article/pii/S0010482524008503,Jun-Ming Su and Su-Yi Hsu and Te-Yung Fang and Pa-Chun Wang,"Otolaryngology, Clinical core medical knowledge, Knowledge-based AI approaches, Adaptive assessment, Knowledge aggregation","Background
Clinical core medical knowledge (CCMK) learning is essential for medical trainees. Adaptive assessment systems can facilitate self-learning, but extracting experts' CCMK is challenging, especially using modern data-driven artificial intelligence (AI) approaches (e.g., deep learning).
Objectives
This study aims to develop a multi-expert knowledge–aggregated adaptive assessment scheme (MEKAS) using knowledge-based AI approaches to facilitate the learning of CCMK in otolaryngology (CCMK-OTO) and validate its effectiveness through a one-month training program for CCMK-OTO education at a tertiary referral hospital.
Methods
The MEKAS utilized the repertory grid technique and case-based reasoning to aggregate experts' knowledge to construct a representative CCMK base, thereby enabling adaptive assessment for CCMK-OTO training. The effects of longitudinal training were compared between the experimental group (EG) and the control group (CG). Both groups received a normal training program (routine meeting, outpatient/operation room teaching, and classroom teaching), while EG received MEKAS for self-learning. The EG comprised 22 UPGY trainees (6 postgraduate [PGY] and 16 undergraduate [UGY] trainees) and 8 otolaryngology residents (ENT-R); the CG comprised 24 UPGY trainees (8 PGY and 16 UGY trainees). The training effectiveness was compared through pre- and post-test CCMK-OTO scores, and user experiences were evaluated using a technology acceptance model-based questionnaire.
Results
Both UPGY (z = −3.976, P < 0.001) and ENT-R (z = −2.038, P = 0.042) groups in EG exhibited significant improvements in their CCMK-OTO scores, while UPGY in CG did not (z = −1.204, P = 0.228). The UPGY group in EG also demonstrated a substantial improvement compared to the UPGY group in CG (z = −4.943, P < 0.001). The EG participants were highly satisfied with the MEKAS system concerning self-learning assistance, adaptive testing, perceived satisfaction, intention to use, perceived usefulness, perceived ease of use, and perceived enjoyment, rating it between an overall average of 3.8 and 4.1 out of 5.0 on all scales.
Conclusions
The MEKAS system facilitates CCMK-OTO learning and provides an efficient knowledge aggregation scheme that can be applied to other medical subjects to efficiently build adaptive assessment systems for CCMK learning. Larger-scale validation across diverse institutions and settings is warranted further to assess MEKAS's scalability, generalizability, and long-term impact.",article,,,,,,,,,
Evaluation of ChatGPT-generated medical responses: A systematic review and meta-analysis,Journal of Biomedical Informatics,151,104620,2024,1532-0464,https://doi.org/10.1016/j.jbi.2024.104620,https://www.sciencedirect.com/science/article/pii/S1532046424000388,Qiuhong Wei and Zhengxiong Yao and Ying Cui and Bo Wei and Zhezhen Jin and Ximing Xu,"ChatGPT, Large language model, Medicine, Evaluation","Objective
Large language models (LLMs) such as ChatGPT are increasingly explored in medical domains. However, the absence of standard guidelines for performance evaluation has led to methodological inconsistencies. This study aims to summarize the available evidence on evaluating ChatGPT’s performance in answering medical questions and provide direction for future research.
Methods
An extensive literature search was conducted on June 15, 2023, across ten medical databases. The keyword used was “ChatGPT,” without restrictions on publication type, language, or date. Studies evaluating ChatGPT's performance in answering medical questions were included. Exclusions comprised review articles, comments, patents, non-medical evaluations of ChatGPT, and preprint studies. Data was extracted on general study characteristics, question sources, conversation processes, assessment metrics, and performance of ChatGPT. An evaluation framework for LLM in medical inquiries was proposed by integrating insights from selected literature. This study is registered with PROSPERO, CRD42023456327.
Results
A total of 3520 articles were identified, of which 60 were reviewed and summarized in this paper and 17 were included in the meta-analysis. ChatGPT displayed an overall integrated accuracy of 56 % (95 % CI: 51 %–60 %, I2 = 87 %) in addressing medical queries. However, the studies varied in question resource, question-asking process, and evaluation metrics. As per our proposed evaluation framework, many studies failed to report methodological details, such as the date of inquiry, version of ChatGPT, and inter-rater consistency.
Conclusion
This review reveals ChatGPT's potential in addressing medical inquiries, but the heterogeneity of the study design and insufficient reporting might affect the results’ reliability. Our proposed evaluation framework provides insights for the future study design and transparent reporting of LLM in responding to medical questions.",article,,,,,,,,,
Large language models for human–robot interaction: A review,Biomimetic Intelligence and Robotics,3,100131,2023,2667-3797,https://doi.org/10.1016/j.birob.2023.100131,https://www.sciencedirect.com/science/article/pii/S2667379723000451,Ceng Zhang and Junxin Chen and Jiatong Li and Yanhong Peng and Zebing Mao,"Large language models, Human–robot interaction, Task completion, Considerations and challenges","The fusion of large language models and robotic systems has introduced a transformative paradigm in human–robot interaction, offering unparalleled capabilities in natural language understanding and task execution. This review paper offers a comprehensive analysis of this nascent but rapidly evolving domain, spotlighting the recent advances of Large Language Models (LLMs) in enhancing their structures and performances, particularly in terms of multimodal input handling, high-level reasoning, and plan generation. Moreover, it probes the current methodologies that integrate LLMs into robotic systems for complex task completion, from traditional probabilistic models to the utilization of value functions and metrics for optimal decision-making. Despite these advancements, the paper also reveals the formidable challenges that confront the field, such as contextual understanding, data privacy and ethical considerations. To our best knowledge, this is the first study to comprehensively analyze the advances and considerations of LLMs in Human–Robot Interaction (HRI) based on recent progress, which provides potential avenues for further research.",article,4,,,,,,,,
Proposals and Methods for Foreign Language Learning Using Machine Translation and Large Language Model,Procedia Computer Science,225,4750-4757,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.10.474,https://www.sciencedirect.com/science/article/pii/S1877050923016320,Kohei Sugiyama and Tsukasa Yamanaka,"English language learning, Foreign language teaching, Machine Translation, Large language model","In this paper, we propose a new learning model that utilizes machine translation and large language models. While English education has traditionally been conducted through the relationship between English teachers and learners, replacing English teachers with machine translation and large language models may offer the potential to provide an equally or even more efficient and high-quality learning environment. The authors have developed a browser-based service to experience this educational environment for Japanese. To experience a new learning model that is high quality and efficient, we have implemented DeepL, a machine translation service that can translate with high accuracy, and ChatGPT, which uses a large language model that can generate natural sentences and adapt to a variety of tasks interactively. By combining these advanced services, it is now possible to provide explanations of the English translations and to evaluate the essays. This newly developed service is currently being experimentally used in English classes at a Japanese university. Interviews with users who used it revealed that they were easily exposed to English above their level. In other words, the results suggest that this proposed model can provide a better environment for English utilization than teachers. The developed service is available to anyone at the following URL. Transable: https://transable.net",article,,27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023),,,,,,,
Multimodal prediction of student performance: A fusion of signed graph neural networks and large language models,Pattern Recognition Letters,181,1-8,2024,0167-8655,https://doi.org/10.1016/j.patrec.2024.03.007,https://www.sciencedirect.com/science/article/pii/S0167865524000758,Sijie Wang and Lin Ni and Zeyu Zhang and Xiaoxuan Li and Xianda Zheng and Jiamou Liu,"Signed network, Graph representations learning, Natural language processing, Multimodal","In online education platforms, accurately predicting student performance is essential for timely dropout prevention and interventions for at-risk students. This task is made difficult by the prevalent use of Multiple-Choice Questions (MCQs) in learnersourcing platforms, where noise in student-generated content and the limitations of existing unsigned graph-based models, specifically their inability to distinguish the semantic meaning between correct and incorrect responses, hinder accurate performance predictions. To address these issues, we introduce the Large Language Model enhanced Signed Bipartite graph Contrastive Learning (LLM-SBCL) model—a novel Multimodal Model utilizing Signed Graph Neural Networks (SGNNs) and a Large Language Model (LLM). Our model uses a signed bipartite graph to represent students’ answers, with positive and negative edges denoting correct and incorrect responses, respectively. To mitigate noise impact, we apply contrastive learning to the signed graphs, combined with knowledge point embeddings from the LLM to further enhance our model’s predictive performance. Upon evaluating our model on five real-world datasets, it demonstrates superior accuracy and stability, exhibiting an average F1 improvement of 3.7% over the best baseline models.",article,,,,,,,,,
GitHub Copilot AI pair programmer: Asset or Liability?,Journal of Systems and Software,203,111734,2023,0164-1212,https://doi.org/10.1016/j.jss.2023.111734,https://www.sciencedirect.com/science/article/pii/S0164121223001292,,"Code completion, Language model, GitHub copilot, Testing","Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot’s proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans’ solutions is greater than Copilot’s suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans’ contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise.",article,,,,,,,,,
"Unveiling security, privacy, and ethical concerns of ChatGPT",Journal of Information and Intelligence,2,102-115,2024,2949-7159,https://doi.org/10.1016/j.jiixd.2023.10.007,https://www.sciencedirect.com/science/article/pii/S2949715923000707,Xiaodong Wu and Ran Duan and Jianbing Ni,"ChatGPT, Large language model (LLM), Security, Privacy, Ethics","This paper delves into the realm of ChatGPT, an AI-powered chatbot that utilizes topic modeling and reinforcement learning to generate natural responses. Although ChatGPT holds immense promise across various industries, such as customer service, education, mental health treatment, personal productivity, and content creation, it is essential to address its security, privacy, and ethical implications. By exploring the upgrade path from GPT-1 to GPT-4, discussing the model's features, limitations, and potential applications, this study aims to shed light on the potential risks of integrating ChatGPT into our daily lives. Focusing on security, privacy, and ethics issues, we highlight the challenges these concerns pose for widespread adoption. Finally, we analyze the open problems in these areas, calling for concerted efforts to ensure the development of secure and ethically sound large language models.",article,2,,,,,,,,
Exploring the REIT architecture for requirements elicitation interview training with robotic and virtual tutors,Journal of Systems and Software,212,112018,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112018,https://www.sciencedirect.com/science/article/pii/S016412122400061X,Binnur Görer and Fatma Başak Aydemir,"Software engineering education, Requirements elicitation interview training, Empirical evaluation, Generic architecture, Robotic tutor, Virtual tutor","Requirements elicitation interviews are a widely adopted technique where the interview success depends on the interviewer’s preparedness and communication skills. Students can enhance these skills through practice interviews. However, organizing practice interviews for many students presents scalability challenges, given the time and effort required to involve stakeholders in each session. To address this, we propose REIT, an extensible architecture for Requirements Elicitation Interview Training system leveraging technologies such as robots and voice systems. REIT has components to support both the interview phase, wherein students act as interviewers while the system assumes the role of an interviewee, and the feedback phase, during which the system assesses students’ performance and offers contextual and behavioral feedback to enhance their interviewing skills. We demonstrate the applicability of REIT through two implementations: RoREIT with a physical robotic agent and VoREIT with a virtual voice-only agent. We empirically evaluated both instances with a group of graduate students. The participants appreciated both systems. They demonstrated higher learning gain when trained with RoREIT, but they found VoREIT more engaging and easier to use. These findings indicate that each system has distinct benefits and drawbacks, suggesting that educators can customize REIT for various settings, considering preferences and available resources.",article,,,,,,,,,
Combining low-code development with ChatGPT to novel no-code approaches: A focus-group study,Intelligent Systems with Applications,20,200289,2023,2667-3053,https://doi.org/10.1016/j.iswa.2023.200289,https://www.sciencedirect.com/science/article/pii/S266730532300114X,José Martins and Frederico Branco and Henrique Mamede,"Low-code, No-code, Artificial intelligence, Software models, ChatGPT, LLM","Low-code tools are a trend in software development for business solutions due to their agility and ease of use. There are a certain number of vendors with such solutions. Still, in most Western countries, there is a clear need for the existence of greater quantities of certified and experienced professionals to work with those tools. This means that companies with more resources can attract and maintain those professionals, whilst other smaller organizations must rely on an endless search for this scarce resource. We will present and validate a model designed to transform ChatGPT into a low-code developer, addressing the demand for a more skilled human resource solution. This innovative tool underwent rigorous validation via a focus group study, engaging a panel of highly experienced experts. Their invaluable insights and feedback on the proposed model were systematically gathered and meticulously analysed.",article,,,,,,,,,
GPTSniffer: A CodeBERT-based classifier to detect source code written by ChatGPT,Journal of Systems and Software,214,112059,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112059,https://www.sciencedirect.com/science/article/pii/S0164121224001043,,"ChatGPT, Code classification, CodeBERT, Pre-trained Models","Since its launch in November 2022, ChatGPT has gained popularity among users, especially programmers who use it to solve development issues. However, while offering a practical solution to programming problems, ChatGPT should be used primarily as a supporting tool (e.g., in software education) rather than as a replacement for humans. Thus, detecting automatically generated source code by ChatGPT is necessary, and tools for identifying AI-generated content need to be adapted to work effectively with code. This paper presents GPTSniffer– a novel approach to the detection of source code written by AI – built on top of CodeBERT. We conducted an empirical study to investigate the feasibility of automated identification of AI-generated code, and the factors that influence this ability. The results show that GPTSniffer can accurately classify whether code is human-written or AI-generated, outperforming two baselines, GPTZero and OpenAI Text Classifier. Also, the study shows how similar training data or a classification context with paired snippets helps boost the prediction. We conclude that GPTSniffer can be leveraged in different contexts, e.g., in software engineering education, where teachers use the tool to detect cheating and plagiarism, or in development, where AI-generated code may require peculiar quality assurance activities.",article,,,,,,,,,
The method of constructing basic-element base using large language model- Take the issue of rice waste,Procedia Computer Science,221,1493-1500,2023,1877-0509,https://doi.org/10.1016/j.procs.2023.08.012,https://www.sciencedirect.com/science/article/pii/S1877050923007706,WANG Long-hao and LI Ding-jie and LI Xing-sen,Large language model;Extenics;Basic-element base;Rice waste;ChatGPT,"The rapid development of artificial intelligence technology has led to the emergence of large language models such as ChatGPT represented by natural language processing technology, but currently there is no effective way to input all the information to be exchanged. In this paper, a method of constructing local basic-element base of input information by combining the large language model with extenics is proposed. Taking rice waste problem as an example, the method is successfully applied to a practical project to verify the feasibility of the method.",article,,Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023),,,,,,,
XLMR4MD: New Vietnamese dataset and framework for detecting the consistency of description and permission in Android applications using large language models,Computers & Security,140,103814,2024,0167-4048,https://doi.org/10.1016/j.cose.2024.103814,https://www.sciencedirect.com/science/article/pii/S0167404824001159,,"Large language models, App Android, Security policy, Application permissions, Application description, Deep learning","Google Play and other application marketplaces have various Android applications and metadata. Among these, description information and privacy policy help explain the application's functionality. They also describe the permission of the application, especially those related to sensitive information. Detecting the inconsistency between the description of the application and privacy information and the permission extracted in the application's source code helps users decide whether to install and use the application. In this research, we propose a new method based on a pre-trained language model to detect inconsistencies between the permission extracted from the description application and privacy policy and the permission extracted from the application's source code (file APK). Related works focus on models of large-scale datasets, especially for resource-rich languages such as English. However, a language with low resources, like Vietnamese, needs more datasets for the task. To solve this problem, we propose the ViDPApp dataset (Description and Privacy Policy of Applications on Vietnamese domains), a high-quality dataset that humans manually annotate with 12,000+ sentences with an inter-annotator agreement (IAA) of over 85%. In addition, we proposed XLMR4MD, a new framework using large language models, outperforming powerful machine models (LSTM, Bi-GRU-LSTM-CNN, WikiBERT, DistilBERT, mBERT, and PhoBERT) and achieving the best with 84.04% F1 score in detecting inconsistencies between Android application permission and description. This framework can be fine-tuned for 100 languages, which benefits low-resource languages like Vietnamese. The dataset is available for research purposes.",article,,,,,,,,,
ChatGPT: A meta-analysis after 2.5 months,Machine Learning with Applications,16,100541,2024,2666-8270,https://doi.org/10.1016/j.mlwa.2024.100541,https://www.sciencedirect.com/science/article/pii/S2666827024000173,Christoph Leiter and Ran Zhang and Yanran Chen and Jonas Belouadi and Daniil Larionov and Vivian Fresen and Steffen Eger,"ChatGPT, Sentiment analysis, Emotion analysis, Science, Large language models","ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and media attention since its release in November 2022. However, little hard evidence is available regarding its perception in various sources. In this paper, we analyze over 300,000 tweets and more than 150 scientific papers to investigate how ChatGPT is perceived and discussed. Our findings show that ChatGPT is generally viewed as of high quality, with positive sentiment and emotions of joy dominating social media. Its perception has slightly decreased since its debut, however, with joy decreasing and (negative) surprise on the rise, and it is perceived more negatively in languages other than English. In recent scientific papers, ChatGPT is characterized as a great opportunity across various fields including the medical domain, but also as a threat concerning ethics and receives mixed assessments for education. Our comprehensive meta-analysis of ChatGPT’s perception after 2.5 months since its release can contribute to shaping the public debate and informing its future development. We make our data available.11https://github.com/NL2G/ChatGPTReview.",article,,,,,,,,,
Beyond human in neurosurgical exams: ChatGPT's success in the Turkish neurosurgical society proficiency board exams,Computers in Biology and Medicine,169,107807,2024,0010-4825,https://doi.org/10.1016/j.compbiomed.2023.107807,https://www.sciencedirect.com/science/article/pii/S0010482523012726,Mustafa Caglar Sahin and Alperen Sozer and Pelin Kuzucu and Tolga Turkmen and Merve Buke Sahin and Ekin Sozer and Ozan Yavuz Tufek and Kerem Nernekli and Hakan Emmez and Emrah Celtikci,"Artificial intelligence, Board, ChatGPT, Education, Exam, Machine learning, Large language model","Chat Generative Pre-Trained Transformer (ChatGPT) is a sophisticated natural language model that employs advanced deep learning techniques and is trained on extensive datasets to produce responses akin to human conversation for user inputs. In this study, ChatGPT's success in the Turkish Neurosurgical Society Proficiency Board Exams (TNSPBE) is compared to the actual candidates who took the exam, along with identifying the types of questions it answered incorrectly, assessing the quality of its responses, and evaluating its performance based on the difficulty level of the questions. Scores of all 260 candidates were recalculated according to the exams they took and included questions in those exams for ranking purposes of this study. The average score of the candidates for a total of 523 questions is 62.02 ± 0.61 compared to ChatGPT, which was 78.77. We have concluded that in addition to ChatGPT's higher response rate, there was also a correlation with the increase in clarity regardless of the difficulty level of the questions with Clarity 1.5, 2.0, 2.5, and 3.0. In the participants, however, there is no such increase in parallel with the increase in clarity.",article,,,,,,,,,
MedChatZH: A tuning LLM for traditional Chinese medicine consultations,Computers in Biology and Medicine,172,108290,2024,0010-4825,https://doi.org/10.1016/j.compbiomed.2024.108290,https://www.sciencedirect.com/science/article/pii/S0010482524003743,Yang Tan and Zhixing Zhang and Mingchen Li and Fei Pan and Hao Duan and Zijie Huang and Hua Deng and Zhuohang Yu and Chen Yang and Guoyang Shen and Peng Qi and Chengyuan Yue and Yuxian Liu and Liang Hong and Huiqun Yu and Guisheng Fan and Yun Tang,"Generative large language models (LLMs), Question-answering (QA), Dialogue model, Traditional Chinese medical QA, Fine-tuning","Generative Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, including Question-Answering (QA) and dialogue systems. However, most models are trained on English data and lack strong generalization in providing answers in Chinese. This limitation is especially evident in specialized domains like traditional Chinese medical QA, where performance suffers due to the absence of fine-tuning and high-quality datasets. To address this, we introduce MedChatZH, a dialogue model optimized for Chinese medical QA based on transformer decoder with LLaMA architecture. Continued pre-training on a curated corpus of Chinese medical books is followed by fine-tuning with a carefully selected medical instruction dataset, resulting in MedChatZH outperforming several Chinese dialogue baselines on a real-world medical dialogue dataset. Our model, code, and dataset are publicly available on GitHub (https://github.com/tyang816/MedChatZH) to encourage further research in traditional Chinese medicine and LLMs.",article,,,,,,,,,
"Decoding ChatGPT: A taxonomy of existing research, current challenges, and possible future directions",Journal of King Saud University - Computer and Information Sciences,35,101675,2023,1319-1578,https://doi.org/10.1016/j.jksuci.2023.101675,https://www.sciencedirect.com/science/article/pii/S131915782300229X,Shahab Saquib Sohail and Faiza Farhat and Yassine Himeur and Mohammad Nadeem and Dag Øivind Madsen and Yashbir Singh and Shadi Atalla and Wathiq Mansoor,"ChatGPT, Large language models (LLMs), Generative Pre-trained Transformer (GPT), AI Generated Content (AIGC), Systematic review, Trustworthy AI","Chat Generative Pre-trained Transformer (ChatGPT) has gained significant interest and attention since its launch in November 2022. It has shown impressive performance in various domains, including passing exams and creative writing. However, challenges and concerns related to biases and trust persist. In this work, we present a comprehensive review of over 100 Scopus-indexed publications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and explore its applications. We critically analyze the existing literature, identifying common approaches employed in the studies. Additionally, we investigate diverse application areas where ChatGPT has found utility, such as healthcare, marketing and financial services, software engineering, academic and scientific writing, research and education, environmental science, and natural language processing. Through examining these applications, we gain valuable insights into the potential of ChatGPT in addressing real-world challenges. We also discuss crucial issues related to ChatGPT, including biases and trustworthiness, emphasizing the need for further research and development in these areas. Furthermore, we identify potential future directions for ChatGPT research, proposing solutions to current challenges and speculating on expected advancements. By fully leveraging the capabilities of ChatGPT, we can unlock its potential across various domains, leading to advancements in conversational AI and transformative impacts in society.",article,8,,,,,,,,
Enhancing large language model capabilities for rumor detection with Knowledge-Powered Prompting,Engineering Applications of Artificial Intelligence,133,108259,2024,0952-1976,https://doi.org/10.1016/j.engappai.2024.108259,https://www.sciencedirect.com/science/article/pii/S0952197624004172,Yeqing Yan and Peng Zheng and Yongjun Wang,"Social networks, Rumor detection, Knowledge augmentation, Prompt tuning, Large language model","Amid the proliferation of misinformation on social networks, automated rumor detection has emerged as a pivotal and pressing research domain. Nonetheless, current methodologies are hindered by constrained feature representations and limited adaptability in effectively addressing diverse and unconventional rumors. The incorporation of large-scale language models holds the promise of delivering heightened semantic comprehension and broader adaptability. Regrettably, prevailing general-purpose prompting approaches frequently fall short in furnishing adequate domain-specific context and guidance, thereby restricting their utility in the context of rumor detection. To ameliorate these concerns, we introduce the Knowledge-Powered Prompting strategy, which imparts task-relevant prompts and context to the model by amalgamating domain expertise with large-scale language models. This fusion equips the model to better align with the exigencies of rumor detection, mitigating the challenges posed by sensitivity to semantic subtleties and a paucity of training samples. In particular, we devise exploration prompts and bolster the prompt representation with a dynamic knowledge injection module, thereby facilitating profound reasoning about pivotal entities. Subsequently, we extract valuable external knowledge through the filtration of interactions between knowledge and claim, thereby diminishing the impact of noise. Concurrently, we undertake joint optimization, encompassing multi-task prompt population and categorical judgment objectives, fostering synergistic semantic modeling and discriminative assessments. Empirical evaluations reveal that our methodology substantially outperforms existing models.",article,,,,,,,,,
EBERT: A lightweight expression-enhanced large-scale pre-trained language model for mathematics education,Knowledge-Based Systems,,112118,2024,0950-7051,https://doi.org/10.1016/j.knosys.2024.112118,https://www.sciencedirect.com/science/article/pii/S0950705124007524,Zhiyi Duan and Hengnian Gu and Yuan Ke and Dongdai Zhou,"Pre-trained model, Question&Answer tree, Expression enhanced matrix, Question&Answer matching","Within the realm of mathematics education, there exist several challenging supervised tasks that educators and researchers encounter, such as question difficulty prediction and mathematical expression understanding. To address these challenges, researchers have introduced unsupervised pre-trained models specifically tailored for mathematics education, yielding promising outcomes. However, the existing literature fails to consider the domain-specific characteristics of mathematics, particularly the structural features in pre-trained corpora and extensive expressions, which makes them costly expensive and time-consuming. To tackle this problem, we propose a lightweight expression-enhanced large-scale pre-trained language model, called EBERT, for mathematics education. Specifically, we select a large number of expression-enriched exercises to further pre-train the original BERT. To depict the inherent structural features existed in expressions, the initial step involves the creation of an Operator Tree for each expression. Subsequently, each exercise is transformed into a corresponding Question&Answer tree (QAT) to serve as the model input. Notably, to ensure the preservation of semantic integrity within the QAT, a specialized Expression Enhanced Matrix is devised to confine the visibility of individual tokens. Additionally, a new pre-training task, referred to as Question&Answer Matching, is introduced to capture exercise-related structural information at the semantic level. Through three downstream tasks in mathematical education, we prove that EBERT outperforms several state-of-the-art baselines (such as MathBERT and GPT-3) in terms of ACC and F1-score.",article,,,,,,,,,
Prompting large language model with context and pre-answer for knowledge-based VQA,Pattern Recognition,151,110399,2024,0031-3203,https://doi.org/10.1016/j.patcog.2024.110399,https://www.sciencedirect.com/science/article/pii/S003132032400150X,Zhongjian Hu and Peng Yang and Yuanshuang Jiang and Zijian Bai,"Visual question answering, Large language model, Knowledge-based VQA, Fine-tuning, In-context learning","Existing studies apply Large Language Model (LLM) to knowledge-based Visual Question Answering (VQA) with encouraging results. Due to the insufficient input information, the previous methods still have shortcomings in constructing the prompt for LLM, and cannot fully activate the capacity of LLM. In addition, previous works adopt GPT-3 for inference, which has expensive costs. In this paper, we propose PCPA: a framework that Prompts LLM with Context and Pre-Answer for VQA. Specifically, we adopt a vanilla VQA model to generate in-context examples and candidate answers, and add a pre-answer selection layer to generate pre-answers. We integrate in-context examples and pre-answers into the prompt to inspire the LLM. In addition, we choose LLaMA instead of GPT-3, which is an open and free model. We build a small dataset to fine-tune the LLM. Compared to existing baselines, the PCPA improves accuracy by more than 2.1 and 1.5 on OK-VQA and A-OKVQA, respectively.",article,,,,,,,,,
Artificial Intelligence as an Innovative Element of Support in Policing,Procedia Computer Science,237,237-244,2024,1877-0509,https://doi.org/10.1016/j.procs.2024.05.101,https://www.sciencedirect.com/science/article/pii/S1877050924011177,Hana Dubravova and Jan Cap and Kristyna Holubova and Lukas Hribnak,"artificial intelligence, police, GPT, large language model, administrative burden, chat","Currently, the public security sector is faced with an increasing administrative burden that limits the ability of police officers to focus on core security tasks. This paper focuses on the possibility of using large-scale language models (LSMs) as an innovative tool to address this challenge. Based on a careful literature review and analysis of current trends in artificial intelligence, the author team develops a concept for integrating GPTs into police practice, with an emphasis on the potential for reducing administrative burden and supporting efficient processing of relevant information. As part of this research, we have identified key areas of policing where AI could bring significant value, including data analysis and document production assistance. However, it should be emphasized that this technology is still in its early stages of development and its implementation would require a carefully considered approach involving interdisciplinary collaboration and further research to test the theoretical assumptions presented in this study. Thus, this paper contributes to a deeper understanding of the potential benefits and challenges of integrating GPT into policing practice and outlines a path towards future innovative solutions in the field of public safety.",article,,International Conference on Industry Sciences and Computer Science Innovation,,,,,,,
"From image to language: A critical analysis of Visual Question Answering (VQA) approaches, challenges, and opportunities",Information Fusion,106,102270,2024,1566-2535,https://doi.org/10.1016/j.inffus.2024.102270,https://www.sciencedirect.com/science/article/pii/S1566253524000484,Md. Farhan Ishmam and Md. Sakib Hossain Shovon and M.F. Mridha and Nilanjan Dey,"Visual Question Answering, Vision language pre-training, Multimodal learning, Multimodal large language models","The multimodal task of Visual Question Answering (VQA) encompassing elements of Computer Vision (CV) and Natural Language Processing (NLP), aims to generate answers to questions on any visual input. Over time, the scope of VQA has expanded from datasets focusing on an extensive collection of natural images to datasets featuring synthetic images, video, 3D environments, and various other visual inputs. The emergence of large pre-trained networks has shifted the early VQA approaches relying on feature extraction and fusion schemes to vision language pre-training (VLP) techniques. However, there is a lack of comprehensive surveys that encompass both traditional VQA architectures and contemporary VLP-based methods. Furthermore, the VLP challenges in the lens of VQA haven’t been thoroughly explored, leaving room for potential open problems to emerge. Our work presents a survey in the domain of VQA that delves into the intricacies of VQA datasets and methods over the field’s history, introduces a detailed taxonomy to categorize the facets of VQA, and highlights the recent trends, challenges, and scopes for improvement. We further generalize VQA to multimodal question answering, explore tasks related to VQA, and present a set of open problems for future investigation. The work aims to navigate both beginners and experts by shedding light on the potential avenues of research and expanding the boundaries of the field.",article,,,,,,,,,
ChatGPT in healthcare: A taxonomy and systematic review,Computer Methods and Programs in Biomedicine,245,108013,2024,0169-2607,https://doi.org/10.1016/j.cmpb.2024.108013,https://www.sciencedirect.com/science/article/pii/S0169260724000087,Jianning Li and Amin Dada and Behrus Puladi and Jens Kleesiek and Jan Egger,"ChatGPT, Healthcare, NLP, Transformer, LLM, OpenAI, Taxonomy, Bard, BERT, LLaMA","The recent release of ChatGPT, a chat bot research project/product of natural language processing (NLP) by OpenAI, stirs up a sensation among both the general public and medical professionals, amassing a phenomenally large user base in a short time. This is a typical example of the ‘productization’ of cutting-edge technologies, which allows the general public without a technical background to gain firsthand experience in artificial intelligence (AI), similar to the AI hype created by AlphaGo (DeepMind Technologies, UK) and self-driving cars (Google, Tesla, etc.). However, it is crucial, especially for healthcare researchers, to remain prudent amidst the hype. This work provides a systematic review of existing publications on the use of ChatGPT in healthcare, elucidating the ‘status quo’ of ChatGPT in medical applications, for general readers, healthcare professionals as well as NLP scientists. The large biomedical literature database PubMed is used to retrieve published works on this topic using the keyword ‘ChatGPT’. An inclusion criterion and a taxonomy are further proposed to filter the search results and categorize the selected publications, respectively. It is found through the review that the current release of ChatGPT has achieved only moderate or ‘passing’ performance in a variety of tests, and is unreliable for actual clinical deployment, since it is not intended for clinical applications by design. We conclude that specialized NLP models trained on (bio)medical datasets still represent the right direction to pursue for critical clinical applications.",article,,,,,,,,,
Transformers and large language models in healthcare: A review,Artificial Intelligence in Medicine,154,102900,2024,0933-3657,https://doi.org/10.1016/j.artmed.2024.102900,https://www.sciencedirect.com/science/article/pii/S0933365724001428,Subhash Nerella and Sabyasachi Bandyopadhyay and Jiaqing Zhang and Miguel Contreras and Scott Siegel and Aysegul Bumin and Brandon Silva and Jessica Sena and Benjamin Shickel and Azra Bihorac and Kia Khezeli and Parisa Rashidi,"Transformers, Healthcare, Electronic Health Records, Large Language Models, Medical Imaging, Natural Language Processing","With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of healthcare data, including clinical NLP, medical imaging, structured Electronic Health Records (EHR), social media, bio-physiological signals, biomolecular sequences. Furthermore, which have also include the articles that used the transformer architecture for generating surgical instructions and predicting adverse outcomes after surgeries under the umbrella of critical care. Under diverse settings, these models have been used for clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. Finally, we also discuss the benefits and limitations of using transformers in healthcare and examine issues such as computational cost, model interpretability, fairness, alignment with human values, ethical implications, and environmental impact.",article,,,,,,,,,
"ChatGPT for digital forensic investigation: The good, the bad, and the unknown",Forensic Science International: Digital Investigation,46,301609,2023,2666-2817,https://doi.org/10.1016/j.fsidi.2023.301609,https://www.sciencedirect.com/science/article/pii/S266628172300121X,Mark Scanlon and Frank Breitinger and Christopher Hargreaves and Jan-Niclas Hilgert and John Sheppard,"ChatGPT, Digital forensics, Artificial intelligence, Generative pre-trained transformers (GPT), Large language models (LLM)","The disruptive application of ChatGPT (GPT-3.5, GPT-4) to a variety of domains has become a topic of much discussion in the scientific community and society at large. Large Language Models (LLMs), e.g., BERT, Bard, Generative Pre-trained Transformers (GPTs), LLaMA, etc., have the ability to take instructions, or prompts, from users and generate answers and solutions based on very large volumes of text-based training data. This paper assesses the impact and potential impact of ChatGPT on the field of digital forensics, specifically looking at its latest pre-trained LLM, GPT-4. A series of experiments are conducted to assess its capability across several digital forensic use cases including artefact understanding, evidence searching, code generation, anomaly detection, incident response, and education. Across these topics, its strengths and risks are outlined and a number of general conclusions are drawn. Overall this paper concludes that while there are some potential low-risk applications of ChatGPT within digital forensics, many are either unsuitable at present, since the evidence would need to be uploaded to the service, or they require sufficient knowledge of the topic being asked of the tool to identify incorrect assumptions, inaccuracies, and mistakes. However, to an appropriately knowledgeable user, it could act as a useful supporting tool in some circumstances.",article,,,,,,,,,
The academic industry’s response to generative artificial intelligence: An institutional analysis of large language models,Telecommunications Policy,48,102760,2024,0308-5961,https://doi.org/10.1016/j.telpol.2024.102760,https://www.sciencedirect.com/science/article/pii/S0308596124000570,Nir Kshetri,"Academic industry, ChatGPT, Generative artificial intelligence, Institutional theory, Large language models, Theorization","This paper examines academic institutions' heterogeneous initial responses to generative AI (GAI) tools like ChatGPT and factors influencing increased acceptance over time. GAI's disruptive nature coupled with uncertainty about impacts poses adoption challenges. However, external pressures from stakeholders seeking GAI integration contribute to changing attitudes. Actions of institutional change agents also drive growing acceptance by increasing awareness of GAI advantages. They challenge prevailing logics emphasizing assessments, proposing new values around employability and job performance. Additionally, academic institutions reevaluating GAI's value creation potential through applications and evolving business models contributes to favorable responses. The paper proposes an institutional theory framework explaining dynamics underpinning academic institutions' assimilation of GAI. It highlights how various mechanisms like external pressures, institutional entrepreneurs' theorization efforts justifying technology use, and internal sensemaking shape institutional norms and values, enabling academic systems' adaptation. The study informs policy and practice while directing future research toward validating propositions empirically and examining contextual dimensions including industry characteristics affecting GAI adoption.",article,5,,,,,,,,
A decision support framework to incorporate textual data for early student dropout prediction in higher education,Decision Support Systems,168,113940,2023,0167-9236,https://doi.org/10.1016/j.dss.2023.113940,https://www.sciencedirect.com/science/article/pii/S0167923623000155,,"Decision support framework, Learning analytics, Student dropout prediction, Textual data, doc2vec, Segmentation","Managing student dropout in higher education is critical, considering its substantial impacts on students' lives, academic institutions, and society as a whole. Using predictive modeling can be instrumental for this task, as a means to identify dropouts proactively on the basis of student characteristics and their academic performance. To enhance these predictions, textual student feedback also might be relevant; this article proposes a hybrid decision support framework that combines predictive modeling with student segmentation efforts. A real-life data set from a French higher education institution, containing information of 14,391 students and 62,545 feedback documents, confirms the superior performance of the proposed framework, in terms of the area under the curve and top decile lift, compared with various benchmarks. In contributing to decision support system research, this study (1) proposes a new framework for automatic, data-driven segmentation of students based on textual data; (2) compares multiple text representation methods and confirms that incorporating student textual feedback data improves the predictive performance of student dropout models; and (3) establishes useful insights to help decision-makers anticipate and manage student dropout behaviors.",article,,,,,,,,,
The Dark Side of Language Models: Exploring the Potential of LLMs in Multimedia Disinformation Generation and Dissemination,Machine Learning with Applications,16,100545,2024,2666-8270,https://doi.org/10.1016/j.mlwa.2024.100545,https://www.sciencedirect.com/science/article/pii/S2666827024000215,Dipto Barman and Ziyi Guo and Owen Conlan,"Llms, Disinformation, Information quality, ChatGPT, Mitigation","Disinformation - the deliberate spread of false or misleading information poses a significant threat to our society by undermining trust, exacerbating polarization, and manipulating public opinion. With the rapid advancement of artificial intelligence and the growing prominence of large language models (LLMs) such as ChatGPT, new avenues for the dissemination of disinformation are emerging. This review paper explores the potential of LLMs to initiate the generation of multi-media disinformation, encompassing text, images, audio, and video. We begin by examining the capabilities of LLMs, highlighting their potential to create compelling, context-aware content that can be weaponized for malicious purposes. Subsequently, we examine the nature of disinformation and the various mechanisms through which it spreads in the digital landscape. Utilizing these advanced models, malicious actors can automate and scale up disinformation effectively. We describe a theoretical pipeline for creating and disseminating disinformation on social media. Existing interventions to combat disinformation are also reviewed. While these efforts have shown success, we argue that they need to be strengthened to effectively counter the escalating threat posed by LLMs. Digital platforms have, unfortunately, enabled malicious actors to extend the reach of disinformation. The advent of LLMs poses an additional concern as they can be harnessed to significantly amplify the velocity, variety, and volume of disinformation. Thus, this review proposes augmenting current interventions with AI tools like LLMs, capable of assessing information more swiftly and comprehensively than human fact-checkers. This paper illuminates the dark side of LLMs and highlights their potential to be exploited as disinformation dissemination tools.",article,,,,,,,,,
An empirical approach to understand the role of emotions in code comprehension,Journal of Computer Languages,79,101269,2024,2590-1184,https://doi.org/10.1016/j.cola.2024.101269,https://www.sciencedirect.com/science/article/pii/S2590118424000121,Divjot Singh and Ashutosh Mishra and Ashutosh Aggarwal,"Code comprehension, Systematic literature review, Emotions, Cognitive skills","Programming and cognitive skills are two pivotal abilities of programmers to maintain software products. First, this study included a systematic literature review on code comprehension, emotions, cognitive psychology, and belief-desire-intention domains to analyse various code comprehension monitoring techniques, performance metrics, and computational methodologies. Second, a case study is conducted to examine the influence of various emotional stages on programmers’ programming and cognitive skills while comprehending the software code. The categorization of the participants is done empirically based on their expertism level, and the same results are verified using various machine learning models and performance metrics.",article,,,,,,,,,
"Identifying social determinants of health from clinical narratives: A study of performance, documentation ratio, and potential bias",Journal of Biomedical Informatics,153,104642,2024,1532-0464,https://doi.org/10.1016/j.jbi.2024.104642,https://www.sciencedirect.com/science/article/pii/S1532046424000601,,"Social determinants of health, Large language model, Transformer, Clinical concept extraction, Natural language processing, Cancer","Objective
To develop a natural language processing (NLP) package to extract social determinants of health (SDoH) from clinical narratives, examine the bias among race and gender groups, test the generalizability of extracting SDoH for different disease groups, and examine population-level extraction ratio.
Methods
We developed SDoH corpora using clinical notes identified at the University of Florida (UF) Health. We systematically compared 7 transformer-based large language models (LLMs) and developed an open-source package – SODA (i.e., SOcial DeterminAnts) to facilitate SDoH extraction from clinical narratives. We examined the performance and potential bias of SODA for different race and gender groups, tested the generalizability of SODA using two disease domains including cancer and opioid use, and explored strategies for improvement. We applied SODA to extract 19 categories of SDoH from the breast (n = 7,971), lung (n = 11,804), and colorectal cancer (n = 6,240) cohorts to assess patient-level extraction ratio and examine the differences among race and gender groups.
Results
We developed an SDoH corpus using 629 clinical notes of cancer patients with annotations of 13,193 SDoH concepts/attributes from 19 categories of SDoH, and another cross-disease validation corpus using 200 notes from opioid use patients with 4,342 SDoH concepts/attributes. We compared 7 transformer models and the GatorTron model achieved the best mean average strict/lenient F1 scores of 0.9122 and 0.9367 for SDoH concept extraction and 0.9584 and 0.9593 for linking attributes to SDoH concepts. There is a small performance gap (∼4%) between Males and Females, but a large performance gap (>16 %) among race groups. The performance dropped when we applied the cancer SDoH model to the opioid cohort; fine-tuning using a smaller opioid SDoH corpus improved the performance. The extraction ratio varied in the three cancer cohorts, in which 10 SDoH could be extracted from over 70 % of cancer patients, but 9 SDoH could be extracted from less than 70 % of cancer patients. Individuals from the White and Black groups have a higher extraction ratio than other minority race groups.
Conclusions
Our SODA package achieved good performance in extracting 19 categories of SDoH from clinical narratives. The SODA package with pre-trained transformer models is available at https://github.com/uf-hobi-informatics-lab/SODA_Docker.",article,,,,,,,,,
Blockchain-based auditing of legal decisions supported by explainable AI and generative AI tools,Engineering Applications of Artificial Intelligence,129,107666,2024,0952-1976,https://doi.org/10.1016/j.engappai.2023.107666,https://www.sciencedirect.com/science/article/pii/S095219762301850X,,"Legal, Law, Explainable AI, Blockchain, Generative AI, Responsible AI","Generative AI tools powered by Large Language Models (LLMs) have demonstrated advanced capabilities in understanding and articulating legal facts closer to the level of legal practitioners. However, scholars hold contrasting views on the reliability of the reasoning behind a decision derived from LLMs due to its black-box nature. Law firms are vigilant in recognizing the potential risks of violating confidentiality and inappropriate exposure of sensitive legal data through the prompt sent to Generative AI. This research attempts to find an equilibrium between responsible usage and control of human legal professionals over content produced by Generative AI through regular audits. It investigates the potential of Generative AI in drafting correspondence for pre-litigation decisions derived from an eXplainable AI (XAI) algorithm. This research presents an end-to-end process of designing the architecture and methodology for a blockchain-based auditing system. It detects unauthorized alterations of data repositories containing the decisions by an XAI model and automated textual explanation by Generative AI. The automated auditing by blockchain facilitates responsible usage of AI technologies and reduces discrepancies in tracing the accountability of adversarial decisions. It conceptualizes the two algorithms. First, strategic on-chain (within blockchain) and off-chain (outside blockchain) data storage in compliance with the data protection laws and critical requirements of stakeholders in a legal firm. Second, auditing by comparison of the unique signature as Merkle roots of files stored off-chain with their immutable blockchain counterpart. A case study on liability cases under tort law demonstrates the system implementation results.",article,,,,,,,,,
TeenyTinyLlama: Open-source tiny language models trained in Brazilian Portuguese,Machine Learning with Applications,16,100558,2024,2666-8270,https://doi.org/10.1016/j.mlwa.2024.100558,https://www.sciencedirect.com/science/article/pii/S2666827024000343,,"Large language models, Portuguese, Text generation, Low-resource settings, Low-resource languages","Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development.",article,,,,,,,,,
AGCVT-prompt for sentiment classification: Automatically generating chain of thought and verbalizer in prompt learning,Engineering Applications of Artificial Intelligence,132,107907,2024,0952-1976,https://doi.org/10.1016/j.engappai.2024.107907,https://www.sciencedirect.com/science/article/pii/S0952197624000654,Xu Gu and Xiaoliang Chen and Peng Lu and Zonggen Li and Yajun Du and Xianyong Li,"Large language models, Prompt learning, Sentiment classification, Chain of thought","Large language models (LLMs) have revolutionized natural language processing, but they require significant data and hardware resources. Prompt learning offers a solution by enabling a single model for multiple downstream tasks. However, current prompt learning methods rely on costly prompt templates for training. This is a challenge for tasks like sentiment classification, where high-quality templates are hard to create and pseudo-token composed templates can be expensive to train. Recent studies on the chain of thought (COT) have shown that enhancing the presentation of certain aspects of the reasoning process can improve the performance of LLMs. With this in mind, this research introduces the auto-generated COT and verbalizer templates (AGCVT-Prompt) technique, which clusters unlabeled texts according to their identified topic and sentiment. Subsequently, it generates dual verbalizers and formulates both topic and sentiment prompt templates, utilizing the categories discerned within the text and verbalizers. This method significantly improves the transparency and interpretability of the model’s decision-making processes. The AGCVT-Prompt technique was evaluated against conventional prompt learning and advanced sentiment classification methods, using state-of-the-art LLMs on both Chinese and English datasets. The results showed superior performance in all evaluations. Specifically, the AGCVT-Prompt method outperformed previous prompt learning techniques in few-shot learning scenarios, providing higher zero-shot and few-shot learning capabilities. Additionally, AGCVT-Prompt was utilized to analyze network comments about Corona Virus Disease 2019, providing valuable insights. These findings indicate that AGCVT-Prompt is a promising alternative for sentiment classification tasks, particularly in situations where labeled data is scarce.",article,,,,,,,,,
ChatReview: A ChatGPT-enabled natural language processing framework to study domain-specific user reviews,Machine Learning with Applications,15,100522,2024,2666-8270,https://doi.org/10.1016/j.mlwa.2023.100522,https://www.sciencedirect.com/science/article/pii/S2666827023000750,Brittany Ho and Ta’Rhonda Mayberry and Khanh Linh Nguyen and Manohar Dhulipala and Vivek Krishnamani Pallipuram,"Natural Language Processing, ChatGPT, Sentiment analysis, Prompt engineering, Intelligent search engines, Recommender system","Intelligent search engines including pre-trained generative transformers (GPT) have revolutionized the user search experience. Several fields including e-commerce, education, and hospitality are increasingly exploring GPT tools to study user reviews and gain critical insights to improve their service quality. However, massive user-review data and imprecise prompt engineering lead to biased, irrelevant, and impersonal search results. In addition, exposing user data to these search engines may pose privacy issues. Motivated by these factors, we present ChatReview, a ChatGPT-enabled natural language processing (NLP) framework that effectively studies domain-specific user reviews to offer relevant and personalized search results at multiple levels of granularity. The framework accomplishes this task using four phases including data collection, tokenization, query construction, and response generation. The data collection phase involves gathering domain-specific user reviews from public and private repositories. In the tokenization phase, ChatReview applies sentiment analysis to extract keywords and categorize them into various sentiment classes. This process creates a token repository that best describes the user sentiments for a given user-review data. In the query construction phase, the framework uses the token repository and domain knowledge to construct three types of ChatGPT prompts including explicit, implicit, and creative. In the response generation phase, ChatReview pipelines these prompts into ChatGPT to generate search results at varying levels of granularity. We analyze our framework using three real-world domains including education, local restaurants, and hospitality. We assert that our framework simplifies prompt engineering for general users to produce effective results while minimizing the exposure of sensitive user data to search engines. We also present a one-of-a-kind Large Language Model (LLM) peer assessment of the ChatReview framework. Specifically, we employ Google’s Bard to objectively and qualitatively analyze the various ChatReview outputs. Our Bard-based analyses yield over 90% satisfaction, establishing ChatReview as a viable survey analysis tool.",article,,,,,,,,,
A survey on machine learning techniques applied to source code,Journal of Systems and Software,209,111934,2024,0164-1212,https://doi.org/10.1016/j.jss.2023.111934,https://www.sciencedirect.com/science/article/pii/S0164121223003291,Tushar Sharma and Maria Kechagia and Stefanos Georgiou and Rohit Tiwari and Indira Vats and Hadi Moazen and Federica Sarro,"Machine learning for software engineering, Source code analysis, Deep learning, Datasets, Tools","The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 494 studies. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.",article,,,,,,,,,
A cross-guidance cross-lingual model on generated parallel corpus for classical Chinese machine reading comprehension,Information Processing & Management,61,103607,2024,0306-4573,https://doi.org/10.1016/j.ipm.2023.103607,https://www.sciencedirect.com/science/article/pii/S0306457323003448,Junyi Xiang and Maofu Liu and Qiyuan Li and Chen Qiu and Huijun Hu,"Classical Chinese machine reading comprehension, Chinese diachronic gap, Cross-guidance cross-lingual model, Parallel corpus generation","Chinese diachronic gap is a key issue in classical Chinese machine reading comprehension (CCMRC). Preceding work on bridging this gap has been mostly restricted to limited monolingual classical Chinese corpora pre-training and lexical knowledge integration, which require a great deal of human resources. In this paper, we propose a cross-guidance cross-lingual model (CGCLM), pre-trained on a classical and modern Chinese parallel corpus generated from a large language model, to bridge the Chinese diachronic gap and reduce the manual effort. The CGCLM facilitates accurate translation by providing in-context examples and feedback based on the longest common substring between source and target sentences, thereby avoiding untranslated Chinese words. Specifically, we consider three pre-training tasks, i.e., cross-masked language modeling, linguistic label cross-prediction, and semantic cross-aware translation language modeling. The knowledge acquired from masked tokens uncovering and linguistic label predicting can lead to the implicit semantic alignment between two language styles. Taking advantage of the semantic similarity between the same syntactic levels of parallel pairs, cross-aware modeling integrates and transmits contextualized semantic information. We utilize an 18.6G monolingual corpus to create a 37.2G parallel corpus. Manual evaluation has resulted in only acceptable discrepancies between our generated and human-edited parallel corpora. Extensive experimental results show that our proposed model outperforms the state-of-the-art by an average accuracy of 3.13%, 2.44%, and 2.17% on CCMRC, classical Chinese language understanding evaluation (CCLUE), and modern Chinese language understanding evaluation (MCLUE) tasks.",article,2,,,,,,,,
Impermanent identifiers: Enhanced source code comprehension and refactoring,Journal of Systems and Software,216,112137,2024,0164-1212,https://doi.org/10.1016/j.jss.2024.112137,https://www.sciencedirect.com/science/article/pii/S0164121224001821,Eduardo Martins Guerra and André A.S. Ivo and Fernando O. Pereira and Romain Robbes and Andrea Janes and Fábio Fagundes Silveira,"Source code identifiers, Program comprehension, Software refactoring, Software evolution","In response to the prevailing challenges in contemporary software development, this article introduces an innovative approach to code augmentation centered around Impermanent Identifiers. The primary goal is to enhance the software development experience by introducing dynamic identifiers that adapt to changing contexts, facilitating more efficient interactions between developers and source code, ultimately advancing comprehension, maintenance, and collaboration in software development. Additionally, this study rigorously evaluates the adoption and acceptance of Impermanent Identifiers within the software development landscape. Through a comprehensive empirical examination, we investigate how developers perceive and integrate this approach into their daily programming practices, exploring perceived benefits, potential barriers, and factors influencing its adoption. In summary, this article charts a new course for code augmentation, proposing Impermanent Identifiers as its cornerstone while assessing their feasibility and acceptance among developers. This interdisciplinary research seeks to contribute to the continuous improvement of software development practices and the progress of code augmentation technology.",article,,,,,,,,,
A comparative analysis of knowledge injection strategies for large language models in the scholarly domain,Engineering Applications of Artificial Intelligence,133,108166,2024,0952-1976,https://doi.org/10.1016/j.engappai.2024.108166,https://www.sciencedirect.com/science/article/pii/S0952197624003245,,"Knowledge injection, Knowledge graphs, Large language models, Transformers, BERT, Classification, Natural language processing","In recent years, transformer-based models have emerged as powerful tools for natural language processing tasks, demonstrating remarkable performance in several domains. However, they still present significant limitations. These shortcomings become more noticeable when dealing with highly specific and complex concepts, particularly within the scientific domain. For example, transformer models have particular difficulties when processing scientific articles due to the domain-specific terminologies and sophisticated ideas often encountered in scientific literature. To overcome these challenges and further enhance the effectiveness of transformers in specific fields, researchers have turned their attention to the concept of knowledge injection. Knowledge injection is the process of incorporating outside knowledge into transformer models to improve their performance on certain tasks. In this paper, we present a comprehensive study of knowledge injection strategies for transformers within the scientific domain. Specifically, we provide a detailed overview and comparative assessment of four primary methodologies, evaluating their efficacy in the task of classifying scientific articles. For this purpose, we constructed a new benchmark including both 24K labelled papers and a knowledge graph of 9.2K triples describing pertinent research topics. We also developed a full codebase to easily re-implement all knowledge injection strategies in different domains. A formal evaluation indicates that the majority of the proposed knowledge injection methodologies significantly outperform the baseline established by Bidirectional Encoder Representations from Transformers.",article,,,,,,,,,
Opportunities and challenges in the application of large artificial intelligence models in radiology,Meta-Radiology,2,100080,2024,2950-1628,https://doi.org/10.1016/j.metrad.2024.100080,https://www.sciencedirect.com/science/article/pii/S295016282400033X,Liangrui Pan and Zhenyu Zhao and Ying Lu and Kewei Tang and Liyong Fu and Qingchun Liang and Shaoliang Peng,"Artificial intelligence large models, Radiology, Progress, Challenges","Influenced by ChatGPT, artificial intelligence (AI) large models have witnessed a global upsurge in large model research and development. As people enjoy the convenience by this AI large model, more and more large models in subdivided fields are gradually being proposed, especially large models in radiology imaging field. This article first introduces the development history of large models, technical details, workflow, working principles of multimodal large models and working principles of video generation large models. Secondly, we summarize the latest research progress of AI large models in radiology education, radiology report generation, applications of unimodal and multimodal radiology. Finally, this paper also summarizes some of the challenges of large AI models in radiology, with the aim of better promoting the rapid revolution in the field of radiography.",article,2,,,,,,,,
"A comprehensive survey of ChatGPT: Advancements, applications, prospects, and challenges",Meta-Radiology,1,100022,2023,2950-1628,https://doi.org/10.1016/j.metrad.2023.100022,https://www.sciencedirect.com/science/article/pii/S295016282300022X,Anam Nazir and Ze Wang,"Large Language Models (LLMs), Generative Pre-trained Transformers (GPT), Natural language processing (NLP), Contextual learning, Trustworthy conversational agents, Human-computer interaction, ChatGPT","Large Language Models (LLMs) especially when combined with Generative Pre-trained Transformers (GPT) represent a groundbreaking in natural language processing. In particular, ChatGPT, a state-of-the-art conversational language model with a user-friendly interface, has garnered substantial attention owing to its remarkable capability for generating human-like responses across a variety of conversational scenarios. This survey offers an overview of ChatGPT, delving into its inception, evolution, and key technology. We summarize the fundamental principles that underpin ChatGPT, encompassing its introduction in conjunction with GPT and LLMs. We also highlight the specific characteristics of GPT models with details of their impressive language understanding and generation capabilities. We then summarize applications of ChatGPT in a few representative domains. In parallel to the many advantages that ChatGPT can provide, we discuss the limitations and challenges along with potential mitigation strategies. Despite various controversial arguments and ethical concerns, ChatGPT has drawn significant attention from research industries and academia in a very short period. The survey concludes with an envision of promising avenues for future research in the field of ChatGPT. It is worth noting that knowing and addressing the challenges faced by ChatGPT will mount the way for more reliable and trustworthy conversational agents in the years to come.",article,2,,,,,,,,
Xiaoqing: A Q&A model for glaucoma based on LLMs,Computers in Biology and Medicine,174,108399,2024,0010-4825,https://doi.org/10.1016/j.compbiomed.2024.108399,https://www.sciencedirect.com/science/article/pii/S0010482524004839,Xiaojuan Xue and Deshiwei Zhang and Chengyang Sun and Yiqiao Shi and Rongsheng Wang and Tao Tan and Peng Gao and Sujie Fan and Guangtao Zhai and Menghan Hu and Yue Wu,"Glaucoma, Large Language Models (LLMs), Medical NLP system, Ophthalmology question and answer system","Glaucoma is one of the leading cause of blindness worldwide. Individuals affected by glaucoma, including patients and their family members, frequently encounter a deficit in dependable support beyond the confines of clinical environments. Seeking advice via the internet can be a difficult task due to the vast amount of disorganized and unstructured material available on these sites, nevertheless. This research explores how Large Language Models (LLMs) can be leveraged to better serve medical research and benefit glaucoma patients. We introduce Xiaoqing, a Natural Language Processing (NLP) model specifically tailored for the glaucoma field, detailing its development and deployment. To evaluate its effectiveness, we conducted two forms of experiments: comparative and experiential. In the comparative analysis, we presented 22 glaucoma-related questions in simplified Chinese to three medical NLP models (Xiaoqing LLMs, HuaTuo, Ivy GPT) and two general models (ChatGPT-3.5 and ChatGPT-4), covering a range of topics from basic glaucoma knowledge to treatment, surgery, research, management standards, and patient lifestyle. Responses were assessed for informativeness and readability. The experiential experiment involved glaucoma patients and non-patients interacting with Xiaoqing, collecting and analyzing their questions and feedback on the same criteria. The findings demonstrated that Xiaoqing notably outperformed the other models in terms of informativeness and readability, suggesting that Xiaoqing is a significant advancement in the management and treatment of glaucoma in China. We also provide a Web-based version of Xiaoqing, allowing readers to directly experience its functionality. The Web-based Xiaoqing is available at https://qa.glaucoma-assistant.com//qa.",article,,,,,,,,,
