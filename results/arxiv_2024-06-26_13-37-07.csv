doi,url,title,abstract,published
10.1145/3545947.3569630,http://arxiv.org/pdf/2212.05113v1.pdf,"Automatically Generating CS Learning Materials with Large Language
  Models","Recent breakthroughs in Large Language Models (LLMs), such as GPT-3 and
Codex, now enable software developers to generate code based on a natural
language prompt. Within computer science education, researchers are exploring
the potential for LLMs to generate code explanations and programming
assignments using carefully crafted prompts. These advances may enable students
to interact with code in new ways while helping instructors scale their
learning materials. However, LLMs also introduce new implications for academic
integrity, curriculum design, and software engineering careers. This workshop
will demonstrate the capabilities of LLMs to help attendees evaluate whether
and how LLMs might be integrated into their pedagogy and research. We will also
engage attendees in brainstorming to consider how LLMs will impact our field.",2022-12-09T20:37:44Z
,http://arxiv.org/pdf/2405.20183v1.pdf,"A Survey Study on the State of the Art of Programming Exercise
  Generation using Large Language Models","This paper analyzes Large Language Models (LLMs) with regard to their
programming exercise generation capabilities. Through a survey study, we
defined the state of the art, extracted their strengths and weaknesses and
finally proposed an evaluation matrix, helping researchers and educators to
decide which LLM is the best fitting for the programming exercise generation
use case. We also found that multiple LLMs are capable of producing useful
programming exercises. Nevertheless, there exist challenges like the ease with
which LLMs might solve exercises generated by LLMs. This paper contributes to
the ongoing discourse on the integration of LLMs in education.",2024-05-30T15:49:34Z
,http://arxiv.org/pdf/2406.13972v1.pdf,"CREF: An LLM-based Conversational Software Repair Framework for
  Programming Tutors","Program repair techniques offer cost-saving benefits for debugging within
software development and programming education scenarios. With the proven
effectiveness of Large Language Models (LLMs) in code-related tasks,
researchers have explored their potential for program repair. However, it is
crucial to recognize that existing repair benchmarks may have influenced LLM
training data, potentially causing data leakage. To evaluate LLMs' realistic
repair capabilities, (1) we introduce an extensive, non-crawled benchmark,
referred to as TutorCode, comprising 1,239 C++ defect codes and associated
information such as tutor guidance, solution description, failing test cases,
and the corrected code. Our work assesses the repair performance of 12 LLMs on
TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision
(RPSR). (2) We then provide a comprehensive investigation into which types of
extra information can help LLMs improve their performance in repairing defects.
Among these types, tutor guidance was found to be the most effective
information in enhancing LLM repair capabilities. To fully harness LLMs'
conversational capabilities and the benefits of augmented information, (3) we
introduce a novel conversational semi-automatic repair framework CREF assisting
human tutor. It demonstrates a remarkable AVG-5 improvement of 17.2%-24.6%
compared to the baseline, achieving an impressive AVG-5 of 76.6% when utilizing
GPT-4. These results highlight the potential for enhancing LLMs' repair
capabilities through interactions with tutors and historical conversations
involving incorrect responses. The successful application of CREF in a
real-world educational setting demonstrates its effectiveness in reducing
tutors' workload and improving students' learning experience, while also
showcasing its promise for facilitating other software engineering tasks, such
as code review.",2024-06-20T03:36:34Z
,http://arxiv.org/pdf/2402.01156v2.pdf,"An Empirical Study on Low Code Programming using Traditional vs Large
  Language Model Support","Low-code programming (LCP) refers to programming using models at higher
levels of abstraction, resulting in less manual and more efficient programming,
and reduced learning effort for amateur developers. Many LCP tools have rapidly
evolved and have benefited from the concepts of visual programming languages
(VPLs) and programming by demonstration (PBD). With huge increase in interest
in using large language models (LLMs) in software engineering, LLM-based LCP
has began to become increasingly important. However, the technical principles
and application scenarios of traditional approaches to LCP and LLM-based LCP
are significantly different. Understanding these key differences and
characteristics in the application of the two approaches to LCP by users is
crucial for LCP providers in improving existing and developing new LCP tools,
and in better assisting users in choosing the appropriate LCP technology. We
conducted an empirical study of both traditional LCP and LLM-based LCP. We
analyzed developers' discussions on Stack Overflow (SO) over the past three
years and then explored the similarities and differences between traditional
LCP and LLM-based LCP features and developer feedback. Our findings reveal that
while traditional LCP and LLM-based LCP share common primary usage scenarios,
they significantly differ in scope, limitations and usage throughout the
software development lifecycle, particularly during the implementation phase.
We also examine how LLMs impact and integrate with LCP, discussing the latest
technological developments in LLM-based LCP, such as its integration with VPLs
and the application of LLM Agents in software engineering.",2024-02-02T05:52:32Z
,http://arxiv.org/pdf/2401.16186v1.pdf,"An Empirical Study on Usage and Perceptions of LLMs in a Software
  Engineering Project","Large Language Models (LLMs) represent a leap in artificial intelligence,
excelling in tasks using human language(s). Although the main focus of
general-purpose LLMs is not code generation, they have shown promising results
in the domain. However, the usefulness of LLMs in an academic software
engineering project has not been fully explored yet. In this study, we explore
the usefulness of LLMs for 214 students working in teams consisting of up to
six members. Notably, in the academic course through which this study is
conducted, students were encouraged to integrate LLMs into their development
tool-chain, in contrast to most other academic courses that explicitly prohibit
the use of LLMs.
  In this paper, we analyze the AI-generated code, prompts used for code
generation, and the human intervention levels to integrate the code into the
code base. We also conduct a perception study to gain insights into the
perceived usefulness, influencing factors, and future outlook of LLM from a
computer science student's perspective. Our findings suggest that LLMs can play
a crucial role in the early stages of software development, especially in
generating foundational code structures, and helping with syntax and error
debugging. These insights provide us with a framework on how to effectively
utilize LLMs as a tool to enhance the productivity of software engineering
students, and highlight the necessity of shifting the educational focus toward
preparing students for successful human-AI collaboration.",2024-01-29T14:32:32Z
,http://arxiv.org/pdf/2405.18062v2.pdf,Towards Integrating Emerging AI Applications in SE Education,"Artificial Intelligence (AI) approaches have been incorporated into modern
learning environments and software engineering (SE) courses and curricula for
several years. However, with the significant rise in popularity of large
language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in
particular in the last year, educators are faced with rapidly changing
classroom environments and disrupted teaching principles. Examples range from
programming assignment solutions that are fully generated via ChatGPT, to
various forms of cheating during exams. However, despite these negative aspects
and emerging challenges, AI tools in general, and LLM applications in
particular, can also provide significant opportunities in a wide variety of SE
courses, supporting both students and educators in meaningful ways. In this
early research paper, we present preliminary results of a systematic analysis
of current trends in the area of AI, and how they can be integrated into
university-level SE curricula, guidelines, and approaches to support both
instructors and learners. We collected both teaching and research papers and
analyzed their potential usage in SE education, using the ACM Computer Science
Curriculum Guidelines CS2023. As an initial outcome, we discuss a series of
opportunities for AI applications and further research areas.",2024-05-28T11:21:45Z
,http://arxiv.org/pdf/2403.18679v2.pdf,"An Exploratory Study on Upper-Level Computing Students' Use of Large
  Language Models as Tools in a Semester-Long Project","Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are
influencing software engineering practice. Software engineering educators must
teach future software engineers how to use such tools well. As of yet, there
have been few studies that report on the use of LLMs in the classroom. It is,
therefore, important to evaluate students' perception of LLMs and possible ways
of adapting the computing curriculum to these shifting paradigms.
  Purpose: The purpose of this study is to explore computing students'
experiences and approaches to using LLMs during a semester-long software
engineering project.
  Design/Method: We collected data from a senior-level software engineering
course at Purdue University. This course uses a project-based learning (PBL)
design. The students used LLMs such as ChatGPT and Copilot in their projects. A
sample of these student teams were interviewed to understand (1) how they used
LLMs in their projects; and (2) whether and how their perspectives on LLMs
changed over the course of the semester. We analyzed the data to identify
themes related to students' usage patterns and learning outcomes.
  Results/Discussion: When computing students utilize LLMs within a project,
their use cases cover both technical and professional applications. In
addition, these students perceive LLMs to be efficient tools in obtaining
information and completion of tasks. However, there were concerns about the
responsible use of LLMs without being detrimental to their own learning
outcomes. Based on our findings, we recommend future research to investigate
the usage of LLM's in lower-level computer engineering courses to understand
whether and how LLMs can be integrated as a learning aid without hurting the
learning outcomes.",2024-03-27T15:21:58Z
10.1007/978-3-031-64302-6_19,http://arxiv.org/pdf/2310.05292v4.pdf,"How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent
  for Debugging","Large Language Models (LLMs) now excel at generative skills and can create
content at impeccable speeds. However, they are imperfect and still make
various mistakes. In a Computer Science education context, as these models are
widely recognized as ""AI pair programmers,"" it becomes increasingly important
to train students on evaluating and debugging the LLM-generated code. In this
work, we introduce HypoCompass, a novel system to facilitate deliberate
practice on debugging, where human novices play the role of Teaching Assistants
and help LLM-powered teachable agents debug code. We enable effective task
delegation between students and LLMs in this learning-by-teaching environment:
students focus on hypothesizing the cause of code errors, while adjacent skills
like code completion are offloaded to LLM-agents. Our evaluations demonstrate
that HypoCompass generates high-quality training materials (e.g., bugs and
fixes), outperforming human counterparts fourfold in efficiency, and
significantly improves student performance on debugging by 12% in the
pre-to-post test.",2023-10-08T21:39:47Z
,http://arxiv.org/pdf/2308.08102v1.pdf,"ChatLogo: A Large Language Model-Driven Hybrid Natural-Programming
  Language Interface for Agent-based Modeling and Programming","Building on Papert (1980)'s idea of children talking to computers, we propose
ChatLogo, a hybrid natural-programming language interface for agent-based
modeling and programming. We build upon previous efforts to scaffold ABM & P
learning and recent development in leveraging large language models (LLMs) to
support the learning of computational programming. ChatLogo aims to support
conversations with computers in a mix of natural and programming languages,
provide a more user-friendly interface for novice learners, and keep the
technical system from over-reliance on any single LLM. We introduced the main
elements of our design: an intelligent command center, and a conversational
interface to support creative expression. We discussed the presentation format
and future work. Responding to the challenges of supporting open-ended
constructionist learning of ABM & P and leveraging LLMs for educational
purposes, we contribute to the field by proposing the first constructionist
LLM-driven interface to support computational and complex systems thinking.",2023-08-16T02:21:52Z
10.1007/978-3-031-48796-5_13,http://arxiv.org/pdf/2310.19813v1.pdf,Enhancing Genetic Improvement Mutations Using Large Language Models,"Large language models (LLMs) have been successfully applied to software
engineering tasks, including program repair. However, their application in
search-based techniques such as Genetic Improvement (GI) is still largely
unexplored. In this paper, we evaluate the use of LLMs as mutation operators
for GI to improve the search process. We expand the Gin Java GI toolkit to call
OpenAI's API to generate edits for the JCodec tool. We randomly sample the
space of edits using 5 different edit types. We find that the number of patches
passing unit tests is up to 75% higher with LLM-based edits than with standard
Insert edits. Further, we observe that the patches found with LLMs are
generally less diverse compared to standard edits. We ran GI with local search
to find runtime improvements. Although many improving patches are found by
LLM-enhanced GI, the best improving patch was found by standard GI.",2023-10-18T10:24:14Z
,http://arxiv.org/pdf/2405.05347v1.pdf,Benchmarking Educational Program Repair,"The emergence of large language models (LLMs) has sparked enormous interest
due to their potential application across a range of educational tasks. For
example, recent work in programming education has used LLMs to generate
learning resources, improve error messages, and provide feedback on code.
However, one factor that limits progress within the field is that much of the
research uses bespoke datasets and different evaluation metrics, making direct
comparisons between results unreliable. Thus, there is a pressing need for
standardization and benchmarks that facilitate the equitable comparison of
competing approaches. One task where LLMs show great promise is program repair,
which can be used to provide debugging support and next-step hints to students.
In this article, we propose a novel educational program repair benchmark. We
curate two high-quality publicly available programming datasets, present a
unified evaluation procedure introducing a novel evaluation metric rouge@k for
approximating the quality of repairs, and evaluate a set of five recent models
to establish baseline performance.",2024-05-08T18:23:59Z
,http://arxiv.org/pdf/2303.07263v1.pdf,InferFix: End-to-End Program Repair with LLMs,"Software development life cycle is profoundly influenced by bugs: their
introduction, identification, and eventual resolution account for a significant
portion of software cost. This has motivated software engineering researchers
and practitioners to propose different approaches for automating the
identification and repair of software defects. Large language models have been
adapted to the program repair task through few-shot demonstration learning and
instruction prompting, treating this as an infilling task. However, these
models have only focused on learning general bug-fixing patterns for
uncategorized bugs mined from public repositories. In this paper, we propose
InferFix: a transformer-based program repair framework paired with a
state-of-the-art static analyzer to fix critical security and performance bugs.
InferFix combines a Retriever -- transformer encoder model pretrained via
contrastive learning objective, which aims at searching for semantically
equivalent bugs and corresponding fixes; and a Generator -- a large language
model (Codex Cushman) finetuned on supervised bug-fix data with prompts
augmented via bug type annotations and semantically similar fixes retrieved
from an external non-parametric memory. To train and evaluate our approach, we
curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by
executing the Infer static analyzer on the change histories of thousands of
Java and C# repositories. Our evaluation demonstrates that InferFix outperforms
strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C#
and 76.8% in Java. We discuss the deployment of InferFix alongside Infer at
Microsoft which offers an end-to-end solution for detection, classification,
and localization of bugs, as well as fixing and validation of candidate
patches, integrated in the continuous integration pipeline to automate the
software development workflow.",2023-03-13T16:42:47Z
,http://arxiv.org/pdf/2307.02792v2.pdf,What Should Data Science Education Do with Large Language Models?,"The rapid advances of large language models (LLMs), such as ChatGPT, are
revolutionizing data science and statistics. These state-of-the-art tools can
streamline complex processes. As a result, it reshapes the role of data
scientists. We argue that LLMs are transforming the responsibilities of data
scientists, shifting their focus from hands-on coding, data-wrangling and
conducting standard analyses to assessing and managing analyses performed by
these automated AIs. This evolution of roles is reminiscent of the transition
from a software engineer to a product manager. We illustrate this transition
with concrete data science case studies using LLMs in this paper. These
developments necessitate a meaningful evolution in data science education.
Pedagogy must now place greater emphasis on cultivating diverse skillsets among
students, such as LLM-informed creativity, critical thinking, AI-guided
programming. LLMs can also play a significant role in the classroom as
interactive teaching and learning tools, contributing to personalized
education. This paper discusses the opportunities, resources and open
challenges for each of these directions. As with any transformative technology,
integrating LLMs into education calls for careful consideration. While LLMs can
perform repetitive tasks efficiently, it's crucial to remember that their role
is to supplement human intelligence and creativity, not to replace it.
Therefore, the new era of data science education should balance the benefits of
LLMs while fostering complementary human expertise and innovations. In
conclusion, the rise of LLMs heralds a transformative period for data science
and its education. This paper seeks to shed light on the emerging trends,
potential opportunities, and challenges accompanying this paradigm shift,
hoping to spark further discourse and investigation into this exciting,
uncharted territory.",2023-07-06T06:07:29Z
,http://arxiv.org/pdf/2310.05727v1.pdf,The Program Testing Ability of Large Language Models for Code,"Recent development of large language models (LLMs) for code like CodeX and
CodeT5+ demonstrates tremendous promise in achieving code intelligence. Their
ability of synthesizing code that completes a program for performing a
pre-defined task has been intensively tested and verified on benchmark datasets
including HumanEval and MBPP. Yet, evaluation of these LLMs from more
perspectives (than just program synthesis) is also anticipated, considering
their broad scope of applications in software engineering. In this paper, we
explore the ability of LLMs for testing programs/code. By performing thorough
analyses of recent LLMs for code in program testing, we show a series of
intriguing properties of these models and demonstrate how program testing
ability of LLMs can be improved. Following recent work which utilizes generated
test cases to enhance program synthesis, we further leverage our findings in
improving the quality of the synthesized programs and show +11.77% and +4.22%
higher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline
and the recent state-of-the-art, respectively.",2023-10-09T13:55:45Z
,http://arxiv.org/pdf/2404.03543v2.pdf,"CodeEditorBench: Evaluating Code Editing Capability of Large Language
  Models","Large Language Models (LLMs) for code are rapidly evolving, with code editing
emerging as a critical capability. We introduce CodeEditorBench, an evaluation
framework designed to rigorously assess the performance of LLMs in code editing
tasks, including debugging, translating, polishing, and requirement switching.
Unlike existing benchmarks focusing solely on code generation, CodeEditorBench
emphasizes real-world scenarios and practical aspects of software development.
We curate diverse coding challenges and scenarios from five sources, covering
various programming languages, complexity levels, and editing tasks. Evaluation
of 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and
GPT-4), outperform open-source models in CodeEditorBench, highlighting
differences in model performance based on problem types and prompt
sensitivities. CodeEditorBench aims to catalyze advancements in LLMs by
providing a robust platform for assessing code editing capabilities. We will
release all prompts and datasets to enable the community to expand the dataset
and benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to
the advancement of LLMs in code editing and provide a valuable resource for
researchers and practitioners.",2024-04-04T15:49:49Z
,http://arxiv.org/pdf/2401.05399v1.pdf,Automated Assessment of Students' Code Comprehension using LLMs,"Assessing student's answers and in particular natural language answers is a
crucial challenge in the field of education. Advances in machine learning,
including transformer-based models such as Large Language Models(LLMs), have
led to significant progress in various natural language tasks. Nevertheless,
amidst the growing trend of evaluating LLMs across diverse tasks, evaluating
LLMs in the realm of automated answer assesment has not received much
attention. To address this gap, we explore the potential of using LLMs for
automated assessment of student's short and open-ended answer. Particularly, we
use LLMs to compare students' explanations with expert explanations in the
context of line-by-line explanations of computer programs.
  For comparison purposes, we assess both Large Language Models (LLMs) and
encoder-based Semantic Textual Similarity (STS) models in the context of
assessing the correctness of students' explanation of computer code. Our
findings indicate that LLMs, when prompted in few-shot and chain-of-thought
setting perform comparable to fine-tuned encoder-based models in evaluating
students' short answers in programming domain.",2023-12-19T20:39:12Z
,http://arxiv.org/pdf/2404.02540v2.pdf,CSEPrompts: A Benchmark of Introductory Computer Science Prompts,"Recent advances in AI, machine learning, and NLP have led to the development
of a new generation of Large Language Models (LLMs) that are trained on massive
amounts of data and often have trillions of parameters. Commercial applications
(e.g., ChatGPT) have made this technology available to the general public, thus
making it possible to use LLMs to produce high-quality texts for academic and
professional purposes. Schools and universities are aware of the increasing use
of AI-generated content by students and they have been researching the impact
of this new technology and its potential misuse. Educational programs in
Computer Science (CS) and related fields are particularly affected because LLMs
are also capable of generating programming code in various programming
languages. To help understand the potential impact of publicly available LLMs
in CS education, we introduce CSEPrompts, a framework with hundreds of
programming exercise prompts and multiple-choice questions retrieved from
introductory CS and programming courses. We also provide experimental results
on CSEPrompts to evaluate the performance of several LLMs with respect to
generating Python code and answering basic computer science and programming
questions.",2024-04-03T07:55:57Z
,http://arxiv.org/pdf/2402.14261v1.pdf,Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming,"The integration of Large Language Models (LLMs) into Development Environments
(IDEs) has become a focal point in modern software development. LLMs such as
OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment
developer productivity by serving as intelligent, chat-driven programming
assistants. However, utilizing LLMs out of the box is unlikely to be optimal
for any given scenario. Rather, each system requires the LLM to be honed to its
set of heuristics to ensure the best performance. In this paper, we introduce
the Copilot evaluation harness: a set of data and tools for evaluating
LLM-guided IDE interactions, covering various programming scenarios and
languages. We propose our metrics as a more robust and information-dense
evaluation than previous state of the art evaluation systems. We design and
compute both static and execution based success metrics for scenarios
encompassing a wide range of developer tasks, including code generation from
natural language (generate), documentation generation from code (doc), test
case generation (test), bug-fixing (fix), and workspace understanding and query
resolution (workspace). These success metrics are designed to evaluate the
performance of LLMs within a given IDE and its respective parameter space. Our
learnings from evaluating three common LLMs using these metrics can inform the
development and validation of future scenarios in LLM guided IDEs.",2024-02-22T03:51:34Z
,http://arxiv.org/pdf/2406.10300v1.pdf,"Large Language Models as Software Components: A Taxonomy for
  LLM-Integrated Applications","Large Language Models (LLMs) have become widely adopted recently. Research
explores their use both as autonomous agents and as tools for software
engineering. LLM-integrated applications, on the other hand, are software
systems that leverage an LLM to perform tasks that would otherwise be
impossible or require significant coding effort. While LLM-integrated
application engineering is emerging as new discipline, its terminology,
concepts and methods need to be established. This study provides a taxonomy for
LLM-integrated applications, offering a framework for analyzing and describing
these systems. It also demonstrates various ways to utilize LLMs in
applications, as well as options for implementing such integrations.
  Following established methods, we analyze a sample of recent LLM-integrated
applications to identify relevant dimensions. We evaluate the taxonomy by
applying it to additional cases. This review shows that applications integrate
LLMs in numerous ways for various purposes. Frequently, they comprise multiple
LLM integrations, which we term ``LLM components''. To gain a clear
understanding of an application's architecture, we examine each LLM component
separately. We identify thirteen dimensions along which to characterize an LLM
component, including the LLM skills leveraged, the format of the output, and
more. LLM-integrated applications are described as combinations of their LLM
components. We suggest a concise representation using feature vectors for
visualization.
  The taxonomy is effective for describing LLM-integrated applications. It can
contribute to theory building in the nascent field of LLM-integrated
application engineering and aid in developing such systems. Researchers and
practitioners explore numerous creative ways to leverage LLMs in applications.
Though challenges persist, integrating LLMs may revolutionize the way software
systems are built.",2024-06-13T21:32:56Z
,http://arxiv.org/pdf/2401.12453v1.pdf,"""The teachers are confused as well"": A Multiple-Stakeholder Ethics
  Discussion on Large Language Models in Computing Education","Large Language Models (LLMs) are advancing quickly and impacting people's
lives for better or worse. In higher education, concerns have emerged such as
students' misuse of LLMs and degraded education outcomes. To unpack the ethical
concerns of LLMs for higher education, we conducted a case study consisting of
stakeholder interviews (n=20) in higher education computer science. We found
that students use several distinct mental models to interact with LLMs - LLMs
serve as a tool for (a) writing, (b) coding, and (c) information retrieval,
which differ somewhat in ethical considerations. Students and teachers brought
up ethical issues that directly impact them, such as inaccurate LLM responses,
hallucinations, biases, privacy leakage, and academic integrity issues.
Participants emphasized the necessity of guidance and rules for the use of LLMs
in higher education, including teaching digital literacy, rethinking education,
and having cautious and contextual policies. We reflect on the ethical
challenges and propose solutions.",2024-01-23T02:43:00Z
,http://arxiv.org/pdf/2402.07081v1.pdf,"Using Large Language Models for Student-Code Guided Test Case Generation
  in Computer Science Education","In computer science education, test cases are an integral part of programming
assignments since they can be used as assessment items to test students'
programming knowledge and provide personalized feedback on student-written
code. The goal of our work is to propose a fully automated approach for test
case generation that can accurately measure student knowledge, which is
important for two reasons. First, manually constructing test cases requires
expert knowledge and is a labor-intensive process. Second, developing test
cases for students, especially those who are novice programmers, is
significantly different from those oriented toward professional-level software
developers. Therefore, we need an automated process for test case generation to
assess student knowledge and provide feedback. In this work, we propose a large
language model-based approach to automatically generate test cases and show
that they are good measures of student knowledge, using a publicly available
dataset that contains student-written Java code. We also discuss future
research directions centered on using test cases to help students.",2024-02-11T01:37:48Z
,http://arxiv.org/pdf/2406.15379v1.pdf,CS1-LLM: Integrating LLMs into CS1 Instruction,"The recent, widespread availability of Large Language Models (LLMs) like
ChatGPT and GitHub Copilot may impact introductory programming courses (CS1)
both in terms of what should be taught and how to teach it. Indeed, recent
research has shown that LLMs are capable of solving the majority of the
assignments and exams we previously used in CS1. In addition, professional
software engineers are often using these tools, raising the question of whether
we should be training our students in their use as well. This experience report
describes a CS1 course at a large research-intensive university that fully
embraces the use of LLMs from the beginning of the course. To incorporate the
LLMs, the course was intentionally altered to reduce emphasis on syntax and
writing code from scratch. Instead, the course now emphasizes skills needed to
successfully produce software with an LLM. This includes explaining code,
testing code, and decomposing large problems into small functions that are
solvable by an LLM. In addition to frequent, formative assessments of these
skills, students were given three large, open-ended projects in three separate
domains (data science, image processing, and game design) that allowed them to
showcase their creativity in topics of their choosing. In an end-of-term
survey, students reported that they appreciated learning with the assistance of
the LLM and that they interacted with the LLM in a variety of ways when writing
code. We provide lessons learned for instructors who may wish to incorporate
LLMs into their course.",2024-04-17T14:44:28Z
10.1145/3587102.3588785,http://arxiv.org/pdf/2304.03938v1.pdf,"Comparing Code Explanations Created by Students and Large Language
  Models","Reasoning about code and explaining its purpose are fundamental skills for
computer scientists. There has been extensive research in the field of
computing education on the relationship between a student's ability to explain
code and other skills such as writing and tracing code. In particular, the
ability to describe at a high-level of abstraction how code will behave over
all possible inputs correlates strongly with code writing skills. However,
developing the expertise to comprehend and explain code accurately and
succinctly is a challenge for many students. Existing pedagogical approaches
that scaffold the ability to explain code, such as producing exemplar code
explanations on demand, do not currently scale well to large classrooms. The
recent emergence of powerful large language models (LLMs) may offer a solution.
In this paper, we explore the potential of LLMs in generating explanations that
can serve as examples to scaffold students' ability to understand and explain
code. To evaluate LLM-created explanations, we compare them with explanations
created by students in a large course ($n \approx 1000$) with respect to
accuracy, understandability and length. We find that LLM-created explanations,
which can be produced automatically on demand, are rated as being significantly
easier to understand and more accurate summaries of code than student-created
explanations. We discuss the significance of this finding, and suggest how such
models can be incorporated into introductory programming education.",2023-04-08T06:52:54Z
,http://arxiv.org/pdf/2308.08572v1.pdf,"Large Language Models in Introductory Programming Education: ChatGPT's
  Performance and Implications for Assessments","This paper investigates the performance of the Large Language Models (LLMs)
ChatGPT-3.5 and GPT-4 in solving introductory programming tasks. Based on the
performance, implications for didactic scenarios and assessment formats
utilizing LLMs are derived. For the analysis, 72 Python tasks for novice
programmers were selected from the free site CodingBat. Full task descriptions
were used as input to the LLMs, while the generated replies were evaluated
using CodingBat's unit tests. In addition, the general availability of textual
explanations and program code was analyzed. The results show high scores of
94.4 to 95.8% correct responses and reliable availability of textual
explanations and program code, which opens new ways to incorporate LLMs into
programming education and assessment.",2023-08-15T19:48:31Z
,http://arxiv.org/pdf/2405.02828v1.pdf,"Trojans in Large Language Models of Code: A Critical Review through a
  Trigger-Based Taxonomy","Large language models (LLMs) have provided a lot of exciting new capabilities
in software development. However, the opaque nature of these models makes them
difficult to reason about and inspect. Their opacity gives rise to potential
security risks, as adversaries can train and deploy compromised models to
disrupt the software development process in the victims' organization.
  This work presents an overview of the current state-of-the-art trojan attacks
on large language models of code, with a focus on triggers -- the main design
point of trojans -- with the aid of a novel unifying trigger taxonomy
framework. We also aim to provide a uniform definition of the fundamental
concepts in the area of trojans in Code LLMs. Finally, we draw implications of
findings on how code models learn on trigger design.",2024-05-05T06:43:52Z
,http://arxiv.org/pdf/2406.12513v1.pdf,"Can We Trust Large Language Models Generated Code? A Framework for
  In-Context Learning, Security Patterns, and Code Evaluations Across Diverse
  LLMs","Large Language Models (LLMs) such as ChatGPT and GitHub Copilot have
revolutionized automated code generation in software engineering. However, as
these models are increasingly utilized for software development, concerns have
arisen regarding the security and quality of the generated code. These concerns
stem from LLMs being primarily trained on publicly available code repositories
and internet-based textual data, which may contain insecure code. This presents
a significant risk of perpetuating vulnerabilities in the generated code,
creating potential attack vectors for exploitation by malicious actors. Our
research aims to tackle these issues by introducing a framework for secure
behavioral learning of LLMs through In-Content Learning (ICL) patterns during
the code generation process, followed by rigorous security evaluations. To
achieve this, we have selected four diverse LLMs for experimentation. We have
evaluated these coding LLMs across three programming languages and identified
security vulnerabilities and code smells. The code is generated through ICL
with curated problem sets and undergoes rigorous security testing to evaluate
the overall quality and trustworthiness of the generated code. Our research
indicates that ICL-driven one-shot and few-shot learning patterns can enhance
code security, reducing vulnerabilities in various programming scenarios.
Developers and researchers should know that LLMs have a limited understanding
of security principles. This may lead to security breaches when the generated
code is deployed in production systems. Our research highlights LLMs are a
potential source of new vulnerabilities to the software supply chain. It is
important to consider this when using LLMs for code generation. This research
article offers insights into improving LLM security and encourages proactive
use of LLMs for code generation to ensure software system safety.",2024-06-18T11:29:34Z
,http://arxiv.org/pdf/2401.05319v1.pdf,"Leveraging Print Debugging to Improve Code Generation in Large Language
  Models","Large language models (LLMs) have made significant progress in code
generation tasks, but their performance in tackling programming problems with
complex data structures and algorithms remains suboptimal. To address this
issue, we propose an in-context learning approach that guides LLMs to debug by
using a ""print debugging"" method, which involves inserting print statements to
trace and analysing logs for fixing the bug. We collect a Leetcode problem
dataset and evaluate our method using the Leetcode online judging system.
Experiments with GPT-4 demonstrate the effectiveness of our approach,
outperforming rubber duck debugging in easy and medium-level Leetcode problems
by 1.5% and 17.9%.",2024-01-10T18:37:59Z
,http://arxiv.org/pdf/2308.13507v2.pdf,"Large Language Models Should Ask Clarifying Questions to Increase
  Confidence in Generated Code","Large language models (LLMs) have significantly improved the ability to
perform tasks in the field of code generation. However, there is still a gap
between LLMs being capable coders and being top-tier software engineers. Based
on the observation that toplevel software engineers often ask clarifying
questions to reduce ambiguity in both requirements and coding solutions, I
argue that the same should be applied to LLMs for code generation tasks. By
asking probing questions in various topics before generating the final code,
the challenges of programming with LLMs, such as unclear intent specification,
lack of computational thinking, and undesired code quality, may be alleviated.
This, in turn, increases confidence in the generated code. In this work, I
explore how to leverage better communication skills to achieve greater
confidence in generated code. I propose a communication-centered process that
uses an LLM-generated communicator to identify issues with high ambiguity or
low confidence in problem descriptions and generated code. I then ask
clarifying questions to obtain responses from users for refining the code.",2023-08-25T17:33:05Z
,http://arxiv.org/pdf/2212.11140v1.pdf,"Benchmarking Large Language Models for Automated Verilog RTL Code
  Generation","Automating hardware design could obviate a significant amount of human error
from the engineering process and lead to fewer errors. Verilog is a popular
hardware description language to model and design digital systems, thus
generating Verilog code is a critical first step. Emerging large language
models (LLMs) are able to write high-quality code in other programming
languages. In this paper, we characterize the ability of LLMs to generate
useful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets
collected from GitHub and Verilog textbooks. We construct an evaluation
framework comprising test-benches for functional analysis and a flow to test
the syntax of Verilog code generated in response to problems of varying
difficulty. Our findings show that across our problem scenarios, the
fine-tuning results in LLMs more capable of producing syntactically correct
code (25.9% overall). Further, when analyzing functional correctness, a
fine-tuned open-source CodeGen LLM can outperform the state-of-the-art
commercial Codex LLM (6.5% overall). Training/evaluation scripts and LLM
checkpoints are available: https://github.com/shailja-thakur/VGen.",2022-12-13T16:34:39Z
10.1145/3568813.3600139,http://arxiv.org/pdf/2306.05715v1.pdf,"Exploring the Responses of Large Language Models to Beginner
  Programmers' Help Requests","Background and Context: Over the past year, large language models (LLMs) have
taken the world by storm. In computing education, like in other walks of life,
many opportunities and threats have emerged as a consequence.
  Objectives: In this article, we explore such opportunities and threats in a
specific area: responding to student programmers' help requests. More
specifically, we assess how good LLMs are at identifying issues in problematic
code that students request help on.
  Method: We collected a sample of help requests and code from an online
programming course. We then prompted two different LLMs (OpenAI Codex and
GPT-3.5) to identify and explain the issues in the students' code and assessed
the LLM-generated answers both quantitatively and qualitatively.
  Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently
find at least one actual issue in each student program (GPT-3.5 in 90% of the
cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57%
of the time). False positives are common (40% chance for GPT-3.5). The advice
that the LLMs provide on the issues is often sensible. The LLMs perform better
on issues involving program logic rather than on output formatting. Model
solutions are frequently provided even when the LLM is prompted not to. LLM
responses to prompts in a non-English language are only slightly worse than
responses to English prompts.
  Implications: Our results continue to highlight the utility of LLMs in
programming education. At the same time, the results highlight the
unreliability of LLMs: LLMs make some of the same mistakes that students do,
perhaps especially when formatting output as required by automated assessment
systems. Our study informs teachers interested in using LLMs as well as future
efforts to customize LLMs for the needs of programming education.",2023-06-09T07:19:43Z
,http://arxiv.org/pdf/2406.04817v1.pdf,"Experiences from Integrating Large Language Model Chatbots into the
  Classroom","In the present study, we provided students an unfiltered access to a
state-of-the-art large language model (LLM) chatbot. The chatbot was
intentionally designed to mimic proprietary commercial chatbots such as ChatGPT
where the chatbot has not been tailored for the educational context; the
underlying engine was OpenAI GPT-4. The chatbot was integrated into online
learning materials of three courses. One of the courses focused on software
engineering with LLMs, while the two other courses were not directly related to
LLMs. Our results suggest that only a minority of students engage with the
chatbot in the courses that do not relate to LLMs. At the same time,
unsurprisingly, nearly all students in the LLM-focused course leveraged the
chatbot. In all courses, the majority of the LLM usage came from a few
superusers, whereas the majority of the students did not heavily use the
chatbot even though it was readily available and effectively provided a free
access to the OpenAI GPT-4 model. We also observe that in addition to students
using the chatbot for course-specific purposes, many use the chatbot for their
own purposes. These results suggest that the worst fears of educators -- all
students overrelying on LLMs -- did not materialize even when the chatbot
access was unfiltered. We finally discuss potential reasons for the low usage,
suggesting the need for more tailored and scaffolded LLM experiences targeted
for specific types of student use cases.",2024-06-07T10:37:14Z
,http://arxiv.org/pdf/2403.16159v2.pdf,"Designing Child-Centric AI Learning Environments: Insights from
  LLM-Enhanced Creative Project-Based Learning","Project-based learning (PBL) is an instructional method that is very helpful
in nurturing students' creativity, but it requires significant time and energy
from both students and teachers. Large language models (LLMs) have been proven
to assist in creative tasks, yet much controversy exists regarding their role
in fostering creativity. This paper explores the potential of LLMs in PBL
settings, with a special focus on fostering creativity. We began with an
exploratory study involving 12 middle school students and identified five
design considerations for LLM applications in PBL. Building on this, we
developed an LLM-empowered, 48-hour PBL program and conducted an instructional
experiment with 31 middle school students. Our results indicated that LLMs can
enhance every stage of PBL. Additionally, we also discovered ambivalent
perspectives among students and mentors toward LLM usage. Furthermore, we
explored the challenge and design implications of integrating LLMs into PBL and
reflected on the program. By bridging AI advancements into educational
practice, our work aims to inspire further discourse and investigation into
harnessing AI's potential in child-centric educational settings.",2024-03-24T13:54:05Z
,http://arxiv.org/pdf/2309.14534v3.pdf,"Teach AI How to Code: Using Large Language Models as Teachable Agents
  for Programming Education","This work investigates large language models (LLMs) as teachable agents for
learning by teaching (LBT). LBT with teachable agents helps learners identify
knowledge gaps and discover new knowledge. However, teachable agents require
expensive programming of subject-specific knowledge. While LLMs as teachable
agents can reduce the cost, LLMs' expansive knowledge as tutees discourages
learners from teaching. We propose a prompting pipeline that restrains LLMs'
knowledge and makes them initiate ""why"" and ""how"" questions for effective
knowledge-building. We combined these techniques into TeachYou, an LBT
environment for algorithm learning, and AlgoBo, an LLM-based tutee chatbot that
can simulate misconceptions and unawareness prescribed in its knowledge state.
Our technical evaluation confirmed that our prompting pipeline can effectively
configure AlgoBo's problem-solving performance. Through a between-subject study
with 40 algorithm novices, we also observed that AlgoBo's questions led to
knowledge-dense conversations (effect size=0.71). Lastly, we discuss design
implications, cost-efficiency, and personalization of LLM-based teachable
agents.",2023-09-25T21:20:04Z
,http://arxiv.org/pdf/2401.07994v1.pdf,"A Novel Approach for Automatic Program Repair using Round-Trip
  Translation with Large Language Models","Research shows that grammatical mistakes in a sentence can be corrected by
translating it to another language and back using neural machine translation
with language models. We investigate whether this correction capability of
Large Language Models (LLMs) extends to Automatic Program Repair (APR). Current
generative models for APR are pre-trained on source code and fine-tuned for
repair. This paper proposes bypassing the fine-tuning step and using Round-Trip
Translation (RTT): translation of code from one programming language to another
programming or natural language, and back. We hypothesize that RTT with LLMs
restores the most commonly seen patterns in code during pre-training, i.e.,
performs a regression toward the mean, which removes bugs as they are a form of
noise w.r.t. the more frequent, natural, bug-free code in the training data. To
test this hypothesis, we employ eight recent LLMs pre-trained on code,
including the latest GPT versions, and four common program repair benchmarks in
Java. We find that RTT with English as an intermediate language repaired 101 of
164 bugs with GPT-4 on the HumanEval-Java dataset. Moreover, 46 of these are
unique bugs that are not repaired by other LLMs fine-tuned for APR. Our
findings highlight the viability of round-trip translation with LLMs as a
technique for automated program repair and its potential for research in
software engineering.
  Keywords: automated program repair, large language model, machine translation",2024-01-15T22:36:31Z
,http://arxiv.org/pdf/2309.00029v1.pdf,"Exploring the Potential of Large Language Models to Generate Formative
  Programming Feedback","Ever since the emergence of large language models (LLMs) and related
applications, such as ChatGPT, its performance and error analysis for
programming tasks have been subject to research. In this work-in-progress
paper, we explore the potential of such LLMs for computing educators and
learners, as we analyze the feedback it generates to a given input containing
program code. In particular, we aim at (1) exploring how an LLM like ChatGPT
responds to students seeking help with their introductory programming tasks,
and (2) identifying feedback types in its responses. To achieve these goals, we
used students' programming sequences from a dataset gathered within a CS1
course as input for ChatGPT along with questions required to elicit feedback
and correct solutions. The results show that ChatGPT performs reasonably well
for some of the introductory programming tasks and student errors, which means
that students can potentially benefit. However, educators should provide
guidance on how to use the provided feedback, as it can contain misleading
information for novices.",2023-08-31T15:22:11Z
10.1145/3639474.3640061,http://arxiv.org/pdf/2404.02548v2.pdf,AI-Tutoring in Software Engineering Education,"With the rapid advancement of artificial intelligence (AI) in various
domains, the education sector is set for transformation. The potential of
AI-driven tools in enhancing the learning experience, especially in
programming, is immense. However, the scientific evaluation of Large Language
Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an
AI-Tutor remains largely unexplored. Therefore, there is a need to understand
how students interact with such AI-Tutors and to analyze their experiences. In
this paper, we conducted an exploratory case study by integrating the
GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a
combination of empirical data collection and an exploratory survey, we
identified different user types based on their interaction patterns with the
AI-Tutor. Additionally, the findings highlight advantages, such as timely
feedback and scalability. However, challenges like generic responses and
students' concerns about a learning progress inhibition when using the AI-Tutor
were also evident. This research adds to the discourse on AI's role in
education.",2024-04-03T08:15:08Z
,http://arxiv.org/pdf/2310.17807v3.pdf,Clover: Closed-Loop Verifiable Code Generation,"The use of large language models for code generation is a rapidly growing
trend in software development. However, without effective methods for ensuring
the correctness of generated code, this trend could lead to any number of
undesirable outcomes. In this paper, we lay out a vision for addressing this
challenge: the Clover paradigm, short for Closed-Loop Verifiable Code
Generation, which reduces correctness checking to the more accessible problem
of consistency checking. At the core of Clover lies a checker that performs
consistency checks among code, docstrings, and formal annotations. The checker
is implemented using a novel integration of formal verification tools and large
language models. We provide a theoretical analysis to support our thesis that
Clover should be effective at consistency checking. We also empirically
investigate its feasibility on a hand-designed dataset (CloverBench) featuring
annotated Dafny programs at a textbook level of difficulty. Experimental
results show that for this dataset, (i) LLMs are reasonably successful at
automatically generating formal specifications; and (ii) our consistency
checker achieves a promising acceptance rate (up to 87%) for correct instances
while maintaining zero tolerance for incorrect ones (no false positives).",2023-10-26T22:58:19Z
,http://arxiv.org/pdf/2402.01687v2.pdf,"""Which LLM should I use?"": Evaluating LLMs for tasks performed by
  Undergraduate Computer Science Students","This study evaluates the effectiveness of various large language models
(LLMs) in performing tasks common among undergraduate computer science
students. Although a number of research studies in the computing education
community have explored the possibility of using LLMs for a variety of tasks,
there is a lack of comprehensive research comparing different LLMs and
evaluating which LLMs are most effective for different tasks. Our research
systematically assesses some of the publicly available LLMs such as Google
Bard, ChatGPT(3.5), GitHub Copilot Chat, and Microsoft Copilot across diverse
tasks commonly encountered by undergraduate computer science students in India.
These tasks include code explanation and documentation, solving class
assignments, technical interview preparation, learning new concepts and
frameworks, and email writing. Evaluation for these tasks was carried out by
pre-final year and final year undergraduate computer science students and
provides insights into the models' strengths and limitations. This study aims
to guide students as well as instructors in selecting suitable LLMs for any
specific task and offers valuable insights on how LLMs can be used
constructively by students and instructors.",2024-01-22T15:11:36Z
,http://arxiv.org/pdf/2402.09299v1.pdf,"Trained Without My Consent: Detecting Code Inclusion In Language Models
  Trained on Code","Code auditing ensures that the developed code adheres to standards,
regulations, and copyright protection by verifying that it does not contain
code from protected sources. The recent advent of Large Language Models (LLMs)
as coding assistants in the software development process poses new challenges
for code auditing. The dataset for training these models is mainly collected
from publicly available sources. This raises the issue of intellectual property
infringement as developers' codes are already included in the dataset.
Therefore, auditing code developed using LLMs is challenging, as it is
difficult to reliably assert if an LLM used during development has been trained
on specific copyrighted codes, given that we do not have access to the training
datasets of these models. Given the non-disclosure of the training datasets,
traditional approaches such as code clone detection are insufficient for
asserting copyright infringement. To address this challenge, we propose a new
approach, TraWiC; a model-agnostic and interpretable method based on membership
inference for detecting code inclusion in an LLM's training dataset. We extract
syntactic and semantic identifiers unique to each program to train a classifier
for detecting code inclusion. In our experiments, we observe that TraWiC is
capable of detecting 83.87% of codes that were used to train an LLM. In
comparison, the prevalent clone detection tool NiCad is only capable of
detecting 47.64%. In addition to its remarkable performance, TraWiC has low
resource overhead in contrast to pair-wise clone detection that is conducted
during the auditing process of tools like CodeWhisperer reference tracker,
across thousands of code snippets.",2024-02-14T16:41:35Z
,http://arxiv.org/pdf/2402.08147v2.pdf,"VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large
  Language Model, and Tree Search","Large Language Models (LLMs) can generate useful code, but often the code
they generate cannot be trusted to be sound. In this paper, we present VerMCTS,
an approach to begin to resolve this issue by generating verified programs in
Dafny and Coq. VerMCTS uses a logical verifier in concert with an LLM to guide
a modified Monte Carlo Tree Search (MCTS). This approach leverages the verifier
to gain intermediate feedback inside the search algorithm by checking partial
programs at each step to estimate an upper bound on the value function. To
measure the performance of VerMCTS, we develop a new suite of multi-step
verified programming problems in Dafny and Coq. In terms of pass@T, a new
metric which computes the pass rate given a budget of T tokens sampled from the
LLM, VerMCTS leads to more than a 30% absolute increase in average pass@5000
across the suite over repeated sampling from the base language model. Our code
and benchmarks are available at
https://github.com/namin/llm-verified-with-monte-carlo-tree-search .",2024-02-13T00:55:14Z
,http://arxiv.org/pdf/2401.08664v3.pdf,"Adapting Large Language Models for Education: Foundational Capabilities,
  Potentials, and Challenges","Online education platforms, leveraging the internet to distribute education
resources, seek to provide convenient education but often fall short in
real-time communication with students. They often struggle to address the
diverse obstacles students encounter throughout their learning journey. Solving
the problems encountered by students poses a significant challenge for
traditional deep learning models, as it requires not only a broad spectrum of
subject knowledge but also the ability to understand what constitutes a
student's individual difficulties. It's challenging for traditional machine
learning models, as they lack the capacity to comprehend students' personalized
needs. Recently, the emergence of large language models (LLMs) offers the
possibility for resolving this issue by comprehending individual requests.
Although LLMs have been successful in various fields, creating an LLM-based
education system is still challenging for the wide range of educational skills
required. This paper reviews the recently emerged LLM research related to
educational capabilities, including mathematics, writing, programming,
reasoning, and knowledge-based question answering, with the aim to explore
their potential in constructing the next-generation intelligent education
system. Specifically, for each capability, we focus on investigating two
aspects. Firstly, we examine the current state of LLMs regarding this
capability: how advanced they have become, whether they surpass human
abilities, and what deficiencies might exist. Secondly, we evaluate whether the
development methods for LLMs in this area are generalizable, that is, whether
these methods can be applied to construct a comprehensive educational
supermodel with strengths across various capabilities, rather than being
effective in only a singular aspect.",2023-12-27T14:37:32Z
,http://arxiv.org/pdf/2305.05176v1.pdf,"FrugalGPT: How to Use Large Language Models While Reducing Cost and
  Improving Performance","There is a rapidly growing number of large language models (LLMs) that users
can query for a fee. We review the cost associated with querying popular LLM
APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have
heterogeneous pricing structures, with fees that can differ by two orders of
magnitude. In particular, using LLMs on large collections of queries and text
can be expensive. Motivated by this, we outline and discuss three types of
strategies that users can exploit to reduce the inference cost associated with
using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As
an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM
cascade which learns which combinations of LLMs to use for different queries in
order to reduce cost and improve accuracy. Our experiments show that FrugalGPT
can match the performance of the best individual LLM (e.g. GPT-4) with up to
98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.
The ideas and findings presented here lay a foundation for using LLMs
sustainably and efficiently.",2023-05-09T05:11:02Z
,http://arxiv.org/pdf/2405.16133v2.pdf,"Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via
  Code Rewriting","Large Language Models (LLMs) have exhibited remarkable proficiency in
generating code. However, the misuse of LLM-generated (Synthetic) code has
prompted concerns within both educational and industrial domains, highlighting
the imperative need for the development of synthetic code detectors. Existing
methods for detecting LLM-generated content are primarily tailored for general
text and often struggle with code content due to the distinct grammatical
structure of programming languages and massive ""low-entropy"" tokens. Building
upon this, our work proposes a novel zero-shot synthetic code detector based on
the similarity between the code and its rewritten variants. Our method relies
on the intuition that the differences between the LLM-rewritten and original
codes tend to be smaller when the original code is synthetic. We utilize
self-supervised contrastive learning to train a code similarity model and
assess our approach on two synthetic code detection benchmarks. Our results
demonstrate a notable enhancement over existing synthetic content detectors
designed for general texts, with an improvement of 20.5% in the APPS benchmark
and 29.1% in the MBPP benchmark.",2024-05-25T08:57:28Z
,http://arxiv.org/pdf/2307.10236v3.pdf,"Look Before You Leap: An Exploratory Study of Uncertainty Measurement
  for Large Language Models","The recent performance leap of Large Language Models (LLMs) opens up new
opportunities across numerous industrial applications and domains. However,
erroneous generations, such as false predictions, misinformation, and
hallucination made by LLMs, have also raised severe concerns for the
trustworthiness of LLMs', especially in safety-, security- and
reliability-sensitive scenarios, potentially hindering real-world adoptions.
While uncertainty estimation has shown its potential for interpreting the
prediction risks made by general machine learning (ML) models, little is known
about whether and to what extent it can help explore an LLM's capabilities and
counteract its undesired behavior. To bridge the gap, in this paper, we
initiate an exploratory study on the risk assessment of LLMs from the lens of
uncertainty. In particular, we experiment with twelve uncertainty estimation
methods and four LLMs on four prominent natural language processing (NLP) tasks
to investigate to what extent uncertainty estimation techniques could help
characterize the prediction risks of LLMs. Our findings validate the
effectiveness of uncertainty estimation for revealing LLMs'
uncertain/non-factual predictions. In addition to general NLP tasks, we
extensively conduct experiments with four LLMs for code generation on two
datasets. We find that uncertainty estimation can potentially uncover buggy
programs generated by LLMs. Insights from our study shed light on future design
and development for reliable LLMs, facilitating further research toward
enhancing the trustworthiness of LLMs.",2023-07-16T08:28:04Z
,http://arxiv.org/pdf/2405.02213v2.pdf,Automatic Programming: Large Language Models and Beyond,"Automatic programming has seen increasing popularity due to the emergence of
tools like GitHub Copilot which rely on Large Language Models (LLMs). At the
same time, automatically generated code faces challenges during deployment due
to concerns around quality and trust. In this article, we study automated
coding in a general sense and study the concerns around code quality, security
and related issues of programmer responsibility. These are key issues for
organizations while deciding on the usage of automatically generated code. We
discuss how advances in software engineering such as program repair and
analysis can enable automatic programming. We conclude with a forward looking
view, focusing on the programming environment of the near future, where
programmers may need to switch to different roles to fully utilize the power of
automatic programming. Automated repair of automatically generated programs
from LLMs, can help produce higher assurance code from LLMs, along with
evidence of assurance",2024-05-03T16:19:24Z
,http://arxiv.org/pdf/2308.10454v1.pdf,"Elucidating STEM Concepts through Generative AI: A Multi-modal
  Exploration of Analogical Reasoning","This study explores the integration of generative artificial intelligence
(AI), specifically large language models, with multi-modal analogical reasoning
as an innovative approach to enhance science, technology, engineering, and
mathematics (STEM) education. We have developed a novel system that utilizes
the capacities of generative AI to transform intricate principles in
mathematics, physics, and programming into comprehensible metaphors. To further
augment the educational experience, these metaphors are subsequently converted
into visual form. Our study aims to enhance the learners' understanding of STEM
concepts and their learning engagement by using the visual metaphors. We
examine the efficacy of our system via a randomized A/B/C test, assessing
learning gains and motivation shifts among the learners. Our study demonstrates
the potential of applying large language models to educational practice on STEM
subjects. The results will shed light on the design of educational system in
terms of harnessing AI's potential to empower educational stakeholders.",2023-08-21T04:00:56Z
10.1145/3643991.3645074,http://arxiv.org/pdf/2402.11702v2.pdf,"Can ChatGPT Support Developers? An Empirical Evaluation of Large
  Language Models for Code Generation","Large language models (LLMs) have demonstrated notable proficiency in code
generation, with numerous prior studies showing their promising capabilities in
various development scenarios. However, these studies mainly provide
evaluations in research settings, which leaves a significant gap in
understanding how effectively LLMs can support developers in real-world. To
address this, we conducted an empirical analysis of conversations in DevGPT, a
dataset collected from developers' conversations with ChatGPT (captured with
the Share Link feature on platforms such as GitHub). Our empirical findings
indicate that the current practice of using LLM-generated code is typically
limited to either demonstrating high-level concepts or providing examples in
documentation, rather than to be used as production-ready code. These findings
indicate that there is much future work needed to improve LLMs in code
generation before they can be integral parts of modern software development.",2024-02-18T20:48:09Z
10.1145/3639476.3639764,http://arxiv.org/pdf/2312.08055v2.pdf,Breaking the Silence: the Threats of Using LLMs in Software Engineering,"Large Language Models (LLMs) have gained considerable traction within the
Software Engineering (SE) community, impacting various SE tasks from code
completion to test generation, from program repair to code summarization.
Despite their promise, researchers must still be careful as numerous intricate
factors can influence the outcomes of experiments involving LLMs. This paper
initiates an open discussion on potential threats to the validity of LLM-based
research including issues such as closed-source models, possible data leakage
between LLM training data and research evaluation, and the reproducibility of
LLM-based findings. In response, this paper proposes a set of guidelines
tailored for SE researchers and Language Model (LM) providers to mitigate these
concerns. The implications of the guidelines are illustrated using existing
good practices followed by LLM providers and a practical example for SE
researchers in the context of test case generation.",2023-12-13T11:02:19Z
,http://arxiv.org/pdf/2310.10690v3.pdf,"Large Language Models for In-Context Student Modeling: Synthesizing
  Student's Behavior in Visual Programming","Student modeling is central to many educational technologies as it enables
predicting future learning outcomes and designing targeted instructional
strategies. However, open-ended learning domains pose challenges for accurately
modeling students due to the diverse behaviors and a large space of possible
misconceptions. To approach these challenges, we explore the application of
large language models (LLMs) for in-context student modeling in open-ended
learning domains. More concretely, given a particular student's attempt on a
reference task as observation, the objective is to synthesize the student's
attempt on a target task. We introduce a novel framework, LLM for Student
Synthesis (LLM-SS), that leverages LLMs for synthesizing a student's behavior.
Our framework can be combined with different LLMs; moreover, we fine-tune LLMs
to boost their student modeling capabilities. We instantiate several methods
based on LLM-SS framework and evaluate them using an existing benchmark,
StudentSyn, for student attempt synthesis in a visual programming domain.
Experimental results show that our methods perform significantly better than
the baseline method NeurSS provided in the StudentSyn benchmark. Furthermore,
our method using a fine-tuned version of the GPT-3.5 model is significantly
better than using the base GPT-3.5 model and gets close to human tutors'
performance.",2023-10-15T12:56:13Z
,http://arxiv.org/pdf/2310.04474v3.pdf,Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning,"While enabling large language models to implement function calling (known as
APIs) can greatly enhance the performance of Large Language Models (LLMs),
function calling is still a challenging task due to the complicated relations
between different APIs, especially in a context-learning setting without
fine-tuning. This paper introduces ``Reverse Chain'', a controllable,
target-driven approach designed to empower LLMs with the capability to operate
external APIs only via prompts. Recognizing that most LLMs have limited
tool-use capabilities, Reverse Chain limits LLMs to executing simple tasks,
e.g., API Selection and Argument Completion. Furthermore, to manage a
controllable multi-function calling, Reverse Chain adopts a generic rule based
on a backward reasoning process. This rule determines when to do API selection
or Argument completion. To evaluate the multi-tool-use capability of LLMs, we
have released a compositional multi-tool task dataset, available at
\url{https://anonymous.4open.science/r/reverse-chain-8681}. Extensive numerical
experiments validate the remarkable proficiency of Reverse Chain in managing
multiple API calls.",2023-10-06T05:20:18Z
,http://arxiv.org/pdf/2307.14936v1.pdf,"PanGu-Coder2: Boosting Large Language Models for Code with Ranking
  Feedback","Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.",2023-07-27T15:28:29Z
,http://arxiv.org/pdf/2305.14752v1.pdf,"A New Era in Software Security: Towards Self-Healing Software via Large
  Language Models and Formal Verification","In this paper we present a novel solution that combines the capabilities of
Large Language Models (LLMs) with Formal Verification strategies to verify and
automatically repair software vulnerabilities. Initially, we employ Bounded
Model Checking (BMC) to locate the software vulnerability and derive a
counterexample. The counterexample provides evidence that the system behaves
incorrectly or contains a vulnerability. The counterexample that has been
detected, along with the source code, are provided to the LLM engine. Our
approach involves establishing a specialized prompt language for conducting
code debugging and generation to understand the vulnerability's root cause and
repair the code. Finally, we use BMC to verify the corrected version of the
code generated by the LLM. As a proof of concept, we create ESBMC-AI based on
the Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained
Transformer model, specifically gpt-3.5-turbo, to detect and fix errors in C
programs. Our experimentation involved generating a dataset comprising 1000 C
code samples, each consisting of 20 to 50 lines of code. Notably, our proposed
method achieved an impressive success rate of up to 80% in repairing vulnerable
code encompassing buffer overflow and pointer dereference failures. We assert
that this automated approach can effectively incorporate into the software
development lifecycle's continuous integration and deployment (CI/CD) process.",2023-05-24T05:54:10Z
,http://arxiv.org/pdf/2304.06815v3.pdf,"Automatic Semantic Augmentation of Language Model Prompts (for Code
  Summarization)","Large Language Models (LLM) are a new class of computation engines,
""programmed"" via prompt engineering. We are still learning how to best
""program"" these LLMs to help developers. We start with the intuition that
developers tend to consciously and unconsciously have a collection of semantics
facts in mind when working on coding tasks. Mostly these are shallow, simple
facts arising from a quick read. For a function, examples of facts might
include parameter and local variable names, return expressions, simple pre- and
post-conditions, and basic control and data flow, etc.
  One might assume that the powerful multi-layer architecture of
transformer-style LLMs makes them inherently capable of doing this simple level
of ""code analysis"" and extracting such information, implicitly, while
processing code: but are they, really? If they aren't, could explicitly adding
this information help? Our goal here is to investigate this question, using the
code summarization task and evaluate whether automatically augmenting an LLM's
prompt with semantic facts explicitly, actually helps.
  Prior work shows that LLM performance on code summarization benefits from
few-shot samples drawn either from the same-project or from examples found via
information retrieval methods (such as BM25). While summarization performance
has steadily increased since the early days, there is still room for
improvement: LLM performance on code summarization still lags its performance
on natural-language tasks like translation and text summarization.
  We find that adding semantic facts actually does help! This approach improves
performance in several different settings suggested by prior work, including
for two different Large Language Models. In most cases, improvement nears or
exceeds 2 BLEU; for the PHP language in the challenging CodeSearchNet dataset,
this augmentation actually yields performance surpassing 30 BLEU.",2023-04-13T20:49:35Z
,http://arxiv.org/pdf/2401.10759v1.pdf,"Interactions with Prompt Problems: A New Way to Teach Programming with
  Large Language Models","Large Language Models (LLMs) have upended decades of pedagogy in computing
education. Students previously learned to code through \textit{writing} many
small problems with less emphasis on code reading and comprehension. Recent
research has shown that free code generation tools powered by LLMs can solve
introductory programming problems presented in natural language with ease. In
this paper, we propose a new way to teach programming with Prompt Problems.
Students receive a problem visually, indicating how input should be transformed
to output, and must translate that to a prompt for an LLM to decipher. The
problem is considered correct when the code that is generated by the student
prompt can pass all test cases. In this paper we present the design of this
tool, discuss student interactions with it as they learn, and provide insights
into this new class of programming problems as well as the design tools that
integrate LLMs.",2024-01-19T15:32:46Z
,http://arxiv.org/pdf/2402.07913v2.pdf,"QACP: An Annotated Question Answering Dataset for Assisting Chinese
  Python Programming Learners","In online learning platforms, particularly in rapidly growing computer
programming courses, addressing the thousands of students' learning queries
requires considerable human cost. The creation of intelligent assistant large
language models (LLMs) tailored for programming education necessitates distinct
data support. However, in real application scenarios, the data resources for
training such LLMs are relatively scarce. Therefore, to address the data
scarcity in intelligent educational systems for programming, this paper
proposes a new Chinese question-and-answer dataset for Python learners. To
ensure the authenticity and reliability of the sources of the questions, we
collected questions from actual student questions and categorized them
according to various dimensions such as the type of questions and the type of
learners. This annotation principle is designed to enhance the effectiveness
and quality of online programming education, providing a solid data foundation
for developing the programming teaching assists (TA). Furthermore, we conducted
comprehensive evaluations of various LLMs proficient in processing and
generating Chinese content, highlighting the potential limitations of general
LLMs as intelligent teaching assistants in computer programming courses.",2024-01-30T13:11:23Z
,http://arxiv.org/pdf/2312.15223v1.pdf,A Survey on Large Language Models for Software Engineering,"Software Engineering (SE) is the systematic design, development, and
maintenance of software applications, underpinning the digital infrastructure
of our modern mainworld. Very recently, the SE community has seen a rapidly
increasing number of techniques employing Large Language Models (LLMs) to
automate a broad range of SE tasks. Nevertheless, existing information of the
applications, effects, and possible limitations of LLMs within SE is still not
well-studied.
  In this paper, we provide a systematic survey to summarize the current
state-of-the-art research in the LLM-based SE community. We summarize 30
representative LLMs of Source Code across three model architectures, 15
pre-training objectives across four categories, and 16 downstream tasks across
five categories. We then present a detailed summarization of the recent SE
studies for which LLMs are commonly utilized, including 155 studies for 43
specific code-related tasks across four crucial phases within the SE workflow.
Besides, we summarize existing attempts to empirically evaluate LLMs in SE,
such as benchmarks, empirical studies, and exploration of SE education. We also
discuss several critical aspects of optimization and applications of LLMs in
SE, such as security attacks, model tuning, and model compression. Finally, we
highlight several challenges and potential opportunities on applying LLMs for
future SE studies, such as exploring domain LLMs and constructing clean
evaluation datasets. Overall, our work can help researchers gain a
comprehensive understanding about the achievements of the existing LLM-based SE
studies and promote the practical application of these techniques. Our
artifacts are publicly available and will continuously updated at the living
repository: \url{https://github.com/iSEngLab/AwesomeLLM4SE}.",2023-12-23T11:09:40Z
,http://arxiv.org/pdf/2306.03438v2.pdf,"Large Language Models of Code Fail at Completing Code with Potential
  Bugs","Large language models of code (Code-LLMs) have recently brought tremendous
advances to code completion, a fundamental feature of programming assistance
and code intelligence. However, most existing works ignore the possible
presence of bugs in the code context for generation, which are inevitable in
software development. Therefore, we introduce and study the buggy-code
completion problem, inspired by the realistic scenario of real-time code
suggestion where the code context contains potential bugs -- anti-patterns that
can become bugs in the completed program. To systematically study the task, we
introduce two datasets: one with synthetic bugs derived from semantics-altering
operator changes (buggy-HumanEval) and one with realistic bugs derived from
user submissions to coding problems (buggy-FixEval). We find that the presence
of potential bugs significantly degrades the generation performance of the
high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO
on test cases of buggy-HumanEval drop more than 50% given a single potential
bug in the context. Finally, we investigate several post-hoc methods for
mitigating the adverse effect of potential bugs and find that there remains a
significant gap in post-mitigation performance.",2023-06-06T06:35:27Z
,http://arxiv.org/pdf/2404.14824v1.pdf,"Automated Commit Message Generation with Large Language Models: An
  Empirical Study and Beyond","Commit Message Generation (CMG) approaches aim to automatically generate
commit messages based on given code diffs, which facilitate collaboration among
developers and play a critical role in Open-Source Software (OSS). Very
recently, Large Language Models (LLMs) have demonstrated extensive
applicability in diverse code-related task. But few studies systematically
explored their effectiveness using LLMs. This paper conducts the first
comprehensive experiment to investigate how far we have been in applying LLM to
generate high-quality commit messages. Motivated by a pilot analysis, we first
clean the most widely-used CMG dataset following practitioners' criteria.
Afterward, we re-evaluate diverse state-of-the-art CMG approaches and make
comparisons with LLMs, demonstrating the superior performance of LLMs against
state-of-the-art CMG approaches. Then, we further propose four manual metrics
following the practice of OSS, including Accuracy, Integrity, Applicability,
and Readability, and assess various LLMs accordingly. Results reveal that
GPT-3.5 performs best overall, but different LLMs carry different advantages.
To further boost LLMs' performance in the CMG task, we propose an Efficient
Retrieval-based In-Context Learning (ICL) framework, namely ERICommiter, which
leverages a two-step filtering to accelerate the retrieval efficiency and
introduces semantic/lexical-based retrieval algorithm to construct the ICL
examples. Extensive experiments demonstrate the substantial performance
improvement of ERICommiter on various LLMs for code diffs of different
programming languages. Meanwhile, ERICommiter also significantly reduces the
retrieval time while keeping almost the same performance. Our research
contributes to the understanding of LLMs' capabilities in the CMG field and
provides valuable insights for practitioners seeking to leverage these tools in
their workflows.",2024-04-23T08:24:43Z
,http://arxiv.org/pdf/2311.03033v1.pdf,"Beyond Words: A Mathematical Framework for Interpreting Large Language
  Models","Large language models (LLMs) are powerful AI tools that can generate and
comprehend natural language text and other complex information. However, the
field lacks a mathematical framework to systematically describe, compare and
improve LLMs. We propose Hex a framework that clarifies key terms and concepts
in LLM research, such as hallucinations, alignment, self-verification and
chain-of-thought reasoning. The Hex framework offers a precise and consistent
way to characterize LLMs, identify their strengths and weaknesses, and
integrate new findings. Using Hex, we differentiate chain-of-thought reasoning
from chain-of-thought prompting and establish the conditions under which they
are equivalent. This distinction clarifies the basic assumptions behind
chain-of-thought prompting and its implications for methods that use it, such
as self-verification and prompt programming.
  Our goal is to provide a formal framework for LLMs that can help both
researchers and practitioners explore new possibilities for generative AI. We
do not claim to have a definitive solution, but rather a tool for opening up
new research avenues. We argue that our formal definitions and results are
crucial for advancing the discussion on how to build generative AI systems that
are safe, reliable, fair and robust, especially in domains like healthcare and
software engineering.",2023-11-06T11:13:17Z
10.1145/3639474.3640052,http://arxiv.org/pdf/2403.06254v1.pdf,"LLMs Still Can't Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4
  and Bard's Capacity to Handle Object-Oriented Programming Assignments","Large Language Models (LLMs) have emerged as promising tools to assist
students while solving programming assignments. However, object-oriented
programming (OOP), with its inherent complexity involving the identification of
entities, relationships, and responsibilities, is not yet mastered by these
tools. Contrary to introductory programming exercises, there exists a research
gap with regard to the behavior of LLMs in OOP contexts. In this study, we
experimented with three prominent LLMs - GPT-3.5, GPT-4, and Bard - to solve
real-world OOP exercises used in educational settings, subsequently validating
their solutions using an Automatic Assessment Tool (AAT). The findings revealed
that while the models frequently achieved mostly working solutions to the
exercises, they often overlooked the best practices of OOP. GPT-4 stood out as
the most proficient, followed by GPT-3.5, with Bard trailing last. We advocate
for a renewed emphasis on code quality when employing these models and explore
the potential of pairing LLMs with AATs in pedagogical settings. In conclusion,
while GPT-4 showcases promise, the deployment of these models in OOP education
still mandates supervision.",2024-03-10T16:40:05Z
,http://arxiv.org/pdf/2205.12615v1.pdf,Autoformalization with Large Language Models,"Autoformalization is the process of automatically translating from natural
language mathematics to formal specifications and proofs. A successful
autoformalization system could advance the fields of formal verification,
program synthesis, and artificial intelligence. While the long-term goal of
autoformalization seemed elusive for a long time, we show large language models
provide new prospects towards this goal. We make the surprising observation
that LLMs can correctly translate a significant portion ($25.3\%$) of
mathematical competition problems perfectly to formal specifications in
Isabelle/HOL. We demonstrate the usefulness of this process by improving a
previously introduced neural theorem prover via training on these
autoformalized theorems. Our methodology results in a new state-of-the-art
result on the MiniF2F theorem proving benchmark, improving the proof rate from
$29.6\%$ to $35.2\%$.",2022-05-25T09:53:30Z
10.1145/3611643.3616271,http://arxiv.org/pdf/2309.00608v3.pdf,"Copiloting the Copilots: Fusing Large Language Models with Completion
  Engines for Automated Program Repair","During Automated Program Repair (APR), it can be challenging to synthesize
correct patches for real-world systems in general-purpose programming
languages. Recent Large Language Models (LLMs) have been shown to be helpful
""copilots"" in assisting developers with various coding tasks, and have also
been directly applied for patch synthesis. However, most LLMs treat programs as
sequences of tokens, meaning that they are ignorant of the underlying semantics
constraints of the target programming language. This results in plenty of
statically invalid generated patches, impeding the practicality of the
technique. Therefore, we propose Repilot, a general code generation framework
to further copilot the AI ""copilots"" (i.e., LLMs) by synthesizing more valid
patches during the repair process. Our key insight is that many LLMs produce
outputs autoregressively (i.e., token by token), resembling human writing
programs, which can be significantly boosted and guided through a Completion
Engine. Repilot synergistically synthesizes a candidate patch through the
interaction between an LLM and a Completion Engine, which 1) prunes away
infeasible tokens suggested by the LLM and 2) proactively completes the token
based on the suggestions provided by the Completion Engine. Our evaluation on a
subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot
outperforms state-of-the-art techniques by fixing 27% and 47% more bugs,
respectively. Moreover, Repilot produces more valid and correct patches than
the base LLM with the same budget. While we focus on leveraging Repilot for APR
in this work, the overall approach is also generalizable to other code
generation tasks.",2023-09-01T17:54:14Z
,http://arxiv.org/pdf/2406.08216v1.pdf,"A Software Engineering Perspective on Testing Large Language Models:
  Research, Practice, Tools and Benchmarks","Large Language Models (LLMs) are rapidly becoming ubiquitous both as
stand-alone tools and as components of current and future software systems. To
enable usage of LLMs in the high-stake or safety-critical systems of 2030, they
need to undergo rigorous testing. Software Engineering (SE) research on testing
Machine Learning (ML) components and ML-based systems has systematically
explored many topics such as test input generation and robustness. We believe
knowledge about tools, benchmarks, research and practitioner views related to
LLM testing needs to be similarly organized. To this end, we present a taxonomy
of LLM testing topics and conduct preliminary studies of state of the art and
practice approaches to research, open-source tools and benchmarks for LLM
testing, mapping results onto this taxonomy. Our goal is to identify gaps
requiring more research and engineering effort and inspire a clearer
communication between LLM practitioners and the SE research community.",2024-06-12T13:45:45Z
10.1145/3632620.3671098,http://arxiv.org/pdf/2406.06451v1.pdf,"Insights from Social Shaping Theory: The Appropriation of Large Language
  Models in an Undergraduate Programming Course","The capability of large language models (LLMs) to generate, debug, and
explain code has sparked the interest of researchers and educators in
undergraduate programming, with many anticipating their transformative
potential in programming education. However, decisions about why and how to use
LLMs in programming education may involve more than just the assessment of an
LLM's technical capabilities. Using the social shaping of technology theory as
a guiding framework, our study explores how students' social perceptions
influence their own LLM usage. We then examine the correlation of self-reported
LLM usage with students' self-efficacy and midterm performances in an
undergraduate programming course. Triangulating data from an anonymous
end-of-course student survey (n = 158), a mid-course self-efficacy survey
(n=158), student interviews (n = 10), self-reported LLM usage on homework, and
midterm performances, we discovered that students' use of LLMs was associated
with their expectations for their future careers and their perceptions of peer
usage. Additionally, early self-reported LLM usage in our context correlated
with lower self-efficacy and lower midterm scores, while students' perceived
over-reliance on LLMs, rather than their usage itself, correlated with
decreased self-efficacy later in the course.",2024-06-10T16:40:14Z
,http://arxiv.org/pdf/2404.08029v1.pdf,"A Multi-Expert Large Language Model Architecture for Verilog Code
  Generation","Recently, there has been a surging interest in using large language models
(LLMs) for Verilog code generation. However, the existing approaches are
limited in terms of the quality of the generated Verilog code. To address such
limitations, this paper introduces an innovative multi-expert LLM architecture
for Verilog code generation (MEV-LLM). Our architecture uniquely integrates
multiple LLMs, each specifically fine-tuned with a dataset that is categorized
with respect to a distinct level of design complexity. It allows more targeted
learning, directly addressing the nuances of generating Verilog code for each
category. Empirical evidence from experiments highlights notable improvements
in terms of the percentage of generated Verilog outputs that are syntactically
and functionally correct. These findings underscore the efficacy of our
approach, promising a forward leap in the field of automated hardware design
through machine learning.",2024-04-11T16:58:29Z
,http://arxiv.org/pdf/2310.06225v2.pdf,"GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using
  Large Language Models","Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs' performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4's ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models' capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.",2023-10-10T00:39:04Z
,http://arxiv.org/pdf/2406.07573v1.pdf,"Investigating the Potential of Using Large Language Models for
  Scheduling","The inaugural ACM International Conference on AI-powered Software introduced
the AIware Challenge, prompting researchers to explore AI-driven tools for
optimizing conference programs through constrained optimization. We investigate
the use of Large Language Models (LLMs) for program scheduling, focusing on
zero-shot learning and integer programming to measure paper similarity. Our
study reveals that LLMs, even under zero-shot settings, create reasonably good
first drafts of conference schedules. When clustering papers, using only titles
as LLM inputs produces results closer to human categorization than using titles
and abstracts with TFIDF. The code has been made publicly available.",2024-06-04T08:56:56Z
,http://arxiv.org/pdf/2307.07924v5.pdf,ChatDev: Communicative Agents for Software Development,"Software development is a complex task that necessitates cooperation among
multiple members with diverse skills. Numerous studies used deep learning to
improve specific phases in a waterfall model, such as design, coding, and
testing. However, the deep learning model in each phase requires unique
designs, leading to technical inconsistencies across various phases, which
results in a fragmented and ineffective development process. In this paper, we
introduce ChatDev, a chat-powered software development framework in which
specialized agents driven by large language models (LLMs) are guided in what to
communicate (via chat chain) and how to communicate (via communicative
dehallucination). These agents actively contribute to the design, coding, and
testing phases through unified language-based communication, with solutions
derived from their multi-turn dialogues. We found their utilization of natural
language is advantageous for system design, and communicating in programming
language proves helpful in debugging. This paradigm demonstrates how linguistic
communication facilitates multi-agent collaboration, establishing language as a
unifying bridge for autonomous task-solving among LLM agents. The code and data
are available at https://github.com/OpenBMB/ChatDev.",2023-07-16T02:11:34Z
,http://arxiv.org/pdf/2402.15100v1.pdf,Studying LLM Performance on Closed- and Open-source Data,"Large Language models (LLMs) are finding wide use in software engineering
practice. These models are extremely data-hungry, and are largely trained on
open-source (OSS) code distributed with permissive licenses. In terms of actual
use however, a great deal of software development still occurs in the
for-profit/proprietary sphere, where the code under development is not, and
never has been, in the public domain; thus, many developers, do their work, and
use LLMs, in settings where the models may not be as familiar with the code
under development. In such settings, do LLMs work as well as they do for OSS
code? If not, what are the differences? When performance differs, what are the
possible causes, and are there work-arounds? In this paper, we examine this
issue using proprietary, closed-source software data from Microsoft, where most
proprietary code is in C# and C++. We find that performance for C# changes
little from OSS --> proprietary code, but does significantly reduce for C++; we
find that this difference is attributable to differences in identifiers. We
also find that some performance degradation, in some cases, can be ameliorated
efficiently by in-context learning.",2024-02-23T05:17:28Z
,http://arxiv.org/pdf/2405.05253v1.pdf,"Open Source Language Models Can Provide Feedback: Evaluating LLMs'
  Ability to Help Students Using GPT-4-As-A-Judge","Large language models (LLMs) have shown great potential for the automatic
generation of feedback in a wide range of computing contexts. However, concerns
have been voiced around the privacy and ethical implications of sending student
work to proprietary models. This has sparked considerable interest in the use
of open source LLMs in education, but the quality of the feedback that such
open models can produce remains understudied. This is a concern as providing
flawed or misleading generated feedback could be detrimental to student
learning. Inspired by recent work that has utilised very powerful LLMs, such as
GPT-4, to evaluate the outputs produced by less powerful models, we conduct an
automated analysis of the quality of the feedback produced by several open
source models using a dataset from an introductory programming course. First,
we investigate the viability of employing GPT-4 as an automated evaluator by
comparing its evaluations with those of a human expert. We observe that GPT-4
demonstrates a bias toward positively rating feedback while exhibiting moderate
agreement with human raters, showcasing its potential as a feedback evaluator.
Second, we explore the quality of feedback generated by several leading
open-source LLMs by using GPT-4 to evaluate the feedback. We find that some
models offer competitive performance with popular proprietary LLMs, such as
ChatGPT, indicating opportunities for their responsible use in educational
settings.",2024-05-08T17:57:39Z
,http://arxiv.org/pdf/2401.00761v1.pdf,The Earth is Flat? Unveiling Factual Errors in Large Language Models,"Large Language Models (LLMs) like ChatGPT are foundational in various
applications due to their extensive knowledge from pre-training and
fine-tuning. Despite this, they are prone to generating factual and commonsense
errors, raising concerns in critical areas like healthcare, journalism, and
education to mislead users. Current methods for evaluating LLMs' veracity are
limited by test data leakage or the need for extensive human labor, hindering
efficient and accurate error detection. To tackle this problem, we introduce a
novel, automatic testing framework, FactChecker, aimed at uncovering factual
inaccuracies in LLMs. This framework involves three main steps: First, it
constructs a factual knowledge graph by retrieving fact triplets from a
large-scale knowledge database. Then, leveraging the knowledge graph,
FactChecker employs a rule-based approach to generates three types of questions
(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and
multi-hop relations, along with correct answers. Lastly, it assesses the LLMs'
responses for accuracy using tailored matching strategies for each question
type. Our extensive tests on six prominent LLMs, including text-davinci-002,
text-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal
that FactChecker can trigger factual errors in up to 45\% of questions in these
models. Moreover, we demonstrate that FactChecker's test cases can improve
LLMs' factual accuracy through in-context learning and fine-tuning (e.g.,
llama-2-13b-chat's accuracy increase from 35.3\% to 68.5\%). We are making all
code, data, and results available for future research endeavors.",2024-01-01T14:02:27Z
,http://arxiv.org/pdf/2307.07699v1.pdf,Leveraging Large Language Models to Generate Answer Set Programs,"Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated
exceptional performance in various natural language processing tasks and have
shown the ability to solve certain reasoning problems. However, their reasoning
capabilities are limited and relatively shallow, despite the application of
various prompting techniques. In contrast, formal logic is adept at handling
complex reasoning, but translating natural language descriptions into formal
logic is a challenging task that non-experts struggle with. This paper proposes
a neuro-symbolic method that combines the strengths of large language models
and answer set programming. Specifically, we employ an LLM to transform natural
language descriptions of logic puzzles into answer set programs. We carefully
design prompts for an LLM to convert natural language descriptions into answer
set programs in a step by step manner. Surprisingly, with just a few in-context
learning examples, LLMs can generate reasonably complex answer set programs.
The majority of errors made are relatively simple and can be easily corrected
by humans, thus enabling LLMs to effectively assist in the creation of answer
set programs.",2023-07-15T03:40:55Z
10.1145/3639477.3639719,http://arxiv.org/pdf/2310.06266v2.pdf,CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,"Code Large Language Models (Code LLMs) have gained significant attention in
the industry due to their wide applications in the full lifecycle of software
engineering. However, the effectiveness of existing models in understanding
non-English inputs for multi-lingual code-related tasks is still far from well
studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code
LLM. It is specifically designed for code-related tasks with both English and
Chinese prompts and supports over 40 programming languages. CodeFuse achieves
its effectiveness by utilizing a high quality pre-training dataset that is
carefully filtered by program analyzers and optimized during the training
process. Extensive experiments are conducted using real-world usage scenarios,
the industry-standard benchmark HumanEval-x, and the specially designed
CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we
actively collected valuable human feedback from the AntGroup's software
development process where CodeFuse has been successfully deployed. The results
demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%,
positioning it as one of the top multi-lingual code LLMs with similar parameter
sizes. In practical scenarios, such as code generation, code translation, code
comments, and testcase generation, CodeFuse performs better than other models
when confronted with Chinese prompts.",2023-10-10T02:38:44Z
,http://arxiv.org/pdf/2404.08877v1.pdf,Aligning LLMs for FL-free Program Repair,"Large language models (LLMs) have achieved decent results on automated
program repair (APR). However, the next token prediction training objective of
decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction
objective of current infilling-style methods, which impedes LLMs from fully
leveraging pre-trained knowledge for program repair. In addition, while some
LLMs are capable of locating and repairing bugs end-to-end when using the
related artifacts (e.g., test cases) as input, existing methods regard them as
separate tasks and ask LLMs to generate patches at fixed locations. This
restriction hinders LLMs from exploring potential patches beyond the given
locations.
  In this paper, we investigate a new approach to adapt LLMs to program repair.
Our core insight is that LLM's APR capability can be greatly improved by simply
aligning the output to their training objective and allowing them to refine the
whole program without first performing fault localization. Based on this
insight, we designed D4C, a straightforward prompting framework for APR. D4C
can repair 180 bugs correctly in Defects4J, with each patch being sampled only
10 times. This surpasses the SOTA APR methods with perfect fault localization
by 10% and reduces the patch sampling number by 90%. Our findings reveal that
(1) objective alignment is crucial for fully exploiting LLM's pre-trained
capability, and (2) replacing the traditional localize-then-repair workflow
with direct debugging is more effective for LLM-based APR methods. Thus, we
believe this paper introduces a new mindset for harnessing LLMs in APR.",2024-04-13T02:36:40Z
,http://arxiv.org/pdf/2404.18496v1.pdf,AI-powered Code Review with LLMs: Early Results,"In this paper, we present a novel approach to improving software quality and
efficiency through a Large Language Model (LLM)-based model designed to review
code and identify potential issues. Our proposed LLM-based AI agent model is
trained on large code repositories. This training includes code reviews, bug
reports, and documentation of best practices. It aims to detect code smells,
identify potential bugs, provide suggestions for improvement, and optimize the
code. Unlike traditional static code analysis tools, our LLM-based AI agent has
the ability to predict future potential risks in the code. This supports a dual
goal of improving code quality and enhancing developer education by encouraging
a deeper understanding of best practices and efficient coding techniques.
Furthermore, we explore the model's effectiveness in suggesting improvements
that significantly reduce post-release bugs and enhance code review processes,
as evidenced by an analysis of developer sentiment toward LLM feedback. For
future work, we aim to assess the accuracy and efficiency of LLM-generated
documentation updates in comparison to manual methods. This will involve an
empirical study focusing on manually conducted code reviews to identify code
smells and bugs, alongside an evaluation of best practice documentation,
augmented by insights from developer discussions and code reviews. Our goal is
to not only refine the accuracy of our LLM-based tool but also to underscore
its potential in streamlining the software development lifecycle through
proactive code improvement and education.",2024-04-29T08:27:50Z
,http://arxiv.org/pdf/2310.13712v2.pdf,"Impact of Guidance and Interaction Strategies for LLM Use on Learner
  Performance and Perception","Personalized chatbot-based teaching assistants can be crucial in addressing
increasing classroom sizes, especially where direct teacher presence is
limited. Large language models (LLMs) offer a promising avenue, with increasing
research exploring their educational utility. However, the challenge lies not
only in establishing the efficacy of LLMs but also in discerning the nuances of
interaction between learners and these models, which impact learners'
engagement and results. We conducted a formative study in an undergraduate
computer science classroom (N=145) and a controlled experiment on Prolific
(N=356) to explore the impact of four pedagogically informed guidance
strategies on the learners' performance, confidence and trust in LLMs. Direct
LLM answers marginally improved performance, while refining student solutions
fostered trust. Structured guidance reduced random queries as well as instances
of students copy-pasting assignment questions to the LLM. Our work highlights
the role that teachers can play in shaping LLM-supported learning environments.",2023-10-13T01:21:52Z
10.1145/3660807,http://arxiv.org/pdf/2306.01220v2.pdf,"Do Large Language Models Pay Similar Attention Like Human Programmers
  When Generating Code?","Large Language Models (LLMs) have recently been widely used for code
generation. Due to the complexity and opacity of LLMs, little is known about
how these models generate code. We made the first attempt to bridge this
knowledge gap by investigating whether LLMs attend to the same parts of a task
description as human programmers during code generation. An analysis of six
LLMs, including GPT-4, on two popular code generation benchmarks revealed a
consistent misalignment between LLMs' and programmers' attention. We manually
analyzed 211 incorrect code snippets and found five attention patterns that can
be used to explain many code generation errors. Finally, a user study showed
that model attention computed by a perturbation-based method is often favored
by human programmers. Our findings highlight the need for human-aligned LLMs
for better interpretability and programmer trust.",2023-06-02T00:57:03Z
,http://arxiv.org/pdf/2401.05856v1.pdf,"Seven Failure Points When Engineering a Retrieval Augmented Generation
  System","Software engineers are increasingly adding semantic search capabilities to
applications using a strategy known as Retrieval Augmented Generation (RAG). A
RAG system involves finding documents that semantically match a query and then
passing the documents to a large language model (LLM) such as ChatGPT to
extract the right answer using an LLM. RAG systems aim to: a) reduce the
problem of hallucinated responses from LLMs, b) link sources/references to
generated responses, and c) remove the need for annotating documents with
meta-data. However, RAG systems suffer from limitations inherent to information
retrieval systems and from reliance on LLMs. In this paper, we present an
experience report on the failure points of RAG systems from three case studies
from separate domains: research, education, and biomedical. We share the
lessons learned and present 7 failure points to consider when designing a RAG
system. The two key takeaways arising from our work are: 1) validation of a RAG
system is only feasible during operation, and 2) the robustness of a RAG system
evolves rather than designed in at the start. We conclude with a list of
potential research directions on RAG systems for the software engineering
community.",2024-01-11T12:04:11Z
,http://arxiv.org/pdf/2310.01726v1.pdf,Large Language Models for Test-Free Fault Localization,"Fault Localization (FL) aims to automatically localize buggy lines of code, a
key first step in many manual and automatic debugging tasks. Previous FL
techniques assume the provision of input tests, and often require extensive
program analysis, program instrumentation, or data preprocessing. Prior work on
deep learning for APR struggles to learn from small datasets and produces
limited results on real-world programs. Inspired by the ability of large
language models (LLMs) of code to adapt to new tasks based on very few
examples, we investigate the applicability of LLMs to line level fault
localization. Specifically, we propose to overcome the left-to-right nature of
LLMs by fine-tuning a small set of bidirectional adapter layers on top of the
representations learned by LLMs to produce LLMAO, the first language model
based fault localization approach that locates buggy lines of code without any
test coverage information. We fine-tune LLMs with 350 million, 6 billion, and
16 billion parameters on small, manually curated corpora of buggy programs such
as the Defects4J corpus. We observe that our technique achieves substantially
more confidence in fault localization when built on the larger models, with bug
localization performance scaling consistently with the LLM size. Our empirical
evaluation shows that LLMAO improves the Top-1 results over the
state-of-the-art machine learning fault localization (MLFL) baselines by
2.3%-54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FL
technique trained using a language model architecture that can detect security
vulnerabilities down to the code line level.",2023-10-03T01:26:39Z
,http://arxiv.org/pdf/2401.09042v1.pdf,LLMs for Relational Reasoning: How Far are We?,"Large language models (LLMs) have revolutionized many areas (e.g. natural
language processing, software engineering, etc.) by achieving state-of-the-art
performance on extensive downstream tasks. Aiming to achieve robust and general
artificial intelligence, there has been a surge of interest in investigating
the reasoning ability of the LLMs. Whereas the textual and numerical reasoning
benchmarks adopted by previous works are rather shallow and simple, it is hard
to conclude that the LLMs possess strong reasoning ability by merely achieving
positive results on these benchmarks. Recent efforts have demonstrated that the
LLMs are poor at solving sequential decision-making problems that require
common-sense planning by evaluating their performance on the reinforcement
learning benchmarks. In this work, we conduct an in-depth assessment of several
state-of-the-art LLMs' reasoning ability based on the inductive logic
programming (ILP) benchmark, which is broadly recognized as a representative
and challenging measurement for evaluating logic program induction/synthesis
systems as it requires inducing strict cause-effect logic to achieve robust
deduction on independent and identically distributed (IID) and
out-of-distribution (OOD) test samples. Our evaluations illustrate that
compared with the neural program induction systems which are much smaller in
model size, the state-of-the-art LLMs are much poorer in terms of reasoning
ability by achieving much lower performance and generalization using either
natural language prompting or truth-value matrix prompting.",2024-01-17T08:22:52Z
,http://arxiv.org/pdf/2405.05455v1.pdf,"Automated Program Repair: Emerging trends pose and expose problems for
  benchmarks","Machine learning (ML) now pervades the field of Automated Program Repair
(APR). Algorithms deploy neural machine translation and large language models
(LLMs) to generate software patches, among other tasks. But, there are
important differences between these applications of ML and earlier work.
Evaluations and comparisons must take care to ensure that results are valid and
likely to generalize. A challenge is that the most popular APR evaluation
benchmarks were not designed with ML techniques in mind. This is especially
true for LLMs, whose large and often poorly-disclosed training datasets may
include problems on which they are evaluated.",2024-05-08T23:09:43Z
,http://arxiv.org/pdf/2404.12636v2.pdf,Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs,"Large language models (LLMs) have demonstrated remarkable capabilities on a
broad spectrum of downstream tasks. Within the realm of software engineering,
specialized tasks on code, such as program repair, present unique challenges,
necessitating fine-tuning to unlock state-of-the-art performance. Fine-tuning
approaches proposed in the literature for LLMs on program repair tasks are
however generally overlooking the need to reason about the logic behind code
changes, beyond syntactic patterns in the data. High-performing fine-tuning
experiments also usually come at very high computational costs. With MORepair,
we propose a novel perspective on the learning focus of LLM fine-tuning for
program repair: we not only adapt the LLM parameters to the syntactic nuances
of the task of code transformation (objective 1), but we also specifically
fine-tune the LLM with respect to the logical reason behind the code change in
the training data (objective 2). Such a multi-objective fine-tuning will
instruct LLMs to generate high-quality patches.
  We apply MORepair to fine-tune four open-source LLMs with different sizes and
architectures. Experimental results on C++ and Java repair benchmarks show that
the implemented fine-tuning effectively boosts LLM repair performance by 7.6%
to 10% in Top-10 repair suggestions. We further show that our fine-tuning
strategy yields superior performance compared to the incumbent state-of-the-art
in fine-tuned models for program repair, Fine-tune-CoT and RepairLLaMA.",2024-04-19T05:36:21Z
,http://arxiv.org/pdf/2311.18450v3.pdf,Lessons from Building StackSpot AI: A Contextualized AI Coding Assistant,"With their exceptional natural language processing capabilities, tools based
on Large Language Models (LLMs) like ChatGPT and Co-Pilot have swiftly become
indispensable resources in the software developer's toolkit. While recent
studies suggest the potential productivity gains these tools can unlock, users
still encounter drawbacks, such as generic or incorrect answers. Additionally,
the pursuit of improved responses often leads to extensive prompt engineering
efforts, diverting valuable time from writing code that delivers actual value.
To address these challenges, a new breed of tools, built atop LLMs, is
emerging. These tools aim to mitigate drawbacks by employing techniques like
fine-tuning or enriching user prompts with contextualized information.
  In this paper, we delve into the lessons learned by a software development
team venturing into the creation of such a contextualized LLM-based
application, using retrieval-based techniques, called CodeBuddy. Over a
four-month period, the team, despite lacking prior professional experience in
LLM-based applications, built the product from scratch. Following the initial
product release, we engaged with the development team responsible for the code
generative components. Through interviews and analysis of the application's
issue tracker, we uncover various intriguing challenges that teams working on
LLM-based applications might encounter. For instance, we found three main group
of lessons: LLM-based lessons, User-based lessons, and Technical lessons. By
understanding these lessons, software development teams could become better
prepared to build LLM-based applications.",2023-11-30T10:51:26Z
10.1145/3613904.3642377,http://arxiv.org/pdf/2401.17163v2.pdf,"Learning Agent-based Modeling with LLM Companions: Experiences of
  Novices and Experts Using ChatGPT & NetLogo Chat","Large Language Models (LLMs) have the potential to fundamentally change the
way people engage in computer programming. Agent-based modeling (ABM) has
become ubiquitous in natural and social sciences and education, yet no prior
studies have explored the potential of LLMs to assist it. We designed NetLogo
Chat to support the learning and practice of NetLogo, a programming language
for ABM. To understand how users perceive, use, and need LLM-based interfaces,
we interviewed 30 participants from global academia, industry, and graduate
schools. Experts reported more perceived benefits than novices and were more
inclined to adopt LLMs in their workflow. We found significant differences
between experts and novices in their perceptions, behaviors, and needs for
human-AI collaboration. We surfaced a knowledge gap between experts and novices
as a possible reason for the benefit gap. We identified guidance,
personalization, and integration as major needs for LLM-based interfaces to
support the programming of ABM.",2024-01-30T16:49:50Z
,http://arxiv.org/pdf/2308.03873v1.pdf,"Evaluating and Explaining Large Language Models for Code Using Syntactic
  Structures","Large Language Models (LLMs) for code are a family of high-parameter,
transformer-based neural networks pre-trained on massive datasets of both
natural and programming languages. These models are rapidly being employed in
commercial AI-based developer tools, such as GitHub CoPilot. However, measuring
and explaining their effectiveness on programming tasks is a challenging
proposition, given their size and complexity. The methods for evaluating and
explaining LLMs for code are inextricably linked. That is, in order to explain
a model's predictions, they must be reliably mapped to fine-grained,
understandable concepts. Once this mapping is achieved, new methods for
detailed model evaluations are possible. However, most current explainability
techniques and evaluation benchmarks focus on model robustness or individual
task performance, as opposed to interpreting model predictions.
  To this end, this paper introduces ASTxplainer, an explainability method
specific to LLMs for code that enables both new methods for LLM evaluation and
visualizations of LLM predictions that aid end-users in understanding model
predictions. At its core, ASTxplainer provides an automated method for aligning
token predictions with AST nodes, by extracting and aggregating normalized
model logits within AST structures. To demonstrate the practical benefit of
ASTxplainer, we illustrate the insights that our framework can provide by
performing an empirical evaluation on 12 popular LLMs for code using a curated
dataset of the most popular GitHub projects. Additionally, we perform a user
study examining the usefulness of an ASTxplainer-derived visualization of model
predictions aimed at enabling model users to explain predictions. The results
of these studies illustrate the potential for ASTxplainer to provide insights
into LLM effectiveness, and aid end-users in understanding predictions.",2023-08-07T18:50:57Z
,http://arxiv.org/pdf/2403.16087v1.pdf,LLMs as Compiler for Arabic Programming Language,"In this paper we introduce APL (Arabic Programming Language) that uses Large
language models (LLM) as semi-compiler to covert Arabic text code to python
code then run the code. Designing a full pipeline from the structure of the APL
text then a prompt (using prompt engineering) then running the prodcued python
code using PyRunner. This project has a three parts first python library, a
playground with simple interface and this research paper.",2024-03-24T10:57:08Z
,http://arxiv.org/pdf/2404.14419v1.pdf,"Enhancing Fault Detection for Large Language Models via Mutation-Based
  Confidence Smoothing","Large language models (LLMs) achieved great success in multiple application
domains and attracted huge attention from different research communities
recently. Unfortunately, even for the best LLM, there still exist many faults
that LLM cannot correctly predict. Such faults will harm the usability of LLMs.
How to quickly reveal them in LLMs is important, but challenging. The reasons
are twofold, 1) the heavy labeling effort for preparing the test data, and 2)
accessing closed-source LLMs such as GPT4 is money-required. To handle this
problem, in the traditional deep learning testing field, test selection methods
have been proposed for efficiently testing deep learning models by prioritizing
faults. However, the usefulness of these methods on LLMs is unclear and under
exploration. In this paper, we first study the effectiveness of existing fault
detection methods for LLMs. Experimental results on four different
tasks~(including both code tasks and natural language processing tasks) and
four LLMs (e.g., LLaMA and GPT4) demonstrated that existing fault detection
methods cannot perform well on LLMs (e.g., seven out of eight methods perform
worse than random selection on LLaMA). To enhance existing fault detection
methods, we propose MuCS, a prompt Mutation-based prediction Confidence
Smoothing method for LLMs. Concretely, we mutate the prompts and compute the
average prediction confidence of all mutants as the input of fault detection
methods. The results show that our proposed solution significantly enhances
existing methods with the improvement of test relative coverage by up to
97.64%.",2024-04-14T07:06:12Z
,http://arxiv.org/pdf/2405.03734v1.pdf,"FOKE: A Personalized and Explainable Education Framework Integrating
  Foundation Models, Knowledge Graphs, and Prompt Engineering","Integrating large language models (LLMs) and knowledge graphs (KGs) holds
great promise for revolutionizing intelligent education, but challenges remain
in achieving personalization, interactivity, and explainability. We propose
FOKE, a Forest Of Knowledge and Education framework that synergizes foundation
models, knowledge graphs, and prompt engineering to address these challenges.
FOKE introduces three key innovations: (1) a hierarchical knowledge forest for
structured domain knowledge representation; (2) a multi-dimensional user
profiling mechanism for comprehensive learner modeling; and (3) an interactive
prompt engineering scheme for generating precise and tailored learning
guidance.
  We showcase FOKE's application in programming education, homework assessment,
and learning path planning, demonstrating its effectiveness and practicality.
Additionally, we implement Scholar Hero, a real-world instantiation of FOKE.
Our research highlights the potential of integrating foundation models,
knowledge graphs, and prompt engineering to revolutionize intelligent education
practices, ultimately benefiting learners worldwide. FOKE provides a principled
and unified approach to harnessing cutting-edge AI technologies for
personalized, interactive, and explainable educational services, paving the way
for further research and development in this critical direction.",2024-05-06T15:11:05Z
,http://arxiv.org/pdf/2308.16557v1.pdf,"Effective Test Generation Using Pre-trained Large Language Models and
  Mutation Testing","One of the critical phases in software development is software testing.
Testing helps with identifying potential bugs and reducing maintenance costs.
The goal of automated test generation tools is to ease the development of tests
by suggesting efficient bug-revealing tests. Recently, researchers have
leveraged Large Language Models (LLMs) of code to generate unit tests. While
the code coverage of generated tests was usually assessed, the literature has
acknowledged that the coverage is weakly correlated with the efficiency of
tests in bug detection. To improve over this limitation, in this paper, we
introduce MuTAP for improving the effectiveness of test cases generated by LLMs
in terms of revealing bugs by leveraging mutation testing. Our goal is achieved
by augmenting prompts with surviving mutants, as those mutants highlight the
limitations of test cases in detecting bugs. MuTAP is capable of generating
effective test cases in the absence of natural language descriptions of the
Program Under Test (PUTs). We employ different LLMs within MuTAP and evaluate
their performance on different benchmarks. Our results show that our proposed
method is able to detect up to 28% more faulty human-written code snippets.
Among these, 17% remained undetected by both the current state-of-the-art fully
automated test generation tool (i.e., Pynguin) and zero-shot/few-shot learning
approaches on LLMs. Furthermore, MuTAP achieves a Mutation Score (MS) of 93.57%
on synthetic buggy code, outperforming all other approaches in our evaluation.
Our findings suggest that although LLMs can serve as a useful tool to generate
test cases, they require specific post-processing steps to enhance the
effectiveness of the generated test cases which may suffer from syntactic or
functional errors and may be ineffective in detecting certain types of bugs and
testing corner cases PUTs.",2023-08-31T08:48:31Z
,http://arxiv.org/pdf/2310.09748v1.pdf,Large Language Model-Aware In-Context Learning for Code Generation,"Large language models (LLMs) have shown impressive in-context learning (ICL)
ability in code generation. LLMs take a prompt consisting of requirement-code
examples and a new requirement as input, and output new programs. Existing
studies have found that ICL is highly dominated by the examples and thus arises
research on example selection. However, existing approaches randomly select
examples or only consider the textual similarity of requirements to retrieve,
leading to sub-optimal performance. In this paper, we propose a novel
learning-based selection approach named LAIL (LLM-Aware In-context Learning)
for code generation. Given a candidate example, we exploit LLMs themselves to
estimate it by considering the generation probabilities of ground-truth
programs given a requirement and the example. We then label candidate examples
as positive or negative through the probability feedback. Based on the labeled
data, we import a contrastive learning objective to train an effective
retriever that acquires the preference of LLMs in code generation. We apply
LAIL to three LLMs and evaluate it on three representative datasets (e.g.,
MBJP, MBPP, and MBCPP). LATA outperforms the state-of-the-art baselines by
11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5 in
terms of Pass@1, respectively.",2023-10-15T06:12:58Z
,http://arxiv.org/pdf/2305.17145v1.pdf,Type Prediction With Program Decomposition and Fill-in-the-Type Training,"TypeScript and Python are two programming languages that support optional
type annotations, which are useful but tedious to introduce and maintain. This
has motivated automated type prediction: given an untyped program, produce a
well-typed output program. Large language models (LLMs) are promising for type
prediction, but there are challenges: fill-in-the-middle performs poorly,
programs may not fit into the context window, generated types may not type
check, and it is difficult to measure how well-typed the output program is. We
address these challenges by building OpenTau, a search-based approach for type
prediction that leverages large language models. We propose a new metric for
type prediction quality, give a tree-based program decomposition that searches
a space of generated types, and present fill-in-the-type fine-tuning for LLMs.
We evaluate our work with a new dataset for TypeScript type prediction, and
show that 47.4% of files type check (14.5% absolute improvement) with an
overall rate of 3.3 type errors per file. All code, data, and models are
available at: https://github.com/GammaTauAI/opentau.",2023-05-25T21:16:09Z
,http://arxiv.org/pdf/2403.09032v1.pdf,"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language
  Models to Coding Preferences","Evaluating the alignment of large language models (LLMs) with user-defined
coding preferences is a challenging endeavour that requires assessing intricate
textual LLMs' outputs. By relying on automated metrics and static analysis
tools, existing benchmarks fail to assess nuances in user instructions and LLM
outputs, highlighting the need for large-scale datasets and benchmarks for LLM
preference alignment. In this paper, we introduce CodeUltraFeedback, a
preference dataset of 10,000 complex instructions to tune and align LLMs to
coding preferences through AI feedback. We generate responses to the
instructions using a pool of 14 diverse LLMs, which we then annotate according
to their alignment with five coding preferences using the LLM-as-a-Judge
approach with GPT-3.5, producing both numerical and textual feedback. We also
present CODAL-Bench, a benchmark for assessing LLM alignment with these coding
preferences. Our results show that CodeLlama-7B-Instruct, aligned through
reinforcement learning from AI feedback (RLAIF) with direct preference
optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B
LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference
tuning. Furthermore, we show our DPO-aligned CodeLlama model improves
functional correctness on HumanEval+ compared to the unaligned base model.
Therefore, our contributions bridge the gap in preference tuning of LLMs for
code and set the stage for further advancements in model alignment and RLAIF
for code intelligence. Our code and data are available at
https://github.com/martin-wey/CodeUltraFeedback.",2024-03-14T01:51:35Z
,http://arxiv.org/pdf/2306.03324v2.pdf,Impact of Large Language Models on Generating Software Specifications,"Software specifications are essential for ensuring the reliability of
software systems. Existing specification extraction approaches, however, suffer
from limited generalizability and require manual efforts. The recent emergence
of Large Language Models (LLMs), which have been successfully applied to
numerous software engineering tasks, offers a promising avenue for automating
this process. In this paper, we conduct the first empirical study to evaluate
the capabilities of LLMs for generating software specifications from software
comments or documentation. We evaluate LLMs' performance with Few Shot Learning
(FSL), enabling LLMs to generalize from a small number of examples, as well as
different prompt construction strategies, and compare the performance of LLMs
with traditional approaches. Additionally, we conduct a comparative diagnosis
of the failure cases from both LLMs and traditional methods, identifying their
unique strengths and weaknesses. Lastly, we conduct extensive experiments on 15
state of the art LLMs, evaluating their performance and cost effectiveness for
generating software specifications.
  Our results show that with FSL, LLMs outperform traditional methods (by
5.6%), and more sophisticated prompt construction strategies can further
enlarge this performance gap (up to 5.1 to 10.0%). Yet, LLMs suffer from their
unique challenges, such as ineffective prompts and the lack of domain
knowledge, which together account for 53 to 60% of LLM unique failures. The
strong performance of open source models (e.g., StarCoder) makes closed source
models (e.g., GPT 3 Davinci) less desirable due to size and cost. Our study
offers valuable insights for future research to improve specification
generation.",2023-06-06T00:28:39Z
10.1007/978-3-031-48796-5_9,http://arxiv.org/pdf/2403.08430v1.pdf,"Search-based Optimisation of LLM Learning Shots for Story Point
  Estimation","One of the ways Large Language Models (LLMs) are used to perform machine
learning tasks is to provide them with a few examples before asking them to
produce a prediction. This is a meta-learning process known as few-shot
learning. In this paper, we use available Search-Based methods to optimise the
number and combination of examples that can improve an LLM's estimation
performance, when it is used to estimate story points for new agile tasks. Our
preliminary results show that our SBSE technique improves the estimation
performance of the LLM by 59.34% on average (in terms of mean absolute error of
the estimation) over three datasets against a zero-shot setting.",2024-03-13T11:29:37Z
,http://arxiv.org/pdf/2404.07940v1.pdf,"InfiCoder-Eval: Systematically Evaluating the Question-Answering
  Capabilities of Code Large Language Models","Large Language Models for understanding and generating code (code LLMs) have
witnessed tremendous progress in recent years. With the rapid development of
code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and
MBPP, have emerged to measure the performance of code LLMs with a particular
focus on code generation tasks. However, they are insufficient to cover the
full range of expected capabilities of code LLMs, which span beyond code
generation to answering diverse coding-related questions. To fill this gap, we
propose InfiCoder-Eval, a large-scale freeform question-answering (QA)
benchmark for code, comprising 234 carefully selected high-quality Stack
Overflow questions that span across 15 programming languages. To evaluate the
response correctness, InfiCoder-Eval supports four types of model-free metrics
and domain experts carefully choose and concretize the criterion for each
question. We conduct a systematic evaluation for more than 80 code LLMs on
InfiCoder-Eval, leading to a series of insightful findings. Furthermore, our
detailed analyses showcase possible directions for further improvement of code
LLMs. InfiCoder-Eval is fully open source at
https://infi-coder.github.io/inficoder-eval/ and continuously maintaining and
expanding to foster more scientific and systematic practices for evaluating
code LLMs.",2024-03-11T02:06:30Z
,http://arxiv.org/pdf/2308.10462v2.pdf,"Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation
  with Large Language Models","Large Language Models (LLMs) demonstrate impressive capabilities to generate
accurate code snippets given natural language intents in zero-shot, i.e.,
without the need for specific fine-tuning. While prior studies have highlighted
the advantages of fine-tuning LLMs, this process incurs high computational
costs, making it impractical in resource-scarce environments, particularly for
models with billions of parameters. To address these challenges, previous
research explored In-Context Learning (ICL) as a strategy to guide the LLM
generative process with task-specific prompt examples. However, ICL introduces
inconveniences, such as the need for designing contextually relevant prompts
and the absence of learning task-specific parameters, thereby limiting
downstream task performance. In this context, we foresee Parameter-Efficient
Fine-Tuning (PEFT) techniques as a promising approach to efficiently specialize
LLMs to task-specific data while maintaining reasonable resource consumption.
In this paper, we deliver a comprehensive study of PEFT techniques for LLMs
under the automated code generation scenario. Our comprehensive investigation
of PEFT techniques for LLMs reveals their superiority and potential over ICL
across a diverse set of LLMs. Additionally, we demonstrate the extended
capabilities of PEFT, showcasing its ability to learn from two distinct
datasets jointly without compromising performance. Furthermore, our study
highlights the potential for tuning larger LLMs and significant reductions in
memory usage by combining PEFT with quantization. Therefore, this study opens
opportunities for broader applications of PEFT in software engineering
scenarios. Our code is available at
https://github.com/martin-wey/peft-llm-code/.",2023-08-21T04:31:06Z
,http://arxiv.org/pdf/2308.10088v2.pdf,"PACE: Improving Prompt with Actor-Critic Editing for Large Language
  Model","Large language models (LLMs) have showcased remarkable potential across
various tasks by conditioning on prompts. However, the quality of different
human-written prompts leads to substantial discrepancies in LLMs' performance,
and improving prompts usually necessitates considerable human effort and
expertise. To this end, this paper proposes Prompt with Actor-Critic Editing
(PACE) for LLMs to enable automatic prompt editing. Drawing inspiration from
the actor-critic algorithm in reinforcement learning, PACE leverages LLMs as
the dual roles of actors and critics, conceptualizing prompt as a type of
policy. PACE refines prompt, taking into account the feedback from both actors
performing prompt and critics criticizing response. This process helps LLMs
better align prompt to a specific task, thanks to real responses and thinking
from LLMs. We conduct extensive experiments on 24 instruction induction tasks
and 21 big-bench tasks. Experimental results indicate that PACE elevates the
relative performance of medium/low-quality human-written prompts by up to 98\%,
which has comparable performance to high-quality human-written prompts.
Moreover, PACE also exhibits notable efficacy for prompt generation.",2023-08-19T18:47:44Z
,http://arxiv.org/pdf/2406.09701v1.pdf,"Towards Effectively Detecting and Explaining Vulnerabilities Using Large
  Language Models","Software vulnerabilities pose significant risks to the security and integrity
of software systems. Prior studies have proposed a series of approaches to
vulnerability detection using deep learning or pre-trained models. However,
there is still a lack of vulnerability's detailed explanation for understanding
apart from detecting its occurrence. Recently, large language models (LLMs)
have shown a remarkable capability in the comprehension of complicated context
and content generation, which brings opportunities for the detection and
explanation of vulnerabilities of LLMs. In this paper, we conduct a
comprehensive study to investigate the capabilities of LLMs in detecting and
explaining vulnerabilities and propose LLMVulExp, a framework that utilizes
LLMs for vulnerability detection and explanation. Under specialized fine-tuning
for vulnerability explanation, LLMVulExp not only detects the types of
vulnerabilities in the code but also analyzes the code context to generate the
cause, location, and repair suggestions for these vulnerabilities. We find that
LLMVulExp can effectively enable the LLMs to perform vulnerability detection
(e.g., over 90% F1 score on SeVC dataset) and explanation. We also explore the
potential of using advanced strategies such as Chain-of-Thought (CoT) to guide
the LLMs concentrating on vulnerability-prone code and achieve promising
results.",2024-06-14T04:01:25Z
,http://arxiv.org/pdf/2208.08289v3.pdf,CCTEST: Testing and Repairing Code Completion Systems,"Code completion, a highly valuable topic in the software development domain,
has been increasingly promoted for use by recent advances in large language
models (LLMs). To date, visible LLM-based code completion frameworks such as
GitHub Copilot and GPT are trained using deep learning over vast quantities of
unstructured text and open source code. As the paramount component and the
cornerstone in daily programming tasks, code completion has largely boosted
professionals' efficiency in building real-world software systems. In contrast
to this flourishing market, we find that code completion systems often output
suspicious results, and to date, an automated testing and enhancement framework
for code completion systems is not available. This research proposes CCTEST, a
framework to test and repair code completion systems in blackbox settings.
CCTEST features a set of novel mutation strategies, namely program
structure-correlated (PSC) mutations, to generate mutated code completion
inputs. Then, it detects inconsistent outputs, representing possibly erroneous
cases, from all the completed code cases. Moreover, CCTEST repairs the code
completion outputs by selecting the output that mostly reflects the ""average""
appearance of all output cases, as the final output of the code completion
systems. We detected a total of 33,540 inputs (with a true positive rate of
86%) that can trigger erroneous cases from eight popular LLM-based code
completion systems. With repairing, we show that the accuracy of code
completion systems is notably increased by 40% and 67% with respect to BLEU
score and Levenshtein edit similarity.",2022-08-17T13:37:03Z
,http://arxiv.org/pdf/2405.15729v2.pdf,Optimizing Large Language Models for OpenAPI Code Completion,"Recent advancements in Large Language Models (LLMs) and their utilization in
code generation tasks have significantly reshaped the field of software
development. Despite the remarkable efficacy of code completion solutions in
mainstream programming languages, their performance lags when applied to less
ubiquitous formats such as OpenAPI definitions. This study evaluates the
OpenAPI completion performance of GitHub Copilot, a prevalent commercial code
completion tool, and proposes a set of task-specific optimizations leveraging
Meta's open-source model Code Llama. A semantics-aware OpenAPI completion
benchmark proposed in this research is used to perform a series of experiments
through which the impact of various prompt-engineering and fine-tuning
techniques on the Code Llama model's performance is analyzed. The fine-tuned
Code Llama model reaches a peak correctness improvement of 55.2% over GitHub
Copilot despite utilizing 25 times fewer parameters than the commercial
solution's underlying Codex model. Additionally, this research proposes an
enhancement to a widely used code infilling training technique, addressing the
issue of underperformance when the model is prompted with context sizes smaller
than those used during training. The dataset, the benchmark, and the model
fine-tuning code are made publicly available.",2024-05-24T17:19:03Z
10.1145/3636243.3636256,http://arxiv.org/pdf/2312.03173v1.pdf,"A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in
  Programming Education","There is a constant need for educators to develop and maintain effective
up-to-date assessments. While there is a growing body of research in computing
education on utilizing large language models (LLMs) in generation and
engagement with coding exercises, the use of LLMs for generating programming
MCQs has not been extensively explored. We analyzed the capability of GPT-4 to
produce multiple-choice questions (MCQs) aligned with specific learning
objectives (LOs) from Python programming classes in higher education.
Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs
from high-level course context and module-level LOs. We evaluated 651
LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python
courses. We found that GPT-4 was capable of producing MCQs with clear language,
a single correct choice, and high-quality distractors. We also observed that
the generated MCQs appeared to be well-aligned with the LOs. Our findings can
be leveraged by educators wishing to take advantage of the state-of-the-art
generative models to support MCQ authoring efforts.",2023-12-05T22:29:43Z
,http://arxiv.org/pdf/2401.10745v1.pdf,"Ethical Artificial Intelligence Principles and Guidelines for the
  Governance and Utilization of Highly Advanced Large Language Models","Given the success of ChatGPT, LaMDA and other large language models (LLMs),
there has been an increase in development and usage of LLMs within the
technology sector and other sectors. While the level in which LLMs has not
reached a level where it has surpassed human intelligence, there will be a time
when it will. Such LLMs can be referred to as advanced LLMs. Currently, there
are limited usage of ethical artificial intelligence (AI) principles and
guidelines addressing advanced LLMs due to the fact that we have not reached
that point yet. However, this is a problem as once we do reach that point, we
will not be adequately prepared to deal with the aftermath of it in an ethical
and optimal way, which will lead to undesired and unexpected consequences. This
paper addresses this issue by discussing what ethical AI principles and
guidelines can be used to address highly advanced LLMs.",2023-12-19T06:28:43Z
,http://arxiv.org/pdf/2403.14668v1.pdf,"Predicting Learning Performance with Large Language Models: A Study in
  Adult Literacy","Intelligent Tutoring Systems (ITSs) have significantly enhanced adult
literacy training, a key factor for societal participation, employment
opportunities, and lifelong learning. Our study investigates the application of
advanced AI models, including Large Language Models (LLMs) like GPT-4, for
predicting learning performance in adult literacy programs in ITSs. This
research is motivated by the potential of LLMs to predict learning performance
based on its inherent reasoning and computational capabilities. By using
reading comprehension datasets from the ITS, AutoTutor, we evaluate the
predictive capabilities of GPT-4 versus traditional machine learning methods in
predicting learning performance through five-fold cross-validation techniques.
Our findings show that the GPT-4 presents the competitive predictive abilities
with traditional machine learning methods such as Bayesian Knowledge Tracing,
Performance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor
factorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained
on local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected
XGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior
performance compared to local machine execution. Moreover, our investigation
into hyper-parameter tuning by GPT-4 versus grid-search suggests comparable
performance, albeit with less stability in the automated approach, using
XGBoost as the case study. Our study contributes to the field by highlighting
the potential of integrating LLMs with traditional machine learning models to
enhance predictive accuracy and personalize adult literacy education, setting a
foundation for future research in applying LLMs within ITSs.",2024-03-04T08:14:07Z
,http://arxiv.org/pdf/2308.00708v1.pdf,VeriGen: A Large Language Model for Verilog Code Generation,"In this study, we explore the capability of Large Language Models (LLMs) to
automate hardware design by generating high-quality Verilog code, a common
language for designing and modeling digital systems. We fine-tune pre-existing
LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We
evaluate the functional correctness of the generated Verilog code using a
specially designed test suite, featuring a custom problem set and testing
benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the
commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase.
Upon testing with a more diverse and complex problem set, we find that the
fine-tuned model shows competitive performance against state-of-the-art
gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41%
improvement in generating syntactically correct Verilog code across various
problem categories compared to its pre-trained counterpart, highlighting the
potential of smaller, in-house LLMs in hardware design automation.",2023-07-28T02:57:14Z
,http://arxiv.org/pdf/2308.10345v1.pdf,Can Large Language Models Find And Fix Vulnerable Software?,"In this study, we evaluated the capability of Large Language Models (LLMs),
particularly OpenAI's GPT-4, in detecting software vulnerabilities, comparing
their performance against traditional static code analyzers like Snyk and
Fortify. Our analysis covered numerous repositories, including those from NASA
and the Department of Defense. GPT-4 identified approximately four times the
vulnerabilities than its counterparts. Furthermore, it provided viable fixes
for each vulnerability, demonstrating a low rate of false positives. Our tests
encompassed 129 code samples across eight programming languages, revealing the
highest vulnerabilities in PHP and JavaScript. GPT-4's code corrections led to
a 90% reduction in vulnerabilities, requiring only an 11% increase in code
lines. A critical insight was LLMs' ability to self-audit, suggesting fixes for
their identified vulnerabilities and underscoring their precision. Future
research should explore system-level vulnerabilities and integrate multiple
static code analyzers for a holistic perspective on LLMs' potential.",2023-08-20T19:33:12Z
,http://arxiv.org/pdf/2401.16797v2.pdf,"Enhancing Translation Validation of Compiler Transformations with Large
  Language Models","This paper presents a framework that integrates Large Language Models (LLMs)
into translation validation, targeting LLVM compiler transformations where
formal verification tools fall short. Our framework first utilizes existing
formal verification tools for translation validation. In this work, we use
Alive2, a well-known tool in LLVM compiler verification, as an example. When
formal verification tools are unable to confirm a transformation's soundness,
our framework employs fine-tuned LLMs for prediction. It then applies fuzzing
to transformations predicted as potentially unsound by the LLMs due to return
values or memory inconsistencies, aiming to find counterexamples. In cases
where transformations are unsound for other reasons or sound, or if no
counterexamples emerge, the framework directly reports these outcomes without
further fuzzing. This methodology has shown effectiveness in complex
application such as deep-learning accelerator designs, where traditional formal
verification tools struggle.",2024-01-30T07:24:04Z
,http://arxiv.org/pdf/2402.02388v1.pdf,"Solution-oriented Agent-based Models Generation with Verifier-assisted
  Iterative In-context Learning","Agent-based models (ABMs) stand as an essential paradigm for proposing and
validating hypothetical solutions or policies aimed at addressing challenges
posed by complex systems and achieving various objectives. This process demands
labor-intensive endeavors and multidisciplinary expertise. Large language
models (LLMs) encapsulating cross-domain knowledge and programming proficiency
could potentially alleviate the difficulty of this process. However, LLMs excel
in handling sequential information, making it challenging for analyzing the
intricate interactions and nonlinear dynamics inherent in ABMs. Additionally,
due to the lack of self-evaluation capability of LLMs, relying solely on LLMs
is insufficient to effectively accomplish this process. In this paper, we
present SAGE, a general solution-oriented ABM generation framework designed for
automatic modeling and generating solutions for targeted problems. Unlike
approaches reliant on expert handcrafting or resource-intensive neural network
training, SAGE establishes a verifier-assisted iterative in-context learning
process employing large language models (LLMs) to leverages their inherent
cross-domain knowledge for tackling intricate demands from diverse domain
scenarios. In SAGE, we introduce an semi-structured conceptual representation
expliciting the intricate structures of ABMs and an objective representation to
guide LLMs in modeling scenarios and proposing hypothetical solutions through
in-context learning. To ensure the model executability and solution
feasibility, SAGE devises a two-level verifier with chain-of-thought prompting
tailored to the complex interactions and non-linear dynamics of ABMs, driving
the iterative generation optimization. Moreover, we construct an evaluation
dataset of solution-oriented ABMs from open sources.It contains practical
models across various domains.",2024-02-04T07:59:06Z
,http://arxiv.org/pdf/2403.15600v1.pdf,"Just another copy and paste? Comparing the security vulnerabilities of
  ChatGPT generated code and StackOverflow answers","Sonatype's 2023 report found that 97% of developers and security leads
integrate generative Artificial Intelligence (AI), particularly Large Language
Models (LLMs), into their development process. Concerns about the security
implications of this trend have been raised. Developers are now weighing the
benefits and risks of LLMs against other relied-upon information sources, such
as StackOverflow (SO), requiring empirical data to inform their choice. In this
work, our goal is to raise software developers awareness of the security
implications when selecting code snippets by empirically comparing the
vulnerabilities of ChatGPT and StackOverflow. To achieve this, we used an
existing Java dataset from SO with security-related questions and answers.
Then, we asked ChatGPT the same SO questions, gathering the generated code for
comparison. After curating the dataset, we analyzed the number and types of
Common Weakness Enumeration (CWE) vulnerabilities of 108 snippets from each
platform using CodeQL. ChatGPT-generated code contained 248 vulnerabilities
compared to the 302 vulnerabilities found in SO snippets, producing 20% fewer
vulnerabilities with a statistically significant difference. Additionally,
ChatGPT generated 19 types of CWE, fewer than the 22 found in SO. Our findings
suggest developers are under-educated on insecure code propagation from both
platforms, as we found 274 unique vulnerabilities and 25 types of CWE. Any code
copied and pasted, created by AI or humans, cannot be trusted blindly,
requiring good software engineering practices to reduce risk. Future work can
help minimize insecure code propagation from any platform.",2024-03-22T20:06:41Z
,http://arxiv.org/pdf/2405.06806v1.pdf,"An Empirical Study on the Effectiveness of Large Language Models for
  SATD Identification and Classification","Self-Admitted Technical Debt (SATD), a concept highlighting sub-optimal
choices in software development documented in code comments or other project
resources, poses challenges in the maintainability and evolution of software
systems. Large language models (LLMs) have demonstrated significant
effectiveness across a broad range of software tasks, especially in software
text generation tasks. Nonetheless, their effectiveness in tasks related to
SATD is still under-researched. In this paper, we investigate the efficacy of
LLMs in both identification and classification of SATD. For both tasks, we
investigate the performance gain from using more recent LLMs, specifically the
Flan-T5 family, across different common usage settings. Our results demonstrate
that for SATD identification, all fine-tuned LLMs outperform the best existing
non-LLM baseline, i.e., the CNN model, with a 4.4% to 7.2% improvement in F1
score. In the SATD classification task, while our largest fine-tuned model,
Flan-T5-XL, still led in performance, the CNN model exhibited competitive
results, even surpassing four of six LLMs. We also found that the largest
Flan-T5 model, i.e., Flan-T5-XXL, when used with a zero-shot in-context
learning (ICL) approach for SATD identification, provides competitive results
with traditional approaches but performs 6.4% to 9.2% worse than fine-tuned
LLMs. For SATD classification, few-shot ICL approach, incorporating examples
and category descriptions in prompts, outperforms the zero-shot approach and
even surpasses the fine-tuned smaller Flan-T5 models. Moreover, our experiments
demonstrate that incorporating contextual information, such as surrounding
code, into the SATD classification task enables larger fine-tuned LLMs to
improve their performance.",2024-05-10T20:39:24Z
,http://arxiv.org/pdf/2406.04379v1.pdf,"VHDL-Eval: A Framework for Evaluating Large Language Models in VHDL Code
  Generation","With the unprecedented advancements in Large Language Models (LLMs), their
application domains have expanded to include code generation tasks across
various programming languages. While significant progress has been made in
enhancing LLMs for popular programming languages, there exists a notable gap in
comprehensive evaluation frameworks tailored for Hardware Description Languages
(HDLs), particularly VHDL. This paper addresses this gap by introducing a
comprehensive evaluation framework designed specifically for assessing LLM
performance in VHDL code generation task. We construct a dataset for evaluating
LLMs on VHDL code generation task. This dataset is constructed by translating a
collection of Verilog evaluation problems to VHDL and aggregating publicly
available VHDL problems, resulting in a total of 202 problems. To assess the
functional correctness of the generated VHDL code, we utilize a curated set of
self-verifying testbenches specifically designed for those aggregated VHDL
problem set. We conduct an initial evaluation of different LLMs and their
variants, including zero-shot code generation, in-context learning (ICL), and
Parameter-efficient fine-tuning (PEFT) methods. Our findings underscore the
considerable challenges faced by existing LLMs in VHDL code generation,
revealing significant scope for improvement. This study emphasizes the
necessity of supervised fine-tuning code generation models specifically for
VHDL, offering potential benefits to VHDL designers seeking efficient code
generation solutions.",2024-06-06T00:06:50Z
,http://arxiv.org/pdf/2404.17739v2.pdf,How LLMs Aid in UML Modeling: An Exploratory Study with Novice Analysts,"Since the emergence of GPT-3, Large Language Models (LLMs) have caught the
eyes of researchers, practitioners, and educators in the field of software
engineering. However, there has been relatively little investigation regarding
the performance of LLMs in assisting with requirements analysis and UML
modeling. This paper explores how LLMs can assist novice analysts in creating
three types of typical UML models: use case models, class diagrams, and
sequence diagrams. For this purpose, we designed the modeling tasks of these
three UML models for 45 undergraduate students who participated in a
requirements modeling course, with the help of LLMs. By analyzing their project
reports, we found that LLMs can assist undergraduate students as novice
analysts in UML modeling tasks, but LLMs also have shortcomings and limitations
that should be considered when using them.",2024-04-27T00:38:20Z
,http://arxiv.org/pdf/2401.15468v1.pdf,"Large Language Model for Vulnerability Detection: Emerging Results and
  Future Directions","Previous learning-based vulnerability detection methods relied on either
medium-sized pre-trained models or smaller neural networks from scratch. Recent
advancements in Large Pre-Trained Language Models (LLMs) have showcased
remarkable few-shot learning capabilities in various tasks. However, the
effectiveness of LLMs in detecting software vulnerabilities is largely
unexplored. This paper aims to bridge this gap by exploring how LLMs perform
with various prompts, particularly focusing on two state-of-the-art LLMs:
GPT-3.5 and GPT-4. Our experimental results showed that GPT-3.5 achieves
competitive performance with the prior state-of-the-art vulnerability detection
approach and GPT-4 consistently outperformed the state-of-the-art.",2024-01-27T17:39:36Z
,http://arxiv.org/pdf/2403.04814v3.pdf,Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks,"We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for
evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM)
task. This benchmark focuses on syntax-aware completions of program structures
such as code blocks and conditional expressions, and includes 17,720 examples
from multiple programming languages, sourced from recent code submissions after
April 2022 to minimize data contamination. SAFIM provides a robust framework
with various prompt designs and novel syntax-aware post-processing techniques,
facilitating accurate and fair comparisons across LLMs. Our comprehensive
evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM
proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our
findings challenge conventional beliefs and suggest that pretraining methods
and data quality have more impact than model size. SAFIM thus serves as a
foundational platform for future research in effective pretraining strategies
for code LLMs. The evaluation toolkit and dataset are available at
https://github.com/gonglinyuan/safim, and the leaderboard is available at
https://safimbenchmark.com.",2024-03-07T05:05:56Z
,http://arxiv.org/pdf/2312.17025v3.pdf,Experiential Co-Learning of Software-Developing Agents,"Recent advancements in large language models (LLMs) have brought significant
changes to various domains, especially through LLM-driven autonomous agents. A
representative scenario is in software development, where LLM agents
demonstrate efficient collaboration, task division, and assurance of software
quality, markedly reducing the need for manual involvement. However, these
agents frequently perform a variety of tasks independently, without benefiting
from past experiences, which leads to repeated mistakes and inefficient
attempts in multi-step task execution. To this end, we introduce Experiential
Co-Learning, a novel LLM-agent learning framework in which instructor and
assistant agents gather shortcut-oriented experiences from their historical
trajectories and use these past experiences for future task execution. The
extensive experiments demonstrate that the framework enables agents to tackle
unseen software-developing tasks more effectively. We anticipate that our
insights will guide LLM agents towards enhanced autonomy and contribute to
their evolutionary growth in cooperative learning. The code and data are
available at https://github.com/OpenBMB/ChatDev.",2023-12-28T13:50:42Z
,http://arxiv.org/pdf/2405.03727v2.pdf,Large Language Models Synergize with Automated Machine Learning,"Recently, program synthesis driven by large language models (LLMs) has become
increasingly popular. However, program synthesis for machine learning (ML)
tasks still poses significant challenges. This paper explores a novel form of
program synthesis, targeting ML programs, by combining LLMs and automated
machine learning (autoML). Specifically, our goal is to fully automate the
generation and optimization of the code of the entire ML workflow, from data
preparation to modeling and post-processing, utilizing only textual
descriptions of the ML tasks. To manage the length and diversity of ML
programs, we propose to break each ML program into smaller, manageable parts.
Each part is generated separately by the LLM, with careful consideration of
their compatibilities. To ensure compatibilities, we design a testing technique
for ML programs. Unlike traditional program synthesis, which typically relies
on binary evaluations (i.e., correct or incorrect), evaluating ML programs
necessitates more than just binary judgments. Therefore, we further assess ML
programs numerically and select the optimal programs from a range of candidates
using AutoML methods. In experiments across various ML tasks, our method
outperforms existing methods in 10 out of 12 tasks for generating ML programs.
In addition, autoML significantly improves the performance of the generated ML
programs. In experiments, given the textual task description, our method,
Text-to-ML, generates the complete and optimized ML program in a fully
autonomous process.",2024-05-06T08:09:46Z
,http://arxiv.org/pdf/2406.07737v1.pdf,The Future of Software Engineering in an AI-Driven World,"A paradigm shift is underway in Software Engineering, with AI systems such as
LLMs gaining increasing importance for improving software development
productivity. This trend is anticipated to persist. In the next five years, we
will likely see an increasing symbiotic partnership between human developers
and AI. The Software Engineering research community cannot afford to overlook
this trend; we must address the key research challenges posed by the
integration of AI into the software development process. In this paper, we
present our vision of the future of software development in an AI-Driven world
and explore the key challenges that our research community should address to
realize this vision.",2024-06-11T21:46:19Z
,http://arxiv.org/pdf/2404.11595v3.pdf,"A Deep Dive into Large Language Models for Automated Bug Localization
  and Repair","Large language models (LLMs) have shown impressive effectiveness in various
software engineering tasks, including automated program repair (APR). In this
study, we take a deep dive into automated bug fixing utilizing LLMs. In
contrast to many deep learning-based APR methods that assume known bug
locations, rely on line-level localization tools, or address bug prediction and
fixing in one step, our approach uniquely employs LLMs to predict bug location
at the token level and subsequently utilizes them for bug fixing. This
methodological separation of bug localization and fixing using different LLMs
enables effective integration of diverse contextual information and improved
incorporation of inductive biases. We introduce Toggle: Token-Granulated Bug
Localization and Repair, a comprehensive program repair framework that
integrates a bug localization model, an adjustment unit, and a bug-fixing
model. Toggle takes a buggy function as input and generates a complete
corrected function. We investigate various styles of prompting to the bug
fixing model to identify the most effective prompts that better utilize the
inductive bias and significantly outperform others. Toggle achieves the new
state-of-the-art (SOTA) performance on the CodeXGLUE code refinement benchmark,
and exhibits better and comparable performance on several other widely-used APR
datasets, including Defects4J.",2024-04-17T17:48:18Z
,http://arxiv.org/pdf/2405.19888v1.pdf,"Parrot: Efficient Serving of LLM-based Applications with Semantic
  Variable","The rise of large language models (LLMs) has enabled LLM-based applications
(a.k.a. AI agents or co-pilots), a new software paradigm that combines the
strength of LLM and conventional software. Diverse LLM applications from
different tenants could design complex workflows using multiple LLM requests to
accomplish one task. However, they have to use the over-simplified
request-level API provided by today's public LLM services, losing essential
application-level information. Public LLM services have to blindly optimize
individual LLM requests, leading to sub-optimal end-to-end performance of LLM
applications.
  This paper introduces Parrot, an LLM service system that focuses on the
end-to-end experience of LLM-based applications. Parrot proposes Semantic
Variable, a unified abstraction to expose application-level knowledge to public
LLM services. A Semantic Variable annotates an input/output variable in the
prompt of a request, and creates the data pipeline when connecting multiple LLM
requests, providing a natural way to program LLM applications. Exposing
Semantic Variables to the public LLM service allows it to perform conventional
data flow analysis to uncover the correlation across multiple LLM requests.
This correlation opens a brand-new optimization space for the end-to-end
performance of LLM-based applications. Extensive evaluations demonstrate that
Parrot can achieve up to an order-of-magnitude improvement for popular and
practical use cases of LLM applications.",2024-05-30T09:46:36Z
,http://arxiv.org/pdf/2402.03396v1.pdf,"UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large
  Language Models for Program Testing","The remarkable capability of large language models (LLMs) in generating
high-quality code has drawn increasing attention in the software testing
community. However, existing code LLMs often demonstrate unsatisfactory
capabilities in generating accurate and complete tests since they were trained
on code snippets collected without differentiating between code for testing
purposes and other code. In this paper, we present a large-scale dataset
UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test
Synthesis. Associating tests with the tested functions is crucial for LLMs to
infer the expected behavior and the logic paths to be verified. By leveraging
Language Server Protocol, UniTSyn achieves the challenging goal of collecting
focal-test pairs without per-project execution setups or per-language
heuristics that tend to be fragile and difficult to scale. It contains 2.7
million focal-test pairs across five mainstream programming languages, making
it possible to be utilized for enhancing the test generation ability of LLMs.
The details of UniTSyn can be found in Table 1. Our experiments demonstrate
that, by building an autoregressive model based on UniTSyn, we can achieve
significant benefits in learning and understanding unit test representations,
resulting in improved generation accuracy and code coverage across all
evaluated programming languages. Code and data will be publicly available.",2024-02-04T22:48:05Z
,http://arxiv.org/pdf/2310.15317v1.pdf,"Exploring the Potential of Large Language Models in Generating
  Code-Tracing Questions for Introductory Programming Courses","In this paper, we explore the application of large language models (LLMs) for
generating code-tracing questions in introductory programming courses. We
designed targeted prompts for GPT4, guiding it to generate code-tracing
questions based on code snippets and descriptions. We established a set of
human evaluation metrics to assess the quality of questions produced by the
model compared to those created by human experts. Our analysis provides
insights into the capabilities and potential of LLMs in generating diverse
code-tracing questions. Additionally, we present a unique dataset of human and
LLM-generated tracing questions, serving as a valuable resource for both the
education and NLP research communities. This work contributes to the ongoing
dialogue on the potential uses of LLMs in educational settings.",2023-10-23T19:35:01Z
10.1145/3657604.3662036,http://arxiv.org/pdf/2404.13414v3.pdf,"Evaluating the Effectiveness of LLMs in Introductory Computer Science
  Education: A Semester-Long Field Study","The integration of AI assistants, especially through the development of Large
Language Models (LLMs), into computer science education has sparked significant
debate. An emerging body of work has looked into using LLMs in education, but
few have examined the impacts of LLMs on students in entry-level programming
courses, particularly in real-world contexts and over extended periods. To
address this research gap, we conducted a semester-long, between-subjects study
with 50 students using CodeTutor, an LLM-powered assistant developed by our
research team. Our study results show that students who used CodeTutor (the
experimental group) achieved statistically significant improvements in their
final scores compared to peers who did not use the tool (the control group).
Within the experimental group, those without prior experience with LLM-powered
tools demonstrated significantly greater performance gain than their
counterparts. We also found that students expressed positive feedback regarding
CodeTutor's capability, though they also had concerns about CodeTutor's limited
role in developing critical thinking skills. Over the semester, students'
agreement with CodeTutor's suggestions decreased, with a growing preference for
support from traditional human teaching assistants. Our analysis further
reveals that the quality of user prompts was significantly correlated with
CodeTutor's response effectiveness. Building upon our results, we discuss the
implications of our findings for integrating Generative AI literacy into
curricula to foster critical thinking skills and turn to examining the temporal
dynamics of user engagement with LLM-powered tools. We further discuss the
discrepancy between the anticipated functions of tools and students' actual
capabilities, which sheds light on the need for tailored strategies to improve
educational outcomes.",2024-04-20T15:58:22Z
,http://arxiv.org/pdf/2404.18864v1.pdf,Performance-Aligned LLMs for Generating Fast Code,"Optimizing scientific software is a difficult task because codebases are
often large and complex, and performance can depend upon several factors
including the algorithm, its implementation, and hardware among others. Causes
of poor performance can originate from disparate sources and be difficult to
diagnose. Recent years have seen a multitude of work that use large language
models (LLMs) to assist in software development tasks. However, these tools are
trained to model the distribution of code as text, and are not specifically
designed to understand performance aspects of code. In this work, we introduce
a reinforcement learning based methodology to align the outputs of code LLMs
with performance. This allows us to build upon the current code modeling
capabilities of LLMs and extend them to generate better performing code. We
demonstrate that our fine-tuned model improves the expected speedup of
generated code over base models for a set of benchmark tasks from 0.9 to 1.6
for serial code and 1.9 to 4.5 for OpenMP code.",2024-04-29T16:52:38Z
,http://arxiv.org/pdf/2402.01706v1.pdf,"MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse
  Worlds","Large Language Model (LLM) alignment aims to ensure that LLM outputs match
with human values. Researchers have demonstrated the severity of alignment
problems with a large spectrum of jailbreak techniques that can induce LLMs to
produce malicious content during conversations. Finding the corresponding
jailbreaking prompts usually requires substantial human intelligence or
computation resources. In this paper, we report that LLMs have different levels
of alignment in various contexts. As such, by systematically constructing many
contexts, called worlds, leveraging a Domain Specific Language describing
possible worlds (e.g., time, location, characters, actions and languages) and
the corresponding compiler, we can cost-effectively expose latent alignment
issues. Given the low cost of our method, we are able to conduct a large scale
study regarding LLM alignment issues in different worlds. Our results show that
our method outperforms the-state-of-the-art jailbreaking techniques on both
effectiveness and efficiency. In addition, our results indicate that existing
LLMs are extremely vulnerable to nesting worlds and programming language
worlds. They imply that existing alignment training focuses on the real-world
and is lacking in various (virtual) worlds where LLMs can be exploited.",2024-01-25T02:57:40Z
,http://arxiv.org/pdf/2404.10779v1.pdf,Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations,"There is a compelling necessity from enterprises for fine tuning LLMs (Large
Language Models) o get them trained on proprietary domain knowledge. The
challenge is to imbibe the LLMs with domain specific knowledge using the most
optimial resource and cost and in the best possible time. Many enterprises rely
on RAG (Retrieval Augmented Generation) which does not need LLMs to be
ine-tuned but they are limited by the quality of vector databases and their
retrieval capabilities rather than the intrinsic capabilities of the LLMs
themselves. In our current work we focus on fine tuning LLaMA, an open source
LLM using proprietary documents and code from an enterprise repository and use
the fine tuned models to evaluate the quality of responses. As part of this
work, we aim to guide beginners on how to start with fine tuning an LLM for
documentation and code by making educated guesses on size of GPU required and
options that are available for formatting the data. We also propose pre
processing recipes for both documentation and code to prepare dataset in
different formats. The proposed methods of data preparation for document
datasets are forming paragraph chunks, forming question and answer pairs and
forming keyword and paragraph chunk pairs. For code dataset we propose forming
summary and function pairs. Further, we qualitatively evaluate the results of
the models for domain specific queries. Finally, we also propose practical
guidelines and recommendations for fine tuning LLMs.",2024-03-23T13:25:01Z
,http://arxiv.org/pdf/2308.04898v1.pdf,"An Empirical Study on Using Large Language Models to Analyze Software
  Supply Chain Security Failures","As we increasingly depend on software systems, the consequences of breaches
in the software supply chain become more severe. High-profile cyber attacks
like those on SolarWinds and ShadowHammer have resulted in significant
financial and data losses, underlining the need for stronger cybersecurity. One
way to prevent future breaches is by studying past failures. However,
traditional methods of analyzing these failures require manually reading and
summarizing reports about them. Automated support could reduce costs and allow
analysis of more failures. Natural Language Processing (NLP) techniques such as
Large Language Models (LLMs) could be leveraged to assist the analysis of
failures. In this study, we assessed the ability of Large Language Models
(LLMs) to analyze historical software supply chain breaches. We used LLMs to
replicate the manual analysis of 69 software supply chain security failures
performed by members of the Cloud Native Computing Foundation (CNCF). We
developed prompts for LLMs to categorize these by four dimensions: type of
compromise, intent, nature, and impact. GPT 3.5s categorizations had an average
accuracy of 68% and Bard had an accuracy of 58% over these dimensions. We
report that LLMs effectively characterize software supply chain failures when
the source articles are detailed enough for consensus among manual analysts,
but cannot yet replace human analysts. Future work can improve LLM performance
in this context, and study a broader range of articles and failures.",2023-08-09T15:35:14Z
,http://arxiv.org/pdf/2406.08316v2.pdf,Is Programming by Example solved by LLMs?,"Programming-by-Examples (PBE) aims to generate an algorithm from input-output
examples. Such systems are practically and theoretically important: from an
end-user perspective, they are deployed to millions of people, and from an AI
perspective, PBE corresponds to a very general form of few-shot inductive
inference. Given the success of Large Language Models (LLMs) in code-generation
tasks, we investigate here the extent to which LLMs can be said to have
`solved' PBE. We experiment on classic domains such as lists and strings, and
an uncommon graphics programming domain not well represented in typical
pretraining data. We find that pretrained models are not effective at PBE, but
that they can be fine-tuned for much higher performance, provided the test
problems are in-distribution. We analyze empirically what causes these models
to succeed and fail, and take steps toward understanding how to achieve better
out-of-distribution generalization. Collectively these results suggest that
LLMs make strong progress toward solving the typical suite of PBE tasks,
potentially increasing the flexibility and applicability of PBE systems, while
also identifying ways in which LLMs still fall short.",2024-06-12T15:16:40Z
,http://arxiv.org/pdf/2401.00757v1.pdf,"A & B == B & A: Triggering Logical Reasoning Failures in Large Language
  Models","Recent advancements in large language models (LLMs) have propelled Artificial
Intelligence (AI) to new heights, enabling breakthroughs in various tasks such
as writing assistance, code generation, and machine translation. A significant
distinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to
""reason."" However, evaluating the reasoning ability of LLMs remains a challenge
as most existing evaluations focus on their accuracy on the downstream tasks
rather than directly assessing their reasoning processes. Efforts have been
made to develop benchmarks and metrics to assess reasoning in LLMs, but they
suffer from data leakage or limited scope. In this paper, we introduce
LogicAsker, an automatic approach that comprehensively evaluates and improves
the logical reasoning abilities of LLMs under a set of atomic reasoning skills
based on propositional and predicate logic. The results provide insights into
LLMs' reasoning abilities and reveal the logical rules the LLMs did not learn
well. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,
ChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases
from LogicAsker can find logical reasoning failures in different LLMs with a
rate of 25\% - 94\%. In addition, the test cases of LogicAsker can be further
used to design demonstration examples for in-context learning, which
effectively improves the logical reasoning ability of LLMs, e.g., 10\% for
GPT-4. As far as we know, our work is the first to create prompts based on
testing results to improve LLMs' formal reasoning ability effectively. All the
code, data, and results will be released for reproduction and future research.",2024-01-01T13:53:53Z
,http://arxiv.org/pdf/2310.06646v1.pdf,"Forgetful Large Language Models: Lessons Learned from Using LLMs in
  Robot Programming","Large language models offer new ways of empowering people to program robot
applications-namely, code generation via prompting. However, the code generated
by LLMs is susceptible to errors. This work reports a preliminary exploration
that empirically characterizes common errors produced by LLMs in robot
programming. We categorize these errors into two phases: interpretation and
execution. In this work, we focus on errors in execution and observe that they
are caused by LLMs being ""forgetful"" of key information provided in user
prompts. Based on this observation, we propose prompt engineering tactics
designed to reduce errors in execution. We then demonstrate the effectiveness
of these tactics with three language models: ChatGPT, Bard, and LLaMA-2.
Finally, we discuss lessons learned from using LLMs in robot programming and
call for the benchmarking of LLM-powered end-user development of robot
applications.",2023-10-10T14:10:39Z
,http://arxiv.org/pdf/2306.00029v1.pdf,CodeTF: One-stop Transformer Library for State-of-the-art Code LLM,"Code intelligence plays a key role in transforming modern software
engineering. Recently, deep learning-based models, especially Transformer-based
large language models (LLMs), have demonstrated remarkable potential in
tackling these tasks by leveraging massive open-source code data and
programming language features. However, the development and deployment of such
models often require expertise in both machine learning and software
engineering, creating a barrier for the model adoption. In this paper, we
present CodeTF, an open-source Transformer-based library for state-of-the-art
Code LLMs and code intelligence. Following the principles of modular design and
extensible framework, we design CodeTF with a unified interface to enable rapid
access and development across different types of models, datasets and tasks.
Our library supports a collection of pretrained Code LLM models and popular
code benchmarks, including a standardized interface to train and serve code
LLMs efficiently, and data features such as language-specific parsers and
utility functions for extracting code attributes. In this paper, we describe
the design principles, the architecture, key modules and components, and
compare with other related library tools. Finally, we hope CodeTF is able to
bridge the gap between machine learning/generative AI and software engineering,
providing a comprehensive open-source solution for developers, researchers, and
practitioners.",2023-05-31T05:24:48Z
,http://arxiv.org/pdf/2307.05950v2.pdf,"Exploring the Effectiveness of LLMs in Automated Logging Generation: An
  Empirical Study","Automated logging statement generation supports developers in documenting
critical software runtime behavior. Given the great success in natural language
generation and programming language comprehension, large language models (LLMs)
might help developers generate logging statements, but this has not yet been
investigated. To fill the gap, this paper performs the first study on exploring
LLMs for logging statement generation.We first build a logging statement
generation dataset, LogBench, with two parts: (1) LogBench-O: logging
statements collected from GitHub repositories, and (2) LogBench-T: the
transformed unseen code from LogBench-O. Then, we leverage LogBench to evaluate
the effectiveness and generalization capabilities (using LogBench-T) of eleven
top-performing LLMs. In addition, we examine the performance of these LLMs
against classical retrieval-based and machine learning-based logging methods
from the era preceding LLMs. We further evaluate LLM's logging generalization
capabilities using unseen data (LogBench-T) derived from code transformation
techniques. While existing LLMs deliver decent predictions on logging levels
and logging variables, our study indicates that they only achieve a maximum
BLEU score of 0.249, thus calling for improvements. The paper also highlights
the importance of prompt constructions and external factors (e.g., programming
contexts and code comments) for LLMs' logging performance. Based on these
findings, we identify five implications and provide practical advice for future
logging research. Our empirical analysis discloses the limitations of current
logging approaches while showcasing the potential of LLM-based logging tools,
and provides actionable guidance for building more practical models.",2023-07-12T06:32:51Z
10.1109/ACCESS.2024.3391815,http://arxiv.org/pdf/2403.14965v1.pdf,"Comprehensive Evaluation and Insights into the Use of Large Language
  Models in the Automation of Behavior-Driven Development Acceptance Test
  Formulation","Behavior-driven development (BDD) is an Agile testing methodology fostering
collaboration among developers, QA analysts, and stakeholders. In this
manuscript, we propose a novel approach to enhance BDD practices using large
language models (LLMs) to automate acceptance test generation. Our study uses
zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B,
and PaLM-2. The paper presents a detailed methodology that includes the
dataset, prompt techniques, LLMs, and the evaluation process. The results
demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests
with better performance. The few-shot prompt technique highlights its ability
to provide higher accuracy by incorporating examples for in-context learning.
Furthermore, the study examines syntax errors, validation accuracy, and
comparative analysis of LLMs, revealing their effectiveness in enhancing BDD
practices. However, our study acknowledges that there are limitations to the
proposed approach. We emphasize that this approach can support collaborative
BDD processes and create opportunities for future research into automated BDD
acceptance test generation using LLMs.",2024-03-22T05:37:52Z
,http://arxiv.org/pdf/2405.11581v2.pdf,DOLLmC: DevOps for Large Language model Customization,"The rapid integration of Large Language Models (LLMs) into various industries
presents both revolutionary opportunities and unique challenges. This research
aims to establish a scalable and efficient framework for LLM customization,
exploring how DevOps practices should be adapted to meet the specific demands
of LLM customization. By integrating ontologies, knowledge maps, and prompt
engineering into the DevOps pipeline, we propose a robust framework that
enhances continuous learning, seamless deployment, and rigorous version control
of LLMs. This methodology is demonstrated through the development of a
domain-specific chatbot for the agricultural sector, utilizing heterogeneous
data to deliver actionable insights. The proposed methodology, so called
DOLLmC, not only addresses the immediate challenges of LLM customization but
also promotes scalability and operational efficiency. However, the
methodology's primary limitation lies in the need for extensive testing,
validation, and broader adoption across different domains.",2024-05-19T15:20:27Z
,http://arxiv.org/pdf/2405.17743v2.pdf,ORLM: Training Large Language Models for Optimization Modeling,"Large Language Models (LLMs) have emerged as powerful tools for tackling
complex Operations Research (OR) problem by providing the capacity in
automating optimization modeling. However, current methodologies heavily rely
on prompt engineering (e.g., multi-agent cooperation) with proprietary LLMs,
raising data privacy concerns that could be prohibitive in industry
applications. To tackle this issue, we propose training open-source LLMs for
optimization modeling. We identify four critical requirements for the training
dataset of OR LLMs, design and implement OR-Instruct, a semi-automated process
for creating synthetic data tailored to specific requirements. We also
introduce the IndustryOR benchmark, the first industrial benchmark for testing
LLMs on solving real-world OR problems. We apply the data from OR-Instruct to
various open-source LLMs of 7b size (termed as ORLMs), resulting in a
significantly improved capability for optimization modeling. Our
best-performing ORLM achieves state-of-the-art performance on the NL4OPT, MAMO,
and IndustryOR benchmarks. Our code and data are available at
\url{https://github.com/Cardinal-Operations/ORLM}.",2024-05-28T01:55:35Z
,http://arxiv.org/pdf/2305.18365v3.pdf,"What can Large Language Models do in chemistry? A comprehensive
  benchmark on eight tasks","Large Language Models (LLMs) with strong abilities in natural language
processing tasks have emerged and have been applied in various kinds of areas
such as science, finance and software engineering. However, the capability of
LLMs to advance the field of chemistry remains unclear. In this paper, rather
than pursuing state-of-the-art performance, we aim to evaluate capabilities of
LLMs in a wide range of tasks across the chemistry domain. We identify three
key chemistry-related capabilities including understanding, reasoning and
explaining to explore in LLMs and establish a benchmark containing eight
chemistry tasks. Our analysis draws on widely recognized datasets facilitating
a broad exploration of the capacities of LLMs within the context of practical
chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are
evaluated for each chemistry task in zero-shot and few-shot in-context learning
settings with carefully selected demonstration examples and specially crafted
prompts. Our investigation found that GPT-4 outperformed other models and LLMs
exhibit different competitive levels in eight chemistry tasks. In addition to
the key findings from the comprehensive benchmark analysis, our work provides
insights into the limitation of current LLMs and the impact of in-context
learning settings on LLMs' performance across various chemistry tasks. The code
and datasets used in this study are available at
https://github.com/ChemFoundationModels/ChemLLMBench.",2023-05-27T14:17:33Z
,http://arxiv.org/pdf/2404.04603v1.pdf,Analyzing LLM Usage in an Advanced Computing Class in India,"This paper investigates the usage patterns of undergraduate and graduate
students when engaging with large language models (LLMs) to tackle programming
assignments in the context of advanced computing courses. Existing work
predominantly focuses on the influence of LLMs in introductory programming
contexts. Additionally, there is a scarcity of studies analyzing actual
conversations between students and LLMs. Our study provides a comprehensive
quantitative and qualitative analysis of raw interactions between students and
LLMs within an advanced computing course (Distributed Systems) at an Indian
University. We further complement this by conducting student interviews to gain
deeper insights into their usage patterns. Our study shows that students make
use of large language models (LLMs) in various ways: generating code or
debugging code by identifying and fixing errors. They also copy and paste
assignment descriptions into LLM interfaces for specific solutions, ask
conceptual questions about complex programming ideas or theoretical concepts,
and generate test cases to check code functionality and robustness. Our
analysis includes over 4,000 prompts from 411 students and conducting
interviews with 10 students. Our analysis shows that LLMs excel at generating
boilerplate code and assisting in debugging, while students handle the
integration of components and system troubleshooting. This aligns with the
learning objectives of advanced computing courses, which are oriented towards
teaching students how to build systems and troubleshoot, with less emphasis on
generating code from scratch. Therefore, LLM tools can be leveraged to increase
student productivity, as shown by the data we collected. This study contributes
to the ongoing discussion on LLM use in education, advocating for their
usefulness in advanced computing courses to complement higher-level learning
and productivity.",2024-04-06T12:06:56Z
,http://arxiv.org/pdf/2405.01553v1.pdf,"Empirical Studies of Parameter Efficient Methods for Large Language
  Models of Code and Knowledge Transfer to R","Recently, Large Langauge Models (LLMs) have gained a lot of attention in the
Software Engineering (SE) community. LLMs or their variants pre-trained on code
are used for many SE tasks. A main approach for adapting LLMs to the downstream
task is to fine-tune the models. However, with having billions-parameters-LLMs,
fine-tuning the models is not practical. An alternative approach is using
Parameter Efficient Fine Tuning (PEFT), in which the model parameters are
frozen and only a few added parameters are trained. Though the LLMs are used
for programming languages such as Python and Java widely, their capability for
low-resource languages is limited. In this work, we empirically study PEFT
methods, LoRA and Compacter, on CodeT5 and CodeLlama. We will assess their
performance compared to fully fine-tuned models, whether they can be used for
knowledge transfer from natural language models to code (using T5 and Llama
models), and their ability to adapt the learned knowledge to an unseen
language. For the unseen language, we aim to study R, as it has a wide
community. The adaptability with less computational costs makes LLMs accessible
in scenarios where heavy computational resources are not available. Moreover,
studying R opens new opportunities for using LLMs for other languages. We
anticipate our findings to showcase the capabilities of PEFT for code LLMs for
R and reveal the improvement areas.",2024-03-16T03:12:45Z
,http://arxiv.org/pdf/2401.03374v2.pdf,"LLM-Powered Code Vulnerability Repair with Reinforcement Learning and
  Semantic Reward","In software development, the predominant emphasis on functionality often
supersedes security concerns, a trend gaining momentum with AI-driven
automation tools like GitHub Copilot. These tools significantly improve
developers' efficiency in functional code development. Nevertheless, it remains
a notable concern that such tools are also responsible for creating insecure
code, predominantly because of pre-training on publicly available repositories
with vulnerable code. Moreover, developers are called the ""weakest link in the
chain"" since they have very minimal knowledge of code security. Although
existing solutions provide a reasonable solution to vulnerable code, they must
adequately describe and educate the developers on code security to ensure that
the security issues are not repeated. Therefore we introduce a multipurpose
code vulnerability analysis system \texttt{SecRepair}, powered by a large
language model, CodeGen2 assisting the developer in identifying and generating
fixed code along with a complete description of the vulnerability with a code
comment. Our innovative methodology uses a reinforcement learning paradigm to
generate code comments augmented by a semantic reward mechanism. Inspired by
how humans fix code issues, we propose an instruction-based dataset suitable
for vulnerability analysis with LLMs. We further identify zero-day and N-day
vulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findings
underscore that incorporating reinforcement learning coupled with semantic
reward augments our model's performance, thereby fortifying its capacity to
address code vulnerabilities with improved efficacy.",2024-01-07T02:46:39Z
,http://arxiv.org/pdf/2401.02985v1.pdf,"Evaluating Large Language Models on the GMAT: Implications for the
  Future of Business Education","The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.",2024-01-02T03:54:50Z
10.1109/APSEC60848.2023.00025,http://arxiv.org/pdf/2311.11690v1.pdf,Refactoring Programs Using Large Language Models with Few-Shot Examples,"A less complex and more straightforward program is a crucial factor that
enhances its maintainability and makes writing secure and bug-free programs
easier. However, due to its heavy workload and the risks of breaking the
working programs, programmers are reluctant to do code refactoring, and thus,
it also causes the loss of potential learning experiences. To mitigate this, we
demonstrate the application of using a large language model (LLM), GPT-3.5, to
suggest less complex versions of the user-written Python program, aiming to
encourage users to learn how to write better programs. We propose a method to
leverage the prompting with few-shot examples of the LLM by selecting the
best-suited code refactoring examples for each target programming problem based
on the prior evaluation of prompting with the one-shot example. The
quantitative evaluation shows that 95.68% of programs can be refactored by
generating 10 candidates each, resulting in a 17.35% reduction in the average
cyclomatic complexity and a 25.84% decrease in the average number of lines
after filtering only generated programs that are semantically correct.
Furthermore, the qualitative evaluation shows outstanding capability in code
formatting, while unnecessary behaviors such as deleting or translating
comments are also observed.",2023-11-20T11:43:45Z
,http://arxiv.org/pdf/2212.14834v4.pdf,"Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-Learning
  Libraries via Large Language Models","Detecting bugs in Deep Learning (DL) libraries (e.g., TensorFlow/PyTorch) is
critical for almost all downstream DL systems in ensuring effectiveness/safety
for end users. Meanwhile, traditional fuzzing techniques can be hardly
effective for such a challenging domain since the input DL programs need to
satisfy both the input language (e.g., Python) syntax/semantics and the DL API
input/shape constraints for tensor computations.
  To address these limitations, we propose TitanFuzz - the first approach to
directly leveraging Large Language Models (LLMs) to generate input programs for
fuzzing DL libraries. LLMs are titanic models trained on billions of code
snippets and can auto-regressively generate human-like code snippets. Our key
insight is that modern LLMs can also include numerous code snippets invoking DL
library APIs in their training corpora, and thus can implicitly learn both
language syntax/semantics and intricate DL API constraints for valid DL program
generation. More specifically, we use both generative and infilling LLMs (e.g.,
Codex/InCoder) to generate and mutate valid/diverse input DL programs for
fuzzing. Our experimental results demonstrate that TitanFuzz can achieve
30.38%/50.84% higher code coverage than state-of-the-art fuzzers on
TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 41
already confirmed as previously unknown bugs.
  This paper demonstrates that modern titanic LLMs can be leveraged to directly
perform both generation-based and mutation-based fuzzing studied for decades,
while being fully automated, generalizable, and applicable to domains
challenging for traditional approaches (such as DL systems). We hope TitanFuzz
can stimulate more work in this promising direction of LLMs for fuzzing.",2022-12-30T17:25:11Z
,http://arxiv.org/pdf/2406.12655v1.pdf,"Benchmarks and Metrics for Evaluations of Code Generation: A Critical
  Review","With the rapid development of Large Language Models (LLMs), a large number of
machine learning models have been developed to assist programming tasks
including the generation of program code from natural language input. However,
how to evaluate such LLMs for this task is still an open problem despite of the
great amount of research efforts that have been made and reported to evaluate
and compare them. This paper provides a critical review of the existing work on
the testing and evaluation of these tools with a focus on two key aspects: the
benchmarks and the metrics used in the evaluations. Based on the review,
further research directions are discussed.",2024-06-18T14:25:34Z
,http://arxiv.org/pdf/2405.12750v1.pdf,"Generative AI and Large Language Models for Cyber Security: All Insights
  You Need","This paper provides a comprehensive review of the future of cybersecurity
through Generative AI and Large Language Models (LLMs). We explore LLM
applications across various domains, including hardware design security,
intrusion detection, software engineering, design verification, cyber threat
intelligence, malware detection, and phishing detection. We present an overview
of LLM evolution and its current state, focusing on advancements in models such
as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends
to LLM vulnerabilities, such as prompt injection, insecure output handling,
data poisoning, DDoS attacks, and adversarial instructions. We delve into
mitigation strategies to protect these models, providing a comprehensive look
at potential attack scenarios and prevention techniques. Furthermore, we
evaluate the performance of 42 LLM models in cybersecurity knowledge and
hardware security, highlighting their strengths and weaknesses. We thoroughly
evaluate cybersecurity datasets for LLM training and testing, covering the
lifecycle from data creation to usage and identifying gaps for future research.
In addition, we review new strategies for leveraging LLMs, including techniques
like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human
Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank
Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim
to enhance real-time cybersecurity defenses and improve the sophistication of
LLM applications in threat detection and response. Our paper provides a
foundational understanding and strategic direction for integrating LLMs into
future cybersecurity frameworks, emphasizing innovation and robust model
deployment to safeguard against evolving cyber threats.",2024-05-21T13:02:27Z
,http://arxiv.org/pdf/2311.14722v1.pdf,"Zero-Shot Question Answering over Financial Documents using Large
  Language Models","We introduce a large language model (LLM) based approach to answer complex
questions requiring multi-hop numerical reasoning over financial reports. While
LLMs have exhibited remarkable performance on various natural language and
reasoning tasks, complex reasoning problems often rely on few-shot prompts that
require carefully crafted examples. In contrast, our approach uses novel
zero-shot prompts that guide the LLM to encode the required reasoning into a
Python program or a domain specific language. The generated program is then
executed by a program interpreter, thus mitigating the limitations of LLM in
performing accurate arithmetic calculations.
  We evaluate the proposed approach on three financial datasets using some of
the recently developed generative pretrained transformer (GPT) models and
perform comparisons with various zero-shot baselines. The experimental results
demonstrate that our approach significantly improves the accuracy for all the
LLMs over their respective baselines. We provide a detailed analysis of the
results, generating insights to support our findings. The success of our
approach demonstrates the enormous potential to extract complex domain specific
numerical reasoning by designing zero-shot prompts to effectively exploit the
knowledge embedded in LLMs.",2023-11-19T16:23:34Z
,http://arxiv.org/pdf/2401.07031v2.pdf,"Code Security Vulnerability Repair Using Reinforcement Learning with
  Large Language Models","With the recent advancement of Large Language Models (LLMs), generating
functionally correct code has become less complicated for a wide array of
developers. While using LLMs has sped up the functional development process, it
poses a heavy risk to code security. Code generation with proper security
measures using LLM is a significantly more challenging task than functional
code generation. Security measures may include adding a pair of lines of code
with the original code, consisting of null pointer checking or prepared
statements for SQL injection prevention. Currently, available code repair LLMs
generate code repair by supervised fine-tuning, where the model looks at
cross-entropy loss. However, the original and repaired codes are mostly similar
in functionality and syntactically, except for a few (1-2) lines, which act as
security measures. This imbalance between the lines needed for security
measures and the functional code enforces the supervised fine-tuned model to
prioritize generating functional code without adding proper security measures,
which also benefits the model by resulting in minimal loss. Therefore, in this
work, for security hardening and strengthening of generated code from LLMs, we
propose a reinforcement learning-based method for program-specific repair with
the combination of semantic and syntactic reward mechanisms that focus heavily
on adding security and functional measures in the code, respectively.",2024-01-13T10:19:26Z
,http://arxiv.org/pdf/2403.11671v1.pdf,HDLdebugger: Streamlining HDL debugging with Large Language Models,"In the domain of chip design, Hardware Description Languages (HDLs) play a
pivotal role. However, due to the complex syntax of HDLs and the limited
availability of online resources, debugging HDL codes remains a difficult and
time-intensive task, even for seasoned engineers. Consequently, there is a
pressing need to develop automated HDL code debugging models, which can
alleviate the burden on hardware engineers. Despite the strong capabilities of
Large Language Models (LLMs) in generating, completing, and debugging software
code, their utilization in the specialized field of HDL debugging has been
limited and, to date, has not yielded satisfactory results. In this paper, we
propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which
consists of HDL debugging data generation via a reverse engineering approach, a
search engine for retrieval-augmented generation, and a retrieval-augmented LLM
fine-tuning approach. Through the integration of these components, HDLdebugger
can automate and streamline HDL debugging for chip design. Our comprehensive
experiments, conducted on an HDL code dataset sourced from Huawei, reveal that
HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional
effectiveness in HDL code debugging.",2024-03-18T11:19:37Z
,http://arxiv.org/pdf/2406.04568v1.pdf,"StackSight: Unveiling WebAssembly through Large Language Models and
  Neurosymbolic Chain-of-Thought Decompilation","WebAssembly enables near-native execution in web applications and is
increasingly adopted for tasks that demand high performance and robust
security. However, its assembly-like syntax, implicit stack machine, and
low-level data types make it extremely difficult for human developers to
understand, spurring the need for effective WebAssembly reverse engineering
techniques. In this paper, we propose StackSight, a novel neurosymbolic
approach that combines Large Language Models (LLMs) with advanced program
analysis to decompile complex WebAssembly code into readable C++ snippets.
StackSight visualizes and tracks virtual stack alterations via a static
analysis algorithm and then applies chain-of-thought prompting to harness LLM's
complex reasoning capabilities. Evaluation results show that StackSight
significantly improves WebAssembly decompilation. Our user study also
demonstrates that code snippets generated by StackSight have significantly
higher win rates and enable a better grasp of code semantics.",2024-06-07T01:08:17Z
,http://arxiv.org/pdf/2306.04556v1.pdf,"StudentEval: A Benchmark of Student-Written Prompts for Large Language
  Models of Code","Code LLMs are being rapidly deployed and there is evidence that they can make
professional programmers more productive. Current benchmarks for code
generation measure whether models generate correct programs given an expert
prompt. In this paper, we present a new benchmark containing multiple prompts
per problem, written by a specific population of non-expert prompters:
beginning programmers. StudentEval contains 1,749 prompts for 48 problems,
written by 80 students who have only completed one semester of Python
programming. Our students wrote these prompts while working interactively with
a Code LLM, and we observed very mixed success rates. We use StudentEval to
evaluate 5 Code LLMs and find that StudentEval is a better discriminator of
model performance than existing benchmarks. We analyze the prompts and find
significant variation in students' prompting techniques. We also find that
nondeterministic LLM sampling could mislead students into thinking that their
prompts are more (or less) effective than they actually are, which has
implications for how to teach with Code LLMs.",2023-06-07T16:03:55Z
,http://arxiv.org/pdf/2308.04748v2.pdf,Fuzz4All: Universal Fuzzing with Large Language Models,"Fuzzing has achieved tremendous success in discovering bugs and
vulnerabilities in various software systems. Systems under test (SUTs) that
take in programming or formal language as inputs, e.g., compilers, runtime
engines, constraint solvers, and software libraries with accessible APIs, are
especially important as they are fundamental building blocks of software
development. However, existing fuzzers for such systems often target a specific
language, and thus cannot be easily applied to other languages or even other
versions of the same language. Moreover, the inputs generated by existing
fuzzers are often limited to specific features of the input language, and thus
can hardly reveal bugs related to other or new features. This paper presents
Fuzz4All, the first fuzzer that is universal in the sense that it can target
many different input languages and many different features of these languages.
The key idea behind Fuzz4All is to leverage large language models (LLMs) as an
input generation and mutation engine, which enables the approach to produce
diverse and realistic inputs for any practically relevant language. To realize
this potential, we present a novel autoprompting technique, which creates LLM
prompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop,
which iteratively updates the prompt to create new fuzzing inputs. We evaluate
Fuzz4All on nine systems under test that take in six different languages (C,
C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all six
languages, that universal fuzzing achieves higher coverage than existing,
language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs in
widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit
quantum computing platform, with 64 bugs already confirmed by developers as
previously unknown.",2023-08-09T07:36:21Z
,http://arxiv.org/pdf/2403.17134v1.pdf,"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair","Automated program repair has emerged as a powerful technique to mitigate the
impact of software bugs on system reliability and user experience. This paper
introduces RepairAgent, the first work to address the program repair challenge
through an autonomous agent based on a large language model (LLM). Unlike
existing deep learning-based approaches, which prompt a model with a fixed
prompt or in a fixed feedback loop, our work treats the LLM as an agent capable
of autonomously planning and executing actions to fix bugs by invoking suitable
tools. RepairAgent freely interleaves gathering information about the bug,
gathering repair ingredients, and validating fixes, while deciding which tools
to invoke based on the gathered information and feedback from previous fix
attempts. Key contributions that enable RepairAgent include a set of tools that
are useful for program repair, a dynamically updated prompt format that allows
the LLM to interact with these tools, and a finite state machine that guides
the agent in invoking the tools. Our evaluation on the popular Defects4J
dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164
bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM
imposes an average cost of 270,000 tokens per bug, which, under the current
pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To
the best of our knowledge, this work is the first to present an autonomous,
LLM-based agent for program repair, paving the way for future agent-based
techniques in software engineering.",2024-03-25T19:17:43Z
,http://arxiv.org/pdf/2307.07411v1.pdf,"Detecting LLM-Generated Text in Computing Education: A Comparative Study
  for ChatGPT Cases","Due to the recent improvements and wide availability of Large Language Models
(LLMs), they have posed a serious threat to academic integrity in education.
Modern LLM-generated text detectors attempt to combat the problem by offering
educators with services to assess whether some text is LLM-generated. In this
work, we have collected 124 submissions from computer science students before
the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this
data to evaluate eight publicly-available LLM-generated text detectors through
the measures of accuracy, false positives, and resilience. The purpose of this
work is to inform the community of what LLM-generated text detectors work and
which do not, but also to provide insights for educators to better maintain
academic integrity in their courses. Our results find that CopyLeaks is the
most accurate LLM-generated text detector, GPTKit is the best LLM-generated
text detector to reduce false positives, and GLTR is the most resilient
LLM-generated text detector. We also express concerns over 52 false positives
(of 114 human written submissions) generated by GPTZero. Finally, we note that
all LLM-generated text detectors are less accurate with code, other languages
(aside from English), and after the use of paraphrasing tools (like QuillBot).
Modern detectors are still in need of improvements so that they can offer a
full-proof solution to help maintain academic integrity. Further, their
usability can be improved by facilitating a smooth API integration, providing
clear documentation of their features and the understandability of their
model(s), and supporting more commonly used languages.",2023-07-10T12:18:34Z
,http://arxiv.org/pdf/2310.14053v3.pdf,"Beyond Accuracy: Evaluating Self-Consistency of Code Large Language
  Models with IdentityChain","Code Large Language Models (Code LLMs) are being increasingly employed in
real-life applications, so evaluating them is critical. While the conventional
accuracy evaluates the performance of Code LLMs on a set of individual tasks,
their self-consistency across different tasks is overlooked. Intuitively, a
trustworthy model should be self-consistent when generating natural language
specifications for its own code and generating code for its own specifications.
Failure to preserve self-consistency reveals a lack of understanding of the
shared semantics underlying natural language and programming language, and
therefore undermines the trustworthiness of a model. In this paper, we first
formally define the self-consistency of Code LLMs and then design a framework,
IdentityChain, which effectively and efficiently evaluates the self-consistency
and conventional accuracy of a model at the same time. We study eleven Code
LLMs and show that they fail to preserve self-consistency, which is indeed a
distinct aspect from conventional accuracy. Furthermore, we show that
IdentityChain can be used as a model debugging tool to expose weaknesses of
Code LLMs by demonstrating three major weaknesses that we identify in current
models using IdentityChain. Our code is available at
https://github.com/marcusm117/IdentityChain.",2023-10-21T16:14:56Z
10.1109/AsianHOST59942.2023.10409307,http://arxiv.org/pdf/2401.16448v1.pdf,"LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware
  Debugging","This paper presents LLM4SecHW, a novel framework for hardware debugging that
leverages domain specific Large Language Model (LLM). Despite the success of
LLMs in automating various software development tasks, their application in the
hardware security domain has been limited due to the constraints of commercial
LLMs and the scarcity of domain specific data. To address these challenges, we
propose a unique approach to compile a dataset of open source hardware design
defects and their remediation steps, utilizing version control data. This
dataset provides a substantial foundation for training machine learning models
for hardware. LLM4SecHW employs fine tuning of medium sized LLMs based on this
dataset, enabling the identification and rectification of bugs in hardware
designs. This pioneering approach offers a reference workflow for the
application of fine tuning domain specific LLMs in other research areas. We
evaluate the performance of our proposed system on various open source hardware
designs, demonstrating its efficacy in accurately identifying and correcting
defects. Our work brings a new perspective on automating the quality control
process in hardware design.",2024-01-28T19:45:25Z
,http://arxiv.org/pdf/2307.10348v1.pdf,Code Detection for Hardware Acceleration Using Large Language Models,"Large language models (LLMs) have been massively applied to many tasks, often
surpassing state-of-the-art approaches. While their effectiveness in code
generation has been extensively studied (e.g., AlphaCode), their potential for
code detection remains unexplored.
  This work presents the first analysis of code detection using LLMs. Our study
examines essential kernels, including matrix multiplication, convolution, and
fast-fourier transform, implemented in C/C++. We propose both a preliminary,
naive prompt and a novel prompting strategy for code detection.
  Results reveal that conventional prompting achieves great precision but poor
accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively)
due to a high number of false positives. Our novel prompting strategy
substantially reduces false positives, resulting in excellent overall accuracy
(91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable
challenge to existing state-of-the-art code detection methods.",2023-07-19T17:21:58Z
,http://arxiv.org/pdf/2312.10055v1.pdf,"Next-Step Hint Generation for Introductory Programming Using Large
  Language Models","Large Language Models possess skills such as answering questions, writing
essays or solving programming exercises. Since these models are easily
accessible, researchers have investigated their capabilities and risks for
programming education. This work explores how LLMs can contribute to
programming education by supporting students with automated next-step hints. We
investigate prompt practices that lead to effective next-step hints and use
these insights to build our StAP-tutor. We evaluate this tutor by conducting an
experiment with students, and performing expert assessments. Our findings show
that most LLM-generated feedback messages describe one specific next step and
are personalised to the student's code and approach. However, the hints may
contain misleading information and lack sufficient detail when students
approach the end of the assignment. This work demonstrates the potential for
LLM-generated feedback, but further research is required to explore its
practical implementation.",2023-12-03T17:51:07Z
,http://arxiv.org/pdf/2403.01131v2.pdf,"LLaMoCo: Instruction Tuning of Large Language Models for Optimization
  Code Generation","Recent research explores optimization using large language models (LLMs) by
either iteratively seeking next-step solutions from LLMs or directly prompting
LLMs for an optimizer. However, these approaches exhibit inherent limitations,
including low operational efficiency, high sensitivity to prompt design, and a
lack of domain-specific knowledge. We introduce LLaMoCo, the first
instruction-tuning framework designed to adapt LLMs for solving optimization
problems in a code-to-code manner. Specifically, we establish a comprehensive
instruction set containing well-described problem prompts and effective
optimization codes. We then develop a novel two-phase learning strategy that
incorporates a contrastive learning-based warm-up procedure before the
instruction-tuning phase to enhance the convergence behavior during model
fine-tuning. The experiment results demonstrate that a CodeGen (350M) model
fine-tuned by our LLaMoCo achieves superior optimization performance compared
to GPT-4 Turbo and the other competitors across both synthetic and realistic
problem sets. The fine-tuned model and the usage instructions are available at
https://anonymous.4open.science/r/LLaMoCo-722A.",2024-03-02T08:21:59Z
10.1145/3649217.3653568,http://arxiv.org/pdf/2404.10990v1.pdf,"Automating Personalized Parsons Problems with Customized Contexts and
  Concepts","Parsons problems provide useful scaffolding for introductory programming
students learning to write code. However, generating large numbers of
high-quality Parsons problems that appeal to the diverse range of interests in
a typical introductory course is a significant challenge for educators. Large
language models (LLMs) may offer a solution, by allowing students to produce
on-demand Parsons problems for topics covering the breadth of the introductory
programming curriculum, and targeting thematic contexts that align with their
personal interests. In this paper, we introduce PuzzleMakerPy, an educational
tool that uses an LLM to generate unlimited contextualized drag-and-drop
programming exercises in the form of Parsons Problems, which introductory
programmers can use as a supplemental learning resource. We evaluated
PuzzleMakerPy by deploying it in a large introductory programming course, and
found that the ability to personalize the contextual framing used in problem
descriptions was highly engaging for students, and being able to customize the
programming topics was reported as being useful for their learning.",2024-04-17T02:01:50Z
,http://arxiv.org/pdf/2403.17218v1.pdf,"A Comprehensive Study of the Capabilities of Large Language Models for
  Vulnerability Detection","Large Language Models (LLMs) have demonstrated great potential for code
generation and other software engineering tasks. Vulnerability detection is of
crucial importance to maintaining the security, integrity, and trustworthiness
of software systems. Precise vulnerability detection requires reasoning about
the code, making it a good case study for exploring the limits of LLMs'
reasoning capabilities. Although recent work has applied LLMs to vulnerability
detection using generic prompting techniques, their full capabilities for this
task and the types of errors they make when explaining identified
vulnerabilities remain unclear.
  In this paper, we surveyed eleven LLMs that are state-of-the-art in code
generation and commonly used as coding assistants, and evaluated their
capabilities for vulnerability detection. We systematically searched for the
best-performing prompts, incorporating techniques such as in-context learning
and chain-of-thought, and proposed three of our own prompting methods. Our
results show that while our prompting methods improved the models' performance,
LLMs generally struggled with vulnerability detection. They reported 0.5-0.63
Balanced Accuracy and failed to distinguish between buggy and fixed versions of
programs in 76% of cases on average. By comprehensively analyzing and
categorizing 287 instances of model reasoning, we found that 57% of LLM
responses contained errors, and the models frequently predicted incorrect
locations of buggy code and misidentified bug types. LLMs only correctly
localized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted
correctly by 70-100% of human participants. These findings suggest that despite
their potential for other tasks, LLMs may fail to properly comprehend critical
code structures and security-related concepts. Our data and code are available
at https://figshare.com/s/78fe02e56e09ec49300b.",2024-03-25T21:47:36Z
,http://arxiv.org/pdf/2310.09690v2.pdf,Configuration Validation with Large Language Models,"Misconfigurations are major causes of software failures. Existing practices
rely on developer-written rules or test cases to validate configurations, which
are expensive. Machine learning (ML) for configuration validation is considered
a promising direction, but has been facing challenges such as the need of
large-scale field data and system-specific models. Recent advances in Large
Language Models (LLMs) show promise in addressing some of the long-lasting
limitations of ML-based configuration validation. We present a first analysis
on the feasibility and effectiveness of using LLMs for configuration
validation. We empirically evaluate LLMs as configuration validators by
developing a generic LLM-based configuration validation framework, named Ciri.
Ciri employs effective prompt engineering with few-shot learning based on both
valid configuration and misconfiguration data. Ciri checks outputs from LLMs
when producing results, addressing hallucination and nondeterminism of LLMs. We
evaluate Ciri's validation effectiveness on eight popular LLMs using
configuration data of ten widely deployed open-source systems. Our analysis (1)
confirms the potential of using LLMs for configuration validation, (2) explores
design space of LLMbased validators like Ciri, and (3) reveals open challenges
such as ineffectiveness in detecting certain types of misconfigurations and
biases towards popular configuration parameters.",2023-10-15T00:50:27Z
,http://arxiv.org/pdf/2311.01020v2.pdf,"Exploring the Problems, their Causes and Solutions of AI Pair
  Programming: A Study with Practitioners of GitHub Copilot","With the recent advancement of Artificial Intelligence (AI) and Large
Language Models (LLMs), AI-based code generation tools become a practical
solution for software development. GitHub Copilot, the AI pair programmer,
utilizes machine learning models trained on a large corpus of code snippets to
generate code suggestions using natural language processing. Despite its
popularity in software development, there is limited empirical evidence on the
actual experiences of practitioners who work with Copilot. To this end, we
conducted an empirical study to understand the problems that practitioners face
when using Copilot, as well as their underlying causes and potential solutions.
We collected data from 476 GitHub issues, 706 GitHub discussions, and 142 Stack
Overflow posts. Our results reveal that (1) Operation Issue and Compatibility
Issue are the most common problems faced by Copilot users, (2) Copilot Internal
Error, Network Connection Error, and Editor/IDE Compatibility Issue are
identified as the most frequent causes, and (3) Bug Fixed by Copilot, Modify
Configuration/Setting, and Use Suitable Version are the predominant solutions.
Based on the results, we discuss the potential areas of Copilot for
enhancement, and provide the implications for the Copilot users, the Copilot
team, and researchers.",2023-11-02T06:24:38Z
,http://arxiv.org/pdf/2308.11873v2.pdf,"Dcc --help: Generating Context-Aware Compiler Error Explanations with
  Large Language Models","In the challenging field of introductory programming, high enrollments and
failure rates drive us to explore tools and systems to enhance student
outcomes, especially automated tools that scale to large cohorts. This paper
presents and evaluates the dcc --help tool, an integration of a Large Language
Model (LLM) into the Debugging C Compiler (DCC) to generate unique,
novice-focused explanations tailored to each error. dcc --help prompts an LLM
with contextual information of compile- and run-time error occurrences,
including the source code, error location and standard compiler error message.
The LLM is instructed to generate novice-focused, actionable error explanations
and guidance, designed to help students understand and resolve problems without
providing solutions. dcc --help was deployed to our CS1 and CS2 courses, with
2,565 students using the tool over 64,000 times in ten weeks. We analysed a
subset of these error/explanation pairs to evaluate their properties, including
conceptual correctness, relevancy, and overall quality. We found that the
LLM-generated explanations were conceptually accurate in 90% of compile-time
and 75% of run-time cases, but often disregarded the instruction not to provide
solutions in code. Our findings, observations and reflections following
deployment indicate that dcc-help provides novel opportunities for scaffolding
students' introduction to programming.",2023-08-23T02:36:19Z
,http://arxiv.org/pdf/2406.07714v2.pdf,LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing,"Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in
programs. However, randomized mutation strategies have limited the fuzzer's
performance on structured data. Specialized fuzzers can handle complex
structured data, but require additional efforts in grammar and suffer from low
throughput.
  In this paper, we explore the potential of utilizing the Large Language Model
to enhance greybox fuzzing for structured data. We utilize the pre-trained
knowledge of LLM about data conversion and format to generate new valid inputs.
We further fine-tuned it with paired mutation seeds to learn structured format
and mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ,
integrates the power of LLM to understand and mutate structured data to
fuzzing. We conduct experiments on the standard bug-based benchmark Magma and a
wide variety of real-world programs. LLAMAFUZZ outperforms our top competitor
by 41 bugs on average. We also identified 47 unique bugs across all trials.
Moreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and
bug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in
real-world program sets on average. We also demonstrate a case study to explain
how LLMs enhance the fuzzing process in terms of code coverage.",2024-06-11T20:48:28Z
,http://arxiv.org/pdf/2404.10304v1.pdf,LLM-Powered Test Case Generation for Detecting Tricky Bugs,"Conventional automated test generation tools struggle to generate test
oracles and tricky bug-revealing test inputs. Large Language Models (LLMs) can
be prompted to produce test inputs and oracles for a program directly, but the
precision of the tests can be very low for complex scenarios (only 6.3% based
on our experiments). To fill this gap, this paper proposes AID, which combines
LLMs with differential testing to generate fault-revealing test inputs and
oracles targeting plausibly correct programs (i.e., programs that have passed
all the existing tests). In particular, AID selects test inputs that yield
diverse outputs on a set of program variants generated by LLMs, then constructs
the test oracle based on the outputs. We evaluate AID on two large-scale
datasets with tricky bugs: TrickyBugs and EvalPlus, and compare it with three
state-of-the-art baselines. The evaluation results show that the recall,
precision, and F1 score of AID outperform the state-of-the-art by up to 1.80x,
2.65x, and 1.66x, respectively.",2024-04-16T06:20:06Z
,http://arxiv.org/pdf/2304.11384v3.pdf,"Large Language Models are Few-Shot Summarizers: Multi-Intent Comment
  Generation via In-Context Learning","Code comment generation aims at generating natural language descriptions for
a code snippet to facilitate developers' program comprehension activities.
Despite being studied for a long time, a bottleneck for existing approaches is
that given a code snippet, they can only generate one comment while developers
usually need to know information from diverse perspectives such as what is the
functionality of this code snippet and how to use it. To tackle this
limitation, this study empirically investigates the feasibility of utilizing
large language models (LLMs) to generate comments that can fulfill developers'
diverse intents. Our intuition is based on the facts that (1) the code and its
pairwise comment are used during the pre-training process of LLMs to build the
semantic connection between the natural language and programming language, and
(2) comments in the real-world projects, which are collected for the
pre-training, usually contain different developers' intents. We thus postulate
that the LLMs can already understand the code from different perspectives after
the pre-training. Indeed, experiments on two large-scale datasets demonstrate
the rationale of our insights: by adopting the in-context learning paradigm and
giving adequate prompts to the LLM (e.g., providing it with ten or more
examples), the LLM can significantly outperform a state-of-the-art supervised
learning approach on generating comments with multiple intents. Results also
show that customized strategies for constructing the prompts and
post-processing strategies for reranking the results can both boost the LLM's
performances, which shed light on future research directions for using LLMs to
achieve comment generation.",2023-04-22T12:26:24Z
,http://arxiv.org/pdf/2305.00948v2.pdf,"Large Linguistic Models: Analyzing theoretical linguistic abilities of
  LLMs","The performance of large language models (LLMs) has recently improved to the
point where the models can perform well on many language tasks. We show here
that for the first time, the models can also generate coherent and valid formal
analyses of linguistic data and illustrate the vast potential of large language
models for analyses of their metalinguistic abilities. LLMs are primarily
trained on language data in the form of text; analyzing and evaluating their
metalinguistic abilities improves our understanding of their general
capabilities and sheds new light on theoretical models in linguistics. In this
paper, we probe into GPT-4's metalinguistic capabilities by focusing on three
subfields of formal linguistics: syntax, phonology, and semantics. We outline a
research program for metalinguistic analyses of large language models, propose
experimental designs, provide general guidelines, discuss limitations, and
offer future directions for this line of research. This line of inquiry also
exemplifies behavioral interpretability of deep learning, where models'
representations are accessed by explicit prompting rather than internal
representations.",2023-05-01T17:09:33Z
,http://arxiv.org/pdf/2310.18355v1.pdf,"Health Disparities through Generative AI Models: A Comparison Study
  Using A Domain Specific large language model","Health disparities are differences in health outcomes and access to
healthcare between different groups, including racial and ethnic minorities,
low-income people, and rural residents. An artificial intelligence (AI) program
called large language models (LLMs) can understand and generate human language,
improving health communication and reducing health disparities. There are many
challenges in using LLMs in human-doctor interaction, including the need for
diverse and representative data, privacy concerns, and collaboration between
healthcare providers and technology experts. We introduce the comparative
investigation of domain-specific large language models such as SciBERT with a
multi-purpose LLMs BERT. We used cosine similarity to analyze text queries
about health disparities in exam rooms when factors such as race are used
alone. Using text queries, SciBERT fails when it doesn't differentiate between
queries text: ""race"" alone and ""perpetuates health disparities."" We believe
clinicians can use generative AI to create a draft response when communicating
asynchronously with patients. However, careful attention must be paid to ensure
they are developed and implemented ethically and equitably.",2023-10-23T21:24:05Z
,http://arxiv.org/pdf/2401.03676v1.pdf,"Assessing AI Detectors in Identifying AI-Generated Code: Implications
  for Education","Educators are increasingly concerned about the usage of Large Language Models
(LLMs) such as ChatGPT in programming education, particularly regarding the
potential exploitation of imperfections in Artificial Intelligence Generated
Content (AIGC) Detectors for academic misconduct. In this paper, we present an
empirical study where the LLM is examined for its attempts to bypass detection
by AIGC Detectors. This is achieved by generating code in response to a given
question using different variants. We collected a dataset comprising 5,069
samples, with each sample consisting of a textual description of a coding
problem and its corresponding human-written Python solution codes. These
samples were obtained from various sources, including 80 from Quescol, 3,264
from Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of
code problem variant prompts, which were used to instruct ChatGPT to generate
the outputs. Subsequently, we assessed the performance of five AIGC detectors.
Our results demonstrate that existing AIGC Detectors perform poorly in
distinguishing between human-written code and AI-generated code.",2024-01-08T05:53:52Z
,http://arxiv.org/pdf/2405.15130v1.pdf,OptLLM: Optimal Assignment of Queries to Large Language Models,"Large Language Models (LLMs) have garnered considerable attention owing to
their remarkable capabilities, leading to an increasing number of companies
offering LLMs as services. Different LLMs achieve different performance at
different costs. A challenge for users lies in choosing the LLMs that best fit
their needs, balancing cost and performance. In this paper, we propose a
framework for addressing the cost-effective query allocation problem for LLMs.
Given a set of input queries and candidate LLMs, our framework, named OptLLM,
provides users with a range of optimal solutions to choose from, aligning with
their budget constraints and performance preferences, including options for
maximizing accuracy and minimizing cost. OptLLM predicts the performance of
candidate LLMs on each query using a multi-label classification model with
uncertainty estimation and then iteratively generates a set of non-dominated
solutions by destructing and reconstructing the current solution. To evaluate
the effectiveness of OptLLM, we conduct extensive experiments on various types
of tasks, including text classification, question answering, sentiment
analysis, reasoning, and log parsing. Our experimental results demonstrate that
OptLLM substantially reduces costs by 2.40% to 49.18% while achieving the same
accuracy as the best LLM. Compared to other multi-objective optimization
algorithms, OptLLM improves accuracy by 2.94% to 69.05% at the same cost or
saves costs by 8.79% and 95.87% while maintaining the highest attainable
accuracy.",2024-05-24T01:05:37Z
,http://arxiv.org/pdf/2406.06025v1.pdf,RepoQA: Evaluating Long Context Code Understanding,"Recent advances have been improving the context windows of Large Language
Models (LLMs). To quantify the real long-context capabilities of LLMs,
evaluators such as the popular Needle in a Haystack have been developed to test
LLMs over a large chunk of raw texts. While effective, current evaluations
overlook the insight of how LLMs work with long-context code, i.e.,
repositories. To this end, we initiate the RepoQA benchmark to evaluate LLMs on
long-context code understanding. Traditional needle testers ask LLMs to
directly retrieve the answer from the context without necessary deep
understanding. In RepoQA, we built our initial task, namely Searching Needle
Function (SNF), which exercises LLMs to search functions given their
natural-language description, i.e., LLMs cannot find the desired function if
they cannot understand the description and code. RepoQA is multilingual and
comprehensive: it includes 500 code search tasks gathered from 50 popular
repositories across 5 modern programming languages. By evaluating 26 general
and code-specific LLMs on RepoQA, we show (i) there is still a small gap
between the best open and proprietary models; (ii) different models are good at
different languages; and (iii) models may understand code better without
comments.",2024-06-10T05:15:30Z
,http://arxiv.org/pdf/2311.16017v1.pdf,"Decoding Logic Errors: A Comparative Study on Bug Detection by Students
  and Large Language Models","Identifying and resolving logic errors can be one of the most frustrating
challenges for novices programmers. Unlike syntax errors, for which a compiler
or interpreter can issue a message, logic errors can be subtle. In certain
conditions, buggy code may even exhibit correct behavior -- in other cases, the
issue might be about how a problem statement has been interpreted. Such errors
can be hard to spot when reading the code, and they can also at times be missed
by automated tests. There is great educational potential in automatically
detecting logic errors, especially when paired with suitable feedback for
novices. Large language models (LLMs) have recently demonstrated surprising
performance for a range of computing tasks, including generating and explaining
code. These capabilities are closely linked to code syntax, which aligns with
the next token prediction behavior of LLMs. On the other hand, logic errors
relate to the runtime performance of code and thus may not be as well suited to
analysis by LLMs. To explore this, we investigate the performance of two
popular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly
explanation of logic errors. We compare LLM performance with a large cohort of
introductory computing students $(n=964)$ solving the same error detection
task. Through a mixed-methods analysis of student and model responses, we
observe significant improvement in logic error identification between the
previous and current generation of LLMs, and find that both LLM generations
significantly outperform students. We outline how such models could be
integrated into computing education tools, and discuss their potential for
supporting students when learning programming.",2023-11-27T17:28:33Z
,http://arxiv.org/pdf/2305.01210v3.pdf,"Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of
  Large Language Models for Code Generation","Program synthesis has been long studied with recent approaches focused on
directly using the power of Large Language Models (LLMs) to generate code.
Programming benchmarks, with curated synthesis problems and test-cases, are
used to measure the performance of various LLMs on code synthesis. However,
these test-cases can be limited in both quantity and quality for fully
assessing the functional correctness of the generated code. Such limitation in
the existing benchmarks begs the following question: In the era of LLMs, is the
code generated really correct? To answer this, we propose EvalPlus -- a code
synthesis evaluation framework to rigorously benchmark the functional
correctness of LLM-synthesized code. EvalPlus augments a given evaluation
dataset with large amounts of test-cases newly produced by an automatic test
input generator, powered by both LLM- and mutation-based strategies. While
EvalPlus is general, we extend the test-cases of the popular HumanEval
benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
catch significant amounts of previously undetected wrong code synthesized by
LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
test insufficiency can lead to mis-ranking. For example, both
WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
while none of them could on HumanEval. Our work not only indicates that prior
popular code synthesis evaluation results do not accurately reflect the true
performance of LLMs for code synthesis, but also opens up a new direction to
improve such programming benchmarks through automated testing. We have
open-sourced our tools, enhanced datasets as well as all LLM-generated code at
https://github.com/evalplus/evalplus to facilitate and accelerate future
LLM-for-code research.",2023-05-02T05:46:48Z
,http://arxiv.org/pdf/2312.09601v1.pdf,"Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large
  Language Models","Binary code summarization, while invaluable for understanding code semantics,
is challenging due to its labor-intensive nature. This study delves into the
potential of large language models (LLMs) for binary code comprehension. To
this end, we present BinSum, a comprehensive benchmark and dataset of over 557K
binary functions and introduce a novel method for prompt synthesis and
optimization. To more accurately gauge LLM performance, we also propose a new
semantic similarity metric that surpasses traditional exact-match approaches.
Our extensive evaluation of prominent LLMs, including ChatGPT, GPT-4, Llama 2,
and Code Llama, reveals 10 pivotal insights. This evaluation generates 4
billion inference tokens, incurred a total expense of 11,418 US dollars and 873
NVIDIA A100 GPU hours. Our findings highlight both the transformative potential
of LLMs in this field and the challenges yet to be overcome.",2023-12-15T08:32:28Z
,http://arxiv.org/pdf/2404.13066v2.pdf,"Leveraging Large Language Model as Simulated Patients for Clinical
  Education","Simulated Patients (SPs) play a crucial role in clinical medical education by
providing realistic scenarios for student practice. However, the high cost of
training and hiring qualified SPs, along with the heavy workload and potential
risks they face in consistently portraying actual patients, limit students'
access to this type of clinical training. Consequently, the integration of
computer program-based simulated patients has emerged as a valuable educational
tool in recent years. With the rapid development of Large Language Models
(LLMs), their exceptional capabilities in conversational artificial
intelligence and role-playing have been demonstrated, making them a feasible
option for implementing Virtual Simulated Patient (VSP). In this paper, we
present an integrated model-agnostic framework called CureFun that harnesses
the potential of LLMs in clinical medical education. This framework facilitates
natural conversations between students and simulated patients, evaluates their
dialogue, and provides suggestions to enhance students' clinical inquiry
skills. Through comprehensive evaluations, our approach demonstrates more
authentic and professional SP-scenario dialogue flows compared to other
LLM-based chatbots, thus proving its proficiency in simulating patients.
Additionally, leveraging CureFun's evaluation ability, we assess several
medical LLMs and discuss the possibilities and limitations of using LLMs as
virtual doctors from the perspective of their diagnostic abilities.",2024-04-13T06:36:32Z
,http://arxiv.org/pdf/2306.00597v2.pdf,Analysis of ChatGPT on Source Code,"This paper explores the use of Large Language Models (LLMs) and in particular
ChatGPT in programming, source code analysis, and code generation. LLMs and
ChatGPT are built using machine learning and artificial intelligence
techniques, and they offer several benefits to developers and programmers.
While these models can save time and provide highly accurate results, they are
not yet advanced enough to replace human programmers entirely. The paper
investigates the potential applications of LLMs and ChatGPT in various areas,
such as code creation, code documentation, bug detection, refactoring, and
more. The paper also suggests that the usage of LLMs and ChatGPT is expected to
increase in the future as they offer unparalleled benefits to the programming
community.",2023-06-01T12:12:59Z
,http://arxiv.org/pdf/2401.16445v3.pdf,OMPGPT: A Generative Pre-trained Transformer Model for OpenMP,"Large language models (LLMs)such as ChatGPT have significantly advanced the
field of Natural Language Processing (NLP). This trend led to the development
of code-based large language models such as StarCoder, WizardCoder, and
CodeLlama, which are trained extensively on vast repositories of code and
programming languages. While the generic abilities of these code LLMs are
useful for many programmers in tasks like code generation, the area of
high-performance computing (HPC) has a narrower set of requirements that make a
smaller and more domain-specific model a smarter choice. This paper presents
OMPGPT, a novel domain-specific model meticulously designed to harness the
inherent strengths of language models for OpenMP pragma generation.
Furthermore, we leverage prompt engineering techniques from the NLP domain to
create Chain-of-OMP, an innovative strategy designed to enhance OMPGPT's
effectiveness. Our extensive evaluations demonstrate that OMPGPT outperforms
existing large language models specialized in OpenMP tasks and maintains a
notably smaller size, aligning it more closely with the typical hardware
constraints of HPC environments. We consider our contribution as a pivotal
bridge, connecting the advantage of language models with the specific demands
of HPC tasks.",2024-01-28T06:06:59Z
,http://arxiv.org/pdf/2401.14423v4.pdf,Prompt Design and Engineering: Introduction and Advanced Methods,"Prompt design and engineering has rapidly become essential for maximizing the
potential of large language models. In this paper, we introduce core concepts,
advanced techniques like Chain-of-Thought and Reflection, and the principles
behind building LLM-based agents. Finally, we provide a survey of tools for
prompt engineers.",2024-01-24T06:20:18Z
,http://arxiv.org/pdf/2306.10509v2.pdf,"Can We Trust AI-Generated Educational Content? Comparative Analysis of
  Human and AI-Generated Learning Resources","As an increasing number of students move to online learning platforms that
deliver personalized learning experiences, there is a great need for the
production of high-quality educational content. Large language models (LLMs)
appear to offer a promising solution to the rapid creation of learning
materials at scale, reducing the burden on instructors. In this study, we
investigated the potential for LLMs to produce learning resources in an
introductory programming context, by comparing the quality of the resources
generated by an LLM with those created by students as part of a learnersourcing
activity. Using a blind evaluation, students rated the correctness and
helpfulness of resources generated by AI and their peers, after both were
initially provided with identical exemplars. Our results show that the quality
of AI-generated resources, as perceived by students, is equivalent to the
quality of resources generated by their peers. This suggests that AI-generated
resources may serve as viable supplementary material in certain contexts.
Resources generated by LLMs tend to closely mirror the given exemplars, whereas
student-generated resources exhibit greater variety in terms of content length
and specific syntax features used. The study highlights the need for further
research exploring different types of learning resources and a broader range of
subject areas, and understanding the long-term impact of AI-generated resources
on learning outcomes.",2023-06-18T09:49:21Z
,http://arxiv.org/pdf/2310.15123v2.pdf,"Branch-Solve-Merge Improves Large Language Model Evaluation and
  Generation","Large Language Models (LLMs) are frequently used for multi-faceted language
generation and evaluation tasks that involve satisfying intricate user
constraints or taking into account multiple aspects and criteria. However,
their performance can fall short, due to the model's lack of coherence and
inability to plan and decompose the problem. We propose Branch-Solve-Merge
(BSM), a Large Language Model program (Schlag et al., 2023) for tackling such
challenging natural language tasks. It consists of branch, solve, and merge
modules that are parameterized with specific prompts to the base LLM. These
three modules plan a decomposition of the task into multiple parallel
sub-tasks, independently solve them, and fuse the solutions to the sub-tasks.
We apply our method to the tasks of LLM response evaluation and constrained
text generation and evaluate its effectiveness with multiple LLMs, including
Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and
consistency for each LLM by enhancing human-LLM agreement by up to 26%,
reducing length and pairwise position biases by up to 50%, and allowing
LLaMA2-chat to match or outperform GPT-4 on most domains. On a constraint story
generation task, BSM improves the coherence of stories while also improving
constraint satisfaction by 12%.",2023-10-23T17:29:48Z
10.1145/3661167.3661171,http://arxiv.org/pdf/2406.16739v1.pdf,Agent-Driven Automatic Software Improvement,"With software maintenance accounting for 50% of the cost of developing
software, enhancing code quality and reliability has become more critical than
ever. In response to this challenge, this doctoral research proposal aims to
explore innovative solutions by focusing on the deployment of agents powered by
Large Language Models (LLMs) to perform software maintenance tasks. The
iterative nature of agents, which allows for continuous learning and
adaptation, can help surpass common challenges in code generation. One distinct
challenge is the last-mile problems, errors at the final stage of producing
functionally and contextually relevant code. Furthermore, this project aims to
surpass the inherent limitations of current LLMs in source code through a
collaborative framework where agents can correct and learn from each other's
errors. We aim to use the iterative feedback in these systems to further
fine-tune the LLMs underlying the agents, becoming better aligned to the task
of automated software improvement. Our main goal is to achieve a leap forward
in the field of automatic software improvement by developing new tools and
frameworks that can enhance the efficiency and reliability of software
development.",2024-06-24T15:45:22Z
,http://arxiv.org/pdf/2302.04662v2.pdf,"Generating High-Precision Feedback for Programming Syntax Errors using
  Large Language Models","Large language models (LLMs), such as Codex, hold great promise in enhancing
programming education by automatically generating feedback for students. We
investigate using LLMs to generate feedback for fixing syntax errors in Python
programs, a key scenario in introductory programming. More concretely, given a
student's buggy program, our goal is to generate feedback comprising a fixed
program along with a natural language explanation describing the errors/fixes,
inspired by how a human tutor would give feedback. While using LLMs is
promising, the critical challenge is to ensure high precision in the generated
feedback, which is imperative before deploying such technology in classrooms.
The main research question we study is: Can we develop LLMs-based feedback
generation techniques with a tunable precision parameter, giving educators
quality control over the feedback that students receive? To this end, we
introduce PyFiXV, our technique to generate high-precision feedback powered by
Codex. The key idea behind PyFiXV is to use a novel run-time validation
mechanism to decide whether the generated feedback is suitable for sharing with
the student; notably, this validation mechanism also provides a precision knob
to educators. We perform an extensive evaluation using two real-world datasets
of Python programs with syntax errors and show the efficacy of PyFiXV in
generating high-precision feedback.",2023-01-24T13:00:25Z
,http://arxiv.org/pdf/2405.00302v3.pdf,"Generating Feedback-Ladders for Logical Errors in Programming using
  Large Language Models","In feedback generation for logical errors in programming assignments, large
language model (LLM)-based methods have shown great promise. These methods ask
the LLM to generate feedback given the problem statement and a student's
(buggy) submission. There are several issues with these types of methods.
First, the generated feedback messages are often too direct in revealing the
error in the submission and thus diminish valuable opportunities for the
student to learn. Second, they do not consider the student's learning context,
i.e., their previous submissions, current knowledge, etc. Third, they are not
layered since existing methods use a single, shared prompt for all student
submissions. In this paper, we explore using LLMs to generate a
""feedback-ladder"", i.e., multiple levels of feedback for the same
problem-submission pair. We evaluate the quality of the generated
feedback-ladder via a user study with students, educators, and researchers. We
have observed diminishing effectiveness for higher-level feedback and
higher-scoring submissions overall in the study. In practice, our method
enables teachers to select an appropriate level of feedback to show to a
student based on their personal learning context, or in a progressive manner to
go more detailed if a higher-level feedback fails to correct the student's
error.",2024-05-01T03:52:39Z
,http://arxiv.org/pdf/2406.06647v2.pdf,"How Efficient is LLM-Generated Code? A Rigorous & High-Standard
  Benchmark","The emergence of large language models (LLMs) has significantly pushed the
frontiers of program synthesis. Advancement of LLM-based program synthesis
calls for a thorough evaluation of LLM-generated code. Most evaluation
frameworks focus on the (functional) correctness of generated code; efficiency,
as an important measure of code quality, has been overlooked in existing
evaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a
rigorous and high-standard benchmark for evaluating the capability of LLMs in
generating efficient code. Firstly, we propose a new efficiency metric called
eff@k, which generalizes the pass@k metric from correctness to efficiency and
appropriately handles right-censored execution time. Furthermore, we derive an
unbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we
also provide a numerically stable implementation for the new estimator.
Secondly, to set a high-standard for efficiency evaluation, we employ a human
expert to design best algorithms and implementations as our reference solutions
of efficiency, many of which are much more efficient than existing canonical
solutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous
evaluation, we employ a human expert to curate strong test case generators to
filter out wrong code and differentiate suboptimal algorithms. An extensive
study across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still
fall short of generating expert-level efficient code. Using two subsets of our
problem set, we demonstrate that such deficiency is because current LLMs
struggle in designing advanced algorithms and are barely aware of
implementation optimization. Our benchmark is publicly available at
https://github.com/q-rz/enamel .",2024-06-10T04:19:20Z
,http://arxiv.org/pdf/2402.10517v4.pdf,"Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs","Recently, considerable efforts have been directed towards compressing Large
Language Models (LLMs), which showcase groundbreaking capabilities across
diverse applications but entail significant deployment costs due to their large
sizes. Meanwhile, much less attention has been given to mitigating the costs
associated with deploying multiple LLMs of varying sizes despite its practical
significance. Thus, this paper introduces \emph{any-precision LLM}, extending
the concept of any-precision DNN to LLMs. Addressing challenges in
any-precision LLM, we propose a lightweight method for any-precision
quantization of LLMs, leveraging a post-training quantization framework, and
develop a specialized software engine for its efficient serving. As a result,
our solution significantly reduces the high costs of deploying multiple,
different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such
as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit
LLM. All the supported LLMs with varying bit-widths demonstrate
state-of-the-art model quality and inference throughput, proving itself to be a
compelling option for deployment of multiple, different-sized LLMs. Our code is
open-sourced and available online.",2024-02-16T09:06:06Z
,http://arxiv.org/pdf/2309.03567v1.pdf,"The Devil is in the Tails: How Long-Tailed Code Distributions Impact
  Large Language Models","Learning-based techniques, especially advanced Large Language Models (LLMs)
for code, have gained considerable popularity in various software engineering
(SE) tasks. However, most existing works focus on designing better
learning-based models and pay less attention to the properties of datasets.
Learning-based models, including popular LLMs for code, heavily rely on data,
and the data's properties (e.g., data distribution) could significantly affect
their behavior. We conducted an exploratory study on the distribution of SE
data and found that such data usually follows a skewed distribution (i.e.,
long-tailed distribution) where a small number of classes have an extensive
collection of samples, while a large number of classes have very few samples.
We investigate three distinct SE tasks and analyze the impacts of long-tailed
distribution on the performance of LLMs for code. Our experimental results
reveal that the long-tailed distribution has a substantial impact on the
effectiveness of LLMs for code. Specifically, LLMs for code perform between
30.0\% and 254.0\% worse on data samples associated with infrequent labels
compared to data samples of frequent labels. Our study provides a better
understanding of the effects of long-tailed distributions on popular LLMs for
code and insights for the future development of SE automation.",2023-09-07T08:53:16Z
,http://arxiv.org/pdf/2404.09384v1.pdf,"Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software
  Verification and Falsification Approaches","Prompting has become one of the main approaches to leverage emergent
capabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al.
TMLR 2022, Wei et al. NeurIPS 2022]. During the last year, researchers and
practitioners have been playing with prompts to see how to make the most of
LLMs. By homogeneously dissecting 80 papers, we investigate in deep how
software testing and verification research communities have been abstractly
architecting their LLM-enabled solutions. More precisely, first, we want to
validate whether downstream tasks are an adequate concept to convey the
blueprint of prompt-based solutions. We also aim at identifying number and
nature of such tasks in solutions. For such goal, we develop a novel downstream
task taxonomy that enables pinpointing some engineering patterns in a rather
varied spectrum of Software Engineering problems that encompasses testing,
fuzzing, debugging, vulnerability detection, static analysis and program
verification approaches.",2024-04-14T23:45:23Z
,http://arxiv.org/pdf/2312.15698v4.pdf,"RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for
  Program Repair","Automated Program Repair (APR) has evolved significantly with the advent of
Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent
avenue of research, with many dimensions which have not been explored. Existing
work mostly fine-tune LLMs with naive code representations and does not scale
to frontier models. To address this problem, we propose RepairLLaMA, a novel
program repair approach that 1) identifies optimal code representations for APR
with fine-tuned models, and 2) pioneers state-of-the-art parameter-efficient
fine-tuning technique (PEFT) for program repair. This results in RepairLLaMA
producing a highly effective `program repair adapter' for fixing bugs with AI.
Our experiments demonstrate the validity of both concepts. First, fine-tuning
adapters with program repair specific code representations enables the model to
use meaningful repair signals and produce better patches. Second,
parameter-efficient fine-tuning helps fine-tuning to converge and clearly
contributes to the effectiveness of RepairLLaMA in fixing bugs outside the
fine-tuning data distribution. Overall, RepairLLaMA correctly fixes 144
Defects4J v2 and 109 HumanEval-Java bugs, outperforming all baselines.",2023-12-25T11:39:46Z
,http://arxiv.org/pdf/2309.17446v2.pdf,"L2CEval: Evaluating Language-to-Code Generation Capabilities of Large
  Language Models","Recently, large language models (LLMs), especially those that are pretrained
on code, have demonstrated strong capabilities in generating programs from
natural language inputs in a few-shot or even zero-shot manner. Despite
promising results, there is a notable lack of a comprehensive evaluation of
these models language-to-code generation capabilities. Existing studies often
focus on specific tasks, model architectures, or learning paradigms, leading to
a fragmented understanding of the overall landscape. In this work, we present
L2CEval, a systematic evaluation of the language-to-code generation
capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing,
math reasoning and Python programming, analyzing the factors that potentially
affect their performance, such as model size, pretraining data, instruction
tuning, and different prompting methods. In addition to assessing model
performance, we measure confidence calibration for the models and conduct human
evaluations of the output programs. This enables us to identify and analyze the
typical failure modes across various tasks and models. L2CEval offers a
comprehensive understanding of the capabilities and limitations of LLMs in
language-to-code generation. We also release the evaluation framework and all
model outputs, hoping to lay the groundwork for further future research in this
domain.",2023-09-29T17:57:00Z
,http://arxiv.org/pdf/2312.15960v2.pdf,"MoTCoder: Elevating Large Language Models with Modular of Thought for
  Challenging Programming Tasks","Large Language Models (LLMs) have showcased impressive capabilities in
handling straightforward programming tasks. However, their performance tends to
falter when confronted with more challenging programming problems. We observe
that conventional models often generate solutions as monolithic code blocks,
restricting their effectiveness in tackling intricate questions. To overcome
this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a
pioneering framework for MoT instruction tuning, designed to promote the
decomposition of tasks into logical sub-tasks and sub-modules. Our
investigations reveal that, through the cultivation and utilization of
sub-modules, MoTCoder significantly improves both the modularity and
correctness of the generated solutions, leading to substantial relative pass@1
improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are
available at https://github.com/dvlab-research/MoTCoder.",2023-12-26T08:49:57Z
,http://arxiv.org/pdf/2402.16896v2.pdf,On Trojan Signatures in Large Language Models of Code,"Trojan signatures, as described by Fields et al. (2021), are noticeable
differences in the distribution of the trojaned class parameters (weights) and
the non-trojaned class parameters of the trojaned model, that can be used to
detect the trojaned model. Fields et al. (2021) found trojan signatures in
computer vision classification tasks with image models, such as, Resnet,
WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in
the classifier layer parameters of large language models of source code.
  Our results suggest that trojan signatures could not generalize to LLMs of
code. We found that trojaned code models are stubborn, even when the models
were poisoned under more explicit settings (finetuned with pre-trained weights
frozen). We analyzed nine trojaned models for two binary classification tasks:
clone and defect detection. To the best of our knowledge, this is the first
work to examine weight-based trojan signature revelation techniques for
large-language models of code and furthermore to demonstrate that detecting
trojans only from the weights in such models is a hard problem.",2024-02-23T22:48:29Z
,http://arxiv.org/pdf/2406.07400v1.pdf,"Guiding LLM Temporal Logic Generation with Explicit Separation of Data
  and Control","Temporal logics are powerful tools that are widely used for the synthesis and
verification of reactive systems. The recent progress on Large Language Models
(LLMs) has the potential to make the process of writing such specifications
more accessible. However, writing specifications in temporal logics remains
challenging for all but the most expert users. A key question in using LLMs for
temporal logic specification engineering is to understand what kind of guidance
is most helpful to the LLM and the users to easily produce specifications.
Looking specifically at the problem of reactive program synthesis, we explore
the impact of providing an LLM with guidance on the separation of control and
data--making explicit for the LLM what functionality is relevant for the
specification, and treating the remaining functionality as an implementation
detail for a series of pre-defined functions and predicates. We present a
benchmark set and find that this separation of concerns improves specification
generation. Our benchmark provides a test set against which to verify future
work in LLM generation of temporal logic specifications.",2024-06-11T16:07:24Z
,http://arxiv.org/pdf/2310.02368v1.pdf,"Reinforcement Learning from Automatic Feedback for High-Quality Unit
  Test Generation","Software testing is a crucial aspect of software development, and the
creation of high-quality tests that adhere to best practices is essential for
effective maintenance. Recently, Large Language Models (LLMs) have gained
popularity for code generation, including the automated creation of test cases.
However, these LLMs are often trained on vast amounts of publicly available
code, which may include test cases that do not adhere to best practices and may
even contain test smells (anti-patterns). To address this issue, we propose a
novel technique called Reinforcement Learning from Static Quality Metrics
(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show
that LLMs can generate undesirable test smells. Thus, we train specific reward
models for each static quality metric, then utilize Proximal Policy
Optimization (PPO) to train models for optimizing a single quality metric at a
time. Furthermore, we amalgamate these rewards into a unified reward model
aimed at capturing different best practices and quality aspects of tests. By
comparing RL-trained models with those trained using supervised learning, we
provide insights into how reliably utilize RL to improve test generation
quality and into the effects of various training strategies. Our experimental
results demonstrate that the RL-optimized model consistently generated
high-quality test cases compared to the base LLM, improving the model by up to
21%, and successfully generates nearly 100% syntactically correct code. RLSQM
also outperformed GPT-4 on four out of seven metrics. This represents a
significant step towards enhancing the overall efficiency and reliability of
software testing through Reinforcement Learning and static quality metrics. Our
data are available at this link: https://figshare.com/s/ded476c8d4c221222849.",2023-10-03T18:48:31Z
,http://arxiv.org/pdf/2405.15512v1.pdf,ChatGPT Code Detection: Techniques for Uncovering the Source of Code,"In recent times, large language models (LLMs) have made significant strides
in generating computer code, blurring the lines between code created by humans
and code produced by artificial intelligence (AI). As these technologies evolve
rapidly, it is crucial to explore how they influence code generation,
especially given the risk of misuse in areas like higher education. This paper
explores this issue by using advanced classification techniques to
differentiate between code written by humans and that generated by ChatGPT, a
type of LLM. We employ a new approach that combines powerful embedding features
(black-box) with supervised learning algorithms - including Deep Neural
Networks, Random Forests, and Extreme Gradient Boosting - to achieve this
differentiation with an impressive accuracy of 98%. For the successful
combinations, we also examine their model calibration, showing that some of the
models are extremely well calibrated. Additionally, we present white-box
features and an interpretable Bayes classifier to elucidate critical
differences between the code sources, enhancing the explainability and
transparency of our approach. Both approaches work well but provide at most
85-88% accuracy. We also show that untrained humans solve the same task not
better than random guessing. This study is crucial in understanding and
mitigating the potential risks associated with using AI in code generation,
particularly in the context of higher education, software development, and
competitive programming.",2024-05-24T12:56:18Z
10.1145/3639474.3640058,http://arxiv.org/pdf/2404.11734v1.pdf,"Let's Ask AI About Their Programs: Exploring ChatGPT's Answers To
  Program Comprehension Questions","Recent research has explored the creation of questions from code submitted by
students. These Questions about Learners' Code (QLCs) are created through
program analysis, exploring execution paths, and then creating code
comprehension questions from these paths and the broader code structure.
Responding to the questions requires reading and tracing the code, which is
known to support students' learning. At the same time, computing education
researchers have witnessed the emergence of Large Language Models (LLMs) that
have taken the community by storm. Researchers have demonstrated the
applicability of these models especially in the introductory programming
context, outlining their performance in solving introductory programming
problems and their utility in creating new learning resources. In this work, we
explore the capability of the state-of-the-art LLMs (GPT-3.5 and GPT-4) in
answering QLCs that are generated from code that the LLMs have created. Our
results show that although the state-of-the-art LLMs can create programs and
trace program execution when prompted, they easily succumb to similar errors
that have previously been recorded for novice programmers. These results
demonstrate the fallibility of these models and perhaps dampen the expectations
fueled by the recent LLM hype. At the same time, we also highlight future
research possibilities such as using LLMs to mimic students as their behavior
can indeed be similar for some specific tasks.",2024-04-17T20:37:00Z
,http://arxiv.org/pdf/2403.09740v1.pdf,Teaching Machines to Code: Smart Contract Translation with LLMs,"The advent of large language models (LLMs) has marked a significant milestone
in the realm of artificial intelligence, with their capabilities often matching
or surpassing human expertise in various domains. Among these achievements,
their adeptness in translation tasks stands out, closely mimicking the
intricate and preliminary processes undertaken by human translators to ensure
the fidelity and quality of the translated content. Despite the advancements in
utilizing LLMs for translating programming code across different languages, the
domain of smart contract translation, particularly into languages not
previously encountered by the LLM, remains largely unexplored. In our research,
we present a pioneering approach, SolMover, which harnesses the synergy of two
distinct LLMs within a unified framework. This framework is designed to grasp
coding principles and apply this understanding to the translation of code into
an unfamiliar language. Our study delves into the capacity of LLMs to mimic
human learning processes, offering an in-depth evaluation of our methodology
for converting smart contracts written in Solidity to Move, a language with
limited resources. The framework employs one LLM to decipher coding conventions
for the new language, creating a blueprint for the second LLM, which, lacking
planning abilities, possesses coding expertise. The empirical evidence from our
experiments suggests that SolMover substantially enhances performance compared
to gpt-3.5-turbo-1106, and achieves superior results over competitors such as
Palm2 and Mixtral-8x7B-Instruct. Additionally, our analysis highlights the
efficacy of our bug mitigation strategy in elevating code quality across all
models, even outside the SolMover framework.",2024-03-13T18:55:20Z
,http://arxiv.org/pdf/2402.15938v3.pdf,"Generalization or Memorization: Data Contamination and Trustworthy
  Evaluation for Large Language Models","Recent statements about the impressive capabilities of large language models
(LLMs) are usually supported by evaluating on open-access benchmarks.
Considering the vast size and wide-ranging sources of LLMs' training data, it
could explicitly or implicitly include test data, leading to LLMs being more
susceptible to data contamination. However, due to the opacity of training
data, the black-box access of models, and the rapid growth of synthetic
training data, detecting and mitigating data contamination for LLMs faces
significant challenges. In this paper, we propose CDD, which stands for
Contamination Detection via output Distribution for LLMs. CDD necessitates only
the sampled texts to detect data contamination, by identifying the peakedness
of LLM's output distribution. To mitigate the impact of data contamination in
evaluation, we also present TED: Trustworthy Evaluation via output
Distribution, based on the correction of LLM's output distribution. To
facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,
for data contamination detection and contamination mitigation evaluation tasks.
Extensive experimental results show that CDD achieves the average relative
improvements of 21.8\%-30.2\% over other contamination detection approaches in
terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect
implicit contamination. TED substantially mitigates performance improvements up
to 66.9\% attributed to data contamination across various contamination setups.
In real-world applications, we reveal that ChatGPT exhibits a high potential to
suffer from data contamination on HumanEval benchmark.",2024-02-24T23:54:41Z
,http://arxiv.org/pdf/2310.03094v3.pdf,"Large Language Model Cascades with Mixture of Thoughts Representations
  for Cost-efficient Reasoning","Large language models (LLMs) such as GPT-4 have exhibited remarkable
performance in a variety of tasks, but this strong performance often comes with
the high expense of using paid API services. In this paper, we are motivated to
study building an LLM cascade to save the cost of using LLMs, particularly for
performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline
follows the intuition that simpler questions can be addressed by a weaker but
more affordable LLM, whereas only the challenging questions necessitate the
stronger and more expensive LLM. To realize this decision-making, we consider
the ""answer consistency"" of the weaker LLM as a signal of the question
difficulty and propose several methods for the answer sampling and consistency
checking, including one leveraging a mixture of two thought representations
(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six
reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and
stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can
achieve performance comparable to using solely the stronger LLM but require
only 40% of its cost.",2023-10-04T18:21:17Z
,http://arxiv.org/pdf/2303.10494v1.pdf,Revisiting the Plastic Surgery Hypothesis via Large Language Models,"Automated Program Repair (APR) aspires to automatically generate patches for
an input buggy program. Traditional APR tools typically focus on specific bug
types and fixes through the use of templates, heuristics, and formal
specifications. However, these techniques are limited in terms of the bug types
and patch variety they can produce. As such, researchers have designed various
learning-based APR tools with recent work focused on directly using Large
Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve
state-of-the-art performance on many repair datasets, the LLMs used for direct
repair are not fully aware of the project-specific information such as unique
variable or method names.
  The plastic surgery hypothesis is a well-known insight for APR, which states
that the code ingredients to fix the bug usually already exist within the same
project. Traditional APR tools have largely leveraged the plastic surgery
hypothesis by designing manual or heuristic-based approaches to exploit such
existing code ingredients. However, as recent APR research starts focusing on
LLM-based approaches, the plastic surgery hypothesis has been largely ignored.
In this paper, we ask the following question: How useful is the plastic surgery
hypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique
opportunity to fully automate the plastic surgery hypothesis via fine-tuning
and prompting. To this end, we propose FitRepair, which combines the direct
usage of LLMs with two domain-specific fine-tuning strategies and one prompting
strategy for more powerful APR. Our experiments on the widely studied Defects4j
1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially
outperforming the best-performing baseline by 15 and 8), respectively,
demonstrating a promising future of the plastic surgery hypothesis in the era
of LLMs.",2023-03-18T20:33:46Z
,http://arxiv.org/pdf/2312.07343v2.pdf,"Can ChatGPT Play the Role of a Teaching Assistant in an Introductory
  Programming Course?","The emergence of Large language models (LLMs) is expected to have a major
impact on education. This paper explores the potential of using ChatGPT, an
LLM, as a virtual Teaching Assistant (TA) in an Introductory Programming
Course. We evaluate ChatGPT's capabilities by comparing its performance with
that of human TAs in some of the important TA functions. The TA functions which
we focus on include (1) grading student code submissions, and (2) providing
feedback to undergraduate students in an introductory programming course.
Firstly, we assess ChatGPT's proficiency in grading student code submissions
using a given grading rubric and compare its performance with the grades
assigned by human TAs. Secondly, we analyze the quality and relevance of the
feedback provided by ChatGPT. This evaluation considers how well ChatGPT
addresses mistakes and offers suggestions for improvement in student solutions
from both code correctness and code quality perspectives. We conclude with a
discussion on the implications of integrating ChatGPT into computing education
for automated grading, personalized learning experiences, and instructional
support.",2023-12-12T15:06:44Z
,http://arxiv.org/pdf/2404.08850v2.pdf,"Assessing Economic Viability: A Comparative Analysis of Total Cost of
  Ownership for Domain-Adapted Large Language Models versus State-of-the-art
  Counterparts in Chip Design Coding Assistance","This paper presents a comparative analysis of total cost of ownership (TCO)
and performance between domain-adapted large language models (LLM) and
state-of-the-art (SoTA) LLMs , with a particular emphasis on tasks related to
coding assistance for chip design. We examine the TCO and performance metrics
of a domain-adaptive LLM, ChipNeMo, against two leading LLMs, Claude 3 Opus and
ChatGPT-4 Turbo, to assess their efficacy in chip design coding generation.
Through a detailed evaluation of the accuracy of the model, training
methodologies, and operational expenditures, this study aims to provide
stakeholders with critical information to select the most economically viable
and performance-efficient solutions for their specific needs. Our results
underscore the benefits of employing domain-adapted models, such as ChipNeMo,
that demonstrate improved performance at significantly reduced costs compared
to their general-purpose counterparts. In particular, we reveal the potential
of domain-adapted LLMs to decrease TCO by approximately 90%-95%, with the cost
advantages becoming increasingly evident as the deployment scale expands. With
expansion of deployment, the cost benefits of ChipNeMo become more pronounced,
making domain-adaptive LLMs an attractive option for organizations with
substantial coding needs supported by LLMs",2024-04-12T23:37:56Z
,http://arxiv.org/pdf/2401.10364v1.pdf,"Using LLM such as ChatGPT for Designing and Implementing a RISC
  Processor: Execution,Challenges and Limitations","This paper discusses the feasibility of using Large Language Models LLM for
code generation with a particular application in designing an RISC. The paper
also reviews the associated steps such as parsing, tokenization, encoding,
attention mechanism, sampling the tokens and iterations during code generation.
The generated code for the RISC components is verified through testbenches and
hardware implementation on a FPGA board. Four metric parameters Correct output
on the first iteration, Number of errors embedded in the code, Number of trials
required to achieve the code and Failure to generate the code after three
iterations, are used to compare the efficiency of using LLM in programming. In
all the cases, the generated code had significant errors and human intervention
was always required to fix the bugs. LLM can therefore be used to complement a
programmer code design.",2024-01-18T20:14:10Z
10.1145/3643796.3648451,http://arxiv.org/pdf/2402.11635v1.pdf,Tool-Augmented LLMs as a Universal Interface for IDEs,"Modern-day Integrated Development Environments (IDEs) have come a long way
from the early text editing utilities to the complex programs encompassing
thousands of functions to help developers. However, with the increasing number
of efficiency-enhancing tools incorporated, IDEs gradually became sophisticated
software with a steep learning curve. The rise of the Large Language Models
(LLMs) capable of both natural language dialogue and code generation leads to a
discourse on the obsolescence of the concept of IDE. In this work, we offer a
view on the place of the LLMs in the IDEs as the universal interface wrapping
the IDE facilities. We envision a model that is able to perform complex actions
involving multiple IDE features upon user command, stripping the user
experience of the tedious work involved in searching through options and
actions. For the practical part of the work, we engage with the works exploring
the ability of LLMs to call for external tools to expedite a given task
execution. We showcase a proof-of-concept of such a tool.",2024-02-18T16:32:28Z
,http://arxiv.org/pdf/2403.16218v1.pdf,CoverUp: Coverage-Guided LLM-Based Test Generation,"This paper presents CoverUp, a novel system that drives the generation of
high-coverage Python regression tests via a combination of coverage analysis
and large-language models (LLMs). CoverUp iteratively improves coverage,
interleaving coverage analysis with dialogs with the LLM to focus its attention
on as yet uncovered lines and branches. The resulting test suites significantly
improve coverage over the current state of the art: compared to CodaMosa, a
hybrid LLM / search-based software testing system, CoverUp substantially
improves coverage across the board. On a per-module basis, CoverUp achieves
median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and
line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative,
coverage-guided approach is crucial to its effectiveness, contributing to
nearly half of its successes.",2024-03-24T16:18:27Z
,http://arxiv.org/pdf/2306.00108v2.pdf,"Better patching using LLM prompting, via Self-Consistency","Large Language models (LLMs) can be induced to solve non-trivial problems
with ""few-shot"" prompts including illustrative problem-solution examples. Now
if the few-shots also include ""chain of thought"" (CoT) explanations, which are
of the form problem-explanation-solution, LLMs will generate a ""explained""
solution, and perform even better. Recently an exciting, substantially better
technique, self-consistency [1] (S-C) has emerged, based on the intuition that
there are many plausible explanations for the right solution; when the LLM is
sampled repeatedly to generate a pool of explanation-solution pairs, for a
given problem, the most frequently occurring solutions in the pool (ignoring
the explanations) tend to be even more likely to be correct! Unfortunately, the
use of this highly-performant S-C (or even CoT) approach in software
engineering settings is hampered by the lack of explanations; most software
datasets lack explanations. In this paper, we describe an application of the
S-C approach to program repair, using the commit log on the fix as the
explanation, only in the illustrative few-shots. We achieve state-of-the art
results, beating previous approaches to prompting-based program repair, on the
MODIT dataset; we also find evidence suggesting that the correct commit
messages are helping the LLM learn to produce better patches.",2023-05-31T18:28:46Z
,http://arxiv.org/pdf/2401.15940v3.pdf,Knowledge-Aware Code Generation with Large Language Models,"Large Language Models (LLMs) perform well on basic programming problems.
However, they encounter challenges when dealing with complex tasks involving
the use of diverse algorithmic and data structure skills, particularly
programming competition-level problems. Notably, ChatGPT exhibits proficient
performance on problems it has encountered during its pre-training phase, but
this performance deteriorates when faced with novel problems. Consequently,
enhancing the ability of LLMs to address unfamiliar problems has emerged as a
pivotal research focus. The problem-solving process of LLMs mirrors human
programmers' approach to a certain extent. When confronted with new programming
tasks, human programmers engage in task planning and code writing with the
previously acquired knowledge about algorithms and data structures. Despite
having learned such knowledge, LLMs struggle to effectively apply it when faced
with specific new problems. To address this issue, we constructed a novel
dataset, CodeF, which contains a portion of programming problems that ChatGPT
has not previously encountered. Furthermore, we developed a Knowledge Library
tailored for Python programming contest problems and introduced the concept of
Knowledge-Aware Code Generation (KareCoder). KareCoder bolsters the models'
understanding and problem-solving capabilities by integrating prompt and
knowledge from the library into the LLMs' code generation reasoning process,
especially on Pass@1 metrics. Upon testing on the CodeF and APPS datasets,
KareCoder demonstrated outstanding performance in handling novel problems
previously unencountered by LLMs. In contrast with the code directly generated
by ChatGPT, KareCoder achieved a relative improvement of 23.3% on the Pass@1
metric on the CodeF post2021-9 dataset. Additionally, it performs well compared
to other methods when dealing with problems that LLMs have previously
encountered.",2024-01-29T08:01:22Z
,http://arxiv.org/pdf/2309.14345v3.pdf,Bias Testing and Mitigation in LLM-based Code Generation,"Utilizing state-of-the-art Large Language Models (LLMs), automatic code
generation models play a pivotal role in enhancing the productivity of software
development procedures. As the adoption of LLMs becomes more widespread in
software coding ecosystems, a pressing issue has emerged: does the generated
code contain social bias and unfairness, such as those related to age, gender,
and race? This issue concerns the integrity, fairness, and ethical foundation
of software applications that depend on the code generated by these models, yet
is under-explored in the literature. This paper presents a novel bias testing
framework that is specifically designed for code generation tasks. Based on
this framework, we conduct an extensive evaluation of the bias in code
generated by five state-of-the-art LLMs. Our findings reveal that 20.29% to
44.93% code functions generated by the models under study are biased when
handling bias sensitive tasks (i.e., tasks that involve sensitive attributes
such as age and gender). This indicates that the existing LLMs can be unfair in
code generation, posing risks of unintended and harmful software behaviors. To
mitigate bias for code generation models, we evaluate five bias mitigation
prompt strategies, i.e., utilizing bias testing results to refine the code
(zero-shot), one-, few-shot, and two Chain-of-Thought (CoT) prompts. Our
evaluation results illustrate that these strategies are all effective in
mitigating bias. Overall, one-shot and few-shot learning are the two most
effective. For GPT-4, 80% to 90% code bias can be removed with one-shot
learning.",2023-09-03T07:14:49Z
10.1145/3613905.3650937,http://arxiv.org/pdf/2404.02213v1.pdf,"Exploring How Multiple Levels of GPT-Generated Programming Hints Support
  or Disappoint Novices","Recent studies have integrated large language models (LLMs) into diverse
educational contexts, including providing adaptive programming hints, a type of
feedback focuses on helping students move forward during problem-solving.
However, most existing LLM-based hint systems are limited to one single hint
type. To investigate whether and how different levels of hints can support
students' problem-solving and learning, we conducted a think-aloud study with
12 novices using the LLM Hint Factory, a system providing four levels of hints
from general natural language guidance to concrete code assistance, varying in
format and granularity. We discovered that high-level natural language hints
alone can be helpless or even misleading, especially when addressing next-step
or syntax-related help requests. Adding lower-level hints, like code examples
with in-line comments, can better support students. The findings open up future
work on customizing help responses from content, format, and granularity levels
to accurately identify and meet students' learning needs.",2024-04-02T18:05:26Z
,http://arxiv.org/pdf/2406.13161v1.pdf,"APPL: A Prompt Programming Language for Harmonious Integration of
  Programs and Large Language Model Prompts","Large Language Models (LLMs) have become increasingly capable of handling
diverse tasks with the aid of well-crafted prompts and integration of external
tools, but as task complexity rises, the workflow involving LLMs can be
complicated and thus challenging to implement and maintain. To address this
challenge, we propose APPL, A Prompt Programming Language that acts as a bridge
between computer programs and LLMs, allowing seamless embedding of prompts into
Python functions, and vice versa. APPL provides an intuitive and Python-native
syntax, an efficient parallelized runtime with asynchronous semantics, and a
tracing module supporting effective failure diagnosis and replaying without
extra costs. We demonstrate that APPL programs are intuitive, concise, and
efficient through three representative scenarios: Chain-of-Thought with
self-consistency (CoT-SC), ReAct tool use agent, and multi-agent chat.
Experiments on three parallelizable workflows further show that APPL can
effectively parallelize independent LLM calls, with a significant speedup ratio
that almost matches the estimation.",2024-06-19T02:29:59Z
,http://arxiv.org/pdf/2404.01754v1.pdf,"Peer-aided Repairer: Empowering Large Language Models to Repair Advanced
  Student Assignments","Automated generation of feedback on programming assignments holds significant
benefits for programming education, especially when it comes to advanced
assignments. Automated Program Repair techniques, especially Large Language
Model based approaches, have gained notable recognition for their potential to
fix introductory assignments. However, the programs used for evaluation are
relatively simple. It remains unclear how existing approaches perform in
repairing programs from higher-level programming courses. To address these
limitations, we curate a new advanced student assignment dataset named
Defects4DS from a higher-level programming course. Subsequently, we identify
the challenges related to fixing bugs in advanced assignments. Based on the
analysis, we develop a framework called PaR that is powered by the LLM. PaR
works in three phases: Peer Solution Selection, Multi-Source Prompt Generation,
and Program Repair. Peer Solution Selection identifies the closely related peer
programs based on lexical, semantic, and syntactic criteria. Then Multi-Source
Prompt Generation adeptly combines multiple sources of information to create a
comprehensive and informative prompt for the last Program Repair stage. The
evaluation on Defects4DS and another well-investigated ITSP dataset reveals
that PaR achieves a new state-of-the-art performance, demonstrating impressive
improvements of 19.94% and 15.2% in repair rate compared to prior
state-of-the-art LLM- and symbolic-based approaches, respectively",2024-04-02T09:12:21Z
,http://arxiv.org/pdf/2304.03816v1.pdf,"Towards Generating Functionally Correct Code Edits from Natural Language
  Issue Descriptions","Large language models (LLMs), such as OpenAI's Codex, have demonstrated their
potential to generate code from natural language descriptions across a wide
range of programming tasks. Several benchmarks have recently emerged to
evaluate the ability of LLMs to generate functionally correct code from natural
language intent with respect to a set of hidden test cases. This has enabled
the research community to identify significant and reproducible advancements in
LLM capabilities. However, there is currently a lack of benchmark datasets for
assessing the ability of LLMs to generate functionally correct code edits based
on natural language descriptions of intended changes. This paper aims to
address this gap by motivating the problem NL2Fix of translating natural
language descriptions of code changes (namely bug fixes described in Issue
reports in repositories) into correct code fixes. To this end, we introduce
Defects4J-NL2Fix, a dataset of 283 Java programs from the popular Defects4J
dataset augmented with high-level descriptions of bug fixes, and empirically
evaluate the performance of several state-of-the-art LLMs for the this task.
Results show that these LLMS together are capable of generating plausible fixes
for 64.6% of the bugs, and the best LLM-based technique can achieve up to
21.20% top-1 and 35.68% top-5 accuracy on this benchmark.",2023-04-07T18:58:33Z
,http://arxiv.org/pdf/2302.08468v3.pdf,LEVER: Learning to Verify Language-to-Code Generation with Execution,"The advent of large language models trained on code (code LLMs) has led to
significant progress in language-to-code generation. State-of-the-art
approaches in this area combine LLM decoding with sample pruning and reranking
using test cases or heuristics based on the execution results. However, it is
challenging to obtain test cases for many real-world language-to-code
applications, and heuristics cannot well capture the semantic features of the
execution results, such as data type and value range, which often indicates the
correctness of the program. In this work, we propose LEVER, a simple approach
to improve language-to-code generation by learning to verify the generated
programs with their execution results. Specifically, we train verifiers to
determine whether a program sampled from the LLMs is correct or not based on
the natural language input, the program itself and its execution results. The
sampled programs are reranked by combining the verification score with the LLM
generation probability, and marginalizing over programs with the same execution
results. On four datasets across the domains of table QA, math QA and basic
Python programming, LEVER consistently improves over the base code LLMs(4.6% to
10.9% with code-davinci-002) and achieves new state-of-the-art results on all
of them.",2023-02-16T18:23:22Z
,http://arxiv.org/pdf/2404.18400v2.pdf,"LLM-SR: Scientific Equation Discovery via Programming with Large
  Language Models","Mathematical equations have been unreasonably effective in describing complex
natural phenomena across various scientific disciplines. However, discovering
such insightful equations from data presents significant challenges due to the
necessity of navigating extremely high-dimensional combinatorial and nonlinear
hypothesis spaces. Traditional methods of equation discovery, commonly known as
symbolic regression, largely focus on extracting equations from data alone,
often neglecting the rich domain-specific prior knowledge that scientists
typically depend on. To bridge this gap, we introduce LLM-SR, a novel approach
that leverages the extensive scientific knowledge and robust code generation
capabilities of Large Language Models (LLMs) to discover scientific equations
from data in an efficient manner. Specifically, LLM-SR treats equations as
programs with mathematical operators and combines LLMs' scientific priors with
evolutionary search over equation programs. The LLM iteratively proposes new
equation skeleton hypotheses, drawing from its physical understanding, which
are then optimized against data to estimate skeleton parameters. We demonstrate
LLM-SR's effectiveness across three diverse scientific domains, where it
discovers physically accurate equations that provide significantly better fits
to in-domain and out-of-domain data compared to the well-established symbolic
regression baselines. Incorporating scientific prior knowledge also enables
LLM-SR to search the equation space more efficiently than baselines. Code is
available at: https://github.com/deep-symbolic-mathematics/LLM-SR",2024-04-29T03:30:06Z
,http://arxiv.org/pdf/2402.08073v2.pdf,Grounding Data Science Code Generation with Input-Output Specifications,"Large language models (LLMs) have recently demonstrated a remarkable ability
to generate code from natural language (NL) prompts. However, in the real
world, NL is often too ambiguous to capture the true intent behind programming
problems, requiring additional input-output (I/O) specifications.
Unfortunately, LLMs can have difficulty aligning their outputs with both the NL
prompt and the I/O specification. In this paper, we give a way to mitigate this
issue in the context of data science programming, where tasks require explicit
I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel
approach for the instruction fine-tuning of LLMs with respect to I/O
specifications. Our method leverages synthetic data produced by the LLM itself
and utilizes execution-derived feedback as a key learning signal. This
feedback, in the form of program I/O specifications, is provided to the LLM to
facilitate instruction fine-tuning. We evaluated our approach on two
challenging data science benchmarks, Arcade and DS-1000. The results
demonstrate a significant improvement in the LLM's ability to generate code
that is not only executable but also accurately aligned with user
specifications, substantially improving the quality of code generation for
complex data science tasks.",2024-02-12T21:32:49Z
,http://arxiv.org/pdf/2309.06236v1.pdf,"The first step is the hardest: Pitfalls of Representing and Tokenizing
  Temporal Data for Large Language Models","Large Language Models (LLMs) have demonstrated remarkable generalization
across diverse tasks, leading individuals to increasingly use them as personal
assistants and universal computing engines. Nevertheless, a notable obstacle
emerges when feeding numerical/temporal data into these models, such as data
sourced from wearables or electronic health records. LLMs employ tokenizers in
their input that break down text into smaller units. However, tokenizers are
not designed to represent numerical values and might struggle to understand
repetitive patterns and context, treating consecutive values as separate tokens
and disregarding their temporal relationships. Here, we discuss recent works
that employ LLMs for human-centric tasks such as in mobile health sensing and
present a case study showing that popular LLMs tokenize temporal data
incorrectly. To address that, we highlight potential solutions such as prompt
tuning with lightweight embedding layers as well as multimodal adapters, that
can help bridge this ""modality gap"". While the capability of language models to
generalize to other modalities with minimal or no finetuning is exciting, this
paper underscores the fact that their outputs cannot be meaningful if they
stumble over input nuances.",2023-09-12T13:51:29Z
,http://arxiv.org/pdf/2402.09649v1.pdf,ProtChatGPT: Towards Understanding Proteins with Large Language Models,"Protein research is crucial in various fundamental disciplines, but
understanding their intricate structure-function relationships remains
challenging. Recent Large Language Models (LLMs) have made significant strides
in comprehending task-specific knowledge, suggesting the potential for
ChatGPT-like systems specialized in protein to facilitate basic research. In
this work, we introduce ProtChatGPT, which aims at learning and understanding
protein structures via natural languages. ProtChatGPT enables users to upload
proteins, ask questions, and engage in interactive conversations to produce
comprehensive answers. The system comprises protein encoders, a
Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and
an LLM. The protein first undergoes protein encoders and PLP-former to produce
protein embeddings, which are then projected by the adapter to conform with the
LLM. The LLM finally combines user questions with projected embeddings to
generate informative answers. Experiments show that ProtChatGPT can produce
promising responses to proteins and their corresponding questions. We hope that
ProtChatGPT could form the basis for further exploration and application in
protein research. Code and our pre-trained model will be publicly available.",2024-02-15T01:22:30Z
,http://arxiv.org/pdf/2406.11930v1.pdf,A Critical Study of What Code-LLMs (Do Not) Learn,"Large Language Models trained on code corpora (code-LLMs) have demonstrated
impressive performance in various coding assistance tasks. However, despite
their increased size and training dataset, code-LLMs still have limitations
such as suggesting codes with syntactic errors, variable misuse etc. Some
studies argue that code-LLMs perform well on coding tasks because they use
self-attention and hidden representations to encode relations among input
tokens. However, previous works have not studied what code properties are not
encoded by code-LLMs. In this paper, we conduct a fine-grained analysis of
attention maps and hidden representations of code-LLMs. Our study indicates
that code-LLMs only encode relations among specific subsets of input tokens.
Specifically, by categorizing input tokens into syntactic tokens and
identifiers, we found that models encode relations among syntactic tokens and
among identifiers, but they fail to encode relations between syntactic tokens
and identifiers. We also found that fine-tuned models encode these relations
poorly compared to their pre-trained counterparts. Additionally, larger models
with billions of parameters encode significantly less information about code
than models with only a few hundred million parameters.",2024-06-17T13:11:17Z
,http://arxiv.org/pdf/2403.00046v2.pdf,"SEED: Customize Large Language Models with Sample-Efficient Adaptation
  for Code Generation","Although Large Language Models (LLMs) have made significant progress in code
generation, they still struggle with code generation tasks in specific
scenarios. These scenarios usually necessitate the adaptation of LLMs to
fulfill specific needs, but the limited training samples available in practice
lead to poor code generation performance. Therefore, how to effectively adapt
LLMs to new scenarios with few training samples is a major challenge for
current code generation. In this paper, we propose a novel adaptation approach
named SEED, which stands for Sample-Efficient adaptation with Error-Driven
learning for code generation. SEED leverages the errors made by LLMs as
learning opportunities, using error revision to overcome its own shortcomings,
thus achieving efficient learning. Specifically, SEED involves identifying
error code generated by LLMs, employing Self-revise for code revision,
optimizing the model with revised code, and iteratively adapting the process
for continuous improvement. Experimental results show that, compared to other
mainstream fine-tuning approaches, SEED achieves superior performance with few
training samples, showing an average relative improvement of 54.7% in Pass@1 on
multiple code generation benchmarks. We also validate the effectiveness of
Self-revise, which generates revised code that optimizes the model more
efficiently compared to the code samples from datasets. Moreover, SEED
consistently demonstrates strong performance across various LLMs, underscoring
its generalizability.",2024-02-29T16:09:02Z
,http://arxiv.org/pdf/2404.00287v1.pdf,"An Empirical Study of Automated Vulnerability Localization with Large
  Language Models","Recently, Automated Vulnerability Localization (AVL) has attracted much
attention, aiming to facilitate diagnosis by pinpointing the lines of code
responsible for discovered vulnerabilities. Large Language Models (LLMs) have
shown potential in various domains, yet their effectiveness in vulnerability
localization remains underexplored. In this work, we perform the first
comprehensive study of LLMs for AVL. Our investigation encompasses 10+ leading
LLMs suitable for code analysis, including ChatGPT and various open-source
models, across three architectural types: encoder-only, encoder-decoder, and
decoder-only, with model sizes ranging from 60M to 16B parameters. We explore
the efficacy of these LLMs using 4 distinct paradigms: zero-shot learning,
one-shot learning, discriminative fine-tuning, and generative fine-tuning. Our
evaluation framework is applied to the BigVul-based dataset for C/C++, and an
additional dataset comprising smart contract vulnerabilities. The results
demonstrate that discriminative fine-tuning of LLMs can significantly
outperform existing learning-based methods for AVL, while other paradigms prove
less effective or unexpectedly ineffective for the task. We also identify
challenges related to input length and unidirectional context in fine-tuning
processes for encoders and decoders. We then introduce two remedial strategies:
the sliding window and the right-forward embedding, both of which substantially
enhance performance. Furthermore, our findings highlight certain generalization
capabilities of LLMs across Common Weakness Enumerations (CWEs) and different
projects, indicating a promising pathway toward their practical application in
vulnerability localization.",2024-03-30T08:42:10Z
,http://arxiv.org/pdf/2403.01709v1.pdf,"Can LLMs Generate Architectural Design Decisions? -An Exploratory
  Empirical study","Architectural Knowledge Management (AKM) involves the organized handling of
information related to architectural decisions and design within a project or
organization. An essential artifact of AKM is the Architecture Decision Records
(ADR), which documents key design decisions. ADRs are documents that capture
decision context, decision made and various aspects related to a design
decision, thereby promoting transparency, collaboration, and understanding.
Despite their benefits, ADR adoption in software development has been slow due
to challenges like time constraints and inconsistent uptake. Recent
advancements in Large Language Models (LLMs) may help bridge this adoption gap
by facilitating ADR generation. However, the effectiveness of LLM for ADR
generation or understanding is something that has not been explored. To this
end, in this work, we perform an exploratory study that aims to investigate the
feasibility of using LLM for the generation of ADRs given the decision context.
In our exploratory study, we utilize GPT and T5-based models with 0-shot,
few-shot, and fine-tuning approaches to generate the Decision of an ADR given
its Context. Our results indicate that in a 0-shot setting, state-of-the-art
models such as GPT-4 generate relevant and accurate Design Decisions, although
they fall short of human-level performance. Additionally, we observe that more
cost-effective models like GPT-3.5 can achieve similar outcomes in a few-shot
setting, and smaller models such as Flan-T5 can yield comparable results after
fine-tuning. To conclude, this exploratory study suggests that LLM can generate
Design Decisions, but further research is required to attain human-level
generation and establish standardized widespread adoption.",2024-03-04T03:56:14Z
,http://arxiv.org/pdf/2404.14662v1.pdf,NExT: Teaching Large Language Models to Reason about Code Execution,"A fundamental skill among human developers is the ability to understand and
reason about program execution. As an example, a programmer can mentally
simulate code execution in natural language to debug and repair code (aka.
rubber duck debugging). However, large language models (LLMs) of code are
typically trained on the surface textual form of programs, thus may lack a
semantic understanding of how programs execute at run-time. To address this
issue, we propose NExT, a method to teach LLMs to inspect the execution traces
of programs (variable states of executed lines) and reason about their run-time
behavior through chain-of-thought (CoT) rationales. Specifically, NExT uses
self-training to bootstrap a synthetic training set of execution-aware
rationales that lead to correct task solutions (e.g., fixed programs) without
laborious manual annotation. Experiments on program repair tasks based on MBPP
and HumanEval demonstrate that NExT improves the fix rate of a PaLM 2 model, by
26.1% and 14.3% absolute, respectively, with significantly improved rationale
quality as verified by automated metrics and human raters. Our model can also
generalize to scenarios where program traces are absent at test-time.",2024-04-23T01:46:32Z
,http://arxiv.org/pdf/2406.09843v1.pdf,An Exploratory Study on Using Large Language Models for Mutation Testing,"The question of how to generate high-utility mutations, to be used for
testing purposes, forms a key challenge in mutation testing literature.
%Existing approaches rely either on human-specified syntactic rules or
learning-based approaches, all of which produce large numbers of redundant
mutants. Large Language Models (LLMs) have shown great potential in
code-related tasks but their utility in mutation testing remains unexplored. To
this end, we systematically investigate the performance of LLMs in generating
effective mutations w.r.t. to their usability, fault detection potential, and
relationship with real bugs. In particular, we perform a large-scale empirical
study involving 4 LLMs, including both open- and closed-source models, and 440
real bugs on two Java benchmarks. We find that compared to existing approaches,
LLMs generate more diverse mutations that are behaviorally closer to real bugs,
which leads to approximately 18% higher fault detection than current approaches
(i.e., 87% vs. 69%) in a newly collected set of bugs, purposely selected for
evaluating learning-based approaches, i.e., mitigating potential data leakage
concerns. Additionally, we explore alternative prompt engineering strategies
and the root causes of uncompilable mutations, produced by the LLMs, and
provide valuable insights for the use of LLMs in the context of mutation
testing.",2024-06-14T08:49:41Z
,http://arxiv.org/pdf/2402.03130v3.pdf,User Centric Evaluation of Code Generation Tools,"With the rapid advance of machine learning (ML) technology, large language
models (LLMs) are increasingly explored as an intelligent tool to generate
program code from natural language specifications. However, existing
evaluations of LLMs have focused on their capabilities in comparison with
humans. It is desirable to evaluate their usability when deciding on whether to
use a LLM in software production. This paper proposes a user centric method for
this purpose. It includes metadata in the test cases of a benchmark to describe
their usages, conducts testing in a multi-attempt process that mimics the uses
of LLMs, measures LLM generated solutions on a set of quality attributes that
reflect usability, and evaluates the performance based on user experiences in
the uses of LLMs as a tool.
  The paper also reports a case study with the method in the evaluation of
ChatGPT's usability as a code generation tool for the R programming language.
Our experiments demonstrated that ChatGPT is highly useful for generating R
program code although it may fail on hard programming tasks. The user
experiences are good with overall average number of attempts being 1.61 and the
average time of completion being 47.02 seconds. Our experiments also found that
the weakest aspect of usability is conciseness, which has a score of 3.80 out
of 5.",2024-02-05T15:56:19Z
,http://arxiv.org/pdf/2404.15236v1.pdf,"Revisiting Unnaturalness for Automated Program Repair in the Era of
  Large Language Models","Language models have improved by orders of magnitude with the recent
emergence of Transformer-based Large Language Models (LLMs). LLMs have
demonstrated their ability to generate natural code that is highly similar to
code written by professional developers. One intermediate value an LLM can emit
is entropy, which measures the naturalness of a token of code. We hypothesize
that entropy can be used to improve the performance of Automated Program Repair
(APR) tasks. While much progress has been made in Automated Program Repair
(APR), fault localization techniques suffer from a lack of diversity in ranking
scores, patch generation tools tend to be inefficient as all tests need to run
before determining if a patch is likely to be correct, and patch ranking often
suffers from the test-suite over-fitting problem. However, using an LLM
directly for APR introduces concerns for training data leakage. In this work,
we introduce a novel way of using the entropy of LLMs in combination with prior
APR tools to improve all stages of APR. We show that entropy is highly
complementary with prior fault localization tools. Our proposed re-ranking
method achieves a 50% Top-5 score improvement over SBFL. We propose a
patch-naturalness measurement, entropy-delta, to improve the efficiency of
template-based repair techniques by ranking plausible patches before undergoing
testing. When using entropy-delta for patch ranking and classification, our
proposed method can rank correct patches more effectively than state-of-the-art
machine learning tools with an 49% improvement in Top-1. Our work suggests that
LLMs can be an effective addition to compliment prior APR tasks while
minimizing both the test-suite overfitting problem and the LLM data leakage
problem.",2024-04-23T17:12:45Z
,http://arxiv.org/pdf/2309.07544v2.pdf,"VerilogEval: Evaluating Large Language Models for Verilog Code
  Generation","The increasing popularity of large language models (LLMs) has paved the way
for their application in diverse domains. This paper proposes a benchmarking
framework tailored specifically for evaluating LLM performance in the context
of Verilog code generation for hardware design and verification. We present a
comprehensive evaluation dataset consisting of 156 problems from the Verilog
instructional website HDLBits. The evaluation set consists of a diverse set of
Verilog code generation tasks, ranging from simple combinational circuits to
complex finite state machines. The Verilog code completions can be
automatically tested for functional correctness by comparing the transient
simulation outputs of the generated design with a golden solution. We also
demonstrate that the Verilog code generation capability of pretrained language
models could be improved with supervised fine-tuning by bootstrapping with LLM
generated synthetic problem-code pairs.",2023-09-14T09:15:34Z
,http://arxiv.org/pdf/2402.07158v1.pdf,"Effort and Size Estimation in Software Projects with Large Language
  Model-based Intelligent Interfaces","The advancement of Large Language Models (LLM) has also resulted in an
equivalent proliferation in its applications. Software design, being one, has
gained tremendous benefits in using LLMs as an interface component that extends
fixed user stories. However, inclusion of LLM-based AI agents in software
design often poses unexpected challenges, especially in the estimation of
development efforts. Through the example of UI-based user stories, we provide a
comparison against traditional methods and propose a new way to enhance
specifications of natural language-based questions that allows for the
estimation of development effort by taking into account data sources,
interfaces and algorithms.",2024-02-11T11:03:08Z
10.1109/TSE.2024.3397822,http://arxiv.org/pdf/2307.00593v3.pdf,"Isolating Compiler Bugs by Generating Effective Witness Programs with
  Large Language Models","Compiler bugs pose a significant threat to safety-critical applications, and
promptly as well as effectively isolating these bugs is crucial for assuring
the quality of compilers. However, the limited availability of debugging
information on reported bugs complicates the compiler bug isolation task.
Existing compiler bug isolation approaches convert the problem into a test
program mutation problem, but they are still limited by ineffective mutation
strategies or high human effort requirements. Drawing inspiration from the
recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT,
in code generation, we propose a new approach named LLM4CBI to utilize LLMs to
generate effective test programs for compiler bug isolation. However, using
LLMs directly for test program mutation may not yield the desired results due
to the challenges associated with formulating precise prompts and selecting
specialized prompts. To overcome the challenges, three new components are
designed in LLM4CBI. First, LLM4CBI utilizes a program complexity-guided prompt
production component, which leverages data and control flow analysis to
identify the most valuable variables and locations in programs for mutation.
Second, LLM4CBI employs a memorized prompt selection component, which adopts
reinforcement learning to select specialized prompts for mutating test programs
continuously. Third, a test program validation component is proposed to select
specialized feedback prompts to avoid repeating the same mistakes during the
mutation process. Compared with state-of-the-art approaches over 120 real bugs
from GCC and LLVM, our evaluation demonstrates the advantages of LLM4CBI: It
can isolate 69.70%/21.74% and 24.44%/8.92% more bugs than DiWi and RecBi within
Top-1/Top-5 ranked results. We also demonstrate that the LLMs component used in
LLM4CBI can be easily replaced while still achieving reasonable results.",2023-07-02T15:20:54Z
,http://arxiv.org/pdf/2206.12839v3.pdf,Repository-Level Prompt Generation for Large Language Models of Code,"With the success of large language models (LLMs) of code and their use as
code assistants (e.g. Codex used in GitHub Copilot), techniques for introducing
domain-specific knowledge in the prompt design process become important. In
this work, we propose a framework called Repo-Level Prompt Generator that
learns to generate example-specific prompts using prompt proposals. The prompt
proposals take context from the entire repository, thereby incorporating both
the structure of the repository and the context from other relevant files (e.g.
imports, parent class files). Our technique doesn't require any access to the
weights of the LLM, making it applicable in cases where we only have black-box
access to the LLM. We conduct experiments on the task of single-line
code-autocompletion using code repositories taken from Google Code archives. We
demonstrate that an oracle constructed from our prompt proposals gives a
remarkably high relative improvement of 36% over Codex, showing the quality of
these proposals. Further, we show that when we train a model to predict a
prompt proposal, we can achieve significant performance gains over Codex and
other baselines. We release our code, data, and trained checkpoints at:
\url{https://github.com/shrivastavadisha/repo_level_prompt_generation}.",2022-06-26T10:51:25Z
,http://arxiv.org/pdf/2305.04701v1.pdf,Differentially Private Attention Computation,"Large language models (LLMs) have had a profound impact on numerous aspects
of daily life including natural language processing, content generation,
research methodologies and so on. However, one crucial issue concerning the
inference results of large language models is security and privacy. In many
scenarios, the results generated by LLMs could possibly leak many confidential
or copyright information. A recent beautiful and breakthrough work [Vyas,
Kakade and Barak 2023] focus on such privacy issue of the LLMs from theoretical
perspective. It is well-known that computing the attention matrix is one of the
major task during the LLMs computation. Thus, how to give a provable privately
guarantees of computing the attention matrix is an important research
direction.
  Previous work [Alman and Song 2023, Brand, Song and Zhou 2023] have proposed
provable tight result for fast computation of attention without considering
privacy concerns. One natural mathematical formulation to quantity the privacy
in theoretical computer science graduate school textbook is differential
privacy. Inspired by [Vyas, Kakade and Barak 2023], in this work, we provide a
provable result for showing how to differentially private approximate the
attention matrix.
  From technique perspective, our result replies on a pioneering work in the
area of differential privacy by [Alabi, Kothari, Tankala, Venkat and Zhang
2022].",2023-05-08T13:32:41Z
,http://arxiv.org/pdf/2310.17064v1.pdf,"math-PVS: A Large Language Model Framework to Map Scientific
  Publications to PVS Theories","As artificial intelligence (AI) gains greater adoption in a wide variety of
applications, it has immense potential to contribute to mathematical discovery,
by guiding conjecture generation, constructing counterexamples, assisting in
formalizing mathematics, and discovering connections between different
mathematical areas, to name a few.
  While prior work has leveraged computers for exhaustive mathematical proof
search, recent efforts based on large language models (LLMs) aspire to position
computing platforms as co-contributors in the mathematical research process.
Despite their current limitations in logic and mathematical tasks, there is
growing interest in melding theorem proving systems with foundation models.
This work investigates the applicability of LLMs in formalizing advanced
mathematical concepts and proposes a framework that can critically review and
check mathematical reasoning in research papers. Given the noted reasoning
shortcomings of LLMs, our approach synergizes the capabilities of proof
assistants, specifically PVS, with LLMs, enabling a bridge between textual
descriptions in academic papers and formal specifications in PVS. By harnessing
the PVS environment, coupled with data ingestion and conversion mechanisms, we
envision an automated process, called \emph{math-PVS}, to extract and formalize
mathematical theorems from research papers, offering an innovative tool for
academic review and discovery.",2023-10-25T23:54:04Z
,http://arxiv.org/pdf/2406.05741v1.pdf,Digital Business Model Analysis Using a Large Language Model,"Digital transformation (DX) has recently become a pressing issue for many
companies as the latest digital technologies, such as artificial intelligence
and the Internet of Things, can be easily utilized. However, devising new
business models is not easy for compa-nies, though they can improve their
operations through digital technologies. Thus, business model design support
methods are needed by people who lack digital tech-nology expertise. In
contrast, large language models (LLMs) represented by ChatGPT and natural
language processing utilizing LLMs have been developed revolutionarily. A
business model design support system that utilizes these technologies has great
potential. However, research on this area is scant. Accordingly, this study
proposes an LLM-based method for comparing and analyzing similar companies from
different business do-mains as a first step toward business model design
support utilizing LLMs. This method can support idea generation in digital
business model design.",2024-06-09T11:16:11Z
,http://arxiv.org/pdf/2308.05177v1.pdf,Fixing Rust Compilation Errors using LLMs,"The Rust programming language, with its safety guarantees, has established
itself as a viable choice for low-level systems programming language over the
traditional, unsafe alternatives like C/C++. These guarantees come from a
strong ownership-based type system, as well as primitive support for features
like closures, pattern matching, etc., that make the code more concise and
amenable to reasoning. These unique Rust features also pose a steep learning
curve for programmers.
  This paper presents a tool called RustAssistant that leverages the emergent
capabilities of Large Language Models (LLMs) to automatically suggest fixes for
Rust compilation errors. RustAssistant uses a careful combination of prompting
techniques as well as iteration with an LLM to deliver high accuracy of fixes.
RustAssistant is able to achieve an impressive peak accuracy of roughly 74% on
real-world compilation errors in popular open-source Rust repositories. We plan
to release our dataset of Rust compilation errors to enable further research.",2023-08-09T18:30:27Z
,http://arxiv.org/pdf/2312.16066v1.pdf,A Prompt Learning Framework for Source Code Summarization,"(Source) code summarization is the task of automatically generating natural
language summaries for given code snippets. Such summaries play a key role in
helping developers understand and maintain source code. Recently, with the
successful application of large language models (LLMs) in numerous fields,
software engineering researchers have also attempted to adapt LLMs to solve
code summarization tasks. The main adaptation schemes include instruction
prompting and task-oriented fine-tuning. However, instruction prompting
involves designing crafted prompts for zero-shot learning or selecting
appropriate samples for few-shot learning and requires users to have
professional domain knowledge, while task-oriented fine-tuning requires high
training costs. In this paper, we propose a novel prompt learning framework for
code summarization called PromptCS. PromptCS trains a prompt agent that can
generate continuous prompts to unleash the potential for LLMs in code
summarization. Compared to the human-written discrete prompt, the continuous
prompts are produced under the guidance of LLMs and are therefore easier to
understand by LLMs. PromptCS freezes the parameters of LLMs when training the
prompt agent, which can greatly reduce the requirements for training resources.
We evaluate PromptCS on the CodeSearchNet dataset involving multiple
programming languages. The results show that PromptCS significantly outperforms
instruction prompting schemes on all four widely used metrics. In some base
LLMs, e.g., CodeGen-Multi-2B and StarCoderBase-1B and -3B, PromptCS even
outperforms the task-oriented fine-tuning scheme. More importantly, the
training efficiency of PromptCS is faster than the task-oriented fine-tuning
scheme, with a more pronounced advantage on larger LLMs. The results of the
human evaluation demonstrate that PromptCS can generate more good summaries
compared to baselines.",2023-12-26T14:37:55Z
,http://arxiv.org/pdf/2404.02183v1.pdf,"Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra
  Large-Scale Code Generation and Optimization","Recent advancements in automatic code generation using large language model
(LLM) agent have brought us closer to the future of automated software
development. However, existing single-agent approaches face limitations in
generating and improving large-scale, complex codebases due to constraints in
context length. To tackle this challenge, we propose Self-Organized multi-Agent
framework (SoA), a novel multi-agent framework that enables the scalable and
efficient generation and optimization of large-scale code. In SoA,
self-organized agents operate independently to generate and modify code
components while seamlessly collaborating to construct the overall codebase. A
key feature of our framework is the automatic multiplication of agents based on
problem complexity, allowing for dynamic scalability. This enables the overall
code volume to be increased indefinitely according to the number of agents,
while the amount of code managed by each agent remains constant. We evaluate
SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent
system, each agent in SoA handles significantly less code, yet the overall
generated code is substantially greater. Moreover, SoA surpasses the powerful
single-agent baseline by 5% in terms of Pass@1 accuracy.",2024-04-02T13:37:28Z
,http://arxiv.org/pdf/2404.09836v2.pdf,"How Far Have We Gone in Stripped Binary Code Understanding Using Large
  Language Models","Binary code analysis plays a pivotal role in various software security
applications, such as software maintenance, malware detection, software
vulnerability discovery, patch analysis, etc. However, unlike source code,
understanding binary code is challenging for reverse engineers due to the
absence of semantic information. Therefore, automated tools are needed to
assist human players in interpreting binary code. In recent years, two groups
of technologies have shown promising prospects: (1) Deep learning-based
technologies have demonstrated competitive results in tasks related to binary
code understanding, furthermore, (2) Large Language Models (LLMs) have been
extensively pre-trained at the source-code level for tasks such as code
understanding and generation. This makes participants wonder about the ability
of LLMs in binary code understanding.
  In this work, we propose a benchmark to evaluate the effectiveness of LLMs in
real-world reverse engineering scenarios. The benchmark covers two key binary
code understanding tasks, including function name recovery and binary code
summarization. We gain valuable insights into their capabilities and
limitations through extensive evaluations of popular LLMs using our benchmark.
Our evaluations reveal that existing LLMs can understand binary code to a
certain extent, thereby improving the efficiency of binary code analysis. Our
results highlight the great potential of the LLMs in advancing the field of
binary code understanding.",2024-04-15T14:44:08Z
,http://arxiv.org/pdf/2310.02031v7.pdf,OceanGPT: A Large Language Model for Ocean Science Tasks,"Ocean science, which delves into the oceans that are reservoirs of life and
biodiversity, is of great significance given that oceans cover over 70% of our
planet's surface. Recently, advances in Large Language Models (LLMs) have
transformed the paradigm in science. Despite the success in other domains,
current LLMs often fall short in catering to the needs of domain experts like
oceanographers, and the potential of LLMs for ocean science is under-explored.
The intrinsic reasons are the immense and intricate nature of ocean data as
well as the necessity for higher granularity and richness in knowledge. To
alleviate these issues, we introduce OceanGPT, the first-ever large language
model in the ocean domain, which is expert in various ocean science tasks. We
also propose OceanGPT, a novel framework to automatically obtain a large volume
of ocean domain instruction data, which generates instructions based on
multi-agent collaboration. Additionally, we construct the first oceanography
benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean
domain. Though comprehensive experiments, OceanGPT not only shows a higher
level of knowledge expertise for oceans science tasks but also gains
preliminary embodied intelligence capabilities in ocean technology.",2023-10-03T13:17:35Z
,http://arxiv.org/pdf/2405.16661v1.pdf,RLSF: Reinforcement Learning via Symbolic Feedback,"In recent years, large language models (LLMs) have had a dramatic impact on
various sub-fields of AI, most notably on natural language understanding tasks.
However, there is widespread agreement that the logical reasoning capabilities
of contemporary LLMs are, at best, fragmentary (i.e., may work well on some
problem instances but fail dramatically on others). While traditional LLM
fine-tuning approaches (e.g., those that use human feedback) do address this
problem to some degree, they suffer from many issues, including unsound
black-box reward models, difficulties in collecting preference data, and sparse
scalar reward values.
  To address these challenges, we propose a new training/fine-tuning paradigm
we refer to as Reinforcement Learning via Symbolic Feedback (RLSF), which is
aimed at enhancing the reasoning capabilities of LLMs. In the RLSF setting, the
LLM that is being trained/fine-tuned is considered as the RL agent, while the
environment is allowed access to reasoning or domain knowledge tools (e.g.,
solvers, algebra systems). Crucially, in RLSF, these reasoning tools can
provide feedback to the LLMs via poly-sized certificates (e.g., proofs), that
characterize errors in the LLM-generated object with respect to some
correctness specification. The ability of RLSF-based training/fine-tuning to
leverage certificate-generating symbolic tools enables sound fine-grained
(token-level) reward signals to LLMs, and thus addresses the limitations of
traditional reward models mentioned above. Via extensive evaluations, we show
that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on
two different applications, namely, program synthesis from natural language
pseudo-code to programming language (C++) and solving the Game of 24.",2024-05-26T18:49:59Z
,http://arxiv.org/pdf/2403.14077v4.pdf,"Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language
  Models for Media Forensics","DeepFakes, which refer to AI-generated media content, have become an
increasing concern due to their use as a means for disinformation. Detecting
DeepFakes is currently solved with programmed machine learning algorithms. In
this work, we investigate the capabilities of multimodal large language models
(LLMs) in DeepFake detection. We conducted qualitative and quantitative
experiments to demonstrate multimodal LLMs and show that they can expose
AI-generated images through careful experimental design and prompt engineering.
This is interesting, considering that LLMs are not inherently tailored for
media forensic tasks, and the process does not require programming. We discuss
the limitations of multimodal LLMs for these tasks and suggest possible
improvements.",2024-03-21T01:57:30Z
,http://arxiv.org/pdf/2202.01142v1.pdf,Pop Quiz! Can a Large Language Model Help With Reverse Engineering?,"Large language models (such as OpenAI's Codex) have demonstrated impressive
zero-shot multi-task capabilities in the software domain, including code
explanation. In this work, we examine if this ability can be used to help with
reverse engineering. Specifically, we investigate prompting Codex to identify
the purpose, capabilities, and important variable names or values from code,
even when the code is produced through decompilation. Alongside an examination
of the model's responses in answering open-ended questions, we devise a
true/false quiz framework to characterize the performance of the language
model. We present an extensive quantitative analysis of the measured
performance of the language model on a set of program purpose identification
and information extraction tasks: of the 136,260 questions we posed, it
answered 72,754 correctly. A key takeaway is that while promising, LLMs are not
yet ready for zero-shot reverse engineering.",2022-02-02T17:09:15Z
,http://arxiv.org/pdf/2402.05939v1.pdf,"Uncertainty Awareness of Large Language Models Under Code Distribution
  Shifts: A Benchmark Study","Large Language Models (LLMs) have been widely employed in programming
language analysis to enhance human productivity. Yet, their reliability can be
compromised by various code distribution shifts, leading to inconsistent
outputs. While probabilistic methods are known to mitigate such impact through
uncertainty calibration and estimation, their efficacy in the language domain
remains underexplored compared to their application in image-based tasks. In
this work, we first introduce a large-scale benchmark dataset, incorporating
three realistic patterns of code distribution shifts at varying intensities.
Then we thoroughly investigate state-of-the-art probabilistic methods applied
to CodeLlama using these shifted code snippets. We observe that these methods
generally improve the uncertainty awareness of CodeLlama, with increased
calibration quality and higher uncertainty estimation~(UE) precision. However,
our study further reveals varied performance dynamics across different criteria
(e.g., calibration error vs misclassification detection) and trade-off between
efficacy and efficiency, highlighting necessary methodological selection
tailored to specific contexts.",2024-01-12T00:00:32Z
,http://arxiv.org/pdf/2406.10279v1.pdf,"We Have a Package for You! A Comprehensive Analysis of Package
  Hallucinations by Code Generating LLMs","The reliance of popular programming languages such as Python and JavaScript
on centralized package repositories and open-source software, combined with the
emergence of code-generating Large Language Models (LLMs), has created a new
type of threat to the software supply chain: package hallucinations. These
hallucinations, which arise from fact-conflicting errors when generating code
using LLMs, represent a novel form of package confusion attack that poses a
critical threat to the integrity of the software supply chain. This paper
conducts a rigorous and comprehensive evaluation of package hallucinations
across different programming languages, settings, and parameters, exploring how
different configurations of LLMs affect the likelihood of generating erroneous
package recommendations and identifying the root causes of this phenomena.
Using 16 different popular code generation models, across two programming
languages and two unique prompt datasets, we collect 576,000 code samples which
we analyze for package hallucinations. Our findings reveal that 19.7% of
generated packages across all the tested LLMs are hallucinated, including a
staggering 205,474 unique examples of hallucinated package names, further
underscoring the severity and pervasiveness of this threat. We also implemented
and evaluated mitigation strategies based on Retrieval Augmented Generation
(RAG), self-detected feedback, and supervised fine-tuning. These techniques
demonstrably reduced package hallucinations, with hallucination rates for one
model dropping below 3%. While the mitigation efforts were effective in
reducing hallucination rates, our study reveals that package hallucinations are
a systemic and persistent phenomenon that pose a significant challenge for code
generating LLMs.",2024-06-12T03:29:06Z
,http://arxiv.org/pdf/2305.03017v4.pdf,"Improving Code Example Recommendations on Informal Documentation Using
  BERT and Query-Aware LSH: A Comparative Study","Our research investigates the recommendation of code examples to aid software
developers, a practice that saves developers significant time by providing
ready-to-use code snippets. The focus of our study is Stack Overflow, a
commonly used resource for coding discussions and solutions, particularly in
the context of the Java programming language. We applied BERT, a powerful Large
Language Model (LLM) that enables us to transform code examples into numerical
vectors by extracting their semantic information. Once these numerical
representations are prepared, we identify Approximate Nearest Neighbors (ANN)
using Locality-Sensitive Hashing (LSH). Our research employed two variants of
LSH: Random Hyperplane-based LSH and Query-Aware LSH. We rigorously compared
these two approaches across four parameters: HitRate, Mean Reciprocal Rank
(MRR), Average Execution Time, and Relevance. Our study revealed that the
Query-Aware (QA) approach showed superior performance over the Random
Hyperplane-based (RH) method. Specifically, it exhibited a notable improvement
of 20\% to 35\% in HitRate for query pairs compared to the RH approach.
Furthermore, the QA approach proved significantly more time-efficient, with its
speed in creating hashing tables and assigning data samples to buckets being at
least four times faster. It can return code examples within milliseconds,
whereas the RH approach typically requires several seconds to recommend code
examples. Due to the superior performance of the QA approach, we tested it
against PostFinder and FaCoY, the state-of-the-art baselines. Our QA method
showed comparable efficiency proving its potential for effective code
recommendation.",2023-05-04T17:43:19Z
,http://arxiv.org/pdf/2307.14991v2.pdf,Multilingual Code Co-Evolution Using Large Language Models,"Many software projects implement APIs and algorithms in multiple programming
languages. Maintaining such projects is tiresome, as developers have to ensure
that any change (e.g., a bug fix or a new feature) is being propagated, timely
and without errors, to implementations in other programming languages. In the
world of ever-changing software, using rule-based translation tools (i.e.,
transpilers) or machine learning models for translating code from one language
to another provides limited value. Translating each time the entire codebase
from one language to another is not the way developers work. In this paper, we
target a novel task: translating code changes from one programming language to
another using large language models (LLMs). We design and implement the first
LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code
changes as edit sequences and learns to correlate changes across programming
languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code
changes from 8 pairs of open-source software projects implementing similar
functionalities in two programming languages (Java and C#). Results show that
Codeditor outperforms the state-of-the-art approaches by a large margin on all
commonly used automatic metrics. Our work also reveals that Codeditor is
complementary to the existing generation-based models, and their combination
ensures even greater performance.",2023-07-27T16:37:30Z
,http://arxiv.org/pdf/2401.01269v2.pdf,LLbezpeky: Leveraging Large Language Models for Vulnerability Detection,"Despite the continued research and progress in building secure systems,
Android applications continue to be ridden with vulnerabilities, necessitating
effective detection methods. Current strategies involving static and dynamic
analysis tools come with limitations like overwhelming number of false
positives and limited scope of analysis which make either difficult to adopt.
Over the past years, machine learning based approaches have been extensively
explored for vulnerability detection, but its real-world applicability is
constrained by data requirements and feature engineering challenges. Large
Language Models (LLMs), with their vast parameters, have shown tremendous
potential in understanding semnatics in human as well as programming languages.
We dive into the efficacy of LLMs for detecting vulnerabilities in the context
of Android security. We focus on building an AI-driven workflow to assist
developers in identifying and rectifying vulnerabilities. Our experiments show
that LLMs outperform our expectations in finding issues within applications
correctly flagging insecure apps in 91.67% of cases in the Ghera benchmark. We
use inferences from our experiments towards building a robust and actionable
vulnerability detection system and demonstrate its effectiveness. Our
experiments also shed light on how different various simple configurations can
affect the True Positive (TP) and False Positive (FP) rates.",2024-01-02T16:14:30Z
,http://arxiv.org/pdf/2403.06050v1.pdf,"Explaining Code with a Purpose: An Integrated Approach for Developing
  Code Comprehension and Prompting Skills","Reading, understanding and explaining code have traditionally been important
skills for novices learning programming. As large language models (LLMs) become
prevalent, these foundational skills are more important than ever given the
increasing need to understand and evaluate model-generated code. Brand new
skills are also needed, such as the ability to formulate clear prompts that can
elicit intended code from an LLM. Thus, there is great interest in integrating
pedagogical approaches for the development of both traditional coding
competencies and the novel skills required to interact with LLMs. One effective
way to develop and assess code comprehension ability is with ``Explain in plain
English'' (EiPE) questions, where students succinctly explain the purpose of a
fragment of code. However, grading EiPE questions has always been difficult
given the subjective nature of evaluating written explanations and this has
stifled their uptake. In this paper, we explore a natural synergy between EiPE
questions and code-generating LLMs to overcome this limitation. We propose
using an LLM to generate code based on students' responses to EiPE questions --
not only enabling EiPE responses to be assessed automatically, but helping
students develop essential code comprehension and prompt crafting skills in
parallel. We investigate this idea in an introductory programming course and
report student success in creating effective prompts for solving EiPE
questions. We also examine student perceptions of this activity and how it
influences their views on the use of LLMs for aiding and assessing learning.",2024-03-10T00:23:08Z
,http://arxiv.org/pdf/2312.01109v1.pdf,"Kattis vs. ChatGPT: Assessment and Evaluation of Programming Tasks in
  the Age of Artificial Intelligence","AI-powered education technologies can support students and teachers in
computer science education. However, with the recent developments in generative
AI, and especially the increasingly emerging popularity of ChatGPT, the
effectiveness of using large language models for solving programming tasks has
been underexplored. The present study examines ChatGPT's ability to generate
code solutions at different difficulty levels for introductory programming
courses. We conducted an experiment where ChatGPT was tested on 127 randomly
selected programming problems provided by Kattis, an automatic software grading
tool for computer science programs, often used in higher education. The results
showed that ChatGPT independently could solve 19 out of 127 programming tasks
generated and assessed by Kattis. Further, ChatGPT was found to be able to
generate accurate code solutions for simple problems but encountered
difficulties with more complex programming tasks. The results contribute to the
ongoing debate on the utility of AI-powered tools in programming education.",2023-12-02T11:09:17Z
10.1109/MS.2024.3418570,http://arxiv.org/pdf/2401.17626v2.pdf,Generative AI to Generate Test Data Generators,"Generating fake data is an essential dimension of modern software testing, as
demonstrated by the number and significance of data faking libraries. Yet,
developers of faking libraries cannot keep up with the wide range of data to be
generated for different natural languages and domains. In this paper, we assess
the ability of generative AI for generating test data in different domains. We
design three types of prompts for Large Language Models (LLMs), which perform
test data generation tasks at different levels of integrability: 1) raw test
data generation, 2) synthesizing programs in a specific language that generate
useful test data, and 3) producing programs that use state-of-the-art faker
libraries. We evaluate our approach by prompting LLMs to generate test data for
11 domains. The results show that LLMs can successfully generate realistic test
data generators in a wide range of domains at all three levels of
integrability.",2024-01-31T06:58:26Z
,http://arxiv.org/pdf/2406.03376v1.pdf,Log Parsing with Self-Generated In-Context Learning and Self-Correction,"Log parsing transforms log messages into structured formats, serving as a
crucial step for log analysis. Despite a variety of log parsing methods that
have been proposed, their performance on evolving log data remains
unsatisfactory due to reliance on human-crafted rules or learning-based models
with limited training data. The recent emergence of large language models
(LLMs) has demonstrated strong abilities in understanding natural language and
code, making it promising to apply LLMs for log parsing. Consequently, several
studies have proposed LLM-based log parsers. However, LLMs may produce
inaccurate templates, and existing LLM-based log parsers directly use the
template generated by the LLM as the parsing result, hindering the accuracy of
log parsing. Furthermore, these log parsers depend heavily on historical log
data as demonstrations, which poses challenges in maintaining accuracy when
dealing with scarce historical log data or evolving log data. To address these
challenges, we propose AdaParser, an effective and adaptive log parsing
framework using LLMs with self-generated in-context learning (SG-ICL) and
self-correction. To facilitate accurate log parsing, AdaParser incorporates a
novel component, a template corrector, which utilizes the LLM to correct
potential parsing errors in the templates it generates. In addition, AdaParser
maintains a dynamic candidate set composed of previously generated templates as
demonstrations to adapt evolving log data. Extensive experiments on public
large-scale datasets show that AdaParser outperforms state-of-the-art methods
across all metrics, even in zero-shot scenarios. Moreover, when integrated with
different LLMs, AdaParser consistently enhances the performance of the utilized
LLMs by a large margin.",2024-06-05T15:31:43Z
10.1145/3624062.3624088,http://arxiv.org/pdf/2308.07505v2.pdf,Data Race Detection Using Large Language Models,"Large language models (LLMs) are demonstrating significant promise as an
alternate strategy to facilitate analyses and optimizations of high-performance
computing programs, circumventing the need for resource-intensive manual tool
creation. In this paper, we explore a novel LLM-based data race detection
approach combining prompting engineering and fine-tuning techniques. We create
a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with
fine-grain labels showing the presence of data race pairs and their associated
variables, line numbers, and read/write information. DRB-ML is then used to
evaluate representative LLMs and fine-tune open-source ones. Our experiment
shows that LLMs can be a viable approach to data race detection. However, they
still cannot compete with traditional data race detection tools when we need
detailed information about variable pairs causing data races.",2023-08-15T00:08:43Z
,http://arxiv.org/pdf/2402.14361v2.pdf,OpenTab: Advancing Large Language Models as Open-domain Table Reasoners,"Large Language Models (LLMs) trained on large volumes of data excel at
various natural language tasks, but they cannot handle tasks requiring
knowledge that has not been trained on previously. One solution is to use a
retriever that fetches relevant information to expand LLM's knowledge scope.
However, existing textual-oriented retrieval-based LLMs are not ideal on
structured table data due to diversified data modalities and large table sizes.
In this work, we propose OpenTab, an open-domain table reasoning framework
powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant
tables and then generates SQL programs to parse the retrieved tables
efficiently. Utilizing the intermediate data derived from the SQL executions,
it conducts grounded inference to produce accurate response. Extensive
experimental evaluation shows that OpenTab significantly outperforms baselines
in both open- and closed-domain settings, achieving up to 21.5% higher
accuracy. We further run ablation studies to validate the efficacy of our
proposed designs of the system.",2024-02-22T08:01:01Z
,http://arxiv.org/pdf/2402.14837v1.pdf,"An Empirical Categorization of Prompting Techniques for Large Language
  Models: A Practitioner's Guide","Due to rapid advancements in the development of Large Language Models (LLMs),
programming these models with prompts has recently gained significant
attention. However, the sheer number of available prompt engineering techniques
creates an overwhelming landscape for practitioners looking to utilize these
tools. For the most efficient and effective use of LLMs, it is important to
compile a comprehensive list of prompting techniques and establish a
standardized, interdisciplinary categorization framework. In this survey, we
examine some of the most well-known prompting techniques from both academic and
practical viewpoints and classify them into seven distinct categories. We
present an overview of each category, aiming to clarify their unique
contributions and showcase their practical applications in real-world examples
in order to equip fellow practitioners with a structured framework for
understanding and categorizing prompting techniques tailored to their specific
domains. We believe that this approach will help simplify the complex landscape
of prompt engineering and enable more effective utilization of LLMs in various
applications. By providing practitioners with a systematic approach to prompt
categorization, we aim to assist in navigating the intricacies of effective
prompt design for conversational pre-trained LLMs and inspire new possibilities
in their respective fields.",2024-02-18T23:03:56Z
,http://arxiv.org/pdf/2406.01566v1.pdf,"Helix: Distributed Serving of Large Language Models via Max-Flow on
  Heterogeneous GPUs","This paper introduces Helix, a distributed system for high-throughput,
low-latency large language model (LLM) serving on heterogeneous GPU clusters. A
key idea behind Helix is to formulate inference computation of LLMs over
heterogeneous GPUs and network connections as a max-flow problem for a
directed, weighted graph, whose nodes represent GPU instances and edges capture
both GPU and network heterogeneity through their capacities. Helix then uses a
mixed integer linear programming (MILP) algorithm to discover highly optimized
strategies to serve LLMs. This approach allows Helix to jointly optimize model
placement and request scheduling, two highly entangled tasks in heterogeneous
LLM serving. Our evaluation on several heterogeneous cluster settings ranging
from 24 to 42 GPU nodes shows that Helix improves serving throughput by up to
2.7$\times$ and reduces prompting and decoding latency by up to 2.8$\times$ and
1.3$\times$, respectively, compared to best existing approaches.",2024-06-03T17:47:53Z
,http://arxiv.org/pdf/2304.02014v1.pdf,"Large Language Models are Edge-Case Fuzzers: Testing Deep Learning
  Libraries via FuzzGPT","Deep Learning (DL) library bugs affect downstream DL applications,
emphasizing the need for reliable systems. Generating valid input programs for
fuzzing DL libraries is challenging due to the need for satisfying both
language syntax/semantics and constraints for constructing valid computational
graphs. Recently, the TitanFuzz work demonstrates that modern Large Language
Models (LLMs) can be directly leveraged to implicitly learn all the constraints
to generate valid DL programs for fuzzing. However, LLMs tend to generate
ordinary programs following similar patterns seen in their massive training
corpora, while fuzzing favors unusual inputs that cover edge cases or are
unlikely to be manually produced.
  To fill this gap, this paper proposes FuzzGPT, the first technique to prime
LLMs to synthesize unusual programs for fuzzing. FuzzGPT is built on the
well-known hypothesis that historical bug-triggering programs may include
rare/valuable code ingredients important for bug finding. Traditional
techniques leveraging such historical information require intensive human
efforts to design dedicated generators and ensure the validity of generated
programs. FuzzGPT demonstrates that this process can be fully automated via the
intrinsic capabilities of LLMs (including fine-tuning and in-context learning),
while being generalizable and applicable to challenging domains. While FuzzGPT
can be applied with different LLMs, this paper focuses on the powerful
GPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potential
of directly leveraging the instruct-following capability of the recent ChatGPT
for effective fuzzing. Evaluation on two popular DL libraries (PyTorch and
TensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz,
detecting 76 bugs, with 49 already confirmed as previously unknown bugs,
including 11 high-priority bugs or security vulnerabilities.",2023-04-04T17:59:52Z
,http://arxiv.org/pdf/2403.00894v1.pdf,"A systematic evaluation of large language models for generating
  programming code","We systematically evaluated the performance of seven large language models in
generating programming code using various prompt strategies, programming
languages, and task difficulties. GPT-4 substantially outperforms other large
language models, including Gemini Ultra and Claude 2. The coding performance of
GPT-4 varies considerably with different prompt strategies. In most LeetCode
and GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the
optimal prompt strategy outperforms 85 percent of human participants.
Additionally, GPT-4 demonstrates strong capabilities in translating code
between different programming languages and in learning from past errors. The
computational efficiency of the code generated by GPT-4 is comparable to that
of human programmers. These results suggest that GPT-4 has the potential to
serve as a reliable assistant in programming code generation and software
development.",2024-03-01T14:43:06Z
,http://arxiv.org/pdf/2308.13062v1.pdf,"ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel
  Patching","Security critical software, e.g., OpenSSL, comes with numerous side-channel
leakages left unpatched due to a lack of resources or experts. The situation
will only worsen as the pace of code development accelerates, with developers
relying on Large Language Models (LLMs) to automatically generate code. In this
work, we explore the use of LLMs in generating patches for vulnerable code with
microarchitectural side-channel leakages. For this, we investigate the
generative abilities of powerful LLMs by carefully crafting prompts following a
zero-shot learning approach. All generated code is dynamically analyzed by
leakage detection tools, which are capable of pinpointing information leakage
at the instruction level leaked either from secret dependent accesses or
branches or vulnerable Spectre gadgets, respectively. Carefully crafted prompts
are used to generate candidate replacements for vulnerable code, which are then
analyzed for correctness and for leakage resilience. From a cost/performance
perspective, the GPT4-based configuration costs in API calls a mere few cents
per vulnerability fixed. Our results show that LLM-based patching is far more
cost-effective and thus provides a scalable solution. Finally, the framework we
propose will improve in time, especially as vulnerability detection tools and
LLMs mature.",2023-08-24T20:04:36Z
,http://arxiv.org/pdf/2305.11202v3.pdf,LLM-based Frameworks for Power Engineering from Routine to Novel Tasks,"The digitalization of energy sectors has expanded the coding responsibilities
for power engineers and researchers. This research article explores the
potential of leveraging Large Language Models (LLMs) to alleviate this burden.
Here, we propose LLM-based frameworks for different programming tasks in power
systems. For well-defined and routine tasks like the classic unit commitment
(UC) problem, we deploy an end-to-end framework to systematically assesses four
leading LLMs-ChatGPT 3.5, ChatGPT 4.0, Claude and Google Bard in terms of
success rate, consistency, and robustness. For complex tasks with limited prior
knowledge, we propose a human-in-the-loop framework to enable engineers and
LLMs to collaboratively solve the problem through interactive-learning of
method recommendation, problem de-composition, subtask programming and
synthesis. Through a comparative study between two frameworks, we find that
human-in-the-loop features like web access, problem decomposition with field
knowledge and human-assisted code synthesis are essential as LLMs currently
still fall short in acquiring cutting-edge and domain-specific knowledge to
complete a holistic problem-solving project.",2023-05-18T15:36:06Z
10.1145/3636243.3636249,http://arxiv.org/pdf/2310.16984v1.pdf,"Patterns of Student Help-Seeking When Using a Large Language
  Model-Powered Programming Assistant","Providing personalized assistance at scale is a long-standing challenge for
computing educators, but a new generation of tools powered by large language
models (LLMs) offers immense promise. Such tools can, in theory, provide
on-demand help in large class settings and be configured with appropriate
guardrails to prevent misuse and mitigate common concerns around learner
over-reliance. However, the deployment of LLM-powered tools in authentic
classroom settings is still rare, and very little is currently known about how
students will use them in practice and what type of help they will seek. To
address this, we examine students' use of an innovative LLM-powered tool that
provides on-demand programming assistance without revealing solutions directly.
We deployed the tool for 12 weeks in an introductory computer and data science
course ($n = 52$), collecting more than 2,500 queries submitted by students
throughout the term. We manually categorized all student queries based on the
type of assistance sought, and we automatically analyzed several additional
query characteristics. We found that most queries requested immediate help with
programming assignments, whereas fewer requests asked for help on related
concepts or for deepening conceptual understanding. Furthermore, students often
provided minimal information to the tool, suggesting this is an area in which
targeted instruction would be beneficial. We also found that students who
achieved more success in the course tended to have used the tool more
frequently overall. Lessons from this research can be leveraged by programming
educators and institutions who plan to augment their teaching with emerging
LLM-powered tools.",2023-10-25T20:36:05Z
,http://arxiv.org/pdf/2402.13291v2.pdf,"DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language
  Models","The automated program repair field has attracted substantial interest over
the years, but despite significant research efforts, creating a system that
works well for complex semantic bugs such as security vulnerabilities has
proven difficult. A promising direction to solve this challenge is by
leveraging large language models (LLMs), which are increasingly used to solve
various programming tasks. In this paper, we investigate the effectiveness of
LLMs for solving code-repair task. We show that the task is difficult as it
requires the model to learn long-range code relationships, a task that
inherently relies on extensive amounts of training data. At the same time,
creating a large, clean dataset for complex program bugs and their
corresponding fixes is non-trivial. We propose a technique to address these
challenges with a new approach for querying and fine-tuning LLMs. The idea is
to use program analysis to limit the LLM's attention mechanism on the portions
of code needed to perform the fix, drastically reducing the amount of required
training data. Concretely, for training and inference, rather than feeding the
entire program to the LLM, we reduce its code to a much shorter snippet that
contains the reported defect together with the necessary context - and use that
instead. Our evaluation shows that this code reduction approach substantially
improves available models such as GPT-4 using few-shot learning, as well as
fine-tuning models. To train and evaluate our system, we created a
comprehensive code fixing dataset by extensively labeling 156 bug patterns
(including 40 security rules), requiring complex interprocedural dataflow to
discover. Our best system with Mixtral-8x7B can remove more than 80% of the
reported defects while exactly matching the human fix in between 10 and 50% of
cases, outperforming baselines based on GPT-3.5 and GPT-4, or based on
window-based models like TFix.",2024-02-19T18:35:40Z
,http://arxiv.org/pdf/2405.11196v1.pdf,"Natural Is The Best: Model-Agnostic Code Simplification for Pre-trained
  Large Language Models","Pre-trained Large Language Models (LLM) have achieved remarkable successes in
several domains. However, code-oriented LLMs are heavy in computational
complexity, and quadratically with the length of the input. Toward simplifying
the input program of an LLM, the state-of-the-art approach has the strategies
to filter the input code tokens based on the attention scores given by the LLM.
The decision to simplify the input should not rely on the attention patterns of
an LLM, as these patterns are influenced by both the model architecture and the
pre-training dataset. Since the model and dataset are part of the solution
domain, not the problem domain where the input belongs, the outcome may differ
when the model is pre-trained on a different dataset. We propose SlimCode, a
model-agnostic code simplification solution for LLMs that depends on the nature
of input code tokens. As an empirical study on the LLMs including CodeBERT,
CodeT5, and GPT-4 for two main tasks: code search and summarization, we
reported that 1) the removal ratio of code has a linear-like relation with the
saving ratio on training time, 2) the impact of categorized tokens on code
simplification can vary significantly, 3) the impact of categorized tokens on
code simplification is task-specific but model-agnostic, and 4) the above
findings hold for the paradigm-prompt engineering and interactive in-context
learning. The empirical results showed that SlimCode can improve the
state-of-the-art technique by 9.46% and 5.15% in terms of MRR and BLEU score on
code search and summarization. Moreover, SlimCode is 133 times faster than the
state-of-the-art approach. Additionally, SlimCode can reduce the cost of
invoking GPT-4 by up to 24% per API query, while still producing comparable
results to those with the original code.",2024-05-18T06:15:52Z
,http://arxiv.org/pdf/2306.01987v3.pdf,"Prompting Is All You Need: Automated Android Bug Replay with Large
  Language Models","Bug reports are vital for software maintenance that allow users to inform
developers of the problems encountered while using the software. As such,
researchers have committed considerable resources toward automating bug replay
to expedite the process of software maintenance. Nonetheless, the success of
current automated approaches is largely dictated by the characteristics and
quality of bug reports, as they are constrained by the limitations of
manually-crafted patterns and pre-defined vocabulary lists. Inspired by the
success of Large Language Models (LLMs) in natural language understanding, we
propose AdbGPT, a new lightweight approach to automatically reproduce the bugs
from bug reports through prompt engineering, without any training and
hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought
reasoning to elicit human knowledge and logical reasoning from LLMs to
accomplish the bug replay in a manner similar to a developer. Our evaluations
demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3%
of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines
and ablation studies. We also conduct a small-scale user study to confirm the
usefulness of AdbGPT in enhancing developers' bug replay capabilities.",2023-06-03T03:03:52Z
,http://arxiv.org/pdf/2406.17224v1.pdf,Large Language Models are Interpretable Learners,"The trade-off between expressiveness and interpretability remains a core
challenge when building human-centric predictive models for classification and
decision-making. While symbolic rules offer interpretability, they often lack
expressiveness, whereas neural networks excel in performance but are known for
being black boxes. In this paper, we show a combination of Large Language
Models (LLMs) and symbolic programs can bridge this gap. In the proposed
LLM-based Symbolic Programs (LSPs), the pretrained LLM with natural language
prompts provides a massive set of interpretable modules that can transform raw
input into natural language concepts. Symbolic programs then integrate these
modules into an interpretable decision rule. To train LSPs, we develop a
divide-and-conquer approach to incrementally build the program from scratch,
where the learning process of each step is guided by LLMs. To evaluate the
effectiveness of LSPs in extracting interpretable and accurate knowledge from
data, we introduce IL-Bench, a collection of diverse tasks, including both
synthetic and real-world scenarios across different modalities. Empirical
results demonstrate LSP's superior performance compared to traditional
neurosymbolic programs and vanilla automatic prompt tuning methods. Moreover,
as the knowledge learned by LSP is a combination of natural language
descriptions and symbolic rules, it is easily transferable to humans
(interpretable), and other LLMs, and generalizes well to out-of-distribution
samples.",2024-06-25T02:18:15Z
,http://arxiv.org/pdf/2309.13078v2.pdf,LPML: LLM-Prompting Markup Language for Mathematical Reasoning,"In utilizing large language models (LLMs) for mathematical reasoning,
addressing the errors in the reasoning and calculation present in the generated
text by LLMs is a crucial challenge. In this paper, we propose a novel
framework that integrates the Chain-of-Thought (CoT) method with an external
tool (Python REPL). We discovered that by prompting LLMs to generate structured
text in XML-like markup language, we could seamlessly integrate CoT and the
external tool and control the undesired behaviors of LLMs. With our approach,
LLMs can utilize Python computation to rectify errors within CoT. We applied
our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and
demonstrated that combining CoT and Python REPL through the markup language
enhances the reasoning capability of LLMs. Our approach enables LLMs to write
the markup language and perform advanced mathematical reasoning using only
zero-shot prompting.",2023-09-21T02:46:20Z
,http://arxiv.org/pdf/2303.09384v1.pdf,"LLMSecEval: A Dataset of Natural Language Prompts for Security
  Evaluations","Large Language Models (LLMs) like Codex are powerful tools for performing
code completion and code generation tasks as they are trained on billions of
lines of code from publicly available sources. Moreover, these models are
capable of generating code snippets from Natural Language (NL) descriptions by
learning languages and programming practices from public GitHub repositories.
Although LLMs promise an effortless NL-driven deployment of software
applications, the security of the code they generate has not been extensively
investigated nor documented. In this work, we present LLMSecEval, a dataset
containing 150 NL prompts that can be leveraged for assessing the security
performance of such models. Such prompts are NL descriptions of code snippets
prone to various security vulnerabilities listed in MITRE's Top 25 Common
Weakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a
secure implementation example to facilitate comparative evaluations against
code produced by LLMs. As a practical application, we show how LLMSecEval can
be used for evaluating the security of snippets automatically generated from NL
descriptions.",2023-03-16T15:13:58Z
,http://arxiv.org/pdf/2311.07635v1.pdf,"Past as a Guide: Leveraging Retrospective Learning for Python Code
  Completion","This work presents Past as a Guide (PaG), a simple approach for Large
Language Models (LLMs) to improve the coding capabilities by integrating the
past history with interactive and iterative code refinements. To be specific,
inspired by human cognitive processes, the proposed method enables LLMs to
utilize previous programming and debugging experiences to enhance the Python
code completion tasks. The framework facilitates LLMs to iteratively refine the
Python code based on previous execution and debugging results and optimize
learning and reasoning capabilities. The proposed methodology achieved a 92\%
pass@1 on HumanEval, demonstrating the potential to advance the field by
leveraging retrospection from past experiences and interactive and iterative
refinement processes without external correctness indicators.",2023-11-13T14:40:33Z
,http://arxiv.org/pdf/2402.16910v2.pdf,"NeSy is alive and well: A LLM-driven symbolic approach for better code
  comment data generation and classification","We present a neuro-symbolic (NeSy) workflow combining a symbolic-based
learning technique with a large language model (LLM) agent to generate
synthetic data for code comment classification in the C programming language.
We also show how generating controlled synthetic data using this workflow fixes
some of the notable weaknesses of LLM-based generation and increases the
performance of classical machine learning models on the code comment
classification task. Our best model, a Neural Network, achieves a Macro-F1
score of 91.412% with an increase of 1.033% after data augmentation.",2024-02-25T13:20:13Z
,http://arxiv.org/pdf/2406.05892v1.pdf,"Security Vulnerability Detection with Multitask Self-Instructed
  Fine-Tuning of Large Language Models","Software security vulnerabilities allow attackers to perform malicious
activities to disrupt software operations. Recent Transformer-based language
models have significantly advanced vulnerability detection, surpassing the
capabilities of static analysis based deep learning models. However, language
models trained solely on code tokens do not capture either the explanation of
vulnerability type or the data flow structure information of code, both of
which are crucial for vulnerability detection. We propose a novel technique
that integrates a multitask sequence-to-sequence LLM with pro-gram control flow
graphs encoded as a graph neural network to achieve sequence-to-classification
vulnerability detection. We introduce MSIVD, multitask self-instructed
fine-tuning for vulnerability detection, inspired by chain-of-thought prompting
and LLM self-instruction. Our experiments demonstrate that MSIVD achieves
superior performance, outperforming the highest LLM-based vulnerability
detector baseline (LineVul), with a F1 score of 0.92 on the BigVul dataset, and
0.48 on the PreciseBugs dataset. By training LLMs and GNNs simultaneously using
a combination of code and explanatory metrics of a vulnerable program, MSIVD
represents a promising direction for advancing LLM-based vulnerability
detection that generalizes to unseen data. Based on our findings, we further
discuss the necessity for new labelled security vulnerability datasets, as
recent LLMs have seen or memorized prior datasets' held-out evaluation data.",2024-06-09T19:18:05Z
,http://arxiv.org/pdf/2307.13383v1.pdf,Predicting Code Coverage without Execution,"Code coverage is a widely used metric for quantifying the extent to which
program elements, such as statements or branches, are executed during testing.
Calculating code coverage is resource-intensive, requiring code building and
execution with additional overhead for the instrumentation. Furthermore,
computing coverage of any snippet of code requires the whole program context.
Using Machine Learning to amortize this expensive process could lower the cost
of code coverage by requiring only the source code context, and the task of
code coverage prediction can be a novel benchmark for judging the ability of
models to understand code. We propose a novel benchmark task called Code
Coverage Prediction for Large Language Models (LLMs). We formalize this task to
evaluate the capability of LLMs in understanding code execution by determining
which lines of a method are executed by a given test case and inputs. We curate
and release a dataset we call COVERAGEEVAL by executing tests and code from the
HumanEval dataset and collecting code coverage information. We report the
performance of four state-of-the-art LLMs used for code-related tasks,
including OpenAI's GPT-4 and GPT-3.5-Turbo, Google's BARD, and Anthropic's
Claude, on the Code Coverage Prediction task. Finally, we argue that code
coverage as a metric and pre-training data source are valuable for overall LLM
performance on software engineering tasks.",2023-07-25T10:07:02Z
,http://arxiv.org/pdf/2312.06121v1.pdf,Can LLMs Configure Software Tools,"In software engineering, the meticulous configuration of software tools is
crucial in ensuring optimal performance within intricate systems. However, the
complexity inherent in selecting optimal configurations is exacerbated by the
high-dimensional search spaces presented in modern applications. Conventional
trial-and-error or intuition-driven methods are both inefficient and
error-prone, impeding scalability and reproducibility. In this study, we embark
on an exploration of leveraging Large-Language Models (LLMs) to streamline the
software configuration process. We identify that the task of hyperparameter
configuration for machine learning components within intelligent applications
is particularly challenging due to the extensive search space and
performance-critical nature. Existing methods, including Bayesian optimization,
have limitations regarding initial setup, computational cost, and convergence
efficiency. Our work presents a novel approach that employs LLMs, such as
Chat-GPT, to identify starting conditions and narrow down the search space,
improving configuration efficiency. We conducted a series of experiments to
investigate the variability of LLM-generated responses, uncovering intriguing
findings such as potential response caching and consistent behavior based on
domain-specific keywords. Furthermore, our results from hyperparameter
optimization experiments reveal the potential of LLMs in expediting
initialization processes and optimizing configurations. While our initial
insights are promising, they also indicate the need for further in-depth
investigations and experiments in this domain.",2023-12-11T05:03:02Z
,http://arxiv.org/pdf/2311.13445v1.pdf,Transfer Attacks and Defenses for Large Language Models on Coding Tasks,"Modern large language models (LLMs), such as ChatGPT, have demonstrated
impressive capabilities for coding tasks including writing and reasoning about
code. They improve upon previous neural network models of code, such as
code2seq or seq2seq, that already demonstrated competitive results when
performing tasks such as code summarization and identifying code
vulnerabilities. However, these previous code models were shown vulnerable to
adversarial examples, i.e. small syntactic perturbations that do not change the
program's semantics, such as the inclusion of ""dead code"" through false
conditions or the addition of inconsequential print statements, designed to
""fool"" the models. LLMs can also be vulnerable to the same adversarial
perturbations but a detailed study on this concern has been lacking so far. In
this paper we aim to investigate the effect of adversarial perturbations on
coding tasks with LLMs. In particular, we study the transferability of
adversarial examples, generated through white-box attacks on smaller code
models, to LLMs. Furthermore, to make the LLMs more robust against such
adversaries without incurring the cost of retraining, we propose prompt-based
defenses that involve modifying the prompt to include additional information
such as examples of adversarially perturbed code and explicit instructions for
reversing adversarial perturbations. Our experiments show that adversarial
examples obtained with a smaller code model are indeed transferable, weakening
the LLMs' performance. The proposed defenses show promise in improving the
model's resilience, paving the way to more robust defensive solutions for LLMs
in code-related applications.",2023-11-22T15:11:35Z
,http://arxiv.org/pdf/2403.07865v4.pdf,"CodeAttack: Revealing Safety Generalization Challenges of Large Language
  Models via Code Completion","The rapid advancement of Large Language Models (LLMs) has brought about
remarkable generative capabilities but also raised concerns about their
potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
new and universal safety vulnerability of these models against code input:
CodeAttack bypasses the safety guardrails of all models more than 80\% of the
time. We find that a larger distribution gap between CodeAttack and natural
language leads to weaker safety generalization, such as encoding natural
language input with data structures. Furthermore, we give our hypotheses about
the success of CodeAttack: the misaligned bias acquired by LLMs during code
training, prioritizing code completion over avoiding the potential safety risk.
Finally, we analyze potential mitigation measures. These findings highlight new
safety risks in the code domain and the need for more robust safety alignment
algorithms to match the code capabilities of LLMs.",2024-03-12T17:55:38Z
,http://arxiv.org/pdf/2211.12821v2.pdf,"Explainable AI for Pre-Trained Code Models: What Do They Learn? When
  They Do Not Work?","In recent years, there has been a wide interest in designing deep neural
network-based models that automate downstream software engineering tasks on
source code, such as code document generation, code search, and program repair.
Although the main objective of these studies is to improve the effectiveness of
the downstream task, many studies only attempt to employ the next best neural
network model, without a proper in-depth analysis of why a particular solution
works or does not, on particular tasks or scenarios. In this paper, using an
example eXplainable AI (XAI) method (attention mechanism), we study two recent
large language models (LLMs) for code (CodeBERT and GraphCodeBERT) on a set of
software engineering downstream tasks: code document generation (CDG), code
refinement (CR), and code translation (CT). Through quantitative and
qualitative studies, we identify what CodeBERT and GraphCodeBERT learn (put the
highest attention on, in terms of source code token types), on these tasks. We
also show some of the common patterns when the model does not work as expected
(performs poorly even on easy problems) and suggest recommendations that may
alleviate the observed challenges.",2022-11-23T10:07:20Z
,http://arxiv.org/pdf/2309.02465v1.pdf,"Towards Foundational AI Models for Additive Manufacturing: Language
  Models for G-Code Debugging, Manipulation, and Comprehension","3D printing or additive manufacturing is a revolutionary technology that
enables the creation of physical objects from digital models. However, the
quality and accuracy of 3D printing depend on the correctness and efficiency of
the G-code, a low-level numerical control programming language that instructs
3D printers how to move and extrude material. Debugging G-code is a challenging
task that requires a syntactic and semantic understanding of the G-code format
and the geometry of the part to be printed. In this paper, we present the first
extensive evaluation of six state-of-the-art foundational large language models
(LLMs) for comprehending and debugging G-code files for 3D printing. We design
effective prompts to enable pre-trained LLMs to understand and manipulate
G-code and test their performance on various aspects of G-code debugging and
manipulation, including detection and correction of common errors and the
ability to perform geometric transformations. We analyze their strengths and
weaknesses for understanding complete G-code files. We also discuss the
implications and limitations of using LLMs for G-code comprehension.",2023-09-04T21:22:28Z
,http://arxiv.org/pdf/2406.01006v1.pdf,SemCoder: Training Code Language Models with Comprehensive Semantics,"Code Large Language Models (Code LLMs) have excelled at tasks like code
completion but often miss deeper semantics such as execution effects and
dynamic states. This paper aims to bridge the gap between Code LLMs' reliance
on static text data and the need for thorough semantic understanding for
complex tasks like debugging and program repair. We introduce a novel strategy
to train Code LLMs with comprehensive semantics, encompassing high-level
functional descriptions, local execution effects of individual statements, and
overall input/output behavior, thereby linking static code text with dynamic
execution states. We begin by collecting PyX, a clean code corpus of fully
executable samples with functional descriptions and execution tracing. We
propose training Code LLMs to write code and represent and reason about
execution behaviors using natural language, mimicking human verbal debugging.
This approach led to the development of SemCoder, a Code LLM with only 6.7B
parameters, which shows competitive performance with GPT-3.5-turbo on code
generation and execution reasoning tasks. SemCoder achieves 81.1% on HumanEval
(GPT-3.5-turbo: 76.8%) and 54.5% on CRUXEval-I (GPT-3.5-turbo: 50.3%). We also
study the effectiveness of SemCoder's monologue-style execution reasoning
compared to concrete scratchpad reasoning, showing that our approach integrates
semantics from multiple dimensions more smoothly. Finally, we demonstrate the
potential of applying learned semantics to improve Code LLMs' debugging and
self-refining capabilities.",2024-06-03T05:36:57Z
,http://arxiv.org/pdf/2406.00037v1.pdf,"Aligning LLMs through Multi-perspective User Preference Ranking-based
  Feedback for Programming Question Answering","Code Community Question Answering (CCQA) seeks to tackle programming-related
issues, thereby boosting productivity in both software engineering and academic
research. Recent advancements in Reinforcement Learning from Human Feedback
(RLHF) have transformed the fine-tuning process of Large Language Models (LLMs)
to produce responses that closely mimic human behavior. Leveraging LLMs with
RLHF for practical CCQA applications has thus emerged as a promising area of
study. Unlike standard code question-answering tasks, CCQA involves multiple
possible answers, with varying user preferences for each response.
Additionally, code communities often show a preference for new APIs. These
challenges prevent LLMs from generating responses that cater to the diverse
preferences of users in CCQA tasks. To address these issues, we propose a novel
framework called Aligning LLMs through Multi-perspective User Preference
Ranking-based Feedback for Programming Question Answering (ALMupQA) to create
user-focused responses. Our approach starts with Multi-perspective Preference
Ranking Alignment (MPRA), which synthesizes varied user preferences based on
the characteristics of answers from code communities. We then introduce a
Retrieval-augmented In-context Learning (RIL) module to mitigate the problem of
outdated answers by retrieving responses to similar questions from a question
bank. Due to the limited availability of high-quality, multi-answer CCQA
datasets, we also developed a dataset named StaCCQA from real code communities.
Extensive experiments demonstrated the effectiveness of the ALMupQA framework
in terms of accuracy and user preference. Compared to the base model, ALMupQA
showed nearly an 11% improvement in BLEU, with increases of 20% and 17.5% in
BERTScore and CodeBERTScore, respectively.",2024-05-27T14:21:31Z
,http://arxiv.org/pdf/2312.05657v1.pdf,"Leveraging Reinforcement Learning and Large Language Models for Code
  Optimization","Code optimization is a daunting task that requires a significant level of
expertise from experienced programmers. This level of expertise is not
sufficient when compared to the rapid development of new hardware
architectures. Towards advancing the whole code optimization process, recent
approaches rely on machine learning and artificial intelligence techniques.
This paper introduces a new framework to decrease the complexity of code
optimization. The proposed framework builds on large language models (LLMs) and
reinforcement learning (RL) and enables LLMs to receive feedback from their
environment (i.e., unit tests) during the fine-tuning process. We compare our
framework with existing state-of-the-art models and show that it is more
efficient with respect to speed and computational usage, as a result of the
decrement in training steps and its applicability to models with fewer
parameters. Additionally, our framework reduces the possibility of logical and
syntactical errors. Toward evaluating our approach, we run several experiments
on the PIE dataset using a CodeT5 language model and RRHF, a new reinforcement
learning algorithm. We adopt a variety of evaluation metrics with regards to
optimization quality, and speedup. The evaluation results demonstrate that the
proposed framework has similar results in comparison with existing models using
shorter training times and smaller pre-trained models. In particular, we
accomplish an increase of 5.6% and 2.2 over the baseline models concerning the
%OP T and SP metrics.",2023-12-09T19:50:23Z
,http://arxiv.org/pdf/2405.16587v1.pdf,Cost-Effective Online Multi-LLM Selection with Versatile Reward Models,"With the rapid advancement of large language models (LLMs), the diversity of
multi-LLM tasks and the variability in their pricing structures have become
increasingly important, as costs can vary greatly between different LLMs. To
tackle these challenges, we introduce the \textit{C2MAB-V}, a
\underline{C}ost-effective \underline{C}ombinatorial \underline{M}ulti-armed
\underline{B}andit with \underline{V}ersatile reward models for optimal LLM
selection and usage. This online model differs from traditional static
approaches or those reliant on a single LLM without cost consideration. With
multiple LLMs deployed on a scheduling cloud and a local server dedicated to
handling user queries, \textit{C2MAB-V} facilitates the selection of multiple
LLMs over a combinatorial search space, specifically tailored for various
collaborative task types with different reward models. Based on our designed
online feedback mechanism and confidence bound technique, \textit{C2MAB-V} can
effectively address the multi-LLM selection challenge by managing the
exploration-exploitation trade-off across different models, while also
balancing cost and reward for diverse tasks. The NP-hard integer linear
programming problem for selecting multiple LLMs with trade-off dilemmas is
addressed by: i) decomposing the integer problem into a relaxed form by the
local server, ii) utilizing a discretization rounding scheme that provides
optimal LLM combinations by the scheduling cloud, and iii) continual online
updates based on feedback. Theoretically, we prove that \textit{C2MAB-V} offers
strict guarantees over versatile reward models, matching state-of-the-art
results for regret and violations in some degenerate cases. Empirically, we
show that \textit{C2MAB-V} effectively balances performance and cost-efficiency
with nine LLMs for three application scenarios.",2024-05-26T14:38:24Z
,http://arxiv.org/pdf/2403.07974v2.pdf,"LiveCodeBench: Holistic and Contamination Free Evaluation of Large
  Language Models for Code","Large Language Models (LLMs) applied to code-related applications have
emerged as a prominent field, attracting significant interest from both
academia and industry. However, as new and improved LLMs are developed,
existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient
for assessing their capabilities. In this work, we propose LiveCodeBench, a
comprehensive and contamination-free evaluation of LLMs for code, which
continuously collects new problems over time from contests across three
competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our
benchmark also focuses on a broader range of code related capabilities, such as
self-repair, code execution, and test output prediction, beyond just code
generation. Currently, LiveCodeBench hosts four hundred high-quality coding
problems that were published between May 2023 and May 2024. We have evaluated
18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present
empirical findings on contamination, holistic performance comparisons,
potential overfitting in existing benchmarks as well as individual model
comparisons. We will release all prompts and model completions for further
community analysis, along with a general toolkit for adding new scenarios and
model",2024-03-12T17:58:04Z
,http://arxiv.org/pdf/2310.15991v1.pdf,White-box Compiler Fuzzing Empowered by Large Language Models,"Compiler correctness is crucial, as miscompilation falsifying the program
behaviors can lead to serious consequences. In the literature, fuzzing has been
extensively studied to uncover compiler defects. However, compiler fuzzing
remains challenging: Existing arts focus on black- and grey-box fuzzing, which
generates tests without sufficient understanding of internal compiler
behaviors. As such, they often fail to construct programs to exercise
conditions of intricate optimizations. Meanwhile, traditional white-box
techniques are computationally inapplicable to the giant codebase of compilers.
Recent advances demonstrate that Large Language Models (LLMs) excel in code
generation/understanding tasks and have achieved state-of-the-art performance
in black-box fuzzing. Nonetheless, prompting LLMs with compiler source-code
information remains a missing piece of research in compiler testing.
  To this end, we propose WhiteFox, the first white-box compiler fuzzer using
LLMs with source-code information to test compiler optimization. WhiteFox
adopts a dual-model framework: (i) an analysis LLM examines the low-level
optimization source code and produces requirements on the high-level test
programs that can trigger the optimization; (ii) a generation LLM produces test
programs based on the summarized requirements. Additionally,
optimization-triggering tests are used as feedback to further enhance the test
generation on the fly. Our evaluation on four popular compilers shows that
WhiteFox can generate high-quality tests to exercise deep optimizations
requiring intricate conditions, practicing up to 80 more optimizations than
state-of-the-art fuzzers. To date, WhiteFox has found in total 96 bugs, with 80
confirmed as previously unknown and 51 already fixed. Beyond compiler testing,
WhiteFox can also be adapted for white-box fuzzing of other complex, real-world
software systems in general.",2023-10-24T16:39:06Z
,http://arxiv.org/pdf/2403.11446v1.pdf,LLM Guided Evolution -- The Automation of Models Advancing Models,"In the realm of machine learning, traditional model development and automated
approaches like AutoML typically rely on layers of abstraction, such as
tree-based or Cartesian genetic programming. Our study introduces ""Guided
Evolution"" (GE), a novel framework that diverges from these methods by
utilizing Large Language Models (LLMs) to directly modify code. GE leverages
LLMs for a more intelligent, supervised evolutionary process, guiding mutations
and crossovers. Our unique ""Evolution of Thought"" (EoT) technique further
enhances GE by enabling LLMs to reflect on and learn from the outcomes of
previous mutations. This results in a self-sustaining feedback loop that
augments decision-making in model evolution. GE maintains genetic diversity,
crucial for evolutionary algorithms, by leveraging LLMs' capability to generate
diverse responses from expertly crafted prompts and modulate model temperature.
This not only accelerates the evolution process but also injects expert like
creativity and insight into the process. Our application of GE in evolving the
ExquisiteNetV2 model demonstrates its efficacy: the LLM-driven GE autonomously
produced variants with improved accuracy, increasing from 92.52% to 93.34%,
without compromising model compactness. This underscores the potential of LLMs
to accelerate the traditional model design pipeline, enabling models to
autonomously evolve and enhance their own designs.",2024-03-18T03:44:55Z
,http://arxiv.org/pdf/2404.07452v1.pdf,"RiskLabs: Predicting Financial Risk Using Large Language Model Based on
  Multi-Sources Data","The integration of Artificial Intelligence (AI) techniques, particularly
large language models (LLMs), in finance has garnered increasing academic
attention. Despite progress, existing studies predominantly focus on tasks like
financial text summarization, question-answering (Q$\&$A), and stock movement
prediction (binary classification), with a notable gap in the application of
LLMs for financial risk prediction. Addressing this gap, in this paper, we
introduce \textbf{RiskLabs}, a novel framework that leverages LLMs to analyze
and predict financial risks. RiskLabs uniquely combines different types of
financial data, including textual and vocal information from Earnings
Conference Calls (ECCs), market-related time series data, and contextual news
data surrounding ECC release dates. Our approach involves a multi-stage
process: initially extracting and analyzing ECC data using LLMs, followed by
gathering and processing time-series data before the ECC dates to model and
understand risk over different timeframes. Using multimodal fusion techniques,
RiskLabs amalgamates these varied data features for comprehensive multi-task
financial risk prediction. Empirical experiment results demonstrate RiskLab's
effectiveness in forecasting both volatility and variance in financial markets.
Through comparative experiments, we demonstrate how different data sources
contribute to financial risk assessment and discuss the critical role of LLMs
in this context. Our findings not only contribute to the AI in finance
application but also open new avenues for applying LLMs in financial risk
assessment.",2024-04-11T03:14:50Z
,http://arxiv.org/pdf/2402.06013v1.pdf,"How to Refactor this Code? An Exploratory Study on Developer-ChatGPT
  Refactoring Conversations","Large Language Models (LLMs), like ChatGPT, have gained widespread popularity
and usage in various software engineering tasks, including refactoring,
testing, code review, and program comprehension. Despite recent studies delving
into refactoring documentation in commit messages, issues, and code review,
little is known about how developers articulate their refactoring needs when
interacting with ChatGPT. In this paper, our goal is to explore conversations
between developers and ChatGPT related to refactoring to better understand how
developers identify areas for improvement in code and how ChatGPT addresses
developers' needs. Our approach relies on text mining refactoring-related
conversations from 17,913 ChatGPT prompts and responses, and investigating
developers' explicit refactoring intention. Our results reveal that (1)
developer-ChatGPT conversations commonly involve generic and specific
terms/phrases; (2) developers often make generic refactoring requests, while
ChatGPT typically includes the refactoring intention; and (3) various learning
settings when prompting ChatGPT in the context of refactoring. We envision that
our findings contribute to a broader understanding of the collaboration between
developers and AI models, in the context of code refactoring, with implications
for model improvement, tool development, and best practices in software
engineering.",2024-02-08T19:24:01Z
,http://arxiv.org/pdf/2402.15943v2.pdf,"Rethinking Software Engineering in the Foundation Model Era: A Curated
  Catalogue of Challenges in the Development of Trustworthy FMware","Foundation models (FMs), such as Large Language Models (LLMs), have
revolutionized software development by enabling new use cases and business
models. We refer to software built using FMs as FMware. The unique properties
of FMware (e.g., prompts, agents, and the need for orchestration), coupled with
the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new
set of software engineering challenges. Based on our industrial experience, we
identified 10 key SE4FMware challenges that have caused enterprise FMware
development to be unproductive, costly, and risky. In this paper, we discuss
these challenges in detail and state the path for innovation that we envision.
Next, we present FMArts, which is our long-term effort towards creating a
cradle-to-grave platform for the engineering of trustworthy FMware. Finally, we
(i) show how the unique properties of FMArts enabled us to design and develop a
complex FMware for a large customer in a timely manner and (ii) discuss the
lessons that we learned in doing so. We hope that the disclosure of the
aforementioned challenges and our associated efforts to tackle them will not
only raise awareness but also promote deeper and further discussions, knowledge
sharing, and innovative solutions across the software engineering discipline.",2024-02-25T00:53:16Z
,http://arxiv.org/pdf/2308.06921v1.pdf,"CodeHelp: Using Large Language Models with Guardrails for Scalable
  Support in Programming Classes","Computing educators face significant challenges in providing timely support
to students, especially in large class settings. Large language models (LLMs)
have emerged recently and show great promise for providing on-demand help at a
large scale, but there are concerns that students may over-rely on the outputs
produced by these models. In this paper, we introduce CodeHelp, a novel
LLM-powered tool designed with guardrails to provide on-demand assistance to
programming students without directly revealing solutions. We detail the design
of the tool, which incorporates a number of useful features for instructors,
and elaborate on the pipeline of prompting strategies we use to ensure
generated outputs are suitable for students. To evaluate CodeHelp, we deployed
it in a first-year computer and data science course with 52 students and
collected student interactions over a 12-week period. We examine students'
usage patterns and perceptions of the tool, and we report reflections from the
course instructor and a series of recommendations for classroom use. Our
findings suggest that CodeHelp is well-received by students who especially
value its availability and help with resolving errors, and that for instructors
it is easy to deploy and complements, rather than replaces, the support that
they provide to students.",2023-08-14T03:52:24Z
,http://arxiv.org/pdf/2310.20105v1.pdf,"Efficient Classification of Student Help Requests in Programming Courses
  Using Large Language Models","The accurate classification of student help requests with respect to the type
of help being sought can enable the tailoring of effective responses.
Automatically classifying such requests is non-trivial, but large language
models (LLMs) appear to offer an accessible, cost-effective solution. This
study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying
help requests from students in an introductory programming class. In zero-shot
trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories,
while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests
related to debugging. Fine-tuning the GPT-3.5 model improved its performance to
such an extent that it approximated the accuracy and consistency across
categories observed between two human raters. Overall, this study demonstrates
the feasibility of using LLMs to enhance educational systems through the
automated classification of student needs.",2023-10-31T00:56:33Z
,http://arxiv.org/pdf/2404.12534v1.pdf,Towards Large Language Models as Copilots for Theorem Proving in Lean,"Theorem proving is an important challenge for large language models (LLMs),
as formal proofs can be checked rigorously by proof assistants such as Lean,
leaving no room for hallucination. Existing LLM-based provers try to prove
theorems in a fully autonomous mode without human intervention. In this mode,
they struggle with novel and challenging theorems, for which human insights may
be critical. In this paper, we explore LLMs as copilots that assist humans in
proving theorems. We introduce Lean Copilot, a framework for running LLM
inference in Lean. It enables programmers to build various LLM-based proof
automation tools that integrate seamlessly into the workflow of Lean users.
Using Lean Copilot, we build tools for suggesting proof steps (tactic
suggestion), completing intermediate proof goals (proof search), and selecting
relevant premises (premise selection) using LLMs. Users can use our pretrained
models or bring their own ones that run either locally (with or without GPUs)
or on the cloud. Experimental results demonstrate the effectiveness of our
method in assisting humans and automating theorem proving process compared to
existing rule-based proof automation in Lean. We open source all codes under a
permissive MIT license to facilitate further research.",2024-04-18T22:54:08Z
,http://arxiv.org/pdf/2402.00905v4.pdf,"Fine-Tuning and Prompt Engineering for Large Language Models-based Code
  Review Automation","Context: The rapid evolution of Large Language Models (LLMs) has sparked
significant interest in leveraging their capabilities for automating code
review processes. Prior studies often focus on developing LLMs for code review
automation, yet require expensive resources, which is infeasible for
organizations with limited budgets and resources. Thus, fine-tuning and prompt
engineering are the two common approaches to leveraging LLMs for code review
automation. Objective: We aim to investigate the performance of LLMs-based code
review automation based on two contexts, i.e., when LLMs are leveraged by
fine-tuning and prompting. Fine-tuning involves training the model on a
specific code review dataset, while prompting involves providing explicit
instructions to guide the model's generation process without requiring a
specific code review dataset. Method: We leverage model fine-tuning and
inference techniques (i.e., zero-shot learning, few-shot learning and persona)
on LLMs-based code review automation. In total, we investigate 12 variations of
two LLMs-based code review automation (i.e., GPT- 3.5 and Magicoder), and
compare them with the Guo et al.'s approach and three existing code review
automation approaches. Results: The fine-tuning of GPT 3.5 with zero-shot
learning helps GPT-3.5 to achieve 73.17% -74.23% higher EM than the Guo et
al.'s approach. In addition, when GPT-3.5 is not fine-tuned, GPT-3.5 with
few-shot learning achieves 46.38% - 659.09% higher EM than GPT-3.5 with
zero-shot learning. Conclusions: Based on our results, we recommend that (1)
LLMs for code review automation should be fine-tuned to achieve the highest
performance; and (2) when data is not sufficient for model fine-tuning (e.g., a
cold-start problem), few-shot learning without a persona should be used for
LLMs for code review automation.",2024-02-01T03:10:26Z
,http://arxiv.org/pdf/2402.16905v1.pdf,"Enforcing Temporal Constraints on Generative Agent Behavior with
  Reactive Synthesis","The surge in popularity of Large Language Models (LLMs) has opened doors for
new approaches to the creation of interactive agents. However, managing the
temporal behavior of such agents over the course of an interaction remains
challenging. The stateful, long-term horizon and quantitative reasoning
required for coherent agent behavior does not fit well into the LLM paradigm.
We propose a combination of formal logic-based program synthesis and LLM
content generation to create generative agents that adhere to temporal
constraints. Our approach uses Temporal Stream Logic (TSL) to generate an
automaton that enforces a temporal structure on an agent and leaves the details
of each action for a moment in time to an LLM. By using TSL, we are able to
augment the generative agent where users have a higher level of guarantees on
behavior, better interpretability of the system, and more ability to build
agents in a modular way. We evaluate our approach on different tasks involved
in creating a coherent interactive agent specialized for various application
domains. We found that over all of the tasks, our approach using TSL achieves
at least 96% adherence, whereas the pure LLM-based approach demonstrates as low
as 14.67% adherence.",2024-02-24T21:36:26Z
,http://arxiv.org/pdf/2403.02583v2.pdf,Generative Software Engineering,"The rapid development of deep learning techniques, improved computational
power, and the availability of vast training data have led to significant
advancements in pre-trained models and large language models (LLMs).
Pre-trained models based on architectures such as BERT and Transformer, as well
as LLMs like ChatGPT, have demonstrated remarkable language capabilities and
found applications in Software engineering. Software engineering tasks can be
divided into many categories, among which generative tasks are the most concern
by researchers, where pre-trained models and LLMs possess powerful language
representation and contextual awareness capabilities, enabling them to leverage
diverse training data and adapt to generative tasks through fine-tuning,
transfer learning, and prompt engineering. These advantages make them effective
tools in generative tasks and have demonstrated excellent performance. In this
paper, we present a comprehensive literature review of generative tasks in SE
using pre-trained models and LLMs. We accurately categorize SE generative tasks
based on software engineering methodologies and summarize the advanced
pre-trained models and LLMs involved, as well as the datasets and evaluation
metrics used. Additionally, we identify key strengths, weaknesses, and gaps in
existing approaches, and propose potential research directions. This review
aims to provide researchers and practitioners with an in-depth analysis and
guidance on the application of pre-trained models and LLMs in generative tasks
within SE.",2024-03-05T01:37:37Z
,http://arxiv.org/pdf/2306.03081v2.pdf,"Sequential Monte Carlo Steering of Large Language Models using
  Probabilistic Programs","Even after fine-tuning and reinforcement learning, large language models
(LLMs) can be difficult, if not impossible, to control reliably with prompts
alone. We propose a new inference-time approach to enforcing syntactic and
semantic constraints on the outputs of LLMs, called sequential Monte Carlo
(SMC) steering. The key idea is to specify language generation tasks as
posterior inference problems in a class of discrete probabilistic sequence
models, and replace standard decoding with sequential Monte Carlo inference.
For a computational cost similar to that of beam search, SMC can steer LLMs to
solve diverse tasks, including infilling, generation under syntactic
constraints, and prompt intersection. To facilitate experimentation with SMC
steering, we present a probabilistic programming library, LLaMPPL
(https://github.com/probcomp/hfppl), for concisely specifying new generation
tasks as language model probabilistic programs, and automating steering of
LLaMA-family Transformers.",2023-06-05T17:55:05Z
,http://arxiv.org/pdf/2404.04966v1.pdf,"Enhancing LLM-based Test Generation for Hard-to-Cover Branches via
  Program Analysis","Automatic test generation plays a critical role in software quality
assurance. While the recent advances in Search-Based Software Testing (SBST)
and Large Language Models (LLMs) have shown promise in generating useful tests,
these techniques still struggle to cover certain branches. Reaching these
hard-to-cover branches usually requires constructing complex objects and
resolving intricate inter-procedural dependencies in branch conditions, which
poses significant challenges for existing test generation techniques. In this
work, we propose TELPA, a novel technique aimed at addressing these challenges.
Its key insight lies in extracting real usage scenarios of the target method
under test to learn how to construct complex objects and extracting methods
entailing inter-procedural dependencies with hard-to-cover branches to learn
the semantics of branch constraints. To enhance efficiency and effectiveness,
TELPA identifies a set of ineffective tests as counter-examples for LLMs and
employs a feedback-based process to iteratively refine these counter-examples.
Then, TELPA integrates program analysis results and counter-examples into the
prompt, guiding LLMs to gain deeper understandings of the semantics of the
target method and generate diverse tests that can reach the hard-to-cover
branches. Our experimental results on 27 open-source Python projects
demonstrate that TELPA significantly outperforms the state-of-the-art SBST and
LLM-based techniques, achieving an average improvement of 31.39% and 22.22% in
terms of branch coverage.",2024-04-07T14:08:28Z
,http://arxiv.org/pdf/2401.13802v3.pdf,"Investigating the Efficacy of Large Language Models for Code Clone
  Detection","Large Language Models (LLMs) have demonstrated remarkable success in various
natural language processing and software engineering tasks, such as code
generation. The LLMs are mainly utilized in the prompt-based zero/few-shot
paradigm to guide the model in accomplishing the task. GPT-based models are one
of the popular ones studied for tasks such as code comment generation or test
generation. These tasks are `generative' tasks. However, there is limited
research on the usage of LLMs for `non-generative' tasks such as classification
using the prompt-based paradigm. In this preliminary exploratory study, we
investigated the applicability of LLMs for Code Clone Detection (CCD), a
non-generative task. By building a mono-lingual and cross-lingual CCD dataset
derived from CodeNet, we first investigated two different prompts using ChatGPT
to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot
setting. We then conducted an analysis to understand the strengths and
weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language
CCD attaining an F1-score of 0.877 and achieves comparable performance to fully
fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the
prompt and the difficulty level of the problems has an impact on the
performance of ChatGPT. Finally we provide insights and future directions based
on our initial analysis",2024-01-24T20:43:36Z
,http://arxiv.org/pdf/2310.02003v5.pdf,"L2MAC: Large Language Model Automatic Computer for Extensive Code
  Generation","Transformer-based large language models (LLMs) are constrained by the fixed
context window of the underlying transformer architecture, hindering their
ability to produce long and coherent outputs. Memory-augmented LLMs are a
promising solution, but current approaches cannot handle long output generation
tasks since they (1) only focus on reading memory and reduce its evolution to
the concatenation of new memories or (2) use very specialized memories that
cannot adapt to other domains. This paper presents L2MAC, the first practical
LLM-based general-purpose stored-program automatic computer (von Neumann
architecture) framework, an LLM-based multi-agent system, for long and
consistent output generation. Its memory has two components: the instruction
registry, which is populated with a prompt program to solve the user-given
task, and a file store, which will contain the final and intermediate outputs.
Each instruction in turn is executed by a separate LLM agent, whose context is
managed by a control unit capable of precise memory reading and writing to
ensure effective interaction with the file store. These components enable L2MAC
to generate extensive outputs, bypassing the constraints of the finite context
window while producing outputs that fulfill a complex user-specified task. We
empirically demonstrate that L2MAC achieves state-of-the-art performance in
generating large codebases for system design tasks, significantly outperforming
other coding methods in implementing the detailed user-specified task; we show
that L2MAC works for general-purpose extensive text-based tasks, such as
writing an entire book; and we provide valuable insights into L2MAC's
performance improvement over existing methods.",2023-10-02T16:55:19Z
,http://arxiv.org/pdf/2310.13006v1.pdf,"Software Metadata Classification based on Generative Artificial
  Intelligence","This paper presents a novel approach to enhance the performance of binary
code comment quality classification models through the application of
Generative Artificial Intelligence (AI). By leveraging the OpenAI API, a
dataset comprising 1239 newly generated code-comment pairs, extracted from
various GitHub repositories and open-source projects, has been labelled as
""Useful"" or ""Not Useful"", and integrated into the existing corpus of 9048 pairs
in the C programming language. Employing a cutting-edge Large Language Model
Architecture, the generated dataset demonstrates notable improvements in model
accuracy. Specifically, when incorporated into the Support Vector Machine (SVM)
model, a 6% increase in precision is observed, rising from 0.79 to 0.85.
Additionally, the Artificial Neural Network (ANN) model exhibits a 1.5%
increase in recall, climbing from 0.731 to 0.746. This paper sheds light on the
potential of Generative AI in augmenting code comment quality classification
models. The results affirm the effectiveness of this methodology, indicating
its applicability in broader contexts within software development and quality
assurance domains. The findings underscore the significance of integrating
generative techniques to advance the accuracy and efficacy of machine learning
models in practical software engineering scenarios.",2023-10-14T07:38:16Z
,http://arxiv.org/pdf/2402.15526v1.pdf,"Chain-of-Specificity: An Iteratively Refining Method for Eliciting
  Knowledge from Large Language Models","Large Language Models (LLMs) exhibit remarkable generative capabilities,
enabling the generation of valuable information. Despite these advancements,
previous research found that LLMs sometimes struggle with adhering to specific
constraints (e.g., in specific place or at specific time), at times even
overlooking them, which leads to responses that are either too generic or not
fully satisfactory. Existing approaches attempted to address this issue by
decomposing or rewriting input instructions, yet they fall short in adequately
emphasizing specific constraints and in unlocking the underlying knowledge
(e.g., programming within the context of software development). In response,
this paper proposes a simple yet effective method named Chain-of-Specificity
(CoS). Specifically, CoS iteratively emphasizes the specific constraints in the
input instructions, unlocks knowledge within LLMs, and refines responses.
Experiments conducted on publicly available and self-build complex datasets
demonstrate that CoS outperforms existing methods in enhancing generated
content especially for the specificity. Besides, as the number of specific
constraints increase, other baselines falter, while CoS still performs well.
Moreover, we show that distilling responses generated by CoS effectively
enhances the ability of smaller models to follow the constrained instructions.
Resources of this paper will be released for further research.",2024-02-20T08:03:05Z
,http://arxiv.org/pdf/2406.15540v1.pdf,"Specify What? Enhancing Neural Specification Synthesis by Symbolic
  Methods","We investigate how combinations of Large Language Models (LLMs) and symbolic
analyses can be used to synthesise specifications of C programs. The LLM
prompts are augmented with outputs from two formal methods tools in the Frama-C
ecosystem, Pathcrawler and EVA, to produce C program annotations in the
specification language ACSL. We demonstrate how the addition of symbolic
analysis to the workflow impacts the quality of annotations: information about
input/output examples from Pathcrawler produce more context-aware annotations,
while the inclusion of EVA reports yields annotations more attuned to runtime
errors. In addition, we show that the method infers rather the programs intent
than its behaviour, by generating specifications for buggy programs and
observing robustness of the result against bugs.",2024-06-21T17:39:57Z
,http://arxiv.org/pdf/2405.19132v1.pdf,"Analyzing Chat Protocols of Novice Programmers Solving Introductory
  Programming Tasks with ChatGPT","Large Language Models (LLMs) have taken the world by storm, and students are
assumed to use related tools at a great scale. In this research paper we aim to
gain an understanding of how introductory programming students chat with LLMs
and related tools, e.g., ChatGPT-3.5. To address this goal, computing students
at a large German university were motivated to solve programming exercises with
the assistance of ChatGPT as part of their weekly introductory course
exercises. Then students (n=213) submitted their chat protocols (with 2335
prompts in sum) as data basis for this analysis. The data was analyzed w.r.t.
the prompts, frequencies, the chats' progress, contents, and other use pattern,
which revealed a great variety of interactions, both potentially supportive and
concerning. Learning about students' interactions with ChatGPT will help inform
and align teaching practices and instructions for future introductory
programming courses in higher education.",2024-05-29T14:38:32Z
,http://arxiv.org/pdf/2401.13810v1.pdf,"Automated Root Causing of Cloud Incidents using In-Context Learning with
  GPT-4","Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis
process for cloud services, requiring on-call engineers to identify the primary
issues and implement corrective actions to prevent future recurrences.
Improving the incident RCA process is vital for minimizing service downtime,
customer impact and manual toil. Recent advances in artificial intelligence
have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which
have proven effective in tackling various AIOps problems, ranging from code
authoring to incident management. Nonetheless, the GPT-4 model's immense size
presents challenges when trying to fine-tune it on user data because of the
significant GPU resource demand and the necessity for continuous model
fine-tuning with the emergence of new data. To address the high cost of
fine-tuning LLM, we propose an in-context learning approach for automated root
causing, which eliminates the need for fine-tuning. We conduct extensive study
over 100,000 production incidents, comparing several large language models
using multiple metrics. The results reveal that our in-context learning
approach outperforms the previous fine-tuned large language models such as
GPT-3 by an average of 24.8\% across all metrics, with an impressive 49.7\%
improvement over the zero-shot model. Moreover, human evaluation involving
actual incident owners demonstrates its superiority over the fine-tuned model,
achieving a 43.5\% improvement in correctness and an 8.7\% enhancement in
readability. The impressive results demonstrate the viability of utilizing a
vanilla GPT model for the RCA task, thereby avoiding the high computational and
maintenance costs associated with a fine-tuned model.",2024-01-24T21:02:07Z
,http://arxiv.org/pdf/2404.15639v1.pdf,"CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models
  of Code","As Large Language Models (LLMs) are increasingly used to automate code
generation, it is often desired to know if the code is AI-generated and by
which model, especially for purposes like protecting intellectual property (IP)
in industry and preventing academic misconduct in education. Incorporating
watermarks into machine-generated content is one way to provide code
provenance, but existing solutions are restricted to a single bit or lack
flexibility. We present CodeIP, a new watermarking technique for LLM-based code
generation. CodeIP enables the insertion of multi-bit information while
preserving the semantics of the generated code, improving the strength and
diversity of the inerseted watermark. This is achieved by training a type
predictor to predict the subsequent grammar type of the next token to enhance
the syntactical and semantic correctness of the generated code. Experiments on
a real-world dataset across five programming languages showcase the
effectiveness of CodeIP.",2024-04-24T04:25:04Z
,http://arxiv.org/pdf/2403.16843v2.pdf,Do LLM Agents Have Regret? A Case Study in Online Learning and Games,"Large language models (LLMs) have been increasingly employed for
(interactive) decision-making, via the development of LLM-based autonomous
agents. Despite their emerging successes, the performance of LLM agents in
decision-making has not been fully investigated through quantitative metrics,
especially in the multi-agent setting when they interact with each other, a
typical scenario in real-world LLM-agent applications. To better understand the
limits of LLM agents in these interactive environments, we propose to study
their interactions in benchmark decision-making settings in online learning and
game theory, through the performance metric of \emph{regret}. We first
empirically study the {no-regret} behaviors of LLMs in canonical
(non-stationary) online learning problems, as well as the emergence of
equilibria when LLM agents interact through playing repeated games. We then
provide some theoretical insights into the no-regret behaviors of LLM agents,
under certain assumptions on the supervised pre-training and the rationality
model of human decision-makers who generate the data. Notably, we also identify
(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To
promote the no-regret behaviors, we propose a novel \emph{unsupervised}
training loss of \emph{regret-loss}, which, in contrast to the supervised
pre-training loss, does not require the labels of (optimal) actions. We then
establish the statistical guarantee of generalization bound for regret-loss
minimization, followed by the optimization guarantee that minimizing such a
loss may automatically lead to known no-regret learning algorithms. Our further
experiments demonstrate the effectiveness of our regret-loss, especially in
addressing the above ``regrettable'' cases.",2024-03-25T15:04:11Z
,http://arxiv.org/pdf/2207.04237v2.pdf,Few-shot training LLMs for project-specific code-summarization,"Very large language models (LLMs), such as GPT-3 and Codex have achieved
state-of-the-art performance on several natural-language tasks, and show great
promise also for code. A particularly exciting aspect of LLMs is their knack
for few-shot and zero-shot learning: they can learn to perform a task with very
few examples. Few-shotting has particular synergies in software engineering,
where there are a lot of phenomena (identifier names, APIs, terminology, coding
patterns) that are known to be highly project-specific. However,
project-specific data can be quite limited, especially early in the history of
a project; thus the few-shot learning capacity of LLMs might be very relevant.
In this paper, we investigate the use few-shot training with the very large GPT
(Generative Pre-trained Transformer) Codex model, and find evidence suggesting
that one can significantly surpass state-of-the-art models for
code-summarization, leveraging project-specific training.",2022-07-09T09:57:11Z
10.1145/3568813.3600142,http://arxiv.org/pdf/2306.10073v1.pdf,"Thrilled by Your Progress! Large Language Models (GPT-4) No Longer
  Struggle to Pass Assessments in Higher Education Programming Courses","This paper studies recent developments in large language models' (LLM)
abilities to pass assessments in introductory and intermediate Python
programming courses at the postsecondary level. The emergence of ChatGPT
resulted in heated debates of its potential uses (e.g., exercise generation,
code explanation) as well as misuses in programming classes (e.g., cheating).
Recent studies show that while the technology performs surprisingly well on
diverse sets of assessment instruments employed in typical programming classes
the performance is usually not sufficient to pass the courses. The release of
GPT-4 largely emphasized notable improvements in the capabilities related to
handling assessments originally designed for human test-takers. This study is
the necessary analysis in the context of this ongoing transition towards mature
generative AI systems. Specifically, we report the performance of GPT-4,
comparing it to the previous generations of GPT models, on three Python courses
with assessments ranging from simple multiple-choice questions (no code
involved) to complex programming projects with code bases distributed into
multiple files (599 exercises overall). Additionally, we analyze the
assessments that were not handled well by GPT-4 to understand the current
limitations of the model, as well as its capabilities to leverage feedback
provided by an auto-grader. We found that the GPT models evolved from
completely failing the typical programming class' assessments (the original
GPT-3) to confidently passing the courses with no human involvement (GPT-4).
While we identified certain limitations in GPT-4's handling of MCQs and coding
exercises, the rate of improvement across the recent generations of GPT models
strongly suggests their potential to handle almost any type of assessment
widely used in higher education programming courses. These findings could be
leveraged by educators and institutions to adapt the design of programming
assessments as well as to fuel the necessary discussions into how programming
classes should be updated to reflect the recent technological developments.
This study provides evidence that programming instructors need to prepare for a
world in which there is an easy-to-use widely accessible technology that can be
utilized by learners to collect passing scores, with no effort whatsoever, on
what today counts as viable programming knowledge and skills assessments.",2023-06-15T22:12:34Z
,http://arxiv.org/pdf/2401.10314v2.pdf,"LangProp: A code optimization framework using Large Language Models
  applied to driving","We propose LangProp, a framework for iteratively optimizing code generated by
large language models (LLMs), in both supervised and reinforcement learning
settings. While LLMs can generate sensible coding solutions zero-shot, they are
often sub-optimal. Especially for code generation tasks, it is likely that the
initial code will fail on certain edge cases. LangProp automatically evaluates
the code performance on a dataset of input-output pairs, catches any
exceptions, and feeds the results back to the LLM in the training loop, so that
the LLM can iteratively improve the code it generates. By adopting a metric-
and data-driven training paradigm for this code optimization procedure, one
could easily adapt findings from traditional machine learning techniques such
as imitation learning, DAgger, and reinforcement learning. We show LangProp's
applicability to general domains such as Sudoku and CartPole, as well as
demonstrate the first proof of concept of automated code optimization for
autonomous driving in CARLA. We show that LangProp can generate interpretable
and transparent policies that can be verified and improved in a metric- and
data-driven way. Our code is available at
https://github.com/shuishida/LangProp.",2024-01-18T18:52:06Z
,http://arxiv.org/pdf/2401.09074v4.pdf,Code Simulation Challenges for Large Language Models,"Many reasoning, planning, and problem-solving tasks share an intrinsic
algorithmic nature: correctly simulating each step is a sufficient condition to
solve them correctly. This work studies to what extent Large Language Models
(LLMs) can simulate coding and algorithmic tasks to provide insights into
general capabilities in such algorithmic reasoning tasks. We introduce
benchmarks for straight-line programs, code that contains critical paths, and
approximate and redundant instructions. We further assess the simulation
capabilities of LLMs with sorting algorithms and nested loops and show that a
routine's computational complexity directly affects an LLM's ability to
simulate its execution. While the most powerful LLMs exhibit relatively strong
simulation capabilities, the process is fragile, seems to rely heavily on
pattern recognition, and is affected by memorisation. We propose a novel
off-the-shelf prompting method, Chain of Simulation (CoSm), which instructs
LLMs to simulate code execution line by line/follow the computation pattern of
compilers. CoSm efficiently helps LLMs reduce memorisation and shallow pattern
recognition while improving simulation performance. We consider the success of
CoSm in code simulation to be inspirational for other general routine
simulation reasoning tasks.",2024-01-17T09:23:59Z
,http://arxiv.org/pdf/2405.10999v1.pdf,Large Language Models for Tuning Evolution Strategies,"Large Language Models (LLMs) exhibit world knowledge and inference
capabilities, making them powerful tools for various applications. This paper
proposes a feedback loop mechanism that leverages these capabilities to tune
Evolution Strategies (ES) parameters effectively. The mechanism involves a
structured process of providing programming instructions, executing the
corresponding code, and conducting thorough analysis. This process is
specifically designed for the optimization of ES parameters. The method
operates through an iterative cycle, ensuring continuous refinement of the ES
parameters. First, LLMs process the instructions to generate or modify the
code. The code is then executed, and the results are meticulously logged.
Subsequent analysis of these results provides insights that drive further
improvements. An experiment on tuning the learning rates of ES using the LLaMA3
model demonstrate the feasibility of this approach. This research illustrates
how LLMs can be harnessed to improve ES algorithms' performance and suggests
broader applications for similar feedback loop mechanisms in various domains.",2024-05-16T21:14:32Z
,http://arxiv.org/pdf/2404.08517v1.pdf,"Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path
  Forward","While Large Language Models (LLMs) have seen widespread applications across
numerous fields, their limited interpretability poses concerns regarding their
safe operations from multiple aspects, e.g., truthfulness, robustness, and
fairness. Recent research has started developing quality assurance methods for
LLMs, introducing techniques such as offline detector-based or uncertainty
estimation methods. However, these approaches predominantly concentrate on
post-generation analysis, leaving the online safety analysis for LLMs during
the generation phase an unexplored area. To bridge this gap, we conduct in this
work a comprehensive evaluation of the effectiveness of existing online safety
analysis methods on LLMs. We begin with a pilot study that validates the
feasibility of detecting unsafe outputs in the early generation process.
Following this, we establish the first publicly available benchmark of online
safety analysis for LLMs, including a broad spectrum of methods, models, tasks,
datasets, and evaluation metrics. Utilizing this benchmark, we extensively
analyze the performance of state-of-the-art online safety analysis methods on
both open-source and closed-source LLMs. This analysis reveals the strengths
and weaknesses of individual methods and offers valuable insights into
selecting the most appropriate method based on specific application scenarios
and task requirements. Furthermore, we also explore the potential of using
hybridization methods, i.e., combining multiple methods to derive a collective
safety conclusion, to enhance the efficacy of online safety analysis for LLMs.
Our findings indicate a promising direction for the development of innovative
and trustworthy quality assurance methodologies for LLMs, facilitating their
reliable deployments across diverse domains.",2024-04-12T14:55:16Z
,http://arxiv.org/pdf/2209.11302v1.pdf,"ProgPrompt: Generating Situated Robot Task Plans using Large Language
  Models","Task planning can require defining myriad domain knowledge about the world in
which a robot needs to act. To ameliorate that effort, large language models
(LLMs) can be used to score potential next actions during task planning, and
even generate action sequences directly, given an instruction in natural
language with no additional domain information. However, such methods either
require enumerating all possible next steps for scoring, or generate free-form
text that may contain actions not possible on a given robot in its current
context. We present a programmatic LLM prompt structure that enables plan
generation functional across situated environments, robot capabilities, and
tasks. Our key insight is to prompt the LLM with program-like specifications of
the available actions and objects in an environment, as well as with example
programs that can be executed. We make concrete recommendations about prompt
structure and generation constraints through ablation experiments, demonstrate
state of the art success rates in VirtualHome household tasks, and deploy our
method on a physical robot arm for tabletop tasks. Website at
progprompt.github.io",2022-09-22T20:29:49Z
10.1609/aaai.v38i21.30364,http://arxiv.org/pdf/2403.14565v1.pdf,"A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students'
  Formative Assessment Responses in Science","This paper explores the use of large language models (LLMs) to score and
explain short-answer assessments in K-12 science. While existing methods can
score more structured math and computer science assessments, they often do not
provide explanations for the scores. Our study focuses on employing GPT-4 for
automated assessment in middle school Earth Science, combining few-shot and
active learning with chain-of-thought reasoning. Using a human-in-the-loop
approach, we successfully score and provide meaningful explanations for
formative assessment responses. A systematic analysis of our method's pros and
cons sheds light on the potential for human-in-the-loop techniques to enhance
automated grading for open-ended science assessments.",2024-03-21T17:09:08Z
,http://arxiv.org/pdf/2305.13592v2.pdf,Understanding Programs by Exploiting (Fuzzing) Test Cases,"Semantic understanding of programs has attracted great attention in the
community. Inspired by recent successes of large language models (LLMs) in
natural language understanding, tremendous progress has been made by treating
programming language as another sort of natural language and training LLMs on
corpora of program code. However, programs are essentially different from texts
after all, in a sense that they are normally heavily structured and
syntax-strict. In particular, programs and their basic units (i.e., functions
and subroutines) are designed to demonstrate a variety of behaviors and/or
provide possible outputs, given different inputs. The relationship between
inputs and possible outputs/behaviors represents the functions/subroutines and
profiles the program as a whole. Therefore, we propose to incorporate such a
relationship into learning, for achieving a deeper semantic understanding of
programs. To obtain inputs that are representative enough to trigger the
execution of most part of the code, we resort to fuzz testing and propose fuzz
tuning to boost the performance of program understanding and code
representation learning, given a pre-trained LLM. The effectiveness of the
proposed method is verified on two program understanding tasks including code
clone detection and code classification, and it outperforms current
state-of-the-arts by large margins. Code is available at
https://github.com/rabbitjy/FuzzTuning.",2023-05-23T01:51:46Z
,http://arxiv.org/pdf/2303.11455v1.pdf,"Large Language Models and Simple, Stupid Bugs","With the advent of powerful neural language models, AI-based systems to
assist developers in coding tasks are becoming widely available; Copilot is one
such system. Copilot uses Codex, a large language model (LLM), to complete code
conditioned on a preceding ""prompt"". Codex, however, is trained on public
GitHub repositories, viz., on code that may include bugs and vulnerabilities.
Previous studies [1], [2] show Codex reproduces vulnerabilities seen in
training. In this study, we examine how prone Codex is to generate an
interesting bug category, single statement bugs, commonly referred to as
simple, stupid bugs or SStuBs in the MSR community. We find that Codex and
similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs
as much as 2x as likely than known, verbatim correct code. We explore the
consequences of the Codex generated SStuBs and propose avoidance strategies
that suggest the possibility of reducing the production of known, verbatim
SStubs, and increase the possibility of producing known, verbatim fixes.",2023-03-20T21:14:06Z
,http://arxiv.org/pdf/2312.15157v1.pdf,CodeScholar: Growing Idiomatic Code Examples,"Programmers often search for usage examples for API methods. A tool that
could generate realistic, idiomatic, and contextual usage examples for one or
more APIs would be immensely beneficial to developers. Such a tool would
relieve the need for a deep understanding of the API landscape, augment
existing documentation, and help discover interactions among APIs. We present
CodeScholar, a tool that generates idiomatic code examples demonstrating the
common usage of API methods. It includes a novel neural-guided search technique
over graphs that grows the query APIs into idiomatic code examples. Our user
study demonstrates that in 70% of cases, developers prefer CodeScholar
generated examples over state-of-the-art large language models (LLM) like
GPT3.5. We quantitatively evaluate 60 single and 25 multi-API queries from 6
popular Python libraries and show that across-the-board CodeScholar generates
more realistic, diverse, and concise examples. In addition, we show that
CodeScholar not only helps developers but also LLM-powered programming
assistants generate correct code in a program synthesis setting.",2023-12-23T04:06:15Z
,http://arxiv.org/pdf/2305.18341v2.pdf,Coarse-Tuning Models of Code with Reinforcement Learning Feedback,"Large Language Models (LLMs) pre-trained on code have recently emerged as the
dominant approach to program synthesis. However, these models are trained using
next-token prediction, which ignores the syntax and semantics of code. We
propose RLCF, that further trains a pre-trained LLM via reinforcement learning,
using feedback from a grounding function that scores the quality of the code.
The grounding function uses (i) compiler-derived feedback on whether the code
it generates passes a set of correctness checks; and (ii) feedback from a
different LLM that compares the generated code to a reference code. RLCF is
model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA
tasks for Java. Our experiments show that RLCF raises the odds that an
LLM-generated program compiles, is executable, and produces the right output on
tests, often allowing LLMs to match the performance of 2x-8x larger LLMs.",2023-05-25T22:09:08Z
,http://arxiv.org/pdf/2307.16696v2.pdf,"Large Language Models for Education: Grading Open-Ended Questions Using
  ChatGPT","As a way of addressing increasingly sophisticated problems, software
professionals face the constant challenge of seeking improvement. However, for
these individuals to enhance their skills, their process of studying and
training must involve feedback that is both immediate and accurate. In the
context of software companies, where the scale of professionals undergoing
training is large, but the number of qualified professionals available for
providing corrections is small, delivering effective feedback becomes even more
challenging. To circumvent this challenge, this work presents an exploration of
using Large Language Models (LLMs) to support the correction process of
open-ended questions in technical training. In this study, we utilized ChatGPT
to correct open-ended questions answered by 42 industry professionals on two
topics. Evaluating the corrections and feedback provided by ChatGPT, we
observed that it is capable of identifying semantic details in responses that
other metrics cannot observe. Furthermore, we noticed that, in general, subject
matter experts tended to agree with the corrections and feedback given by
ChatGPT.",2023-07-31T14:12:06Z
,http://arxiv.org/pdf/2312.17256v1.pdf,"From Bytes to Biases: Investigating the Cultural Self-Perception of
  Large Language Models","Large language models (LLMs) are able to engage in natural-sounding
conversations with humans, showcasing unprecedented capabilities for
information retrieval and automated decision support. They have disrupted
human-technology interaction and the way businesses operate. However,
technologies based on generative artificial intelligence (GenAI) are known to
hallucinate, misinform, and display biases introduced by the massive datasets
on which they are trained. Existing research indicates that humans may
unconsciously internalize these biases, which can persist even after they stop
using the programs. This study explores the cultural self-perception of LLMs by
prompting ChatGPT (OpenAI) and Bard (Google) with value questions derived from
the GLOBE project. The findings reveal that their cultural self-perception is
most closely aligned with the values of English-speaking countries and
countries characterized by sustained economic competitiveness. Recognizing the
cultural biases of LLMs and understanding how they work is crucial for all
members of society because one does not want the black box of artificial
intelligence to perpetuate bias in humans, who might, in turn, inadvertently
create and train even more biased algorithms.",2023-12-21T22:50:14Z
,http://arxiv.org/pdf/2406.00628v1.pdf,"Transforming Computer Security and Public Trust Through the Exploration
  of Fine-Tuning Large Language Models","Large language models (LLMs) have revolutionized how we interact with
machines. However, this technological advancement has been paralleled by the
emergence of ""Mallas,"" malicious services operating underground that exploit
LLMs for nefarious purposes. Such services create malware, phishing attacks,
and deceptive websites, escalating the cyber security threats landscape. This
paper delves into the proliferation of Mallas by examining the use of various
pre-trained language models and their efficiency and vulnerabilities when
misused. Building on a dataset from the Common Vulnerabilities and Exposures
(CVE) program, it explores fine-tuning methodologies to generate code and
explanatory text related to identified vulnerabilities. This research aims to
shed light on the operational strategies and exploitation techniques of Mallas,
leading to the development of more secure and trustworthy AI applications. The
paper concludes by emphasizing the need for further research, enhanced
safeguards, and ethical guidelines to mitigate the risks associated with the
malicious application of LLMs.",2024-06-02T06:10:31Z
,http://arxiv.org/pdf/2308.00229v1.pdf,"Prompts Matter: Insights and Strategies for Prompt Engineering in
  Automated Software Traceability","Large Language Models (LLMs) have the potential to revolutionize automated
traceability by overcoming the challenges faced by previous methods and
introducing new possibilities. However, the optimal utilization of LLMs for
automated traceability remains unclear. This paper explores the process of
prompt engineering to extract link predictions from an LLM. We provide detailed
insights into our approach for constructing effective prompts, offering our
lessons learned. Additionally, we propose multiple strategies for leveraging
LLMs to generate traceability links, improving upon previous zero-shot methods
on the ranking of candidate links after prompt refinement. The primary
objective of this paper is to inspire and assist future researchers and
engineers by highlighting the process of constructing traceability prompts to
effectively harness LLMs for advancing automatic traceability.",2023-08-01T01:56:22Z
,http://arxiv.org/pdf/2308.09183v2.pdf,RatGPT: Turning online LLMs into Proxies for Malware Attacks,"The evolution of Generative AI and the capabilities of the newly released
Large Language Models (LLMs) open new opportunities in software engineering.
However, they also lead to new challenges in cybersecurity. Recently,
researchers have shown the possibilities of using LLMs such as ChatGPT to
generate malicious content that can directly be exploited or guide
inexperienced hackers to weaponize tools and code. These studies covered
scenarios that still require the attacker to be in the middle of the loop. In
this study, we leverage openly available plugins and use an LLM as proxy
between the attacker and the victim. We deliver a proof-of-concept where
ChatGPT is used for the dissemination of malicious software while evading
detection, alongside establishing the communication to a command and control
(C2) server to receive commands to interact with a victim's system. Finally, we
present the general approach as well as essential elements in order to stay
undetected and make the attack a success. This proof-of-concept highlights
significant cybersecurity issues with openly available plugins and LLMs, which
require the development of security guidelines, controls, and mitigation
strategies.",2023-08-17T20:54:39Z
,http://arxiv.org/pdf/2405.20684v1.pdf,Joint Embeddings for Graph Instruction Tuning,"Large Language Models (LLMs) have achieved impressive performance in text
understanding and have become an essential tool for building smart assistants.
Originally focusing on text, they have been enhanced with multimodal
capabilities in recent works that successfully built visual instruction
following assistants. As far as the graph modality goes, however, no such
assistants have yet been developed. Graph structures are complex in that they
represent relation between different features and are permutation invariant.
Moreover, representing them in purely textual form does not always lead to good
LLM performance even for finetuned models. As a result, there is a need to
develop a new method to integrate graphs in LLMs for general graph
understanding. This work explores the integration of the graph modality in LLM
for general graph instruction following tasks. It aims at producing a deep
learning model that enhances an underlying LLM with graph embeddings and trains
it to understand them and to produce, given an instruction, an answer grounded
in the graph representation. The approach performs significantly better than a
graph to text approach and remains consistent even for larger graphs.",2024-05-31T08:26:47Z
,http://arxiv.org/pdf/2406.12950v1.pdf,"MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular
  Property Prediction","Molecular property prediction (MPP) is a fundamental and crucial task in drug
discovery. However, prior methods are limited by the requirement for a large
number of labeled molecules and their restricted ability to generalize for
unseen and new tasks, both of which are essential for real-world applications.
To address these challenges, we present MolecularGPT for few-shot MPP. From a
perspective on instruction tuning, we fine-tune large language models (LLMs)
based on curated molecular instructions spanning over 1000 property prediction
tasks. This enables building a versatile and specialized LLM that can be
adapted to novel MPP tasks without any fine-tuning through zero- and few-shot
in-context learning (ICL). MolecularGPT exhibits competitive in-context
reasoning capabilities across 10 downstream evaluation datasets, setting new
benchmarks for few-shot molecular prediction tasks. More importantly, with just
two-shot examples, MolecularGPT can outperform standard supervised graph neural
network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM
baselines by up to 16.6% increase on classification accuracy and decrease of
199.17 on regression metrics (e.g., RMSE) under zero-shot. This study
demonstrates the potential of LLMs as effective few-shot molecular property
predictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.",2024-06-18T12:54:47Z
,http://arxiv.org/pdf/2403.13271v1.pdf,"Enhancing Code Generation Performance of Smaller Models by Distilling
  the Reasoning Ability of LLMs","Large Language Models (LLMs) have recently made significant advances in code
generation through the 'Chain-of-Thought' prompting technique. This technique
empowers the model to autonomously devise ""solution plans"" to tackle intricate
programming challenges, thereby improving its performance in code generation.
Nevertheless, smaller models have been struggling to keep up with LLMs in
deducing these plans, adversely affecting their code generation capabilities.
Given the considerable size and associated deployment costs, along with
concerns about data security, many teams opt for deploying smaller models for
code generation. Consequently, there arises a compelling need for transferring
LLMs' code generation reasoning abilities to the smaller models. In this paper,
we propose the CodePLAN framework, which aims to transfer LLMs' reasoning
capabilities to smaller models through distillation. We adopt a multi-task
learning approach, jointly undertaking code generation and solution plan
generation tasks, to enhance the code generation capabilities of the smaller
model. To ensure the superior quality of the solution plans, we advocate for
the utilization of backward reasoning and plan sampling strategies. Our
experiments show that in comparison to the conventional fine-tuning approach,
our approach improves the smaller model's code generation performance (measured
in pass@1 metric) by over 130% on the challenging APPS benchmark.",2024-03-20T03:09:54Z
,http://arxiv.org/pdf/2404.14646v2.pdf,"Exploring and Unleashing the Power of Large Language Models in Automated
  Code Translation","Code translation tools (transpilers) are developed for automatic
source-to-source translation. Although learning-based transpilers have shown
impressive enhancement against rule-based counterparts, owing to their
task-specific pre-training on extensive monolingual corpora. Their current
performance still remains unsatisfactory for practical deployment, and the
associated training resources are also prohibitively expensive. LLMs
pre-trained on huge amounts of human-written code/text have shown remarkable
performance in many code intelligence tasks due to their powerful generality,
even without task-specific training. Thus, LLMs can potentially circumvent the
above limitations, but they have not been exhaustively explored yet. This paper
investigates diverse LLMs and learning-based transpilers for automated code
translation tasks, finding that: although certain LLMs have outperformed
current transpilers, they still have some accuracy issues, where most of the
failures are induced by a lack of comprehension of source programs, missing
clear instructions on I/O types in translation, and ignoring discrepancies
between source and target programs. Enlightened by the above findings, we
further propose UniTrans, a Unified code Translation framework, applicable to
various LLMs, for unleashing their power in this field. Specifically, UniTrans
first crafts a series of test cases for target programs with the assistance of
source programs. Next, it harnesses the above auto-generated test cases to
augment the code translation and then evaluate their correctness via execution.
Afterward, UniTrans further (iteratively) repairs incorrectly translated
programs prompted by test case execution results. Extensive experiments are
conducted on six settings of translation datasets between Python, Java, and
C++. Three recent LLMs of diverse sizes are tested with UniTrans, and all
achieve substantial improvements.",2024-04-23T00:49:46Z
,http://arxiv.org/pdf/2406.12902v1.pdf,"Can AI Beat Undergraduates in Entry-level Java Assignments? Benchmarking
  Large Language Models on JavaBench","Code generation benchmarks such as HumanEval are widely adopted to evaluate
LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we
noticed three significant imbalances. First, imbalanced programming language.
95.8% of benchmarks involve Python, while only 5 benchmarks involve Java.
Second, imbalanced code granularity. Function-/statement-level benchmarks
account for over 83.3% of benchmarks. Only a mere handful extends to
class-/project-levels, and all are limited to Python. Third, lacking advanced
features. Existing benchmarks primarily assess basic coding skills, while
overlooking advanced Object-Oriented Programming (OOP) features (i.e.,
encapsulation, inheritance, and polymorphism).
  To fill these gaps, we propose JavaBench, a project-level Java benchmark that
exercises OOP features. It comprises four Java projects with 389 methods in 106
Java classes. The test coverage is up to 92%, and JavaBench is attested by 282
undergraduate students, reaching a 90.93/100 average score (i.e., pass rate
against the test suite), ensuring the quality of documentation, code skeleton,
and tests. To better evaluate LLM's capability against JavaBench, we introduce
a systematic evaluation design covering three context settings and five
synthesis strategies at two granularities using three hierarchical metrics. Our
extensive experiment yields several interesting findings. First, we noticed
that regarding project-level Java programming, LLMs are far behind
undergraduate students (no project can be correctly completed by any studied
LLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using
method signature as prompt context may strike an ideal balance for
project-level code generation. JavaBench is publicly available at
https://github.com/java-bench/JavaBench.",2024-06-10T06:43:25Z
10.1145/3636243.3636257,http://arxiv.org/pdf/2311.09651v2.pdf,"""It's not like Jarvis, but it's pretty close!"" -- Examining ChatGPT's
  Usage among Undergraduate Students in Computer Science","Large language models (LLMs) such as ChatGPT and Google Bard have garnered
significant attention in the academic community. Previous research has
evaluated these LLMs for various applications such as generating programming
exercises and solutions. However, these evaluations have predominantly been
conducted by instructors and researchers, not considering the actual usage of
LLMs by students. This study adopts a student-first approach to comprehensively
understand how undergraduate computer science students utilize ChatGPT, a
popular LLM, released by OpenAI. We employ a combination of student surveys and
interviews to obtain valuable insights into the benefits, challenges, and
suggested improvements related to ChatGPT. Our findings suggest that a majority
of students (over 57%) have a convincingly positive outlook towards adopting
ChatGPT as an aid in coursework-related tasks. However, our research also
highlights various challenges that must be resolved for long-term acceptance of
ChatGPT amongst students. The findings from this investigation have broader
implications and may be applicable to other LLMs and their role in computing
education.",2023-11-16T08:10:18Z
,http://arxiv.org/pdf/2310.14735v4.pdf,"Unleashing the potential of prompt engineering in Large Language Models:
  a comprehensive review","This paper delves into the pivotal role of prompt engineering in unleashing
the capabilities of Large Language Models (LLMs). Prompt engineering is the
process of structuring input text for LLMs and is a technique integral to
optimizing the efficacy of LLMs. This survey elucidates foundational principles
of prompt engineering, such as role-prompting, one-shot, and few-shot
prompting, as well as more advanced methodologies such as the chain-of-thought
and tree-of-thoughts prompting. The paper sheds light on how external
assistance in the form of plugins can assist in this task, and reduce machine
hallucination by retrieving external knowledge. We subsequently delineate
prospective directions in prompt engineering research, emphasizing the need for
a deeper understanding of structures and the role of agents in Artificial
Intelligence-Generated Content (AIGC) tools. We discuss how to assess the
efficacy of prompt methods from different perspectives and using different
methods. Finally, we gather information about the application of prompt
engineering in such fields as education and programming, showing its
transformative potential. This comprehensive survey aims to serve as a friendly
guide for anyone venturing through the big world of LLMs and prompt
engineering.",2023-10-23T09:15:18Z
,http://arxiv.org/pdf/2310.10275v1.pdf,A ML-LLM pairing for better code comment classification,"The ""Information Retrieval in Software Engineering (IRSE)"" at FIRE 2023
shared task introduces code comment classification, a challenging task that
pairs a code snippet with a comment that should be evaluated as either useful
or not useful to the understanding of the relevant code. We answer the code
comment classification shared task challenge by providing a two-fold
evaluation: from an algorithmic perspective, we compare the performance of
classical machine learning systems and complement our evaluations from a
data-driven perspective by generating additional data with the help of large
language model (LLM) prompting to measure the potential increase in
performance. Our best model, which took second place in the shared task, is a
Neural Network with a Macro-F1 score of 88.401% on the provided seed data and a
1.5% overall increase in performance on the data generated by the LLM.",2023-10-13T12:43:13Z
,http://arxiv.org/pdf/2401.03003v4.pdf,"AST-T5: Structure-Aware Pretraining for Code Generation and
  Understanding","Large language models (LLMs) have made significant advancements in
code-related tasks, yet many LLMs treat code as simple sequences, neglecting
its structured nature. We introduce AST-T5, a novel pretraining paradigm that
leverages the Abstract Syntax Tree (AST) for enhanced code generation,
transpilation, and understanding. Using dynamic programming, our AST-Aware
Segmentation retains code structure, while our AST-Aware Span Corruption
objective equips the model to reconstruct various code structures. Unlike other
models, AST-T5 avoids intricate program analyses or architectural changes, so
it integrates seamlessly with any encoder-decoder Transformer. Evaluations show
that AST-T5 consistently outperforms similar-sized LMs across various
code-related tasks. Structure-awareness makes AST-T5 particularly powerful in
code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the
Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in
CodeXGLUE. Our code and model are publicly available at
https://github.com/gonglinyuan/ast_t5.",2024-01-05T06:51:08Z
,http://arxiv.org/pdf/2403.04123v1.pdf,Exploring LLM-based Agents for Root Cause Analysis,"The growing complexity of cloud based software systems has resulted in
incident management becoming an integral part of the software development
lifecycle. Root cause analysis (RCA), a critical part of the incident
management process, is a demanding task for on-call engineers, requiring deep
domain knowledge and extensive experience with a team's specific services.
Automation of RCA can result in significant savings of time, and ease the
burden of incident management on on-call engineers. Recently, researchers have
utilized Large Language Models (LLMs) to perform RCA, and have demonstrated
promising results. However, these approaches are not able to dynamically
collect additional diagnostic information such as incident related logs,
metrics or databases, severely restricting their ability to diagnose root
causes. In this work, we explore the use of LLM based agents for RCA to address
this limitation. We present a thorough empirical evaluation of a ReAct agent
equipped with retrieval tools, on an out-of-distribution dataset of production
incidents collected at Microsoft. Results show that ReAct performs
competitively with strong retrieval and reasoning baselines, but with highly
increased factual accuracy. We then extend this evaluation by incorporating
discussions associated with incident reports as additional inputs for the
models, which surprisingly does not yield significant performance improvements.
Lastly, we conduct a case study with a team at Microsoft to equip the ReAct
agent with tools that give it access to external diagnostic services that are
used by the team for manual RCA. Our results show how agents can overcome the
limitations of prior work, and practical considerations for implementing such a
system in practice.",2024-03-07T00:44:01Z
,http://arxiv.org/pdf/2402.01619v1.pdf,"KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce
  Programs over Low-resourced Knowledge Bases","Program induction (PI) has become a promising paradigm for using knowledge
bases (KBs) to help large language models (LLMs) answer complex
knowledge-intensive questions. Nonetheless, PI typically relies on a large
number of parallel question-program pairs to make the LLM aware of the schema
of the given KB, and is thus challenging for many low-resourced KBs that lack
annotated data. To this end, we propose KB-Plugin, a plug-and-play framework
that enables LLMs to induce programs over any low-resourced KB. Firstly,
KB-Plugin adopts self-supervised learning to encode the detailed schema
information of a given KB into a pluggable module, namely schema plugin.
Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB
to train another pluggable module, namely PI plugin, which can help the LLM
extract question-relevant schema information from the schema plugin of any KB
and utilize this information to induce programs over this KB. Experiments on
five heterogeneous KBQA datasets show that KB-Plugin achieves better or
comparable performance with 25$\times$ smaller backbone LLM compared to SoTA PI
methods for low-resourced KBs, and even approaches the performance of
supervised methods. Our code and data are available at
https://github.com/THU-KEG/KB-Plugin.",2024-02-02T18:32:24Z
,http://arxiv.org/pdf/2308.09895v5.pdf,"Knowledge Transfer from High-Resource to Low-Resource Programming
  Languages for Code LLMs","Over the past few years, Large Language Models of Code (Code LLMs) have
started to have a significant impact on programming practice. Code LLMs are
also emerging as building blocks for research in programming languages and
software engineering. However, Code LLMs produce impressive results on
programming languages that are well represented in their training data (e.g.,
Java, Python, or JavaScript), but struggle with low-resource languages that
have limited training data available. Low resource languages include OCaml,
Racket, and several others.
  This paper presents an effective approach for boosting the performance of
Code LLMs on low-resource languages using semi-synthetic data. Our approach,
MultiPL-T, translates training data from high-resource languages into training
data for low-resource languages in the following way. 1) We use a Code LLM to
synthesize tests for commented code from a high-resource language, filtering
out faulty tests and code with low test coverage. 2) We use a Code LLM to
translate Python code to a target low-resource language, and use tests to
validate the translation. We apply this approach to generate tens of thousands
of validated training items for Julia, Lua, OCaml, R, and Racket. Furthermore,
we use an open model (StarCoderBase) with open training data (The Stack), which
allows us to decontaminate benchmarks, train models without violating licenses,
and run experiments that could not otherwise be done.
  With MultiPL-T generated data, we present fine-tuned versions of
StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket. On
established benchmarks (MultiPL-E), these models outperform other open Code
LLMs. The MultiPL-T approach is easy to apply to new languages, and is
significantly more efficient and effective than alternatives such as training
longer.",2023-08-19T03:19:01Z
,http://arxiv.org/pdf/2310.20329v3.pdf,InstructCoder: Instruction Tuning Large Language Models for Code Editing,"Code editing encompasses a variety of pragmatic tasks that developers deal
with daily. Despite its relevance and practical usefulness, automatic code
editing remains an underexplored area in the evolution of deep learning models,
partly due to data scarcity. In this work, we explore the use of Large Language
Models (LLMs) to edit code based on user instructions. Evaluated on a novel
human-written execution-based benchmark dubbed EditEval, we found current
models often struggle to fulfill the instructions. In light of this, we
contribute InstructCoder, the first instruction-tuning dataset designed to
adapt LLMs for general-purpose code editing, containing high-diversity
code-editing tasks such as comment insertion, code optimization, and code
refactoring. It consists of over 114,000 instruction-input-output triplets and
covers multiple distinct code editing scenarios. The collection process starts
with filtered commit data sourced from GitHub Python repositories as seeds.
Subsequently, the dataset is systematically expanded through an iterative
process, where both seed and generated tasks are used to prompt ChatGPT for
more data. Our findings reveal that open-source LLMs fine-tuned on
InstructCoder can significantly enhance the accuracy of code edits, exhibiting
superior code-editing performance matching advanced proprietary LLMs. The
datasets and the source code are publicly available at
https://github.com/qishenghu/CodeInstruct.",2023-10-31T10:15:35Z
,http://arxiv.org/pdf/2404.10678v1.pdf,Automating REST API Postman Test Cases Using LLM,"In the contemporary landscape of technological advancements, the automation
of manual processes is crucial, compelling the demand for huge datasets to
effectively train and test machines. This research paper is dedicated to the
exploration and implementation of an automated approach to generate test cases
specifically using Large Language Models. The methodology integrates the use of
Open AI to enhance the efficiency and effectiveness of test case generation for
training and evaluating Large Language Models. This formalized approach with
LLMs simplifies the testing process, making it more efficient and
comprehensive. Leveraging natural language understanding, LLMs can
intelligently formulate test cases that cover a broad range of REST API
properties, ensuring comprehensive testing. The model that is developed during
the research is trained using manually collected postman test cases or
instances for various Rest APIs. LLMs enhance the creation of Postman test
cases by automating the generation of varied and intricate test scenarios.
Postman test cases offer streamlined automation, collaboration, and dynamic
data handling, providing a user-friendly and efficient approach to API testing
compared to traditional test cases. Thus, the model developed not only conforms
to current technological standards but also holds the promise of evolving into
an idea of substantial importance in future technological advancements.",2024-04-16T15:53:41Z
,http://arxiv.org/pdf/2406.07174v1.pdf,"ULog: Unsupervised Log Parsing with Large Language Models through Log
  Contrastive Units","Log parsing serves as an essential prerequisite for various log analysis
tasks. Recent advancements in this field have improved parsing accuracy by
leveraging the semantics in logs through fine-tuning large language models
(LLMs) or learning from in-context demonstrations. However, these methods
heavily depend on labeled examples to achieve optimal performance. In practice,
collecting sufficient labeled data is challenging due to the large scale and
continuous evolution of logs, leading to performance degradation of existing
log parsers after deployment. To address this issue, we propose ULog, an
unsupervised LLM-based method for efficient and off-the-shelf log parsing. Our
key insight is that while LLMs may struggle with direct log parsing, their
performance can be significantly enhanced through comparative analysis across
multiple logs that differ only in their parameter parts. We refer to such
groups of logs as Log Contrastive Units (LCUs). Given the vast volume of logs,
obtaining LCUs is difficult. Therefore, ULog introduces a hybrid ranking scheme
to effectively search for LCUs by jointly considering the commonality and
variability among logs. Additionally, ULog crafts a novel parsing prompt for
LLMs to identify contrastive patterns and extract meaningful log structures
from LCUs. Experiments on large-scale public datasets demonstrate that ULog
significantly outperforms state-of-the-art log parsers in terms of accuracy and
efficiency, providing an effective and scalable solution for real-world
deployment.",2024-06-11T11:32:01Z
,http://arxiv.org/pdf/2406.09757v1.pdf,"Evaluating LLM-driven User-Intent Formalization for Verification-Aware
  Languages","Verification-aware programming languages such as Dafny and F* provide means
to formally specify and prove properties of programs. Although the problem of
checking an implementation against a specification can be defined mechanically,
there is no algorithmic way of ensuring the correctness of the user-intent
formalization for programs -- that a specification adheres to the user's intent
behind the program. The intent or requirement is expressed informally in
natural language and the specification is a formal artefact. The advent of
large language models (LLMs) has made strides bridging the gap between informal
intent and formal program implementations recently, driven in large parts due
to benchmarks and automated metrics for evaluation.
  Recent work has proposed evaluating {\it user-intent formalization} problem
for mainstream programming languages~\cite{endres-fse24}. However, such an
approach does not readily extend to verification-aware languages that support
rich specifications (containing quantifiers and ghost variables) that cannot be
evaluated through dynamic execution. Previous work also required generating
program mutants using LLMs to create the benchmark. We advocate an alternate
approach of {\it symbolically testing specifications} to provide an intuitive
metric for evaluating the quality of specifications for verification-aware
languages. We demonstrate that our automated metric agrees closely with mostly
GPT-4 generated and human-labeled dataset of roughly 150 Dafny specifications
for the popular MBPP code-generation benchmark, yet demonstrates cases where
the human labeling is not perfect. We believe our work provides a stepping
stone to enable the establishment of a benchmark and research agenda for the
problem of user-intent formalization for programs.",2024-06-14T06:52:08Z
,http://arxiv.org/pdf/2405.03256v1.pdf,MARE: Multi-Agents Collaboration Framework for Requirements Engineering,"Requirements Engineering (RE) is a critical phase in the software development
process that generates requirements specifications from stakeholders' needs.
Recently, deep learning techniques have been successful in several RE tasks.
However, obtaining high-quality requirements specifications requires
collaboration across multiple tasks and roles. In this paper, we propose an
innovative framework called MARE, which leverages collaboration among large
language models (LLMs) throughout the entire RE process. MARE divides the RE
process into four tasks: elicitation, modeling, verification, and
specification. Each task is conducted by engaging one or two specific agents
and each agent can conduct several actions. MARE has five agents and nine
actions. To facilitate collaboration between agents, MARE has designed a
workspace for agents to upload their generated intermediate requirements
artifacts and obtain the information they need. We conduct experiments on five
public cases, one dataset, and four new cases created by this work. We compared
MARE with three baselines using three widely used metrics for the generated
requirements models. Experimental results show that MARE can generate more
correct requirements models and outperform the state-of-the-art approaches by
15.4%. For the generated requirements specifications, we conduct a human
evaluation in three aspects and provide insights about the quality",2024-05-06T08:24:55Z
,http://arxiv.org/pdf/2404.04949v1.pdf,"SilverSight: A Multi-Task Chinese Financial Large Language Model Based
  on Adaptive Semantic Space Learning","Large language models (LLMs) are increasingly being applied across various
specialized fields, leveraging their extensive knowledge to empower a multitude
of scenarios within these domains. However, each field encompasses a variety of
specific tasks that require learning, and the diverse, heterogeneous data
across these domains can lead to conflicts during model task transfer. In
response to this challenge, our study introduces an Adaptive Semantic Space
Learning (ASSL) framework, which utilizes the adaptive reorganization of data
distributions within the semantic space to enhance the performance and
selection efficacy of multi-expert models. Utilizing this framework, we trained
a financial multi-task LLM named ""SilverSight"". Our research findings
demonstrate that our framework can achieve results close to those obtained with
full data training using only 10% of the data, while also exhibiting strong
generalization capabilities.",2024-04-07T13:02:21Z
10.1145/3613904.3642229,http://arxiv.org/pdf/2402.04975v1.pdf,"ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming
  Learning for Children Aged 6-12","As Computational Thinking (CT) continues to permeate younger age groups in
K-12 education, established CT platforms such as Scratch face challenges in
catering to these younger learners, particularly those in the elementary school
(ages 6-12). Through formative investigation with Scratch experts, we uncover
three key obstacles to children's autonomous Scratch learning: artist's block
in project planning, bounded creativity in asset creation, and inadequate
coding guidance during implementation. To address these barriers, we introduce
ChatScratch, an AI-augmented system to facilitate autonomous programming
learning for young children. ChatScratch employs structured interactive
storyboards and visual cues to overcome artist's block, integrates digital
drawing and advanced image generation technologies to elevate creativity, and
leverages Scratch-specialized Large Language Models (LLMs) for professional
coding guidance. Our study shows that, compared to Scratch, ChatScratch
efficiently fosters autonomous programming learning, and contributes to the
creation of high-quality, personally meaningful Scratch projects for children.",2024-02-07T15:55:51Z
,http://arxiv.org/pdf/2404.08710v1.pdf,Do Large Language Models Learn Human-Like Strategic Preferences?,"We evaluate whether LLMs learn to make human-like preference judgements in
strategic scenarios as compared with known empirical results. We show that
Solar and Mistral exhibit stable value-based preference consistent with human
in the prisoner's dilemma, including stake-size effect, and traveler's dilemma,
including penalty-size effect. We establish a relationship between model size,
value based preference, and superficiality. Finally, we find that models that
tend to be less brittle were trained with sliding window attention.
Additionally, we contribute a novel method for constructing preference
relations from arbitrary LLMs and support for a hypothesis regarding human
behavior in the traveler's dilemma.",2024-04-11T19:13:24Z
,http://arxiv.org/pdf/2404.03887v3.pdf,"SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical
  Reasoning in Large Language Models","This study presents a novel learning approach designed to enhance both
mathematical reasoning and problem-solving abilities of Large Language Models
(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the
Program-of-Thought (PoT) learning, hypothesizing that prioritizing the learning
of mathematical reasoning ability is helpful for the amplification of
problem-solving ability. Thus, the initial learning with CoT is essential for
solving challenging mathematical problems. To this end, we propose a sequential
learning approach, named SAAS (Solving Ability Amplification Strategy), which
strategically transitions from CoT learning to PoT learning. Our empirical
study, involving an extensive performance comparison using several benchmarks,
demonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The
results underscore the effectiveness of our sequential learning approach,
marking a significant advancement in the field of mathematical reasoning in
LLMs.",2024-04-05T04:25:47Z
,http://arxiv.org/pdf/2406.06555v1.pdf,An Evaluation Benchmark for Autoformalization in Lean4,"Large Language Models (LLMs) hold the potential to revolutionize
autoformalization. The introduction of Lean4, a mathematical programming
language, presents an unprecedented opportunity to rigorously assess the
autoformalization capabilities of LLMs. This paper introduces a novel
evaluation benchmark designed for Lean4, applying it to test the abilities of
state-of-the-art LLMs, including GPT-3.5, GPT-4, and Gemini Pro. Our
comprehensive analysis reveals that, despite recent advancements, these LLMs
still exhibit limitations in autoformalization, particularly in more complex
areas of mathematics. These findings underscore the need for further
development in LLMs to fully harness their potential in scientific research and
development. This study not only benchmarks current LLM capabilities but also
sets the stage for future enhancements in autoformalization.",2024-06-01T07:06:57Z
,http://arxiv.org/pdf/2404.01245v1.pdf,"A Statistical Framework of Watermarks for Large Language Models: Pivot,
  Detection Efficiency and Optimal Rules","Since ChatGPT was introduced in November 2022, embedding (nearly)
unnoticeable statistical signals into text generated by large language models
(LLMs), also known as watermarking, has been used as a principled approach to
provable detection of LLM-generated text from its human-written counterpart. In
this paper, we introduce a general and flexible framework for reasoning about
the statistical efficiency of watermarks and designing powerful detection
rules. Inspired by the hypothesis testing formulation of watermark detection,
our framework starts by selecting a pivotal statistic of the text and a secret
key -- provided by the LLM to the verifier -- to enable controlling the false
positive rate (the error of mistakenly detecting human-written text as
LLM-generated). Next, this framework allows one to evaluate the power of
watermark detection rules by obtaining a closed-form expression of the
asymptotic false negative rate (the error of incorrectly classifying
LLM-generated text as human-written). Our framework further reduces the problem
of determining the optimal detection rule to solving a minimax optimization
program. We apply this framework to two representative watermarks -- one of
which has been internally implemented at OpenAI -- and obtain several findings
that can be instrumental in guiding the practice of implementing watermarks. In
particular, we derive optimal detection rules for these watermarks under our
framework. These theoretically derived detection rules are demonstrated to be
competitive and sometimes enjoy a higher power than existing detection
approaches through numerical experiments.",2024-04-01T17:03:41Z
,http://arxiv.org/pdf/2312.05356v3.pdf,Neuron-level LLM Patching for Code Generation,"Large Language Models (LLMs) have found widespread adoption in software
engineering, particularly in code generation tasks. However, updating these
models with new knowledge can be prohibitively expensive, yet it is essential
for maximizing their utility. In this paper, we propose a novel and effective
model editing approach, \textsc{MENT}, to patch LLMs in coding tasks.
\textsc{MENT} is effective, efficient, and reliable. It can correct a neural
model by patching 1 or 2 neurons. As the pioneer work on neuron-level model
editing of generative models, we formalize the editing process and introduce
the involved concepts. Besides, we also introduce new measures to evaluate its
generalization ability, and build a benchmark for further study. Our approach
is evaluated on three coding tasks, including API-seq recommendation,
line-level code generation, and pseudocode-to-code transaction. The
experimental results show that the proposed approach outperforms the state of
the arts by a significant margin in both effectiveness and efficiency measures.
In addition, we demonstrate the usages of \textsc{MENT} for LLM reasoning in
software engineering. By editing LLM knowledge, the directly or indirectly
dependent behaviors of API invocation in the chain-of-thought will change
accordingly. It explained the significance of repairing LLMs.",2023-12-08T20:28:08Z
,http://arxiv.org/pdf/2304.00385v1.pdf,"Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each
  using ChatGPT","Automated Program Repair (APR) aims to automatically generate patches for
buggy programs. Recent APR work has been focused on leveraging modern Large
Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR
tools work by first constructing an input prompt built using the original buggy
code and then queries the LLM to generate patches. While the LLM-based APR
tools are able to achieve state-of-the-art results, it still follows the
classic Generate and Validate repair paradigm of first generating lots of
patches and then validating each one afterwards. This not only leads to many
repeated patches that are incorrect but also miss the crucial information in
test failures as well as in plausible patches.
  To address these limitations, we propose ChatRepair, the first fully
automated conversation-driven APR approach that interleaves patch generation
with instant feedback to perform APR in a conversational style. ChatRepair
first feeds the LLM with relevant test failure information to start with, and
then learns from both failures and successes of earlier patching attempts of
the same bug for more powerful APR. For earlier patches that failed to pass all
tests, we combine the incorrect patches with their corresponding relevant test
failure information to construct a new prompt for the LLM to generate the next
patch. In this way, we can avoid making the same mistakes. For earlier patches
that passed all the tests, we further ask the LLM to generate alternative
variations of the original plausible patches. In this way, we can further build
on and learn from earlier successes to generate more plausible patches to
increase the chance of having correct patches. While our approach is general,
we implement ChatRepair using state-of-the-art dialogue-based LLM -- ChatGPT.
By calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugs
for \$0.42 each!",2023-04-01T20:57:33Z
,http://arxiv.org/pdf/2302.06527v4.pdf,"An Empirical Evaluation of Using Large Language Models for Automated
  Unit Test Generation","Unit tests play a key role in ensuring the correctness of software. However,
manually creating unit tests is a laborious task, motivating the need for
automation. Large Language Models (LLMs) have recently been applied to this
problem, utilizing additional training or few-shot learning on examples of
existing tests. This paper presents a large-scale empirical evaluation on the
effectiveness of LLMs for automated unit test generation without additional
training or manual effort, providing the LLM with the signature and
implementation of the function under test, along with usage examples extracted
from documentation. We also attempt to repair failed generated tests by
re-prompting the model with the failing test and error message. We implement
our approach in TestPilot, a test generation tool for JavaScript that
automatically generates unit tests for all API functions in an npm package. We
evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a
total of 1,684 API functions. The generated tests achieve a median statement
coverage of 70.2% and branch coverage of 52.8%, significantly improving on
Nessie, a recent feedback-directed JavaScript test generation technique, which
achieves only 51.3% statement coverage and 25.6% branch coverage. We also find
that 92.8% of TestPilot's generated tests have no more than 50% similarity with
existing tests (as measured by normalized edit distance), with none of them
being exact copies. Finally, we run TestPilot with two additional LLMs,
OpenAI's older code-cushman-002 LLM and the open LLM StarCoder. Overall, we
observed similar results with the former (68.2% median statement coverage), and
somewhat worse results with the latter (54.0% median statement coverage),
suggesting that the effectiveness of the approach is influenced by the size and
training set of the LLM, but does not fundamentally depend on the specific
model.",2023-02-13T17:13:41Z
,http://arxiv.org/pdf/2312.00024v3.pdf,Can LLMs Patch Security Issues?,"Large Language Models (LLMs) have shown impressive proficiency in code
generation. Nonetheless, similar to human developers, these models might
generate code that contains security vulnerabilities and flaws. Writing secure
code remains a substantial challenge, as vulnerabilities often arise during
interactions between programs and external systems or services, such as
databases and operating systems. In this paper, we propose a novel approach,
Feedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMs
in receiving feedback from Bandit, which is a static code analysis tool, and
then the LLMs generate potential solutions to resolve security vulnerabilities.
Each solution, along with the vulnerable code, is then sent back to the LLM for
code refinement. Our approach shows a significant improvement over the baseline
and outperforms existing approaches. Furthermore, we introduce a new dataset,
PythonSecurityEval, collected from real-world scenarios on Stack Overflow to
evaluate the LLMs' ability to generate secure code. Code and data are available
at \url{https://github.com/Kamel773/LLM-code-refine}",2023-11-13T08:54:37Z
,http://arxiv.org/pdf/2406.08587v1.pdf,"CS-Bench: A Comprehensive Benchmark for Large Language Models towards
  Computer Science Mastery","Computer Science (CS) stands as a testament to the intricacies of human
intelligence, profoundly advancing the development of artificial intelligence
and modern society. However, the current community of large language models
(LLMs) overly focuses on benchmarks for analyzing specific foundational skills
(e.g. mathematics and code generation), neglecting an all-round evaluation of
the computer science field. To bridge this gap, we introduce CS-Bench, the
first bilingual (Chinese-English) benchmark dedicated to evaluating the
performance of LLMs in computer science. CS-Bench comprises approximately 5K
meticulously curated test samples, covering 26 subfields across 4 key areas of
computer science, encompassing various task forms and divisions of knowledge
and reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of
over 30 mainstream LLMs, revealing the relationship between CS performance and
model scales. We also quantitatively analyze the reasons for failures in
existing LLMs and highlight directions for improvements, including knowledge
supplementation and CS-specific reasoning. Further cross-capability experiments
show a high correlation between LLMs' capabilities in computer science and
their abilities in mathematics and coding. Moreover, expert LLMs specialized in
mathematics and coding also demonstrate strong performances in several CS
subfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM
applications in the CS field and paving new avenues in assessing LLMs' diverse
reasoning capabilities. The CS-Bench data and evaluation code are available at
https://github.com/csbench/csbench.",2024-06-12T18:47:28Z
,http://arxiv.org/pdf/2406.07571v1.pdf,"Supporting Self-Reflection at Scale with Large Language Models: Insights
  from Randomized Field Experiments in Classrooms","Self-reflection on learning experiences constitutes a fundamental cognitive
process, essential for the consolidation of knowledge and the enhancement of
learning efficacy. However, traditional methods to facilitate reflection often
face challenges in personalization, immediacy of feedback, engagement, and
scalability. Integration of Large Language Models (LLMs) into the reflection
process could mitigate these limitations. In this paper, we conducted two
randomized field experiments in undergraduate computer science courses to
investigate the potential of LLMs to help students engage in post-lesson
reflection. In the first experiment (N=145), students completed a take-home
assignment with the support of an LLM assistant; half of these students were
then provided access to an LLM designed to facilitate self-reflection. The
results indicated that the students assigned to LLM-guided reflection reported
increased self-confidence and performed better on a subsequent exam two weeks
later than their peers in the control condition. In the second experiment
(N=112), we evaluated the impact of LLM-guided self-reflection against other
scalable reflection methods, such as questionnaire-based activities and review
of key lecture slides, after assignment. Our findings suggest that the students
in the questionnaire and LLM-based reflection groups performed equally well and
better than those who were only exposed to lecture slides, according to their
scores on a proctored exam two weeks later on the same subject matter. These
results underscore the utility of LLM-guided reflection and questionnaire-based
activities in improving learning outcomes. Our work highlights that focusing
solely on the accuracy of LLMs can overlook their potential to enhance
metacognitive skills through practices such as self-reflection. We discuss the
implications of our research for the Edtech community.",2024-06-01T02:41:59Z
,http://arxiv.org/pdf/2212.06742v2.pdf,"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for
  Programming Languages","Software engineers working with the same programming language (PL) may speak
different natural languages (NLs) and vice versa, erecting huge barriers to
communication and working efficiency. Recent studies have demonstrated the
effectiveness of generative pre-training in computer programs, yet they are
always English-centric. In this work, we step towards bridging the gap between
multilingual NLs and multilingual PLs for large language models (LLMs). We
release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.
We employ two methods for universal cross-lingual pre-training: span-corruption
language modeling that learns patterns from monolingual NL or PL; and
pivot-based translation language modeling that relies on parallel data of many
NLs and PLs. Extensive results show that ERNIE-Code outperforms previous
multilingual LLMs for PL or NL across a wide range of end tasks of code
intelligence, including multilingual code-to-text, text-to-code, code-to-code,
and text-to-text generation. We further show its advantage of zero-shot
prompting on multilingual code summarization and text-to-text translation. We
release our code and pre-trained checkpoints.",2022-12-13T17:21:44Z
10.1145/3585059.3611431,http://arxiv.org/pdf/2307.16650v1.pdf,"ChatGPT for Teaching and Learning: An Experience from Data Science
  Education","ChatGPT, an implementation and application of large language models, has
gained significant popularity since its initial release. Researchers have been
exploring ways to harness the practical benefits of ChatGPT in real-world
scenarios. Educational researchers have investigated its potential in various
subjects, e.g., programming, mathematics, finance, clinical decision support,
etc. However, there has been limited attention given to its application in data
science education. This paper aims to bridge that gap by utilizing ChatGPT in a
data science course, gathering perspectives from students, and presenting our
experiences and feedback on using ChatGPT for teaching and learning in data
science education. The findings not only distinguish data science education
from other disciplines but also uncover new opportunities and challenges
associated with incorporating ChatGPT into the data science curriculum.",2023-07-31T13:31:19Z
,http://arxiv.org/pdf/2303.08268v3.pdf,"Chat with the Environment: Interactive Multimodal Perception Using Large
  Language Models","Programming robot behavior in a complex world faces challenges on multiple
levels, from dextrous low-level skills to high-level planning and reasoning.
Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning
ability in few-shot robotic planning. However, it remains challenging to ground
LLMs in multimodal sensory input and continuous action output, while enabling a
robot to interact with its environment and acquire novel information as its
policies unfold. We develop a robot interaction scenario with a partially
observable state, which necessitates a robot to decide on a range of epistemic
actions in order to sample sensory information among multiple modalities,
before being able to execute the task correctly. Matcha (Multimodal environment
chatting) agent, an interactive perception framework, is therefore proposed
with an LLM as its backbone, whose ability is exploited to instruct epistemic
actions and to reason over the resulting multimodal sensations (vision, sound,
haptics, proprioception), as well as to plan an entire task execution based on
the interactively acquired information. Our study demonstrates that LLMs can
provide high-level planning and reasoning skills and control interactive robot
behavior in a multimodal environment, while multimodal modules with the context
of the environmental state help ground the LLMs and extend their processing
ability. The project website can be found at https://matcha-agent.github.io.",2023-03-14T23:01:27Z
,http://arxiv.org/pdf/2304.09842v3.pdf,"Chameleon: Plug-and-Play Compositional Reasoning with Large Language
  Models","Large language models (LLMs) have achieved remarkable progress in solving
various natural language processing tasks due to emergent reasoning abilities.
However, LLMs have inherent limitations as they are incapable of accessing
up-to-date information (stored on the Web or in task-specific knowledge bases),
using external tools, and performing precise mathematical and logical
reasoning. In this paper, we present Chameleon, an AI system that mitigates
these limitations by augmenting LLMs with plug-and-play modules for
compositional reasoning. Chameleon synthesizes programs by composing various
tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python
functions, and heuristic-based modules) for accomplishing complex reasoning
tasks. At the heart of Chameleon is an LLM-based planner that assembles a
sequence of tools to execute to generate the final response. We showcase the
effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning
tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%
overall accuracy on ScienceQA, improving the best published few-shot result by
11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,
lifting the state of the art to 98.78%. Our analysis also shows that the
GPT-4-powered planner exhibits more consistent and rational tool selection via
inferring potential constraints from instructions, compared to a
ChatGPT-powered planner. The project is available at
https://chameleon-llm.github.io.",2023-04-19T17:47:47Z
,http://arxiv.org/pdf/2406.04712v1.pdf,"AICoderEval: Improving AI Domain Code Generation of Large Language
  Models","Automated code generation is a pivotal capability of large language models
(LLMs). However, assessing this capability in real-world scenarios remains
challenging. Previous methods focus more on low-level code generation, such as
model loading, instead of generating high-level codes catering for real-world
tasks, such as image-to-text, text classification, in various domains.
Therefore, we construct AICoderEval, a dataset focused on real-world tasks in
various domains based on HuggingFace, PyTorch, and TensorFlow, along with
comprehensive metrics for evaluation and enhancing LLMs' task-specific code
generation capability. AICoderEval contains test cases and complete programs
for automated evaluation of these tasks, covering domains such as natural
language processing, computer vision, and multimodal learning. To facilitate
research in this area, we open-source the AICoderEval dataset at
\url{https://huggingface.co/datasets/vixuowis/AICoderEval}. After that, we
propose CoderGen, an agent-based framework, to help LLMs generate codes related
to real-world tasks on the constructed AICoderEval. Moreover, we train a more
powerful task-specific code generation model, named AICoder, which is refined
on llama-3 based on AICoderEval. Our experiments demonstrate the effectiveness
of CoderGen in improving LLMs' task-specific code generation capability (by
12.00\% on pass@1 for original model and 9.50\% on pass@1 for ReAct Agent).
AICoder also outperforms current code generation LLMs, indicating the great
quality of the AICoderEval benchmark.",2024-06-07T07:45:38Z
,http://arxiv.org/pdf/2406.14408v2.pdf,"FVEL: Interactive Formal Verification Environment with Large Language
  Models via Theorem Proving","Formal verification (FV) has witnessed growing significance with current
emerging program synthesis by the evolving large language models (LLMs).
However, current formal verification mainly resorts to symbolic verifiers or
hand-craft rules, resulting in limitations for extensive and flexible
verification. On the other hand, formal languages for automated theorem
proving, such as Isabelle, as another line of rigorous verification, are
maintained with comprehensive rules and theorems. In this paper, we propose
FVEL, an interactive Formal Verification Environment with LLMs. Specifically,
FVEL transforms a given code to be verified into Isabelle, and then conducts
verification via neural automated theorem proving with an LLM. The joined
paradigm leverages the rigorous yet abundant formulated and organized rules in
Isabelle and is also convenient for introducing and adjusting cutting-edge
LLMs. To achieve this goal, we extract a large-scale FVELER3. The FVELER
dataset includes code dependencies and verification processes that are
formulated in Isabelle, containing 758 theories, 29,125 lemmas, and 200,646
proof steps in total with in-depth dependencies. We benchmark FVELER in the
FVEL environment by first fine-tuning LLMs with FVELER and then evaluating them
on Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned
Llama3- 8B solves 17.39% (69 -> 81) more problems, and Mistral-7B 12% (75 ->
84) more problems in SV-COMP. And the proportion of proof errors is reduced.
Project page: https://fveler.github.io/.",2024-06-20T15:31:05Z
,http://arxiv.org/pdf/2404.14901v2.pdf,"Beyond Code Generation: An Observational Study of ChatGPT Usage in
  Software Engineering Practice","Large Language Models (LLMs) are frequently discussed in academia and the
general public as support tools for virtually any use case that relies on the
production of text, including software engineering. Currently there is much
debate, but little empirical evidence, regarding the practical usefulness of
LLM-based tools such as ChatGPT for engineers in industry. We conduct an
observational study of 24 professional software engineers who have been using
ChatGPT over a period of one week in their jobs, and qualitatively analyse
their dialogues with the chatbot as well as their overall experience (as
captured by an exit survey). We find that, rather than expecting ChatGPT to
generate ready-to-use software artifacts (e.g., code), practitioners more often
use ChatGPT to receive guidance on how to solve their tasks or learn about a
topic in more abstract terms. We also propose a theoretical framework for how
(i) purpose of the interaction, (ii) internal factors (e.g., the user's
personality), and (iii) external factors (e.g., company policy) together shape
the experience (in terms of perceived usefulness and trust). We envision that
our framework can be used by future research to further the academic discussion
on LLM usage by software engineering practitioners, and to serve as a reference
point for the design of future empirical LLM research in this domain.",2024-04-23T10:34:16Z
,http://arxiv.org/pdf/2406.10903v1.pdf,"New Solutions on LLM Acceleration, Optimization, and Application","Large Language Models (LLMs) have become extremely potent instruments with
exceptional capacities for comprehending and producing human-like text in a
wide range of applications. However, the increasing size and complexity of LLMs
present significant challenges in both training and deployment, leading to
substantial computational and storage costs as well as heightened energy
consumption. In this paper, we provide a review of recent advancements and
research directions aimed at addressing these challenges and enhancing the
efficiency of LLM-based systems. We begin by discussing algorithm-level
acceleration techniques focused on optimizing LLM inference speed and resource
utilization. We also explore LLM-hardware co-design strategies with a vision to
improve system efficiency by tailoring hardware architectures to LLM
requirements. Further, we delve into LLM-to-accelerator compilation approaches,
which involve customizing hardware accelerators for efficient LLM deployment.
Finally, as a case study to leverage LLMs for assisting circuit design, we
examine LLM-aided design methodologies for an important task: High-Level
Synthesis (HLS) functional verification, by creating a new dataset that
contains a large number of buggy and bug-free codes, which can be essential for
training LLMs to specialize on HLS verification and debugging. For each aspect
mentioned above, we begin with a detailed background study, followed by the
presentation of several novel solutions proposed to overcome specific
challenges. We then outline future research directions to drive further
advancements. Through these efforts, we aim to pave the way for more efficient
and scalable deployment of LLMs across a diverse range of applications.",2024-06-16T11:56:50Z
,http://arxiv.org/pdf/2406.09671v1.pdf,"Evaluating ChatGPT-4 Vision on Brazil's National Undergraduate Computer
  Science Exam","The recent integration of visual capabilities into Large Language Models
(LLMs) has the potential to play a pivotal role in science and technology
education, where visual elements such as diagrams, charts, and tables are
commonly used to improve the learning experience. This study investigates the
performance of ChatGPT-4 Vision, OpenAI's most advanced visual model at the
time the study was conducted, on the Bachelor in Computer Science section of
Brazil's 2021 National Undergraduate Exam (ENADE). By presenting the model with
the exam's open and multiple-choice questions in their original image format
and allowing for reassessment in response to differing answer keys, we were
able to evaluate the model's reasoning and self-reflecting capabilities in a
large-scale academic assessment involving textual and visual content. ChatGPT-4
Vision significantly outperformed the average exam participant, positioning
itself within the top 10 best score percentile. While it excelled in questions
that incorporated visual elements, it also encountered challenges with question
interpretation, logical reasoning, and visual acuity. The involvement of an
independent expert panel to review cases of disagreement between the model and
the answer key revealed some poorly constructed questions containing vague or
ambiguous statements, calling attention to the critical need for improved
question design in future exams. Our findings suggest that while ChatGPT-4
Vision shows promise in multimodal academic evaluations, human oversight
remains crucial for verifying the model's accuracy and ensuring the fairness of
high-stakes educational exams. The paper's research materials are publicly
available at https://github.com/nabormendonca/gpt-4v-enade-cs-2021.",2024-06-14T02:42:30Z
,http://arxiv.org/pdf/2308.15452v6.pdf,When Do Program-of-Thoughts Work for Reasoning?,"In the realm of embodied artificial intelligence, the reasoning capabilities
of Large Language Models (LLMs) play a pivotal role. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.",2023-08-29T17:22:39Z
,http://arxiv.org/pdf/2406.07572v1.pdf,"Domain-specific ReAct for physics-integrated iterative modeling: A case
  study of LLM agents for gas path analysis of gas turbines","This study explores the application of large language models (LLMs) with
callable tools in energy and power engineering domain, focusing on gas path
analysis of gas turbines. We developed a dual-agent tool-calling process to
integrate expert knowledge, predefined tools, and LLM reasoning. We evaluated
various LLMs, including LLama3, Qwen1.5 and GPT. Smaller models struggled with
tool usage and parameter extraction, while larger models demonstrated favorable
capabilities. All models faced challenges with complex, multi-component
problems. Based on the test results, we infer that LLMs with nearly 100 billion
parameters could meet professional scenario requirements with fine-tuning and
advanced prompt design. Continued development are likely to enhance their
accuracy and effectiveness, paving the way for more robust AI-driven solutions.",2024-06-01T13:35:18Z
,http://arxiv.org/pdf/2406.04693v1.pdf,LLM-Vectorizer: LLM-based Verified Loop Vectorizer,"Vectorization is a powerful optimization technique that significantly boosts
the performance of high performance computing applications operating on large
data arrays. Despite decades of research on auto-vectorization, compilers
frequently miss opportunities to vectorize code. On the other hand, writing
vectorized code manually using compiler intrinsics is still a complex,
error-prone task that demands deep knowledge of specific architecture and
compilers.
  In this paper, we evaluate the potential of large-language models (LLMs) to
generate vectorized (Single Instruction Multiple Data) code from scalar
programs that process individual array elements. We propose a novel
finite-state machine multi-agents based approach that harnesses LLMs and
test-based feedback to generate vectorized code. Our findings indicate that
LLMs are capable of producing high performance vectorized code with run-time
speedup ranging from 1.1x to 9.4x as compared to the state-of-the-art compilers
such as Intel Compiler, GCC, and Clang.
  To verify the correctness of vectorized code, we use Alive2, a leading
bounded translation validation tool for LLVM IR. We describe a few
domain-specific techniques to improve the scalability of Alive2 on our
benchmark dataset. Overall, our approach is able to verify 38.2% of
vectorizations as correct on the TSVC benchmark dataset.",2024-06-07T07:04:26Z
,http://arxiv.org/pdf/2405.01202v1.pdf,"DLAP: A Deep Learning Augmented Large Language Model Prompting Framework
  for Software Vulnerability Detection","Software vulnerability detection is generally supported by automated static
analysis tools, which have recently been reinforced by deep learning (DL)
models. However, despite the superior performance of DL-based approaches over
rule-based ones in research, applying DL approaches to software vulnerability
detection in practice remains a challenge due to the complex structure of
source code, the black-box nature of DL, and the domain knowledge required to
understand and validate the black-box results for addressing tasks after
detection. Conventional DL models are trained by specific projects and, hence,
excel in identifying vulnerabilities in these projects but not in others. These
models with poor performance in vulnerability detection would impact the
downstream tasks such as location and repair. More importantly, these models do
not provide explanations for developers to comprehend detection results. In
contrast, Large Language Models (LLMs) have made lots of progress in addressing
these issues by leveraging prompting techniques. Unfortunately, their
performance in identifying vulnerabilities is unsatisfactory. This paper
contributes \textbf{\DLAP}, a \underline{\textbf{D}}eep
\underline{\textbf{L}}earning \underline{\textbf{A}}ugmented LLMs
\underline{\textbf{P}}rompting framework that combines the best of both DL
models and LLMs to achieve exceptional vulnerability detection performance.
Experimental evaluation results confirm that \DLAP outperforms state-of-the-art
prompting frameworks, including role-based prompts, auxiliary information
prompts, chain-of-thought prompts, and in-context learning prompts, as well as
fine-turning on multiple metrics.",2024-05-02T11:44:52Z
,http://arxiv.org/pdf/2308.09890v1.pdf,"Inductive-bias Learning: Generating Code Models with Large Language
  Model","Large Language Models(LLMs) have been attracting attention due to a ability
called in-context learning(ICL). ICL, without updating the parameters of a LLM,
it is possible to achieve highly accurate inference based on rules ``in the
context'' by merely inputting a training data into the prompt. Although ICL is
a developing field with many unanswered questions, LLMs themselves serves as a
inference model, seemingly realizing inference without explicitly indicate
``inductive bias''. On the other hand, a code generation is also a highlighted
application of LLMs. The accuracy of code generation has dramatically improved,
enabling even non-engineers to generate code to perform the desired tasks by
crafting appropriate prompts. In this paper, we propose a novel ``learning''
method called an ``Inductive-Bias Learning (IBL)'', which combines the
techniques of ICL and code generation. An idea of IBL is straightforward. Like
ICL, IBL inputs a training data into the prompt and outputs a code with a
necessary structure for inference (we referred to as ``Code Model'') from a
``contextual understanding''. Despite being a seemingly simple approach, IBL
encompasses both a ``property of inference without explicit inductive bias''
inherent in ICL and a ``readability and explainability'' of the code
generation. Surprisingly, generated Code Models have been found to achieve
predictive accuracy comparable to, and in some cases surpassing, ICL and
representative machine learning models. Our IBL code is open source:
https://github.com/fuyu-quant/IBLM",2023-08-19T03:01:45Z
,http://arxiv.org/pdf/2308.10410v4.pdf,"Large Language Models on Wikipedia-Style Survey Generation: an
  Evaluation in NLP Concepts","Educational materials such as survey articles in specialized fields like
computer science traditionally require tremendous expert inputs and are
therefore expensive to create and update. Recently, Large Language Models
(LLMs) have achieved significant success across various general tasks. However,
their effectiveness and limitations in the education domain are yet to be fully
explored. In this work, we examine the proficiency of LLMs in generating
succinct survey articles specific to the niche field of NLP in computer
science, focusing on a curated list of 99 topics. Automated benchmarks reveal
that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by
margins ranging from 2% to 20% in comparison to the established ground truth.
We compare both human and GPT-based evaluation scores and provide in-depth
analysis. While our findings suggest that GPT-created surveys are more
contemporary and accessible than human-authored ones, certain limitations were
observed. Notably, GPT-4, despite often delivering outstanding content,
occasionally exhibited lapses like missing details or factual errors. At last,
we compared the rating behavior between humans and GPT-4 and found systematic
bias in using GPT evaluation.",2023-08-21T01:32:45Z
,http://arxiv.org/pdf/2312.11567v1.pdf,"Students' Perceptions and Preferences of Generative Artificial
  Intelligence Feedback for Programming","The rapid evolution of artificial intelligence (AI), specifically large
language models (LLMs), has opened opportunities for various educational
applications. This paper explored the feasibility of utilizing ChatGPT, one of
the most popular LLMs, for automating feedback for Java programming assignments
in an introductory computer science (CS1) class. Specifically, this study
focused on three questions: 1) To what extent do students view LLM-generated
feedback as formative? 2) How do students see the comparative affordances of
feedback prompts that include their code, vs. those that exclude it? 3) What
enhancements do students suggest for improving AI-generated feedback? To
address these questions, we generated automated feedback using the ChatGPT API
for four lab assignments in the CS1 class. The survey results revealed that
students perceived the feedback as aligning well with formative feedback
guidelines established by Shute. Additionally, students showed a clear
preference for feedback generated by including the students' code as part of
the LLM prompt, and our thematic study indicated that the preference was mainly
attributed to the specificity, clarity, and corrective nature of the feedback.
Moreover, this study found that students generally expected specific and
corrective feedback with sufficient code examples, but had diverged opinions on
the tone of the feedback. This study demonstrated that ChatGPT could generate
Java programming assignment feedback that students perceived as formative. It
also offered insights into the specific improvements that would make the
ChatGPT-generated feedback useful for students.",2023-12-17T22:26:53Z
,http://arxiv.org/pdf/2311.07948v1.pdf,Finding Inductive Loop Invariants using Large Language Models,"Loop invariants are fundamental to reasoning about programs with loops. They
establish properties about a given loop's behavior. When they additionally are
inductive, they become useful for the task of formal verification that seeks to
establish strong mathematical guarantees about program's runtime behavior. The
inductiveness ensures that the invariants can be checked locally without
consulting the entire program, thus are indispensable artifacts in a formal
proof of correctness. Finding inductive loop invariants is an undecidable
problem, and despite a long history of research towards practical solutions, it
remains far from a solved problem. This paper investigates the capabilities of
the Large Language Models (LLMs) in offering a new solution towards this old,
yet important problem. To that end, we first curate a dataset of verification
problems on programs with loops. Next, we design a prompt for exploiting LLMs,
obtaining inductive loop invariants, that are checked for correctness using
sound symbolic tools. Finally, we explore the effectiveness of using an
efficient combination of a symbolic tool and an LLM on our dataset and compare
it against a purely symbolic baseline. Our results demonstrate that LLMs can
help improve the state-of-the-art in automated program verification.",2023-11-14T06:58:09Z
,http://arxiv.org/pdf/2305.14129v3.pdf,GrACE: Generation using Associated Code Edits,"Developers expend a significant amount of time in editing code for a variety
of reasons such as bug fixing or adding new features. Designing effective
methods to predict code edits has been an active yet challenging area of
research due to the diversity of code edits and the difficulty of capturing the
developer intent. In this work, we address these challenges by endowing
pre-trained large language models (LLMs) of code with the knowledge of prior,
relevant edits. The generative capability of the LLMs helps address the
diversity in code changes and conditioning code generation on prior edits helps
capture the latent developer intent. We evaluate two well-known LLMs, Codex and
CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments
with two datasets, the knowledge of prior edits boosts the performance of the
LLMs significantly and enables them to generate 29% and 54% more correctly
edited code in top-1 suggestions relative to the current state-of-the-art
symbolic and neural approaches, respectively.",2023-05-23T14:55:44Z
,http://arxiv.org/pdf/2404.00725v1.pdf,"The Larger the Better? Improved LLM Code-Generation via Budget
  Reallocation","It is a common belief that large language models (LLMs) are better than
smaller-sized ones. However, larger models also require significantly more time
and compute during inference. This begs the question: what happens when both
models operate under the same budget? (e.g., compute, run-time). To address
this question, we analyze code generation LLMs of various sizes and make
comparisons such as running a 70B model once vs. generating five outputs from a
13B model and selecting one. Our findings reveal that, in a standard unit-test
setup, the repeated use of smaller models can yield consistent improvements,
with gains of up to 15% across five tasks. On the other hand, in scenarios
where unit-tests are unavailable, a ranking-based selection of candidates from
the smaller model falls short of the performance of a single output from larger
ones. Our results highlight the potential of using smaller models instead of
larger ones, and the importance of studying approaches for ranking LLM outputs.",2024-03-31T15:55:49Z
,http://arxiv.org/pdf/2406.05940v1.pdf,M2CVD: Multi-Model Collaboration for Code Vulnerability Detection,"Large Language Models (LLMs) have strong capabilities in code comprehension,
but fine-tuning costs and semantic alignment issues limit their
project-specific optimization; conversely, code models such CodeBERT are easy
to fine-tune, but it is often difficult to learn vulnerability semantics from
complex code languages. To address these challenges, this paper introduces the
Multi-Model Collaborative Vulnerability Detection approach (M2CVD) that
leverages the strong capability of analyzing vulnerability semantics from LLMs
to improve the detection accuracy of code models. M2CVD employs a novel
collaborative process: first enhancing the quality of vulnerability semantic
description produced by LLMs through the understanding of project code by code
models, and then using these improved vulnerability semantic description to
boost the detection accuracy of code models. We demonstrated M2CVD's
effectiveness on two real-world datasets, where M2CVD significantly
outperformed the baseline. In addition, we demonstrate that the M2CVD
collaborative method can extend to other different LLMs and code models to
improve their accuracy in vulnerability detection tasks.",2024-06-10T00:05:49Z
,http://arxiv.org/pdf/2406.12334v1.pdf,"What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to
  Prompt Engineering","Large Language Models (LLMs) changed the way we design and interact with
software systems. Their ability to process and extract information from text
has drastically improved productivity in a number of routine tasks. Developers
that want to include these models in their software stack, however, face a
dreadful challenge: debugging their inconsistent behavior across minor
variations of the prompt. We therefore introduce two metrics for classification
tasks, namely sensitivity and consistency, which are complementary to task
performance. First, sensitivity measures changes of predictions across
rephrasings of the prompt, and does not require access to ground truth labels.
Instead, consistency measures how predictions vary across rephrasings for
elements of the same class. We perform an empirical comparison of these metrics
on text classification tasks, using them as guideline for understanding failure
modes of the LLM. Our hope is that sensitivity and consistency will be powerful
allies in automatic prompt engineering frameworks to obtain LLMs that balance
robustness with performance.",2024-06-18T06:59:24Z
,http://arxiv.org/pdf/2311.05943v1.pdf,Prompt Problems: A New Programming Exercise for the Generative AI Era,"Large Language Models (LLMs) are revolutionizing the field of computing
education with their powerful code-generating capabilities. Traditional
pedagogical practices have focused on code writing tasks, but there is now a
shift in importance towards code reading, comprehension and evaluation of
LLM-generated code. Alongside this shift, an important new skill is emerging --
the ability to solve programming tasks by constructing good prompts for
code-generating models. In this work we introduce a new type of programming
exercise to hone this nascent skill: 'Prompt Problems'. Prompt Problems are
designed to help students learn how to write effective prompts for AI code
generators. A student solves a Prompt Problem by crafting a natural language
prompt which, when provided as input to an LLM, outputs code that successfully
solves a specified programming task. We also present a new web-based tool
called Promptly which hosts a repository of Prompt Problems and supports the
automated evaluation of prompt-generated code. We deploy Promptly for the first
time in one CS1 and one CS2 course and describe our experiences, which include
student perceptions of this new type of activity and their interactions with
the tool. We find that students are enthusiastic about Prompt Problems, and
appreciate how the problems engage their computational thinking skills and
expose them to new programming constructs. We discuss ideas for the future
development of new variations of Prompt Problems, and the need to carefully
study their integration into classroom practice.",2023-11-10T09:01:34Z
,http://arxiv.org/pdf/2303.04864v1.pdf,"nl2spec: Interactively Translating Unstructured Natural Language to
  Temporal Logics with Large Language Models","A rigorous formalization of desired system requirements is indispensable when
performing any verification task. This often limits the application of
verification techniques, as writing formal specifications is an error-prone and
time-consuming manual task. To facilitate this, we present nl2spec, a framework
for applying Large Language Models (LLMs) to derive formal specifications (in
temporal logics) from unstructured natural language. In particular, we
introduce a new methodology to detect and resolve the inherent ambiguity of
system requirements in natural language: we utilize LLMs to map subformulas of
the formalization back to the corresponding natural language fragments of the
input. Users iteratively add, delete, and edit these sub-translations to amend
erroneous formalizations, which is easier than manually redrafting the entire
formalization. The framework is agnostic to specific application domains and
can be extended to similar specification languages and new neural models. We
perform a user study to obtain a challenging dataset, which we use to run
experiments on the quality of translations. We provide an open-source
implementation, including a web-based frontend.",2023-03-08T20:08:53Z
,http://arxiv.org/pdf/2312.13264v1.pdf,"dIR -- Discrete Information Retrieval: Conversational Search over
  Unstructured (and Structured) Data with Large Language Models","Data is stored in both structured and unstructured form. Querying both, to
power natural language conversations, is a challenge. This paper introduces
dIR, Discrete Information Retrieval, providing a unified interface to query
both free text and structured knowledge. Specifically, a Large Language Model
(LLM) transforms text into expressive representation. After the text is
extracted into columnar form, it can then be queried via a text-to-SQL Semantic
Parser, with an LLM converting natural language into SQL. Where desired, such
conversation may be effected by a multi-step reasoning conversational agent. We
validate our approach via a proprietary question/answer data set, concluding
that dIR makes a whole new class of queries on free text possible when compared
to traditionally fine-tuned dense-embedding-model-based Information Retrieval
(IR) and SQL-based Knowledge Bases (KB). For sufficiently complex queries, dIR
can succeed where no other method stands a chance.",2023-12-20T18:41:44Z
,http://arxiv.org/pdf/2306.11981v1.pdf,"A Chain of AI-based Solutions for Resolving FQNs and Fixing Syntax
  Errors in Partial Code","API documentation, technical blogs and programming Q&A sites contain numerous
partial code that can be reused in programming tasks, but often these code are
uncompilable due to unresolved names and syntax errors. To facilitate partial
code reuse, we propose the Partial Code Reuse Chain (PCR-Chain) for resolving
fully-qualified names (FQNs) and fixing last-mile syntax errors in partial code
based on a giant large language model (LLM) like ChatGPT. Methodologically,
PCR-Chain is backed up by the underlying global-level prompt architecture
(which combines three design ideas: hierarchical task breakdown, prompt
composition, and a mix of prompt-based AI and non-AI units) and the local-level
prompt design. Technically, we propose PCR-Chain, which employs in-context
learning rather than symbolic, costly training methods. Experimental results
demonstrate that in dynamically-typed languages (Python), PCR-Chain outperforms
current state-of-the-art (SOTA) 5% accuracy like RING. For statically-type
languages (Java), our approach achieves high accuracy of 80.5% in resolving
both non-FQNs and last-mile syntax errors, surpassing SOTA methods (RING) that
can only address last-mile syntax errors. The correct execution of the unit,
module, and PCR-Chain demonstrates the effectiveness of the prompt design,
composition, and architecture and opens up possibilities for building software
engineering tools based on LLMs, replacing traditional program analysis
methods.",2023-06-21T02:13:32Z
,http://arxiv.org/pdf/2405.11361v1.pdf,"An Opportunistically Parallel Lambda Calculus for Performant Composition
  of Large Language Models","Large language models (LLMs) have shown impressive results at a wide-range of
tasks. However, they have limitations, such as hallucinating facts and
struggling with arithmetic. Recent work has addressed these issues with
sophisticated decoding techniques. However, performant decoding, particularly
for sophisticated techniques, relies crucially on parallelization and batching,
which are difficult for developers.
  We make two observations: 1) existing approaches are high-level
domain-specific languages for gluing expensive black-box calls, but are not
general or compositional; 2) LLM programs are essentially pure (all effects
commute). Guided by these observations, we develop a novel, general-purpose
lambda calculus for automatically parallelizing a wide-range of LLM
interactions, without user intervention. The key difference versus standard
lambda calculus is a novel ""opportunistic"" evaluation strategy, which steps
independent parts of a program in parallel, dispatching black-box external
calls as eagerly as possible, even while data-independent parts of the program
are waiting for their own external calls to return. To maintain the simplicity
of the language and to ensure uniformity of opportunistic evaluation,
control-flow and looping constructs are implemented in-language, via Church
encodings.
  We implement this approach in a framework called EPIC, embedded in--and
interoperating closely with--Python. We demonstrate its versatility and
performance with three case studies drawn from the machine learning literature:
Tree-of-Thoughts (LLMs embedded in classic search procedures), nested tool use,
and constrained decoding. Our experiments show that opportunistic evaluation
offers a $1.5\times$ to $4.8\times$ speedup over sequential evaluation, while
still allowing practitioners to write straightforward and composable programs,
without any manual parallelism or batching.",2024-05-18T18:13:31Z
,http://arxiv.org/pdf/2405.00218v2.pdf,Constrained Decoding for Secure Code Generation,"Code Large Language Models (Code LLMs) have been increasingly used by
developers to boost productivity, but they often generate vulnerable code.
Thus, there is an urgent need to ensure that code generated by Code LLMs is
correct and secure. Previous research has primarily focused on generating
secure code, overlooking the fact that secure code also needs to be correct.
This oversight can lead to a false sense of security. Currently, the community
lacks a method to measure actual progress in this area, and we need solutions
that address both security and correctness of code generation.
  This paper introduces a new benchmark, CodeGuard+, along with two new
metrics, to measure Code LLMs' ability to generate both secure and correct
code. Using our new evaluation methods, we show that the state-of-the-art
defense technique, prefix tuning, may not be as strong as previously believed,
since it generates secure code but sacrifices functional correctness. We also
demonstrate that different decoding methods significantly affect the security
of Code LLMs.
  Furthermore, we explore a new defense direction: constrained decoding for
secure code generation. We propose new constrained decoding techniques to
generate secure code. Our results reveal that constrained decoding is more
effective than prefix tuning to improve the security of Code LLMs, without
requiring a specialized training dataset. Moreover, our evaluations over eight
state-of-the-art Code LLMs show that constrained decoding has strong
performance to improve the security of Code LLMs, and our technique outperforms
GPT-4.",2024-04-30T21:52:19Z
,http://arxiv.org/pdf/2211.01910v2.pdf,Large Language Models Are Human-Level Prompt Engineers,"By conditioning on natural language instructions, large language models
(LLMs) have displayed impressive capabilities as general-purpose computers.
However, task performance depends significantly on the quality of the prompt
used to steer the model, and most effective prompts have been handcrafted by
humans. Inspired by classical program synthesis and the human approach to
prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic
instruction generation and selection. In our method, we treat the instruction
as the ""program,"" optimized by searching over a pool of instruction candidates
proposed by an LLM in order to maximize a chosen score function. To evaluate
the quality of the selected instruction, we evaluate the zero-shot performance
of another LLM following the selected instruction. Experiments on 24 NLP tasks
show that our automatically generated instructions outperform the prior LLM
baseline by a large margin and achieve better or comparable performance to the
instructions generated by human annotators on 19/24 tasks. We conduct extensive
qualitative and quantitative analyses to explore the performance of APE. We
show that APE-engineered prompts can be applied to steer models toward
truthfulness and/or informativeness, as well as to improve few-shot learning
performance by simply prepending them to standard in-context learning prompts.
Please check out our webpage at
https://sites.google.com/view/automatic-prompt-engineer.",2022-11-03T15:43:03Z
,http://arxiv.org/pdf/2310.15780v1.pdf,"Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI
  Testing via Functionality-aware Decisions","Automated Graphical User Interface (GUI) testing plays a crucial role in
ensuring app quality, especially as mobile applications have become an integral
part of our daily lives. Despite the growing popularity of learning-based
techniques in automated GUI testing due to their ability to generate human-like
interactions, they still suffer from several limitations, such as low testing
coverage, inadequate generalization capabilities, and heavy reliance on
training data. Inspired by the success of Large Language Models (LLMs) like
ChatGPT in natural language understanding and question answering, we formulate
the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM
to chat with the mobile apps by passing the GUI page information to LLM to
elicit testing scripts, and executing them to keep passing the app feedback to
LLM, iterating the whole process. Within this framework, we have also
introduced a functionality-aware memory prompting mechanism that equips the LLM
with the ability to retain testing knowledge of the whole process and conduct
long-term, functionality-based reasoning to guide exploration. We evaluate it
on 93 apps from Google Play and demonstrate that it outperforms the best
baseline by 32% in activity coverage, and detects 31% more bugs at a faster
rate. Moreover, GPTDroid identify 53 new bugs on Google Play, of which 35 have
been confirmed and fixed.",2023-10-24T12:30:26Z
,http://arxiv.org/pdf/2403.04790v1.pdf,Online Training of Large Language Models: Learn while chatting,"Large Language Models(LLMs) have dramatically revolutionized the field of
Natural Language Processing(NLP), offering remarkable capabilities that have
garnered widespread usage. However, existing interaction paradigms between LLMs
and users are constrained by either inflexibility, limitations in
customization, or a lack of persistent learning. This inflexibility is
particularly evident as users, especially those without programming skills,
have restricted avenues to enhance or personalize the model. Existing
frameworks further complicate the model training and deployment process due to
their computational inefficiencies and lack of user-friendly interfaces. To
overcome these challenges, this paper introduces a novel interaction
paradigm-'Online Training using External Interactions'-that merges the benefits
of persistent, real-time model updates with the flexibility for individual
customization through external interactions such as AI agents or online/offline
knowledge bases.",2024-03-04T10:00:55Z
,http://arxiv.org/pdf/2308.06261v1.pdf,"Enhancing Network Management Using Code Generated by Large Language
  Models","Analyzing network topologies and communication graphs plays a crucial role in
contemporary network management. However, the absence of a cohesive approach
leads to a challenging learning curve, heightened errors, and inefficiencies.
In this paper, we introduce a novel approach to facilitate a
natural-language-based network management experience, utilizing large language
models (LLMs) to generate task-specific code from natural language queries.
This method tackles the challenges of explainability, scalability, and privacy
by allowing network operators to inspect the generated code, eliminating the
need to share network data with LLMs, and concentrating on application-specific
requests combined with general program synthesis techniques. We design and
evaluate a prototype system using benchmark applications, showcasing high
accuracy, cost-effectiveness, and the potential for further enhancements using
complementary program synthesis techniques.",2023-08-11T17:49:15Z
10.1145/3545945.3569770,http://arxiv.org/pdf/2210.11630v1.pdf,Using Large Language Models to Enhance Programming Error Messages,"A key part of learning to program is learning to understand programming error
messages. They can be hard to interpret and identifying the cause of errors can
be time-consuming. One factor in this challenge is that the messages are
typically intended for an audience that already knows how to program, or even
for programming environments that then use the information to highlight areas
in code. Researchers have been working on making these errors more novice
friendly since the 1960s, however progress has been slow. The present work
contributes to this stream of research by using large language models to
enhance programming error messages with explanations of the errors and
suggestions on how to fix the error. Large language models can be used to
create useful and novice-friendly enhancements to programming error messages
that sometimes surpass the original programming error messages in
interpretability and actionability. These results provide further evidence of
the benefits of large language models for computing educators, highlighting
their use in areas known to be challenging for students. We further discuss the
benefits and downsides of large language models and highlight future streams of
research for enhancing programming error messages.",2022-10-20T23:17:26Z
,http://arxiv.org/pdf/2404.12833v1.pdf,How Far Can We Go with Practical Function-Level Program Repair?,"Recently, multiple Automated Program Repair (APR) techniques based on Large
Language Models (LLMs) have been proposed to enhance the repair performance.
While these techniques mainly focus on the single-line or hunk-level repair,
they face significant challenges in real-world application due to the limited
repair task scope and costly statement-level fault localization. However, the
more practical function-level APR, which broadens the scope of APR task to fix
entire buggy functions and requires only cost-efficient function-level fault
localization, remains underexplored. In this paper, we conduct the first
comprehensive study of LLM-based function-level APR including investigating the
effect of the few-shot learning mechanism and the auxiliary repair-relevant
information. Specifically, we adopt six widely-studied LLMs and construct a
benchmark in both the Defects4J 1.2 and 2.0 datasets. Our study demonstrates
that LLMs with zero-shot learning are already powerful function-level APR
techniques, while applying the few-shot learning mechanism leads to disparate
repair performance. Moreover, we find that directly applying the auxiliary
repair-relevant information to LLMs significantly increases function-level
repair performance. Inspired by our findings, we propose an LLM-based
function-level APR technique, namely SRepair, which adopts a dual-LLM framework
to leverage the power of the auxiliary repair-relevant information for
advancing the repair performance. The evaluation results demonstrate that
SRepair can correctly fix 300 single-function bugs in the Defects4J dataset,
largely surpassing all previous APR techniques by at least 85%, without the
need for the costly statement-level fault location information. Furthermore,
SRepair successfully fixes 32 multi-function bugs in the Defects4J dataset,
which is the first time achieved by any APR technique ever to our best
knowledge.",2024-04-19T12:14:09Z
,http://arxiv.org/pdf/2403.15274v2.pdf,Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review,"The year 2023 marked a significant surge in the exploration of applying large
language model (LLM) chatbots, notably ChatGPT, across various disciplines. We
surveyed the applications of ChatGPT in bioinformatics and biomedical
informatics throughout the year, covering omics, genetics, biomedical text
mining, drug discovery, biomedical image understanding, bioinformatics
programming, and bioinformatics education. Our survey delineates the current
strengths and limitations of this chatbot in bioinformatics and offers insights
into potential avenues for future developments.",2024-03-22T15:16:23Z
,http://arxiv.org/pdf/2405.04760v2.pdf,Large Language Models for Cyber Security: A Systematic Literature Review,"The rapid advancement of Large Language Models (LLMs) has opened up new
opportunities for leveraging artificial intelligence in various domains,
including cybersecurity. As the volume and sophistication of cyber threats
continue to grow, there is an increasing need for intelligent systems that can
automatically detect vulnerabilities, analyze malware, and respond to attacks.
In this survey, we conduct a comprehensive review of the literature on the
application of LLMs in cybersecurity (LLM4Security). By comprehensively
collecting over 30K relevant papers and systematically analyzing 127 papers
from top security and software engineering venues, we aim to provide a holistic
view of how LLMs are being used to solve diverse problems across the
cybersecurity domain. Through our analysis, we identify several key findings.
First, we observe that LLMs are being applied to a wide range of cybersecurity
tasks, including vulnerability detection, malware analysis, network intrusion
detection, and phishing detection. Second, we find that the datasets used for
training and evaluating LLMs in these tasks are often limited in size and
diversity, highlighting the need for more comprehensive and representative
datasets. Third, we identify several promising techniques for adapting LLMs to
specific cybersecurity domains, such as fine-tuning, transfer learning, and
domain-specific pre-training. Finally, we discuss the main challenges and
opportunities for future research in LLM4Security, including the need for more
interpretable and explainable models, the importance of addressing data privacy
and security concerns, and the potential for leveraging LLMs for proactive
defense and threat hunting. Overall, our survey provides a comprehensive
overview of the current state-of-the-art in LLM4Security and identifies several
promising directions for future research.",2024-05-08T02:09:17Z
,http://arxiv.org/pdf/2402.10754v1.pdf,When Dataflow Analysis Meets Large Language Models,"Dataflow analysis is a powerful code analysis technique that reasons
dependencies between program values, offering support for code optimization,
program comprehension, and bug detection. Existing approaches require the
successful compilation of the subject program and customizations for downstream
applications. This paper introduces LLMDFA, an LLM-powered dataflow analysis
framework that analyzes arbitrary code snippets without requiring a compilation
infrastructure and automatically synthesizes downstream applications. Inspired
by summary-based dataflow analysis, LLMDFA decomposes the problem into three
sub-problems, which are effectively resolved by several essential strategies,
including few-shot chain-of-thought prompting and tool synthesis. Our
evaluation has shown that the design can mitigate the hallucination and improve
the reasoning ability, obtaining high precision and recall in detecting
dataflow-related bugs upon benchmark programs, outperforming state-of-the-art
(classic) tools, including a very recent industrial analyzer.",2024-02-16T15:21:35Z
,http://arxiv.org/pdf/2309.00363v1.pdf,"FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large
  Language Models in Federated Learning","LLMs have demonstrated great capabilities in various NLP tasks. Different
entities can further improve the performance of those LLMs on their specific
downstream tasks by fine-tuning LLMs. When several entities have similar
interested tasks, but their data cannot be shared because of privacy concerns
regulations, federated learning (FL) is a mainstream solution to leverage the
data of different entities. However, fine-tuning LLMs in federated learning
settings still lacks adequate support from existing FL frameworks because it
has to deal with optimizing the consumption of significant communication and
computational resources, data preparation for different tasks, and distinct
information protection demands. This paper first discusses these challenges of
federated fine-tuning LLMs, and introduces our package FS-LLM as a main
contribution, which consists of the following components: (1) we build an
end-to-end benchmarking pipeline, automizing the processes of dataset
preprocessing, federated fine-tuning execution, and performance evaluation on
federated LLM fine-tuning; (2) we provide comprehensive federated
parameter-efficient fine-tuning algorithm implementations and versatile
programming interfaces for future extension in FL scenarios with low
communication and computation costs, even without accessing the full model; (3)
we adopt several accelerating and resource-efficient operators for fine-tuning
LLMs with limited resources and the flexible pluggable sub-routines for
interdisciplinary study. We conduct extensive experiments to validate the
effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art
parameter-efficient fine-tuning algorithms in FL settings, which also yields
valuable insights into federated fine-tuning LLMs for the research community.
To facilitate further research and adoption, we release FS-LLM at
https://github.com/alibaba/FederatedScope/tree/llm.",2023-09-01T09:40:36Z
,http://arxiv.org/pdf/2303.04142v1.pdf,From Copilot to Pilot: Towards AI Supported Software Development,"AI-supported programming has arrived, as shown by the introduction and
successes of large language models for code, such as Copilot/Codex
(Github/OpenAI) and AlphaCode (DeepMind). Above human average performance on
programming challenges is now possible. However, software engineering is much
more than solving programming contests. Moving beyond code completion to
AI-supported software engineering will require an AI system that can, among
other things, understand how to avoid code smells, to follow language idioms,
and eventually (maybe!) propose rational software designs. In this study, we
explore the current limitations of AI-supported code completion tools like
Copilot and offer a simple taxonomy for understanding the classification of
AI-supported code completion tools in this space. We first perform an
exploratory study on Copilot's code suggestions for language idioms and code
smells. Copilot does not follow language idioms and avoid code smells in most
of our test scenarios. We then conduct additional investigation to determine
the current boundaries of AI-supported code completion tools like Copilot by
introducing a taxonomy of software abstraction hierarchies where 'basic
programming functionality' such as code compilation and syntax checking is at
the least abstract level, software architecture analysis and design are at the
most abstract level. We conclude by providing a discussion on challenges for
future development of AI-supported code completion tools to reach the design
level of abstraction in our taxonomy.",2023-03-07T18:56:52Z
10.1145/3650212.3652126,http://arxiv.org/pdf/2312.13064v3.pdf,LPR: Large Language Models-Aided Program Reduction,"Program reduction is a prevalent technique to facilitate compilers' debugging
by automatically minimizing bug-triggering programs. Existing program reduction
techniques are either generic across languages (e.g., Perses and Vulcan) or
specifically customized for one certain language by employing language-specific
features, like C-Reduce. However, striking the balance between generality
across multiple programming languages and specificity to individual languages
in program reduction is yet to be explored. This paper proposes LPR, the first
technique utilizing LLMs to perform language-specific program reduction for
multiple languages. The core insight is to utilize both the language-generic
syntax level program reduction (e.g., Perses) and the language-specific
semantic level program transformations learned by LLMs. Alternately,
language-generic program reducers efficiently reduce programs into
1-tree-minimality, which is small enough to be manageable for LLMs; LLMs
effectively transform programs via the learned semantics to expose new
reduction opportunities for the language-generic program reducers to further
reduce the programs. Our extensive evaluation on 50 benchmarks across three
languages (C, Rust, and JavaScript) has highlighted LPR's practicality and
superiority over Vulcan, the state-of-the-art language-generic program reducer.
For effectiveness, LPR surpasses Vulcan by producing 24.93%, 4.47%, and 11.71%
smaller programs on benchmarks in C, Rust and JavaScript. Moreover, LPR and
Vulcan have demonstrated their potential to complement each other. By using
Vulcan on LPR's output for C programs, we achieve program sizes comparable to
those reduced by C-Reduce. For efficiency, LPR takes 10.77%, 34.88%, 36.96%
less time than Vulcan to finish all benchmarks in C, Rust and JavaScript,
separately.",2023-12-20T14:43:36Z
,http://arxiv.org/pdf/2406.11346v1.pdf,WaDec: Decompile WebAssembly Using Large Language Model,"WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web
development, offering a compact binary format that allows high-performance
applications to run at near-native speeds in web browsers. Despite its
advantages, Wasm's binary nature presents significant challenges for developers
and researchers, particularly regarding readability when debugging or analyzing
web applications. Therefore, effective decompilation becomes crucial.
Unfortunately, traditional decompilers often struggle with producing readable
outputs. While some large language model (LLM)-based decompilers have shown
good compatibility with general binary files, they still face specific
challenges when dealing with Wasm.
  In this paper, we introduce a novel approach, WaDec, which is the first use
of a fine-tuned LLM to interpret and decompile Wasm binary code into a
higher-level, more comprehensible source code representation. The LLM was
meticulously fine-tuned using a specialized dataset of wat-c code snippets,
employing self-supervised learning techniques. This enables WaDec to
effectively decompile not only complete wat functions but also finer-grained
wat code snippets. Our experiments demonstrate that WaDec markedly outperforms
current state-of-the-art tools, offering substantial improvements across
several metrics. It achieves a code inflation rate of only 3.34%, a dramatic
97% reduction compared to the state-of-the-art's 116.94%. Unlike baselines'
output that cannot be directly compiled or executed, WaDec maintains a
recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output
consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art
performance in AST edit distance by 185%, cyclomatic complexity by 8%, and
cosine similarity by 41%, achieving an average code similarity above 50%.",2024-06-17T09:08:30Z
,http://arxiv.org/pdf/2401.16467v2.pdf,ReGAL: Refactoring Programs to Discover Generalizable Abstractions,"While large language models (LLMs) are increasingly being used for program
synthesis, they lack the global view needed to develop useful abstractions;
they generally predict programs one at a time, often repeating the same
functionality. Generating redundant code from scratch is both inefficient and
error-prone. To address this, we propose Refactoring for Generalizable
Abstraction Learning (ReGAL), a gradient-free method for learning a library of
reusable functions via code refactorization, i.e., restructuring code without
changing its execution output. ReGAL learns from a small set of existing
programs, iteratively verifying and refining its abstractions via execution. We
find that the shared function libraries discovered by ReGAL make programs
easier to predict across diverse domains. On five datasets -- LOGO graphics
generation, Date reasoning, TextCraft (a Minecraft-based text-game) MATH, and
TabMWP -- both open-source and proprietary LLMs improve in accuracy when
predicting programs with ReGAL functions. For CodeLlama-13B, ReGAL results in
absolute accuracy increases of 11.5% on LOGO, 26.1% on date understanding, and
8.1% on TextCraft, outperforming GPT-3.5 in two of three domains. Our analysis
reveals ReGAL's abstractions encapsulate frequently-used subroutines as well as
environment dynamics.",2024-01-29T18:45:30Z
,http://arxiv.org/pdf/2312.06571v1.pdf,"From Text to Motion: Grounding GPT-4 in a Humanoid Robot ""Alter3""","We report the development of Alter3, a humanoid robot capable of generating
spontaneous motion using a Large Language Model (LLM), specifically GPT-4. This
achievement was realized by integrating GPT-4 into our proprietary android,
Alter3, thereby effectively grounding the LLM with Alter's bodily movement.
Typically, low-level robot control is hardware-dependent and falls outside the
scope of LLM corpora, presenting challenges for direct LLM-based robot control.
However, in the case of humanoid robots like Alter3, direct control is feasible
by mapping the linguistic expressions of human actions onto the robot's body
through program code. Remarkably, this approach enables Alter3 to adopt various
poses, such as a 'selfie' stance or 'pretending to be a ghost,' and generate
sequences of actions over time without explicit programming for each body part.
This demonstrates the robot's zero-shot learning capabilities. Additionally,
verbal feedback can adjust poses, obviating the need for fine-tuning. A video
of Alter3's generated motions is available at
https://tnoinkwms.github.io/ALTER-LLM/",2023-12-11T17:57:11Z
,http://arxiv.org/pdf/2305.01157v3.pdf,"Complex Logical Reasoning over Knowledge Graphs using Large Language
  Models","Reasoning over knowledge graphs (KGs) is a challenging task that requires a
deep understanding of the complex relationships between entities and the
underlying logic of their relations. Current approaches rely on learning
geometries to embed entities in vector space for logical query operations, but
they suffer from subpar performance on complex queries and dataset-specific
representations. In this paper, we propose a novel decoupled approach,
Language-guided Abstract Reasoning over Knowledge graphs (LARK), that
formulates complex KG reasoning as a combination of contextual KG search and
logical query reasoning, to leverage the strengths of graph extraction
algorithms and large language models (LLM), respectively. Our experiments
demonstrate that the proposed approach outperforms state-of-the-art KG
reasoning methods on standard benchmark datasets across several logical query
constructs, with significant performance gain for queries of higher complexity.
Furthermore, we show that the performance of our approach improves
proportionally to the increase in size of the underlying LLM, enabling the
integration of the latest advancements in LLMs for logical reasoning over KGs.
Our work presents a new direction for addressing the challenges of complex KG
reasoning and paves the way for future research in this area.",2023-05-02T02:21:49Z
,http://arxiv.org/pdf/2404.04108v1.pdf,"Large language models as oracles for instantiating ontologies with
  domain-specific knowledge","Background. Endowing intelligent systems with semantic data commonly requires
designing and instantiating ontologies with domain-specific knowledge.
Especially in the early phases, those activities are typically performed
manually by human experts possibly leveraging on their own experience. The
resulting process is therefore time-consuming, error-prone, and often biased by
the personal background of the ontology designer. Objective. To mitigate that
issue, we propose a novel domain-independent approach to automatically
instantiate ontologies with domain-specific knowledge, by leveraging on large
language models (LLMs) as oracles. Method. Starting from (i) an initial schema
composed by inter-related classes andproperties and (ii) a set of query
templates, our method queries the LLM multiple times, and generates instances
for both classes and properties from its replies. Thus, the ontology is
automatically filled with domain-specific knowledge, compliant to the initial
schema. As a result, the ontology is quickly and automatically enriched with
manifold instances, which experts may consider to keep, adjust, discard, or
complement according to their own needs and expertise. Contribution. We
formalise our method in general way and instantiate it over various LLMs, as
well as on a concrete case study. We report experiments rooted in the
nutritional domain where an ontology of food meals and their ingredients is
semi-automatically instantiated from scratch, starting from a categorisation of
meals and their relationships. There, we analyse the quality of the generated
ontologies and compare ontologies attained by exploiting different LLMs.
Finally, we provide a SWOT analysis of the proposed method.",2024-04-05T14:04:07Z
,http://arxiv.org/pdf/2401.14936v1.pdf,Reassessing Java Code Readability Models with a Human-Centered Approach,"To ensure that Large Language Models (LLMs) effectively support user
productivity, they need to be adjusted. Existing Code Readability (CR) models
can guide this alignment. However, there are concerns about their relevance in
modern software engineering since they often miss the developers' notion of
readability and rely on outdated code. This research assesses existing Java CR
models for LLM adjustments, measuring the correlation between their and
developers' evaluations of AI-generated Java code. Using the Repertory Grid
Technique with 15 developers, we identified 12 key code aspects influencing CR
that were consequently assessed by 390 programmers when labeling 120
AI-generated snippets. Our findings indicate that when AI generates concise and
executable code, it is often considered readable by CR models and developers.
However, a limited correlation between these evaluations underscores the
importance of future research on learning objectives for adjusting LLMs and on
the aspects influencing CR evaluations included in predictive models.",2024-01-26T15:18:22Z
,http://arxiv.org/pdf/2405.16450v1.pdf,"Synthesizing Programmatic Reinforcement Learning Policies with Large
  Language Model Guided Search","Programmatic reinforcement learning (PRL) has been explored for representing
policies through programs as a means to achieve interpretability and
generalization. Despite promising outcomes, current state-of-the-art PRL
methods are hindered by sample inefficiency, necessitating tens of millions of
program-environment interactions. To tackle this challenge, we introduce a
novel LLM-guided search framework (LLM-GS). Our key insight is to leverage the
programming expertise and common sense reasoning of LLMs to enhance the
efficiency of assumption-free, random-guessing search methods. We address the
challenge of LLMs' inability to generate precise and grammatically correct
programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL
strategy - an LLM is instructed to initially generate Python codes and then
convert them into DSL programs. To further optimize the LLM-generated programs,
we develop a search algorithm named Scheduled Hill Climbing, designed to
efficiently explore the programmatic search space to consistently improve the
programs. Experimental results in the Karel domain demonstrate the superior
effectiveness and efficiency of our LLM-GS framework. Extensive ablation
studies further verify the critical role of our Pythonic-DSL strategy and
Scheduled Hill Climbing algorithm.",2024-05-26T06:33:48Z
,http://arxiv.org/pdf/2306.17820v4.pdf,"Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language
  Models","Neural-symbolic methods have demonstrated efficiency in enhancing the
reasoning abilities of large language models (LLMs). However, existing methods
mainly rely on syntactically mapping natural languages to complete formal
languages like Python and SQL. Those methods require that reasoning tasks be
convertible into programs, which cater to the computer execution mindset and
deviate from human reasoning habits. To broaden symbolic methods' applicability
and adaptability in the real world, we propose the Meta-Reasoning from a
linguistic perspective. This method empowers LLMs to deconstruct
reasoning-independent semantic information into generic symbolic
representations, thereby efficiently capturing more generalized reasoning
knowledge. We conduct extensive experiments on more than ten datasets
encompassing conventional reasoning tasks like arithmetic, symbolic, and
logical reasoning, and the more complex interactive reasoning tasks like
theory-of-mind reasoning. Experimental results demonstrate that Meta-Reasoning
significantly enhances in-context reasoning accuracy, learning efficiency,
out-of-domain generalization, and output stability compared to the
Chain-of-Thought technique. Code and data are publicly available at
\url{https://github.com/Alsace08/Meta-Reasoning}.",2023-06-30T17:38:10Z
,http://arxiv.org/pdf/2404.08148v1.pdf,"Distilling Algorithmic Reasoning from LLMs via Explaining Solution
  Programs","Distilling explicit chain-of-thought reasoning paths has emerged as an
effective method for improving the reasoning abilities of large language models
(LLMs) across various tasks. However, when tackling complex tasks that pose
significant challenges for state-of-the-art models, this technique often
struggles to produce effective chains of thought that lead to correct answers.
In this work, we propose a novel approach to distill reasoning abilities from
LLMs by leveraging their capacity to explain solutions. We apply our method to
solving competitive-level programming challenges. More specifically, we employ
an LLM to generate explanations for a set of <problem, solution-program> pairs,
then use <problem, explanation> pairs to fine-tune a smaller language model,
which we refer to as the Reasoner, to learn algorithmic reasoning that can
generate ""how-to-solve"" hints for unseen problems. Our experiments demonstrate
that learning from explanations enables the Reasoner to more effectively guide
program implementation by a Coder, resulting in higher solve rates than strong
chain-of-thought baselines on competitive-level programming problems. It also
outperforms models that learn directly from <problem, solution-program> pairs.
We curated an additional test set in the CodeContests format, which includes
246 more recent problems posted after the models' knowledge cutoff.",2024-04-11T22:19:50Z
,http://arxiv.org/pdf/2307.16364v1.pdf,"Promptly: Using Prompt Problems to Teach Learners How to Effectively
  Utilize AI Code Generators","With their remarkable ability to generate code, large language models (LLMs)
are a transformative technology for computing education practice. They have
created an urgent need for educators to rethink pedagogical approaches and
teaching strategies for newly emerging skill sets. Traditional approaches to
learning programming have focused on frequent and repeated practice at writing
code. The ease with which code can now be generated has resulted in a shift in
focus towards reading, understanding and evaluating LLM-generated code. In
parallel with this shift, a new essential skill is emerging -- the ability to
construct good prompts for code-generating models. This paper introduces a
novel pedagogical concept known as a `Prompt Problem', designed to help
students learn how to craft effective prompts for LLMs. A Prompt Problem
challenges a student to create a natural language prompt that leads an LLM to
produce the correct code for a specific problem. To support the delivery of
Prompt Problems at scale, in this paper we also present a novel tool called
Promptly which hosts a repository of Prompt Problems and automates the
evaluation of prompt-generated code. We report empirical findings from a field
study in which Promptly was deployed in a first-year Python programming course
(n=54). We explore student interactions with the tool and their perceptions of
the Prompt Problem concept. We found that Promptly was largely well-received by
students for its ability to engage their computational thinking skills and
expose them to new programming constructs. We also discuss avenues for future
work, including variations on the design of Prompt Problems and the need to
study their integration into the curriculum and teaching practice.",2023-07-31T01:46:42Z
,http://arxiv.org/pdf/2308.02618v2.pdf,ChatGPT for GTFS: Benchmarking LLMs on GTFS Understanding and Retrieval,"The General Transit Feed Specification (GTFS) standard for publishing transit
data is ubiquitous. GTFS being tabular data, with information spread across
different files, necessitates specialized tools or packages to retrieve
information. Concurrently, the use of Large Language Models(LLMs) for text and
information retrieval is growing. The idea of this research is to see if the
current widely adopted LLMs (ChatGPT) are able to understand GTFS and retrieve
information from GTFS using natural language instructions without explicitly
providing information. In this research, we benchmark OpenAI's GPT-3.5-Turbo
and GPT-4 LLMs which are the backbone of ChatGPT. ChatGPT demonstrates a
reasonable understanding of GTFS by answering 59.7% (GPT-3.5-Turbo) and 73.3%
(GPT-4) of our multiple-choice questions (MCQ) correctly. Furthermore, we
evaluated the LLMs on information extraction tasks using a filtered GTFS feed
containing four routes. We found that program synthesis techniques outperformed
zero-shot approaches, achieving up to 93% (90%) accuracy for simple queries and
61% (41%) for complex ones using GPT-4 (GPT-3.5-Turbo).",2023-08-04T14:50:37Z
,http://arxiv.org/pdf/2402.02611v2.pdf,"PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial
  Reasoning Problems?","Recent works show that the largest of the large language models (LLMs) can
solve many simple reasoning tasks expressed in natural language, without
any/much supervision. But, can they also solve challenging first-order
combinatorial reasoning problems, such as graph coloring, knapsack and
cryptarithmetic? To answer this question, we present PuzzleBench, a dataset of
31 such challenging problems along with a few solved instances for each
problem. These problems are all first order, i.e., they can be instantiated
with problem instances of varying sizes, and most of them are NP-hard,
requiring several reasoning steps to reach the solution. We first observe that
LLMs, even when aided by symbolic solvers, perform rather poorly on our
dataset. In response, we propose a new approach, Puzzle-LM, which combines LLMs
with both symbolic solvers and program interpreters, along with feedback from
solved examples, to achieve huge performance gains. Our extensive
experimentation and analyses offer new insights into the reasoning abilities
and limitations of present-day LLMs.",2024-02-04T20:56:09Z
,http://arxiv.org/pdf/2404.00413v1.pdf,Language Models are Spacecraft Operators,"Recent trends are emerging in the use of Large Language Models (LLMs) as
autonomous agents that take actions based on the content of the user text
prompts. We intend to apply these concepts to the field of Guidance,
Navigation, and Control in space, enabling LLMs to have a significant role in
the decision-making process for autonomous satellite operations. As a first
step towards this goal, we have developed a pure LLM-based solution for the
Kerbal Space Program Differential Games (KSPDG) challenge, a public software
design competition where participants create autonomous agents for maneuvering
satellites involved in non-cooperative space operations, running on the KSP
game engine. Our approach leverages prompt engineering, few-shot prompting, and
fine-tuning techniques to create an effective LLM-based agent that ranked 2nd
in the competition. To the best of our knowledge, this work pioneers the
integration of LLM agents into space research. Code is available at
https://github.com/ARCLab-MIT/kspdg.",2024-03-30T16:43:59Z
,http://arxiv.org/pdf/2405.17438v1.pdf,An LLM-Tool Compiler for Fused Parallel Function Calling,"State-of-the-art sequential reasoning in Large Language Models (LLMs) has
expanded the capabilities of Copilots beyond conversational tasks to complex
function calling, managing thousands of API calls. However, the tendency of
compositional prompting to segment tasks into multiple steps, each requiring a
round-trip to the GPT APIs, leads to increased system latency and costs.
Although recent advancements in parallel function calling have improved tool
execution per API call, they may necessitate more detailed in-context
instructions and task breakdown at the prompt level, resulting in higher
engineering and production costs. Inspired by the hardware design principles of
multiply-add (MAD) operations, which fuse multiple arithmetic operations into a
single task from the compiler's perspective, we propose LLM-Tool Compiler,
which selectively fuses similar types of tool operations under a single
function at runtime, presenting them as a unified task to the LLM. This
selective fusion inherently enhances parallelization and efficiency.
Benchmarked on a large-scale Copilot platform, LLM-Tool Compiler achieves up to
four times more parallel calls than existing methods, reducing token costs and
latency by up to 40% and 12%, respectively.",2024-05-07T18:55:50Z
,http://arxiv.org/pdf/2306.03314v1.pdf,"Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM
  Agents","In this paper, we present a novel framework for enhancing the capabilities of
large language models (LLMs) by leveraging the power of multi-agent systems.
Our framework introduces a collaborative environment where multiple intelligent
agent components, each with distinctive attributes and roles, work together to
handle complex tasks more efficiently and effectively. We demonstrate the
practicality and versatility of our framework through case studies in
artificial general intelligence (AGI), specifically focusing on the Auto-GPT
and BabyAGI models. We also examine the ""Gorilla"" model, which integrates
external APIs into the LLM. Our framework addresses limitations and challenges
such as looping issues, security risks, scalability, system evaluation, and
ethical considerations. By modeling various domains such as courtroom
simulations and software development scenarios, we showcase the potential
applications and benefits of our proposed multi-agent system. Our framework
provides an avenue for advancing the capabilities and performance of LLMs
through collaboration and knowledge exchange among intelligent agents.",2023-06-05T23:55:37Z
,http://arxiv.org/pdf/2301.12867v4.pdf,"Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and
  Toxicity","Recent breakthroughs in natural language processing (NLP) have permitted the
synthesis and comprehension of coherent text in an open-ended way, therefore
translating the theoretical algorithms into practical applications. The large
language models (LLMs) have significantly impacted businesses such as report
summarization software and copywriters. Observations indicate, however, that
LLMs may exhibit social prejudice and toxicity, posing ethical and societal
dangers of consequences resulting from irresponsibility. Large-scale benchmarks
for accountable LLMs should consequently be developed. Although several
empirical investigations reveal the existence of a few ethical difficulties in
advanced LLMs, there is little systematic examination and user study of the
risks and harmful behaviors of current LLM usage. To further educate future
efforts on constructing ethical LLMs responsibly, we perform a qualitative
research method called ``red teaming'' on OpenAI's ChatGPT\footnote{In this
paper, ChatGPT refers to the version released on Dec 15th.} to better
understand the practical features of ethical dangers in recent LLMs. We analyze
ChatGPT comprehensively from four perspectives: 1) \textit{Bias} 2)
\textit{Reliability} 3) \textit{Robustness} 4) \textit{Toxicity}. In accordance
with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample
datasets. We find that a significant number of ethical risks cannot be
addressed by existing benchmarks, and hence illustrate them via additional case
studies. In addition, we examine the implications of our findings on AI ethics
and harmal behaviors of ChatGPT, as well as future problems and practical
design considerations for responsible LLMs. We believe that our findings may
give light on future efforts to determine and mitigate the ethical hazards
posed by machines in LLM applications.",2023-01-30T13:20:48Z
10.1145/3649217.3653574,http://arxiv.org/pdf/2405.14178v1.pdf,"Desirable Characteristics for AI Teaching Assistants in Programming
  Education","Providing timely and personalized feedback to large numbers of students is a
long-standing challenge in programming courses. Relying on human teaching
assistants (TAs) has been extensively studied, revealing a number of potential
shortcomings. These include inequitable access for students with low confidence
when needing support, as well as situations where TAs provide direct solutions
without helping students to develop their own problem-solving skills. With the
advent of powerful large language models (LLMs), digital teaching assistants
configured for programming contexts have emerged as an appealing and scalable
way to provide instant, equitable, round-the-clock support. Although digital
TAs can provide a variety of help for programming tasks, from high-level
problem solving advice to direct solution generation, the effectiveness of such
tools depends on their ability to promote meaningful learning experiences. If
students find the guardrails implemented in digital TAs too constraining, or if
other expectations are not met, they may seek assistance in ways that do not
help them learn. Thus, it is essential to identify the features that students
believe make digital teaching assistants valuable. We deployed an LLM-powered
digital assistant in an introductory programming course and collected student
feedback ($n=813$) on the characteristics of the tool they perceived to be most
important. Our results highlight that students value such tools for their
ability to provide instant, engaging support, particularly during peak times
such as before assessment deadlines. They also expressed a strong preference
for features that enable them to retain autonomy in their learning journey,
such as scaffolding that helps to guide them through problem-solving steps
rather than simply being shown direct solutions.",2024-05-23T05:03:49Z
,http://arxiv.org/pdf/2311.07957v2.pdf,Language Models are Better Bug Detector Through Code-Pair Classification,"Large language models (LLMs) such as GPT-3.5 and CodeLlama are powerful
models for code generation and understanding. Fine-tuning these models comes
with a high computational cost and requires a large labeled dataset.
Alternatively, in-context learning techniques allow models to learn downstream
tasks with only a few examples. Recently, researchers have shown how in-context
learning performs well in bug detection and repair. In this paper, we propose
code-pair classification task in which both the buggy and non-buggy versions
are given to the model, and the model identifies the buggy ones. We evaluate
our task in real-world dataset of bug detection and two most powerful LLMs. Our
experiments indicate that an LLM can often pick the buggy from the non-buggy
version of the code, and the code-pair classification task is much easier
compared to be given a snippet and deciding if and where a bug exists.",2023-11-14T07:20:57Z
,http://arxiv.org/pdf/2308.11526v1.pdf,Learning Representations on Logs for AIOps,"AI for IT Operations (AIOps) is a powerful platform that Site Reliability
Engineers (SREs) use to automate and streamline operational workflows with
minimal human intervention. Automated log analysis is a critical task in AIOps
as it provides key insights for SREs to identify and address ongoing faults.
Tasks such as log format detection, log classification, and log parsing are key
components of automated log analysis. Most of these tasks require supervised
learning; however, there are multiple challenges due to limited labelled log
data and the diverse nature of log data. Large Language Models (LLMs) such as
BERT and GPT3 are trained using self-supervision on a vast amount of unlabeled
data. These models provide generalized representations that can be effectively
used for various downstream tasks with limited labelled data. Motivated by the
success of LLMs in specific domains like science and biology, this paper
introduces a LLM for log data which is trained on public and proprietary log
data. The results of our experiments demonstrate that the proposed LLM
outperforms existing models on multiple downstream tasks. In summary, AIOps
powered by LLMs offers an efficient and effective solution for automating log
analysis tasks and enabling SREs to focus on higher-level tasks. Our proposed
LLM, trained on public and proprietary log data, offers superior performance on
multiple downstream tasks, making it a valuable addition to the AIOps platform.",2023-08-18T20:34:46Z
10.1145/3586183.3606719,http://arxiv.org/pdf/2308.03921v1.pdf,"Spellburst: A Node-based Interface for Exploratory Creative Coding with
  Natural Language Prompts","Creative coding tasks are often exploratory in nature. When producing digital
artwork, artists usually begin with a high-level semantic construct such as a
""stained glass filter"" and programmatically implement it by varying code
parameters such as shape, color, lines, and opacity to produce visually
appealing results. Based on interviews with artists, it can be effortful to
translate semantic constructs to program syntax, and current programming tools
don't lend well to rapid creative exploration. To address these challenges, we
introduce Spellburst, a large language model (LLM) powered creative-coding
environment. Spellburst provides (1) a node-based interface that allows artists
to create generative art and explore variations through branching and merging
operations, (2) expressive prompt-based interactions to engage in semantic
programming, and (3) dynamic prompt-driven interfaces and direct code editing
to seamlessly switch between semantic and syntactic exploration. Our evaluation
with artists demonstrates Spellburst's potential to enhance creative coding
practices and inform the design of computational creativity tools that bridge
semantic and syntactic spaces.",2023-08-07T21:54:58Z
,http://arxiv.org/pdf/2406.06316v1.pdf,Tx-LLM: A Large Language Model for Therapeutics,"Developing therapeutics is a lengthy and expensive process that requires the
satisfaction of many different criteria, and AI models capable of expediting
the process would be invaluable. However, the majority of current AI approaches
address only a narrowly defined set of tasks, often circumscribed within a
particular domain. To bridge this gap, we introduce Tx-LLM, a generalist large
language model (LLM) fine-tuned from PaLM-2 which encodes knowledge about
diverse therapeutic modalities. Tx-LLM is trained using a collection of 709
datasets that target 66 tasks spanning various stages of the drug discovery
pipeline. Using a single set of weights, Tx-LLM simultaneously processes a wide
variety of chemical or biological entities(small molecules, proteins, nucleic
acids, cell lines, diseases) interleaved with free-text, allowing it to predict
a broad range of associated properties, achieving competitive with
state-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on
22. Among these, Tx-LLM is particularly powerful and exceeds best-in-class
performance on average for tasks combining molecular SMILES representations
with text such as cell line names or disease names, likely due to context
learned during pretraining. We observe evidence of positive transfer between
tasks with diverse drug types (e.g.,tasks involving small molecules and tasks
involving proteins), and we study the impact of model size, domain finetuning,
and prompting strategies on performance. We believe Tx-LLM represents an
important step towards LLMs encoding biochemical knowledge and could have a
future role as an end-to-end tool across the drug discovery development
pipeline.",2024-06-10T14:33:02Z
,http://arxiv.org/pdf/2308.08473v1.pdf,"DataRaceBench V1.4.1 and DataRaceBench-ML V0.1: Benchmark Suites for
  Data Race Detection","Data races pose a significant threat in multi-threaded parallel applications
due to their negative impact on program correctness. DataRaceBench, an
open-source benchmark suite, is specifically crafted to assess these data race
detection tools in a systematic and measurable manner. Machine learning
techniques have recently demonstrated considerable potential in
high-performance computing (HPC) program analysis and optimization. However,
these techniques require specialized data formats for training and refinement.
This paper presents the latest update to DataRaceBench, incorporating new data
race contributions from Wu et al. \cite{wu2023model}, and introduces a derived
dataset named DataRaceBench-ML (DRB-ML) \cite{drbml}. DRB-ML aligns with the
emerging trend of machine learning and large language models. Originating from
DataRaceBench, this dataset includes detailed labels that denote the presence
of a data race and provides comprehensive details of associated variables, such
as variable names, line numbers, and the operation (read/write). Unique to
DRB-ML, we have also integrated a series of tailored prompt-response pairs
specifically designed for LLM fine-tuning.",2023-08-16T16:23:13Z
,http://arxiv.org/pdf/2308.11148v2.pdf,"LLaMA-Reviewer: Advancing Code Review Automation with Large Language
  Models through Parameter-Efficient Fine-Tuning","The automation of code review activities, a long-standing pursuit in software
engineering, has been primarily addressed by numerous domain-specific
pre-trained models. Despite their success, these models frequently demand
extensive resources for pre-training from scratch. In contrast, Large Language
Models (LLMs) provide an intriguing alternative, given their remarkable
capabilities when supplemented with domain-specific knowledge. However, their
potential for automating code review tasks remains largely unexplored.
  In response to this research gap, we present LLaMA-Reviewer, an innovative
framework that leverages the capabilities of LLaMA, a popular LLM, in the realm
of code review. Mindful of resource constraints, this framework employs
parameter-efficient fine-tuning (PEFT) methods, delivering high performance
while using less than 1% of trainable parameters.
  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse,
publicly available datasets. Notably, even with the smallest LLaMA base model
consisting of 6.7B parameters and a limited number of tuning epochs,
LLaMA-Reviewer equals the performance of existing code-review-focused models.
  The ablation experiments provide insights into the influence of various
fine-tuning process components, including input representation, instruction
tuning, and different PEFT methods. To foster continuous progress in this
field, the code and all PEFT-weight plugins have been made open-source.",2023-08-22T03:10:40Z
,http://arxiv.org/pdf/2405.15880v1.pdf,HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis,"Many structured prediction and reasoning tasks can be framed as program
synthesis problems, where the goal is to generate a program in a
domain-specific language (DSL) that transforms input data into the desired
output. Unfortunately, purely neural approaches, such as large language models
(LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while
purely symbolic methods based on combinatorial search scale poorly to complex
problems. Motivated by these limitations, we introduce a hybrid approach, where
LLM completions for a given task are used to learn a task-specific,
context-free surrogate model, which is then used to guide program synthesis. We
evaluate this hybrid approach on three domains, and show that it outperforms
both unguided search and direct sampling from LLMs, as well as existing program
synthesizers.",2024-05-24T18:45:51Z
,http://arxiv.org/pdf/2311.13721v4.pdf,"Nova: Generative Language Models for Assembly Code with Hierarchical
  Attention and Contrastive Learning","Binary code analysis is the foundation of crucial tasks in the security
domain; thus building effective binary analysis techniques is more important
than ever. Large language models (LLMs) although have brought impressive
improvement to source code tasks, do not directly generalize to assembly code
due to the unique challenges of assembly: (1) the low information density of
assembly and (2) the diverse optimizations in assembly code. To overcome these
challenges, this work proposes a hierarchical attention mechanism that builds
attention summaries to capture the semantics more effectively, and designs
contrastive learning objectives to train LLMs to learn assembly optimization.
Equipped with these techniques, this work develops Nova, a generative LLM for
assembly code. Nova outperforms existing techniques on binary code
decompilation by up to 146.54%, and outperforms the latest binary code
similarity detection techniques by up to 6.17%, showing promising abilities on
both assembly generation and understanding tasks.",2023-11-22T22:27:54Z
,http://arxiv.org/pdf/2210.03696v2.pdf,"LLMEffiChecker: Understanding and Testing Efficiency Degradation of
  Large Language Models","In this paper, we make the first attempt to understand and test potential
computation efficiency robustness in state-of-the-art LLMs. By analyzing the
working mechanism and implementation of 20,543 public-accessible LLMs, we
observe a fundamental property in LLMs that could be manipulated in an
adversarial manner to reduce computation efficiency significantly. Our key
motivation is to generate test inputs that could sufficiently delay the
generation of EOS such that LLMs would have to go through enough iterations to
satisfy the pre-configured threshold. We present \tool, which can work under
both white-box setting and black-box setting. In the white-box scenario, \tool
develops a gradient-guided technique that searches for a minimal and
unnoticeable perturbation at character-level, token-level, and structure-level.
In the black-box scenario, \tool employs a causal inference-based approach to
find critical tokens and similarly applies three levels of imperceptible
perturbation to them. Both the white-box and black-box settings effectively
delay the appearance of EOS, compelling these inputs to reach the
naturally-unreachable threshold. To demonstrate the effectiveness of \tool, we
conduct a systematic evaluation on nine public-available LLMs: Google T5,
AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL
translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT and Salesforce CodeGen.
Experimental results show that \tool can increase on average LLMs' response
latency and energy consumption by 325\% to 3244\% and 344\% to 3616\%,
respectively, by perturbing just one character or token in the input sentence.",2022-10-07T17:01:01Z
,http://arxiv.org/pdf/2402.00097v2.pdf,"Code-Aware Prompting: A study of Coverage Guided Test Generation in
  Regression Setting using LLM","Testing plays a pivotal role in ensuring software quality, yet conventional
Search Based Software Testing (SBST) methods often struggle with complex
software units, achieving suboptimal test coverage. Recent works using large
language models (LLMs) for test generation have focused on improving generation
quality through optimizing the test generation context and correcting errors in
model outputs, but use fixed prompting strategies that prompt the model to
generate tests without additional guidance. As a result LLM-generated
testsuites still suffer from low coverage. In this paper, we present SymPrompt,
a code-aware prompting strategy for LLMs in test generation. SymPrompt's
approach is based on recent work that demonstrates LLMs can solve more complex
logical problems when prompted to reason about the problem in a multi-step
fashion. We apply this methodology to test generation by deconstructing the
testsuite generation process into a multi-stage sequence, each of which is
driven by a specific prompt aligned with the execution paths of the method
under test, and exposing relevant type and dependency focal context to the
model. Our approach enables pretrained LLMs to generate more complete test
cases without any additional training. We implement SymPrompt using the
TreeSitter parsing framework and evaluate on a benchmark challenging methods
from open source Python projects. SymPrompt enhances correct test generations
by a factor of 5 and bolsters relative coverage by 26% for CodeGen2. Notably,
when applied to GPT-4, SymPrompt improves coverage by over 2x compared to
baseline prompting strategies.",2024-01-31T18:21:49Z
,http://arxiv.org/pdf/2405.04994v1.pdf,NAVRepair: Node-type Aware C/C++ Code Vulnerability Repair,"The rapid advancement of deep learning has led to the development of Large
Language Models (LLMs). In the field of vulnerability repair, previous research
has leveraged rule-based fixing, pre-trained models, and LLM's prompt
engineering. However, existing approaches have limitations in terms of the
integration of code structure with error types. Besides, due to certain
features of C/C++ language, vulnerability repair in C/C++ proves to be
exceptionally challenging. To address these challenges, we propose NAVRepair, a
novel framework that combines the node-type information extracted from Abstract
Syntax Trees (ASTs) with error types, specifically targeting C/C++
vulnerabilities. Specifically, our approach employs type analysis to localize
the minimum edit node (MEN) and customizes context information collection based
on different error types. In the offline stage, NAVRepair parses code patches
to locate MENs and designs rules to extract relevant contextual information for
each MEN type. In the online repairing stage, it analyzes the suspicious code,
combines it with vulnerability type templates derived from the Common Weakness
Enumeration (CWE), and generates targeted repair prompts. We evaluate NAVRepair
on multiple popular LLMs and demonstrate its effectiveness in improving the
performance of code vulnerability repair. Notably, our framework is independent
of any specific LLMs and can quickly adapt to new vulnerability types.
Extensive experiments validate that NAVRepair achieves excellent results in
assisting LLMs to accurately detect and fix C/C++ vulnerabilities. We achieve a
26% higher accuracy compared to an existing LLM-based C/C++ vulnerability
repair method. We believe our node type-aware approach has promising
application prospects for enhancing real-world C/C++ code security.",2024-05-08T11:58:55Z
,http://arxiv.org/pdf/2401.07930v1.pdf,"On Inter-dataset Code Duplication and Data Leakage in Large Language
  Models","Motivation. Large language models (LLMs) have exhibited remarkable
proficiency in diverse software engineering (SE) tasks. Handling such tasks
typically involves acquiring foundational coding knowledge on large,
general-purpose datasets during a pre-training phase, and subsequently refining
on smaller, task-specific datasets as part of a fine-tuning phase.
  Problem statement. Data leakage is a well-known issue in training of machine
learning models. A manifestation of this issue is the intersection of the
training and testing splits. While intra-dataset code duplication examines this
intersection within a given dataset and has been addressed in prior research,
inter-dataset code duplication, which gauges the overlap between different
datasets, remains largely unexplored. If this phenomenon exists, it could
compromise the integrity of LLM evaluations because of the inclusion of
fine-tuning test samples that were already encountered during pre-training,
resulting in inflated performance metrics.
  Contribution. This paper explores the phenomenon of inter-dataset code
duplication and its impact on evaluating LLMs across diverse SE tasks.
  Study design. We conduct an empirical study using the CSN dataset, a widely
adopted pre-training dataset, and five fine-tuning datasets used for various SE
tasks. We first identify the intersection between the pre-training and
fine-tuning datasets using a deduplication process. Then, we fine-tune four
models pre-trained on CSN to evaluate their performance on samples encountered
during pre-training and those unseen during that phase.
  Results. Our findings reveal a potential threat to the evaluation of various
LLMs across multiple SE tasks, stemming from the inter-dataset code duplication
phenomenon. Moreover, we demonstrate that this threat is accentuated by factors
like the LLM's size and the chosen fine-tuning technique.",2024-01-15T19:46:40Z
,http://arxiv.org/pdf/2312.12450v5.pdf,"Can It Edit? Evaluating the Ability of Large Language Models to Follow
  Code Editing Instructions","A significant amount of research is focused on developing and evaluating
large language models for a variety of code synthesis tasks. These include
synthesizing code from natural language, synthesizing tests from code, and
synthesizing explanations of code. In contrast, the behavior of instructional
code editing with LLMs is understudied. These are tasks in which the model is
provided a block of code and an instruction to modify the code. The editing
instruction may ask for a feature to be added or removed, describe a bug and
ask for a fix, or ask for a different kind of solution. We introduce a
carefully crafted benchmark of code editing tasks and use it to evaluate
several cutting edge LLMs. Our evaluation exposes a significant gap between the
capabilities of state-of-the-art open and closed models. For example, even
GPT-3.5-Turbo is better than the best open model at code editing tasks. We also
introduce a new, carefully curated, permissively licensed training dataset of
code editing tasks coupled with natural language instructions. Using this
training dataset, we show that we can fine-tune open Code LLMs to significantly
improve their code editing capabilities, closing the gap between open and
closed models. All code, data, and models are available at
https://github.com/nuprl/CanItEdit.",2023-12-11T02:27:45Z
,http://arxiv.org/pdf/2404.15317v1.pdf,Concept-Guided LLM Agents for Human-AI Safety Codesign,"Generative AI is increasingly important in software engineering, including
safety engineering, where its use ensures that software does not cause harm to
people. This also leads to high quality requirements for generative AI.
Therefore, the simplistic use of Large Language Models (LLMs) alone will not
meet these quality demands. It is crucial to develop more advanced and
sophisticated approaches that can effectively address the complexities and
safety concerns of software systems. Ultimately, humans must understand and
take responsibility for the suggestions provided by generative AI to ensure
system safety. To this end, we present an efficient, hybrid strategy to
leverage LLMs for safety analysis and Human-AI codesign. In particular, we
develop a customized LLM agent that uses elements of prompt engineering,
heuristic reasoning, and retrieval-augmented generation to solve tasks
associated with predefined safety concepts, in interaction with a system model
graph. The reasoning is guided by a cascade of micro-decisions that help
preserve structured information. We further suggest a graph verbalization which
acts as an intermediate representation of the system model to facilitate
LLM-graph interactions. Selected pairs of prompts and responses relevant for
safety analytics illustrate our method for the use case of a simplified
automated driving system.",2024-04-03T11:37:01Z
,http://arxiv.org/pdf/2311.02433v1.pdf,Can ChatGPT support software verification?,"Large language models have become increasingly effective in software
engineering tasks such as code generation, debugging and repair. Language
models like ChatGPT can not only generate code, but also explain its inner
workings and in particular its correctness. This raises the question whether we
can utilize ChatGPT to support formal software verification.
  In this paper, we take some first steps towards answering this question. More
specifically, we investigate whether ChatGPT can generate loop invariants. Loop
invariant generation is a core task in software verification, and the
generation of valid and useful invariants would likely help formal verifiers.
To provide some first evidence on this hypothesis, we ask ChatGPT to annotate
106 C programs with loop invariants. We check validity and usefulness of the
generated invariants by passing them to two verifiers, Frama-C and CPAchecker.
Our evaluation shows that ChatGPT is able to produce valid and useful
invariants allowing Frama-C to verify tasks that it could not solve before.
Based on our initial insights, we propose ways of combining ChatGPT (or large
language models in general) and software verifiers, and discuss current
limitations and open issues.",2023-11-04T15:25:18Z
,http://arxiv.org/pdf/2406.05514v2.pdf,RAG-Enhanced Commit Message Generation,"Commit message is one of the most important textual information in software
development and maintenance. However, it is time-consuming and labor-intensive
to write commit messages manually. Commit Message Generation (CMG) has become a
research hotspot in automated software engineering. Researchers have proposed
several methods for CMG and achieved great results. In recent years, CodeBERT,
CodeT5, and other Pre-trained Language Models (PLMs) for code have been
proposed. These models can be easily transferred to code-related downstream
tasks including CMG with simple fine-tuning and can achieve impressive
performance. Moreover, Large Language Models (LLMs) with code capabilities
(e.g., ChatGPT, Llama 3, Gemma) can be directly applied to various tasks by
designing instruct prompts without training. This brings new possibilities to
the CMG task. In this work, we propose REACT, a novel REtrieval-Augmented
framework for CommiT message generation, which effectively integrates advanced
retrieval techniques with different PLMs and LLMs and can broadly enhance the
performance of various models on the CMG task. Specifically, we design and
build a hybrid retriever to retrieve the most relevant code diff and commit
message pair from the code base as an ""exemplar"". Then, the retrieved pair is
utilized to guide and enhance the generation of commit messages by PLMs and
LLMs through fine-tuning and in-context learning. Our approach is evaluated on
a widely-used dataset. The experimental results show that REACT significantly
enhances the performance of various models on the CMG task, improving the BLEU
score of CodeT5 by up to 55%, boosting Llama 3's BLEU score by 102%, and
substantially surpassing all baselines, achieving a new SOTA. This demonstrates
the effectiveness and broad applicability of our framework that can enhance CMG
by a large margin.",2024-06-08T16:24:24Z
,http://arxiv.org/pdf/2405.03709v2.pdf,Generating Probabilistic Scenario Programs from Natural Language,"For cyber-physical systems (CPS), including robotics and autonomous vehicles,
mass deployment has been hindered by fatal errors that occur when operating in
rare events. To replicate rare events such as vehicle crashes, many companies
have created logging systems and employed crash reconstruction experts to
meticulously recreate these valuable events in simulation. However, in these
methods, ""what if"" questions are not easily formulated and answered. We present
ScenarioNL, an AI System for creating scenario programs from natural language.
Specifically, we generate these programs from police crash reports. Reports
normally contain uncertainty about the exact details of the incidents which we
represent through a Probabilistic Programming Language (PPL), Scenic. By using
Scenic, we can clearly and concisely represent uncertainty and variation over
CPS behaviors, properties, and interactions. We demonstrate how commonplace
prompting techniques with the best Large Language Models (LLM) are incapable of
reasoning about probabilistic scenario programs and generating code for
low-resource languages such as Scenic. Our system is comprised of several LLMs
chained together with several kinds of prompting strategies, a compiler, and a
simulator. We evaluate our system on publicly available autonomous vehicle
crash reports in California from the last five years and share insights into
how we generate code that is both semantically meaningful and syntactically
correct.",2024-05-03T23:06:31Z
,http://arxiv.org/pdf/2309.05660v2.pdf,Hypothesis Search: Inductive Reasoning with Language Models,"Inductive reasoning is a core problem-solving capacity: humans can identify
underlying principles from a few examples, which robustly generalize to novel
scenarios. Recent work evaluates large language models (LLMs) on inductive
reasoning tasks by directly prompting them yielding ""in context learning."" This
works well for straightforward inductive tasks but performs poorly on complex
tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we
propose to improve the inductive reasoning ability of LLMs by generating
explicit hypotheses at multiple levels of abstraction: we prompt the LLM to
propose multiple abstract hypotheses about the problem, in natural language,
then implement the natural language hypotheses as concrete Python programs.
These programs can be verified by running on observed examples and generalized
to novel inputs. To reduce the hypothesis search space, we explore steps to
filter the set of hypotheses to implement: we either ask the LLM to summarize
them into a smaller set of hypotheses or ask human annotators to select a
subset. We verify our pipeline's effectiveness on the ARC visual inductive
reasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS,
and list transformation dataset List Functions. On a random 100-problem subset
of ARC, our automated pipeline using LLM summaries achieves 30% accuracy,
outperforming the direct prompting baseline (accuracy of 17%). With the minimal
human input of selecting from LLM-generated candidates, performance is boosted
to 33%. Our ablations show that both abstract hypothesis generation and
concrete program representations benefit LLMs on inductive reasoning tasks.",2023-09-11T17:56:57Z
,http://arxiv.org/pdf/2208.05950v2.pdf,Interactive Code Generation via Test-Driven User-Intent Formalization,"Large language models (LLMs) have shown great potential in automating
significant aspects of coding by producing natural code from informal natural
language (NL) intent. However, when interacting with LLMs, users have no
guarantees that the code suggestions produced correctly satisfy the intent they
provided. In fact, it is hard to define a notion of correctness since natural
language can be ambiguous and lacks a formal semantics.
  In this paper, we propose the workflow of {\it interactive test-driven code
generation}, which leverages lightweight user feedback to (a) formalize the
user intent using generated tests that can be useful for debugging, and (b)
produce an improved set of code suggestions by pruning and ranking candidate
code suggestions. We describe a language-agnostic abstract algorithm and a
concrete implementation TiCoder. We perform an automated evaluation of TiCoder
on the \emph{MBPP} and \emph{HumanEval} code generation benchmarks. Our results
are promising with using the OpenAI Codex LLM: our best algorithm improves the
\passk{1} code generation accuracy (in absolute percentages) between $22.49\%$
to $37.71\%$ for MBPP and between $24.79\%$ to $53.98\%$ for HumanEval using
between 1 to 5 simulated user queries.",2022-08-11T17:41:08Z
,http://arxiv.org/pdf/2310.11546v1.pdf,"Bias and Error Mitigation in Software-Generated Data: An Advanced Search
  and Optimization Framework Leveraging Generative Code Models","Data generation and analysis is a fundamental aspect of many industries and
disciplines, from strategic decision making in business to research in the
physical and social sciences. However, data generated using software and
algorithms can be subject to biases and errors. These can be due to problems
with the original software, default settings that do not align with the
specific needs of the situation, or even deeper problems with the underlying
theories and models. This paper proposes an advanced search and optimization
framework aimed at generating and choosing optimal source code capable of
correcting errors and biases from previous versions to address typical problems
in software systems specializing in data analysis and generation, especially
those in the corporate and data science world. Applying this framework multiple
times on the same software system would incrementally improve the quality of
the output results. It uses Solomonoff Induction as a sound theoretical basis,
extending it with Kolmogorov Conditional Complexity, a novel adaptation, to
evaluate a set of candidate programs. We propose the use of generative models
for the creation of this set of programs, with special emphasis on the
capabilities of Large Language Models (LLMs) to generate high quality code.",2023-10-17T19:31:05Z
,http://arxiv.org/pdf/2402.04119v1.pdf,"Scientific Language Modeling: A Quantitative Review of Large Language
  Models in Molecular Science","Efficient molecular modeling and design are crucial for the discovery and
exploration of novel molecules, and the incorporation of deep learning methods
has revolutionized this field. In particular, large language models (LLMs)
offer a fresh approach to tackle scientific problems from a natural language
processing (NLP) perspective, introducing a research paradigm called scientific
language modeling (SLM). However, two key issues remain: how to quantify the
match between model and data modalities and how to identify the
knowledge-learning preferences of models. To address these challenges, we
propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263
experiments to assess the model's compatibility with data modalities and
knowledge acquisition. Through the modal transition probability matrix, we
provide insights into the most suitable modalities for tasks. Furthermore, we
introduce a statistically interpretable approach to discover context-specific
knowledge mapping by localized feature filtering. Our pioneering analysis
offers an exploration of the learning mechanism and paves the way for advancing
SLM in molecular science.",2024-02-06T16:12:36Z
,http://arxiv.org/pdf/2303.10131v1.pdf,"She Elicits Requirements and He Tests: Software Engineering Gender Bias
  in Large Language Models","Implicit gender bias in software development is a well-documented issue, such
as the association of technical roles with men. To address this bias, it is
important to understand it in more detail. This study uses data mining
techniques to investigate the extent to which 56 tasks related to software
development, such as assigning GitHub issues and testing, are affected by
implicit gender bias embedded in large language models. We systematically
translated each task from English into a genderless language and back, and
investigated the pronouns associated with each task. Based on translating each
task 100 times in different permutations, we identify a significant disparity
in the gendered pronoun associations with different tasks. Specifically,
requirements elicitation was associated with the pronoun ""he"" in only 6% of
cases, while testing was associated with ""he"" in 100% of cases. Additionally,
tasks related to helping others had a 91% association with ""he"" while the same
association for tasks related to asking coworkers was only 52%. These findings
reveal a clear pattern of gender bias related to software development tasks and
have important implications for addressing this issue both in the training of
large language models and in broader society.",2023-03-17T17:16:53Z
,http://arxiv.org/pdf/2310.01796v3.pdf,LILAC: Log Parsing using LLMs with Adaptive Parsing Cache,"Log parsing transforms log messages into structured formats, serving as the
prerequisite step for various log analysis tasks. Although a variety of log
parsing approaches have been proposed, their performance on complicated log
data remains compromised due to the use of human-crafted rules or
learning-based models with limited training data. The recent emergence of
powerful large language models (LLMs) demonstrates their vast pre-trained
knowledge related to code and logging, making it promising to apply LLMs for
log parsing. However, their lack of specialized log parsing capabilities
currently hinders their accuracy in parsing. Moreover, the inherent
inconsistent answers, as well as the substantial overhead, prevent the
practical adoption of LLM-based log parsing.
  To address these challenges, we propose LILAC, the first practical log
parsing framework using LLMs with adaptive parsing cache. To facilitate
accurate and robust log parsing, LILAC leverages the in-context learning (ICL)
capability of the LLM by performing a hierarchical candidate sampling algorithm
and selecting high-quality demonstrations. Furthermore, LILAC incorporates a
novel component, an adaptive parsing cache, to store and refine the templates
generated by the LLM. It helps mitigate LLM's inefficiency issue by enabling
rapid retrieval of previously processed log templates. In this process, LILAC
adaptively updates the templates within the parsing cache to ensure the
consistency of parsed results. The extensive evaluation on public large-scale
datasets shows that LILAC outperforms state-of-the-art methods by 69.5% in
terms of the average F1 score of template accuracy. In addition, LILAC reduces
the query times to LLMs by several orders of magnitude, achieving a comparable
efficiency to the fastest baseline.",2023-10-03T04:46:59Z
,http://arxiv.org/pdf/2310.04870v5.pdf,"Lemur: Integrating Large Language Models in Automated Program
  Verification","The demonstrated code-understanding capability of LLMs raises the question of
whether they can be used for automated program verification, a task that
demands high-level abstract reasoning about program properties that is
challenging for verification tools. We propose a general methodology to combine
the power of LLMs and automated reasoners for automated program verification.
We formally describe this methodology as a set of transition rules and prove
its soundness. We instantiate the calculus as a sound automated verification
procedure and demonstrate practical improvements on a set of synthetic and
competition benchmarks.",2023-10-07T16:44:53Z
10.1007/s11704-024-40231-1,http://arxiv.org/pdf/2308.11432v5.pdf,A Survey on Large Language Model based Autonomous Agents,"Autonomous agents have long been a prominent research focus in both academic
and industry communities. Previous research in this field often focuses on
training agents with limited knowledge within isolated environments, which
diverges significantly from human learning processes, and thus makes the agents
hard to achieve human-like decisions. Recently, through the acquisition of vast
amounts of web knowledge, large language models (LLMs) have demonstrated
remarkable potential in achieving human-level intelligence. This has sparked an
upsurge in studies investigating LLM-based autonomous agents. In this paper, we
present a comprehensive survey of these studies, delivering a systematic review
of the field of LLM-based autonomous agents from a holistic perspective. More
specifically, we first discuss the construction of LLM-based autonomous agents,
for which we propose a unified framework that encompasses a majority of the
previous work. Then, we present a comprehensive overview of the diverse
applications of LLM-based autonomous agents in the fields of social science,
natural science, and engineering. Finally, we delve into the evaluation
strategies commonly used for LLM-based autonomous agents. Based on the previous
studies, we also present several challenges and future directions in this
field. To keep track of this field and continuously update our survey, we
maintain a repository of relevant references at
https://github.com/Paitesanshi/LLM-Agent-Survey.",2023-08-22T13:30:37Z
,http://arxiv.org/pdf/2306.08018v5.pdf,"Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for
  Large Language Models","Large Language Models (LLMs), with their remarkable task-handling
capabilities and innovative outputs, have catalyzed significant advancements
across a spectrum of fields. However, their proficiency within specialized
domains such as biomolecular studies remains limited. To address this
challenge, we introduce Mol-Instructions, a comprehensive instruction dataset
designed for the biomolecular domain. Mol-Instructions encompasses three key
components: molecule-oriented instructions, protein-oriented instructions, and
biomolecular text instructions. Each component aims to improve the
understanding and prediction capabilities of LLMs concerning biomolecular
features and behaviors. Through extensive instruction tuning experiments on
LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large
models' performance in the intricate realm of biomolecular studies, thus
fostering progress in the biomolecular research community. Mol-Instructions is
publicly available for ongoing research and will undergo regular updates to
enhance its applicability.",2023-06-13T14:35:34Z
,http://arxiv.org/pdf/2311.10388v2.pdf,"Automatic Smart Contract Comment Generation via Large Language Models
  and In-Context Learning","The previous smart contract code comment (SCC) generation approaches can be
divided into two categories: fine-tuning paradigm-based approaches and
information retrieval-based approaches. However, for the fine-tuning
paradigm-based approaches, the performance may be limited by the quality of the
gathered dataset for the downstream task and they may have knowledge-forgetting
issues. While for the information retrieval-based approaches, it is difficult
for them to generate high-quality comments if similar code does not exist in
the historical repository. Therefore we want to utilize the domain knowledge
related to SCC generation in large language models (LLMs) to alleviate the
disadvantages of these two types of approaches. In this study, we propose an
approach SCCLLM based on LLMs and in-context learning. Specifically, in the
demonstration selection phase, SCCLLM retrieves the top-k code snippets from
the historical corpus by considering syntax, semantics, and lexical
information. In the in-context learning phase, SCCLLM utilizes the retrieved
code snippets as demonstrations, which can help to utilize the related
knowledge for this task. We select a large corpus from a smart contract
community Etherscan.io as our experimental subject. Extensive experimental
results show the effectiveness of SCCLLM when compared with baselines in
automatic evaluation and human evaluation.",2023-11-17T08:31:09Z
,http://arxiv.org/pdf/2406.10305v1.pdf,"Unlock the Correlation between Supervised Fine-Tuning and Reinforcement
  Learning in Training Code Large Language Models","Automatic code generation has been a longstanding research topic. With the
advancement of general-purpose large language models (LLMs), the ability to
code stands out as one important measure to the model's reasoning performance.
Usually, a two-stage training paradigm is implemented to obtain a Code LLM,
namely the pretraining and the fine-tuning. Within the fine-tuning, supervised
fine-tuning (SFT), and reinforcement learning (RL) are often used to improve
the model's zero-shot ability. A large number of work has been conducted to
improve the model's performance on code-related benchmarks with either
modifications to the algorithm or refinement of the dataset. However, we still
lack a deep insight into the correlation between SFT and RL. For instance, what
kind of dataset should be used to ensure generalization, or what if we abandon
the SFT phase in fine-tuning. In this work, we make an attempt to understand
the correlation between SFT and RL. To facilitate our research, we manually
craft 100 basis python functions, called atomic functions, and then a
synthesizing pipeline is deployed to create a large number of synthetic
functions on top of the atomic ones. In this manner, we ensure that the train
and test sets remain distinct, preventing data contamination. Through
comprehensive ablation study, we find: (1) Both atomic and synthetic functions
are indispensable for SFT's generalization, and only a handful of synthetic
functions are adequate; (2) Through RL, the SFT's generalization to target
domain can be greatly enhanced, even with the same training prompts; (3)
Training RL from scratch can alleviate the over-fitting issue introduced in the
SFT phase.",2024-06-14T03:39:01Z
,http://arxiv.org/pdf/2303.08819v1.pdf,"Ask and You Shall Receive (a Graph Drawing): Testing ChatGPT's Potential
  to Apply Graph Layout Algorithms","Large language models (LLMs) have recently taken the world by storm. They can
generate coherent text, hold meaningful conversations, and be taught concepts
and basic sets of instructions - such as the steps of an algorithm. In this
context, we are interested in exploring the application of LLMs to graph
drawing algorithms by performing experiments on ChatGPT. These algorithms are
used to improve the readability of graph visualizations. The probabilistic
nature of LLMs presents challenges to implementing algorithms correctly, but we
believe that LLMs' ability to learn from vast amounts of data and apply complex
operations may lead to interesting graph drawing results. For example, we could
enable users with limited coding backgrounds to use simple natural language to
create effective graph visualizations. Natural language specification would
make data visualization more accessible and user-friendly for a wider range of
users. Exploring LLMs' capabilities for graph drawing can also help us better
understand how to formulate complex algorithms for LLMs; a type of knowledge
that could transfer to other areas of computer science. Overall, our goal is to
shed light on the exciting possibilities of using LLMs for graph drawing while
providing a balanced assessment of the challenges and opportunities they
present. A free copy of this paper with all supplemental materials required to
reproduce our results is available on
https://osf.io/n5rxd/?view_only=f09cbc2621f44074810b7d843f1e12f9",2023-03-03T04:26:43Z
10.1145/3501385.3543957,http://arxiv.org/pdf/2206.11861v2.pdf,"Automatic Generation of Programming Exercises and Code Explanations
  using Large Language Models","This article explores the natural language generation capabilities of large
language models with application to the production of two types of learning
resources common in programming courses. Using OpenAI Codex as the large
language model, we create programming exercises (including sample solutions and
test cases) and code explanations, assessing these qualitatively and
quantitatively. Our results suggest that the majority of the automatically
generated content is both novel and sensible, and in some cases ready to use as
is. When creating exercises we find that it is remarkably easy to influence
both the programming concepts and the contextual themes they contain, simply by
supplying keywords as input to the model. Our analysis suggests that there is
significant value in massive generative machine learning models as a tool for
instructors, although there remains a need for some oversight to ensure the
quality of the generated content before it is delivered to students. We further
discuss the implications of OpenAI Codex and similar tools for introductory
programming education and highlight future research streams that have the
potential to improve the quality of the educational experience for both
teachers and students alike.",2022-06-03T11:00:43Z
10.5281/zenodo.8267597,http://arxiv.org/pdf/2309.03044v1.pdf,Method-Level Bug Severity Prediction using Source Code Metrics and LLMs,"In the past couple of decades, significant research efforts are devoted to
the prediction of software bugs. However, most existing work in this domain
treats all bugs the same, which is not the case in practice. It is important
for a defect prediction method to estimate the severity of the identified bugs
so that the higher-severity ones get immediate attention. In this study, we
investigate source code metrics, source code representation using large
language models (LLMs), and their combination in predicting bug severity labels
of two prominent datasets. We leverage several source metrics at method-level
granularity to train eight different machine-learning models. Our results
suggest that Decision Tree and Random Forest models outperform other models
regarding our several evaluation metrics. We then use the pre-trained CodeBERT
LLM to study the source code representations' effectiveness in predicting bug
severity. CodeBERT finetuning improves the bug severity prediction results
significantly in the range of 29%-140% for several evaluation metrics, compared
to the best classic prediction model on source code metric. Finally, we
integrate source code metrics into CodeBERT as an additional input, using our
two proposed architectures, which both enhance the CodeBERT model
effectiveness.",2023-09-06T14:38:07Z
,http://arxiv.org/pdf/2402.08699v2.pdf,Unsupervised Evaluation of Code LLMs with Round-Trip Correctness,"To evaluate code large language models (LLMs), research has relied on a few
small manually curated benchmarks, such as HumanEval and MBPP, which represent
a narrow part of the real-world software domains. In this work, we introduce
round-trip correctness (RTC) as an alternative evaluation method. RTC allows
Code LLM evaluation on a broader spectrum of real-world software domains
without the need for costly human curation. RTC rests on the idea that we can
ask a model to make a prediction (e.g., describe some code using natural
language), feed that prediction back (e.g., synthesize code from the predicted
description), and check if this round-trip leads to code that is semantically
equivalent to the original input. We show how to employ RTC to evaluate code
synthesis and editing. We find that RTC strongly correlates with model
performance on existing narrow-domain code synthesis benchmarks while allowing
us to expand to a much broader set of domains and tasks which was not
previously possible without costly human annotations.",2024-02-13T11:08:08Z
,http://arxiv.org/pdf/2404.06371v1.pdf,Model Generation from Requirements with LLMs: an Exploratory Study,"Complementing natural language (NL) requirements with graphical models can
improve stakeholders' communication and provide directions for system design.
However, creating models from requirements involves manual effort. The advent
of generative large language models (LLMs), ChatGPT being a notable example,
offers promising avenues for automated assistance in model generation. This
paper investigates the capability of ChatGPT to generate a specific type of
model, i.e., UML sequence diagrams, from NL requirements. We conduct a
qualitative study in which we examine the sequence diagrams generated by
ChatGPT for 28 requirements documents of various types and from different
domains. Observations from the analysis of the generated diagrams have
systematically been captured through evaluation logs, and categorized through
thematic analysis. Our results indicate that, although the models generally
conform to the standard and exhibit a reasonable level of understandability,
their completeness and correctness with respect to the specified requirements
often present challenges. This issue is particularly pronounced in the presence
of requirements smells, such as ambiguity and inconsistency. The insights
derived from this study can influence the practical utilization of LLMs in the
RE process, and open the door to novel RE-specific prompting strategies
targeting effective model generation.",2024-04-09T15:07:25Z
,http://arxiv.org/pdf/2406.04202v1.pdf,"Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language
  Model","With the development of large-scale Language Models (LLM), fine-tuning
pre-trained LLM has become a mainstream paradigm for solving downstream tasks
of natural language processing. However, training a language model in the legal
field requires a large number of legal documents so that the language model can
learn legal terminology and the particularity of the format of legal documents.
The typical NLP approaches usually rely on many manually annotated data sets
for training. However, in the legal field application, it is difficult to
obtain a large number of manually annotated data sets, which restricts the
typical method applied to the task of drafting legal documents. The
experimental results of this paper show that not only can we leverage a large
number of annotation-free legal documents without Chinese word segmentation to
fine-tune a large-scale language model, but more importantly, it can fine-tune
a pre-trained LLM on the local computer to achieve the generating legal
document drafts task, and at the same time achieve the protection of
information privacy and to improve information security issues.",2024-06-06T16:00:20Z
,http://arxiv.org/pdf/2406.12952v1.pdf,Code Agents are State of the Art Software Testers,"Rigorous software testing is crucial for developing and maintaining
high-quality code, making automated test generation a promising avenue for both
improving software quality and boosting the effectiveness of code generation
methods. However, while code generation with Large Language Models (LLMs) is an
extraordinarily active research area, test generation remains relatively
unexplored. We address this gap and investigate the capability of LLM-based
Code Agents for formalizing user issues into test cases. To this end, we
propose a novel benchmark based on popular GitHub repositories, containing
real-world issues, ground-truth patches, and golden tests. We find that LLMs
generally perform surprisingly well at generating relevant test cases with Code
Agents designed for code repair exceeding the performance of systems designed
specifically for test generation. Further, as test generation is a similar but
more structured task than code generation, it allows for a more fine-grained
analysis using fail-to-pass rate and coverage metrics, providing a dual metric
for analyzing systems designed for code repair. Finally, we find that generated
tests are an effective filter for proposed code fixes, doubling the precision
of SWE-Agent.",2024-06-18T14:54:37Z
,http://arxiv.org/pdf/2401.15843v1.pdf,APIGen: Generative API Method Recommendation,"Automatic API method recommendation is an essential task of code
intelligence, which aims to suggest suitable APIs for programming queries.
Existing approaches can be categorized into two primary groups: retrieval-based
and learning-based approaches. Although these approaches have achieved
remarkable success, they still come with notable limitations. The
retrieval-based approaches rely on the text representation capabilities of
embedding models, while the learning-based approaches require extensive
task-specific labeled data for training. To mitigate the limitations, we
propose APIGen, a generative API recommendation approach through enhanced
in-context learning (ICL). APIGen involves two main components: (1) Diverse
Examples Selection. APIGen searches for similar posts to the programming
queries from the lexical, syntactical, and semantic perspectives, providing
more informative examples for ICL. (2) Guided API Recommendation. APIGen
enables large language models (LLMs) to perform reasoning before generating API
recommendations, where the reasoning involves fine-grained matching between the
task intent behind the queries and the factual knowledge of the APIs. With the
reasoning process, APIGen makes recommended APIs better meet the programming
requirement of queries and also enhances the interpretability of results. We
compare APIGen with four existing approaches on two publicly available
benchmarks. Experiments show that APIGen outperforms the best baseline CLEAR by
105.8% in method-level API recommendation and 54.3% in class-level API
recommendation in terms of SuccessRate@1. Besides, APIGen achieves an average
49.87% increase compared to the zero-shot performance of popular LLMs such as
GPT-4 in method-level API recommendation regarding the SuccessRate@3 metric.",2024-01-29T02:35:42Z
,http://arxiv.org/pdf/2404.00756v1.pdf,Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,"Recognizing failures during task execution and implementing recovery
procedures is challenging in robotics. Traditional approaches rely on the
availability of extensive data or a tight set of constraints, while more recent
approaches leverage large language models (LLMs) to verify task steps and
replan accordingly. However, these methods often operate offline, necessitating
scene resets and incurring in high costs. This paper introduces Recover, a
neuro-symbolic framework for online failure identification and recovery. By
integrating ontologies, logical rules, and LLM-based planners, Recover exploits
symbolic information to enhance the ability of LLMs to generate recovery plans
and also to decrease the associated costs. In order to demonstrate the
capabilities of our method in a simulated kitchen environment, we introduce
OntoThor, an ontology describing the AI2Thor simulator setting. Empirical
evaluation shows that OntoThor's logical rules accurately detect all failures
in the analyzed tasks, and that Recover considerably outperforms, for both
failure detection and recovery, a baseline method reliant solely on LLMs.",2024-03-31T17:54:22Z
,http://arxiv.org/pdf/2406.09459v1.pdf,Ad Auctions for LLMs via Retrieval Augmented Generation,"In the field of computational advertising, the integration of ads into the
outputs of large language models (LLMs) presents an opportunity to support
these services without compromising content integrity. This paper introduces
novel auction mechanisms for ad allocation and pricing within the textual
outputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a
segment auction where an ad is probabilistically retrieved for each discourse
segment (paragraph, section, or entire output) according to its bid and
relevance, following the RAG framework, and priced according to competing bids.
We show that our auction maximizes logarithmic social welfare, a new notion of
welfare that balances allocation efficiency and fairness, and we characterize
the associated incentive-compatible pricing rule. These results are extended to
multi-ad allocation per segment. An empirical evaluation validates the
feasibility and effectiveness of our approach over several ad auction
scenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more
flexibility to allocate ads.",2024-06-12T22:05:51Z
,http://arxiv.org/pdf/2303.14310v1.pdf,GPT is becoming a Turing machine: Here are some ways to program it,"We demonstrate that, through appropriate prompting, GPT-3 family of models
can be triggered to perform iterative behaviours necessary to execute (rather
than just write or recall) programs that involve loops, including several
popular algorithms found in computer science curricula or software developer
interviews. We trigger execution and description of Iterations by Regimenting
Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong
repetitive structure in an example of an execution path of a target program for
one particular input, 2) Prompting with fragments of execution paths, and 3)
Explicitly forbidding (skipping) self-attention to parts of the generated text.
On a dynamic program execution, IRSA leads to larger accuracy gains than
replacing the model with the much more powerful GPT-4. IRSA has promising
applications in education, as the prompts and responses resemble student
assignments in data structures and algorithms classes. Our findings hold
implications for evaluating LLMs, which typically target the in-context
learning: We show that prompts that may not even cover one full task example
can trigger algorithmic behaviour, allowing solving problems previously thought
of as hard for LLMs, such as logical puzzles. Consequently, prompt design plays
an even more critical role in LLM performance than previously recognized.",2023-03-25T00:43:41Z
,http://arxiv.org/pdf/2312.16351v1.pdf,"LLMs with User-defined Prompts as Generic Data Operators for Reliable
  Data Processing","Data processing is one of the fundamental steps in machine learning pipelines
to ensure data quality. Majority of the applications consider the user-defined
function (UDF) design pattern for data processing in databases. Although the
UDF design pattern introduces flexibility, reusability and scalability, the
increasing demand on machine learning pipelines brings three new challenges to
this design pattern -- not low-code, not dependency-free and not
knowledge-aware. To address these challenges, we propose a new design pattern
that large language models (LLMs) could work as a generic data operator
(LLM-GDO) for reliable data cleansing, transformation and modeling with their
human-compatible performance. In the LLM-GDO design pattern, user-defined
prompts (UDPs) are used to represent the data processing logic rather than
implementations with a specific programming language. LLMs can be centrally
maintained so users don't have to manage the dependencies at the run-time.
Fine-tuning LLMs with domain-specific data could enhance the performance on the
domain-specific tasks which makes data processing knowledge-aware. We
illustrate these advantages with examples in different data processing tasks.
Furthermore, we summarize the challenges and opportunities introduced by LLMs
to provide a complete view of this design pattern for more discussions.",2023-12-26T23:08:38Z
,http://arxiv.org/pdf/2310.14211v2.pdf,"LUNA: A Model-Based Universal Analysis Framework for Large Language
  Models","Over the past decade, Artificial Intelligence (AI) has had great success
recently and is being used in a wide range of academic and industrial fields.
More recently, LLMs have made rapid advancements that have propelled AI to a
new level, enabling even more diverse applications and industrial domains with
intelligence, particularly in areas like software engineering and natural
language processing. Nevertheless, a number of emerging trustworthiness
concerns and issues exhibited in LLMs have already recently received much
attention, without properly solving which the widespread adoption of LLMs could
be greatly hindered in practice. The distinctive characteristics of LLMs, such
as the self-attention mechanism, extremely large model scale, and
autoregressive generation schema, differ from classic AI software based on CNNs
and RNNs and present new challenges for quality analysis. Up to the present, it
still lacks universal and systematic analysis techniques for LLMs despite the
urgent industrial demand. Towards bridging this gap, we initiate an early
exploratory study and propose a universal analysis framework for LLMs, LUNA,
designed to be general and extensible, to enable versatile analysis of LLMs
from multiple quality perspectives in a human-interpretable manner. In
particular, we first leverage the data from desired trustworthiness
perspectives to construct an abstract model as an auxiliary analysis asset,
which is empowered by various abstract model construction methods. To assess
the quality of the abstract model, we collect and define a number of evaluation
metrics, aiming at both abstract model level and the semantics level. Then, the
semantics, which is the degree of satisfaction of the LLM w.r.t. the
trustworthiness perspective, is bound to and enriches the abstract model with
semantics, which enables more detailed analysis applications for diverse
purposes.",2023-10-22T07:26:21Z
,http://arxiv.org/pdf/2311.18251v1.pdf,"Can Large Language Models Be Good Companions? An LLM-Based Eyewear
  System with Conversational Common Ground","Developing chatbots as personal companions has long been a goal of artificial
intelligence researchers. Recent advances in Large Language Models (LLMs) have
delivered a practical solution for endowing chatbots with anthropomorphic
language capabilities. However, it takes more than LLMs to enable chatbots that
can act as companions. Humans use their understanding of individual
personalities to drive conversations. Chatbots also require this capability to
enable human-like companionship. They should act based on personalized,
real-time, and time-evolving knowledge of their owner. We define such essential
knowledge as the \textit{common ground} between chatbots and their owners, and
we propose to build a common-ground-aware dialogue system from an LLM-based
module, named \textit{OS-1}, to enable chatbot companionship. Hosted by
eyewear, OS-1 can sense the visual and audio signals the user receives and
extract real-time contextual semantics. Those semantics are categorized and
recorded to formulate historical contexts from which the user's profile is
distilled and evolves over time, i.e., OS-1 gradually learns about its user.
OS-1 combines knowledge from real-time semantics, historical contexts, and
user-specific profiles to produce a common-ground-aware prompt input into the
LLM module. The LLM's output is converted to audio, spoken to the wearer when
appropriate.We conduct laboratory and in-field studies to assess OS-1's ability
to build common ground between the chatbot and its user. The technical
feasibility and capabilities of the system are also evaluated. OS-1, with its
common-ground awareness, can significantly improve user satisfaction and
potentially lead to downstream tasks such as personal emotional support and
assistance.",2023-11-30T04:59:34Z
,http://arxiv.org/pdf/2403.16354v1.pdf,ChatDBG: An AI-Powered Debugging Assistant,"This paper presents ChatDBG, the first AI-powered debugging assistant.
ChatDBG integrates large language models (LLMs) to significantly enhance the
capabilities and user-friendliness of conventional debuggers. ChatDBG lets
programmers engage in a collaborative dialogue with the debugger, allowing them
to pose complex questions about program state, perform root cause analysis for
crashes or assertion failures, and explore open-ended queries like ""why is x
null?"". To handle these queries, ChatDBG grants the LLM autonomy to take the
wheel and drive debugging by issuing commands to navigate through stacks and
inspect program state; it then reports its findings and yields back control to
the programmer. Our ChatDBG prototype integrates with standard debuggers
including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our
evaluation across a diverse set of code, including C/C++ code with known bugs
and a suite of Python code including standalone scripts and Jupyter notebooks,
demonstrates that ChatDBG can successfully analyze root causes, explain bugs,
and generate accurate fixes for a wide range of real-world errors. For the
Python programs, a single query led to an actionable bug fix 67% of the time;
one additional follow-up query increased the success rate to 85%. ChatDBG has
seen rapid uptake; it has already been downloaded nearly 30,000 times.",2024-03-25T01:12:57Z
,http://arxiv.org/pdf/2310.15127v2.pdf,"Open-Ended Instructable Embodied Agents with Memory-Augmented Large
  Language Models","Pre-trained and frozen large language models (LLMs) can effectively map
simple scene rearrangement instructions to programs over a robot's visuomotor
functions through appropriate few-shot example prompting. To parse open-domain
natural language and adapt to a user's idiosyncratic procedures, not known
during prompt engineering time, fixed prompts fall short. In this paper, we
introduce HELPER, an embodied agent equipped with an external memory of
language-program pairs that parses free-form human-robot dialogue into action
programs through retrieval-augmented LLM prompting: relevant memories are
retrieved based on the current dialogue, instruction, correction, or VLM
description, and used as in-context prompt examples for LLM querying. The
memory is expanded during deployment to include pairs of user's language and
action plans, to assist future inferences and personalize them to the user's
language and routines. HELPER sets a new state-of-the-art in the TEACh
benchmark in both Execution from Dialog History (EDH) and Trajectory from
Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for
TfD. Our models, code, and video results can be found in our project's website:
https://helper-agent-llm.github.io.",2023-10-23T17:31:55Z
,http://arxiv.org/pdf/2311.09835v4.pdf,"ML-Bench: Evaluating Large Language Models and Agents for Machine
  Learning Tasks on Repository-Level Code","Despite Large Language Models (LLMs) like GPT-4 achieving impressive results
in function-level code generation, they struggle with repository-scale code
understanding (e.g., coming up with the right arguments for calling routines),
requiring a deeper comprehension of complex file interactions. Also, recently,
people have developed LLM agents that attempt to interact with repository code
(e.g., compiling and evaluating its execution), prompting the need to evaluate
their performance. These gaps have motivated our development of ML-Bench, a
benchmark rooted in real-world programming applications that leverage existing
code repositories to perform tasks. Addressing the need for LLMs to interpret
long code contexts and translate instructions into precise, executable scripts,
ML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories,
challenging LLMs to accommodate user-specified arguments and documentation
intricacies effectively. To evaluate both LLMs and AI agents, two setups are
employed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a
predefined deployment environment, and ML-Agent-Bench for testing autonomous
agents in an end-to-end task execution within a Linux sandbox environment. Our
findings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%,
there remains significant scope for improvement, highlighted by issues such as
hallucinated outputs and difficulties with bash script generation. Notably, in
the more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate,
reflecting the efficacy of iterative action and feedback in complex task
resolution. Our code, dataset, and models are available at
https://github.com/gersteinlab/ML-bench.",2023-11-16T12:03:21Z
,http://arxiv.org/pdf/2403.11984v1.pdf,"Using Generative Text Models to Create Qualitative Codebooks for Student
  Evaluations of Teaching","Feedback is a critical aspect of improvement. Unfortunately, when there is a
lot of feedback from multiple sources, it can be difficult to distill the
information into actionable insights. Consider student evaluations of teaching
(SETs), which are important sources of feedback for educators. They can give
instructors insights into what worked during a semester. A collection of SETs
can also be useful to administrators as signals for courses or entire programs.
However, on a large scale as in high-enrollment courses or administrative
records over several years, the volume of SETs can render them difficult to
analyze. In this paper, we discuss a novel method for analyzing SETs using
natural language processing (NLP) and large language models (LLMs). We
demonstrate the method by applying it to a corpus of 5,000 SETs from a large
public university. We show that the method can be used to extract, embed,
cluster, and summarize the SETs to identify the themes they express. More
generally, this work illustrates how to use the combination of NLP techniques
and LLMs to generate a codebook for SETs. We conclude by discussing the
implications of this method for analyzing SETs and other types of student
writing in teaching and research settings.",2024-03-18T17:21:35Z
,http://arxiv.org/pdf/2303.16749v2.pdf,Improving Code Generation by Training with Natural Language Feedback,"The potential for pre-trained large language models (LLMs) to use natural
language feedback at inference time has been an exciting recent development. We
build upon this observation by formalizing an algorithm for learning from
natural language feedback at training time instead, which we call Imitation
learning from Language Feedback (ILF). ILF requires only a small amount of
human-written feedback during training and does not require the same feedback
at test time, making it both user-friendly and sample-efficient. We further
show that ILF can be seen as a form of minimizing the KL divergence to the
ground truth distribution and demonstrate a proof-of-concept on a neural
program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's
pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python
Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and
fine-tuning on repaired programs written by humans. Overall, our results
suggest that learning from human-written natural language feedback is both more
effective and sample-efficient than training exclusively on demonstrations for
improving an LLM's performance on code generation tasks.",2023-03-28T16:15:31Z
,http://arxiv.org/pdf/2402.11755v1.pdf,SPML: A DSL for Defending Language Models Against Prompt Attacks,"Large language models (LLMs) have profoundly transformed natural language
applications, with a growing reliance on instruction-based definitions for
designing chatbots. However, post-deployment the chatbot definitions are fixed
and are vulnerable to attacks by malicious users, emphasizing the need to
prevent unethical applications and financial losses. Existing studies explore
user prompts' impact on LLM-based chatbots, yet practical methods to contain
attacks on application-specific chatbots remain unexplored. This paper presents
System Prompt Meta Language (SPML), a domain-specific language for refining
prompts and monitoring the inputs to the LLM-based chatbots. SPML actively
checks attack prompts, ensuring user inputs align with chatbot definitions to
prevent malicious execution on the LLM backbone, optimizing costs. It also
streamlines chatbot definition crafting with programming language capabilities,
overcoming natural language design challenges. Additionally, we introduce a
groundbreaking benchmark with 1.8k system prompts and 20k user inputs, offering
the inaugural language and benchmark for chatbot definition evaluation.
Experiments across datasets demonstrate SPML's proficiency in understanding
attacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA. Our data
and codes are publicly available at: https://prompt-compiler.github.io/SPML/.",2024-02-19T00:53:48Z
,http://arxiv.org/pdf/2403.04811v1.pdf,"Quantifying Contamination in Evaluating Code Generation Capabilities of
  Language Models","While large language models have achieved remarkable performance on various
code generation benchmarks, there have been growing concerns regarding
potential contamination of these benchmarks as they may be leaked into
pretraining and finetuning data. While recent work has investigated
contamination in natural language generation and understanding tasks, there has
been less extensive research into how data contamination impacts the evaluation
of code generation, which is critical for understanding the robustness and
reliability of LLMs in programming contexts. In this work, we perform a
comprehensive study of data contamination of popular code generation
benchmarks, and precisely quantify their overlap with pretraining corpus
through both surface-level and semantic-level matching. In our experiments, we
show that there are substantial overlap between popular code generation
benchmarks and open training corpus, and models perform significantly better on
the subset of the benchmarks where similar solutions are seen during training.
We also conduct extensive analysis on the factors that affects model
memorization and generalization, such as model size, problem difficulty, and
question length. We release all resulting files from our matching pipeline for
future research.",2024-03-06T21:45:35Z
,http://arxiv.org/pdf/2406.11232v1.pdf,A Collaborative Data Analytics System with Recommender for Diverse Users,"This paper presents the SLEGO (Software-Lego) system, a collaborative
analytics platform that bridges the gap between experienced developers and
novice users using a cloud-based platform with modular, reusable microservices.
These microservices enable developers to share their analytical tools and
workflows, while a simple graphical user interface (GUI) allows novice users to
build comprehensive analytics pipelines without programming skills. Supported
by a knowledge base and a Large Language Model (LLM) powered recommendation
system, SLEGO enhances the selection and integration of microservices,
increasing the efficiency of analytics pipeline construction. Case studies in
finance and machine learning illustrate how SLEGO promotes the sharing and
assembly of modular microservices, significantly improving resource reusability
and team collaboration. The results highlight SLEGO's role in democratizing
data analytics by integrating modular design, knowledge bases, and
recommendation systems, fostering a more inclusive and efficient analytical
environment.",2024-06-17T05:59:13Z
,http://arxiv.org/pdf/2401.14196v2.pdf,"DeepSeek-Coder: When the Large Language Model Meets Programming -- The
  Rise of Code Intelligence","The rapid development of large language models has revolutionized code
intelligence in software development. However, the predominance of
closed-source models has restricted extensive research and development. To
address this, we introduce the DeepSeek-Coder series, a range of open-source
code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion
tokens. These models are pre-trained on a high-quality project-level code
corpus and employ a fill-in-the-blank task with a 16K window to enhance code
generation and infilling. Our extensive evaluations demonstrate that
DeepSeek-Coder not only achieves state-of-the-art performance among open-source
code models across multiple benchmarks but also surpasses existing
closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models
are under a permissive license that allows for both research and unrestricted
commercial use.",2024-01-25T14:17:53Z
,http://arxiv.org/pdf/2405.21047v1.pdf,Grammar-Aligned Decoding,"Large Language Models (LLMs) struggle with reliably generating highly
structured outputs, such as program code, mathematical formulas, or well-formed
markup. Constrained decoding approaches mitigate this problem by greedily
restricting what tokens an LLM can output at each step to guarantee that the
output matches a given constraint. Specifically, in grammar-constrained
decoding (GCD), the LLM's output must follow a given grammar. In this paper we
demonstrate that GCD techniques (and in general constrained decoding
techniques) can distort the LLM's distribution, leading to outputs that are
grammatical but appear with likelihoods that are not proportional to the ones
given by the LLM, and so ultimately are low-quality. We call the problem of
aligning sampling with a grammar constraint, grammar-aligned decoding (GAD),
and propose adaptive sampling with approximate expected futures (ASAp), a
decoding algorithm that guarantees the output to be grammatical while provably
producing outputs that match the conditional probability of the LLM's
distribution conditioned on the given grammar constraint. Our algorithm uses
prior sample outputs to soundly overapproximate the future grammaticality of
different output prefixes. Our evaluation on code generation and structured NLP
tasks shows how ASAp often produces outputs with higher likelihood (according
to the LLM's distribution) than existing GCD techniques, while still enforcing
the desired grammatical constraints.",2024-05-31T17:39:15Z
,http://arxiv.org/pdf/2403.12999v1.pdf,"Prompt Selection and Augmentation for Few Examples Code Generation in
  Large Language Model and its Application in Robotics Control","Few-shot prompting and step-by-step reasoning have enhanced the capabilities
of Large Language Models (LLMs) in tackling complex tasks including code
generation. In this paper, we introduce a prompt selection and augmentation
algorithm aimed at improving mathematical reasoning and robot arm operations.
Our approach incorporates a multi-stage example augmentation scheme combined
with an example selection scheme. This algorithm improves LLM performance by
selecting a set of examples that increase diversity, minimize redundancy, and
increase relevance to the question. When combined with the Program-of-Thought
prompting, our algorithm demonstrates an improvement in performance on the
GSM8K and SVAMP benchmarks, with increases of 0.3% and 1.1% respectively.
Furthermore, in simulated tabletop environments, our algorithm surpasses the
Code-as-Policies approach by achieving a 3.4% increase in successful task
completions and a decrease of over 70% in the number of examples used. Its
ability to discard examples that contribute little to solving the problem
reduces the inferencing time of an LLM-powered robotics system. This algorithm
also offers important benefits for industrial process automation by
streamlining the development and deployment process, reducing manual
programming effort, and enhancing code reusability.",2024-03-11T04:13:29Z
,http://arxiv.org/pdf/2405.15383v1.pdf,"Generating Code World Models with Large Language Models Guided by Monte
  Carlo Tree Search","In this work we consider Code World Models, world models generated by a Large
Language Model (LLM) in the form of Python code for model-based Reinforcement
Learning (RL). Calling code instead of LLMs for planning has the advantages of
being precise, reliable, interpretable, and extremely efficient. However,
writing appropriate Code World Models requires the ability to understand
complex instructions, to generate exact code with non-trivial logic and to
self-debug a long program with feedback from unit tests and environment
trajectories. To address these challenges, we propose Generate, Improve and Fix
with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for
LLMs. To test our approach, we introduce the Code World Models Benchmark
(CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse
RL environments paired with corresponding textual descriptions and curated
trajectories. GIF-MCTS surpasses all baselines on the CWMB and two other
benchmarks, and we show that the Code World Models synthesized with it can be
successfully used for planning, resulting in model-based RL agents with greatly
improved sample efficiency and inference speed.",2024-05-24T09:31:26Z
,http://arxiv.org/pdf/2308.03784v2.pdf,"Improving Requirements Completeness: Automated Assistance through Large
  Language Models","Natural language (NL) is arguably the most prevalent medium for expressing
systems and software requirements. Detecting incompleteness in NL requirements
is a major challenge. One approach to identify incompleteness is to compare
requirements with external sources. Given the rise of large language models
(LLMs), an interesting question arises: Are LLMs useful external sources of
knowledge for detecting potential incompleteness in NL requirements? This
article explores this question by utilizing BERT. Specifically, we employ
BERT's masked language model (MLM) to generate contextualized predictions for
filling masked slots in requirements. To simulate incompleteness, we withhold
content from the requirements and assess BERT's ability to predict terminology
that is present in the withheld content but absent in the disclosed content.
BERT can produce multiple predictions per mask. Our first contribution is
determining the optimal number of predictions per mask, striking a balance
between effectively identifying omissions in requirements and mitigating noise
present in the predictions. Our second contribution involves designing a
machine learning-based filter to post-process BERT's predictions and further
reduce noise. We conduct an empirical evaluation using 40 requirements
specifications from the PURE dataset. Our findings indicate that: (1) BERT's
predictions effectively highlight terminology that is missing from
requirements, (2) BERT outperforms simpler baselines in identifying relevant
yet missing terminology, and (3) our filter significantly reduces noise in the
predictions, enhancing BERT's effectiveness as a tool for completeness checking
of requirements.",2023-08-03T19:49:18Z
,http://arxiv.org/pdf/2401.04514v2.pdf,"Rewriting the Code: A Simple Method for Large Language Model Augmented
  Code Search","In code search, the Generation-Augmented Retrieval (GAR) framework, which
generates exemplar code snippets to augment queries, has emerged as a promising
strategy to address the principal challenge of modality misalignment between
code snippets and natural language queries, particularly with the demonstrated
code generation capabilities of Large Language Models (LLMs). Nevertheless, our
preliminary investigations indicate that the improvements conferred by such an
LLM-augmented framework are somewhat constrained. This limitation could
potentially be ascribed to the fact that the generated codes, albeit
functionally accurate, frequently display a pronounced stylistic deviation from
the ground truth code in the codebase. In this paper, we extend the
foundational GAR framework and propose a simple yet effective method that
additionally Rewrites the Code (ReCo) within the codebase for style
normalization. Experimental results demonstrate that ReCo significantly boosts
retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%),
and fine-tuned dense (up to 23.6%) retrieval settings in diverse search
scenarios. To further elucidate the advantages of ReCo and stimulate research
in code style normalization, we introduce Code Style Similarity, the first
metric tailored to quantify stylistic similarities in code. Notably, our
empirical findings reveal the inadequacy of existing metrics in capturing
stylistic nuances. The source code and data are available at
\url{https://github.com/Alex-HaochenLi/ReCo}.",2024-01-09T12:12:50Z
,http://arxiv.org/pdf/2309.05833v3.pdf,"PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation
  with GPT-4 in Cloud Incident Root Cause Analysis","Major cloud providers have employed advanced AI-based solutions like large
language models to aid humans in identifying the root causes of cloud
incidents. Despite the growing prevalence of AI-driven assistants in the root
cause analysis process, their effectiveness in assisting on-call engineers is
constrained by low accuracy due to the intrinsic difficulty of the task, a
propensity for LLM-based approaches to hallucinate, and difficulties in
distinguishing these well-disguised hallucinations. To address this challenge,
we propose to perform confidence estimation for the predictions to help on-call
engineers make decisions on whether to adopt the model prediction. Considering
the black-box nature of many LLM-based root cause predictors, fine-tuning or
temperature-scaling-based approaches are inapplicable. We therefore design an
innovative confidence estimation framework based on prompting
retrieval-augmented large language models (LLMs) that demand a minimal amount
of information from the root cause predictor. This approach consists of two
scoring phases: the LLM-based confidence estimator first evaluates its
confidence in making judgments in the face of the current incident that
reflects its ``grounded-ness"" level in reference data, then rates the root
cause prediction based on historical references. An optimization step combines
these two scores for a final confidence assignment. We show that our method is
able to produce calibrated confidence estimates for predicted root causes,
validate the usefulness of retrieved historical data and the prompting strategy
as well as the generalizability across different root cause prediction models.
Our study takes an important move towards reliably and effectively embedding
LLMs into cloud incident management systems.",2023-09-11T21:24:00Z
,http://arxiv.org/pdf/2401.01600v1.pdf,PLLaMa: An Open-source Large Language Model for Plant Science,"Large Language Models (LLMs) have exhibited remarkable capabilities in
understanding and interacting with natural language across various sectors.
However, their effectiveness is limited in specialized areas requiring high
accuracy, such as plant science, due to a lack of specific expertise in these
fields. This paper introduces PLLaMa, an open-source language model that
evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising
more than 1.5 million scholarly articles in plant science. This development
significantly enriches PLLaMa with extensive knowledge and proficiency in plant
and agricultural sciences. Our initial tests, involving specific datasets
related to plants and agriculture, show that PLLaMa substantially improves its
understanding of plant science-related topics. Moreover, we have formed an
international panel of professionals, including plant scientists, agricultural
engineers, and plant breeders. This team plays a crucial role in verifying the
accuracy of PLLaMa's responses to various academic inquiries, ensuring its
effective and reliable application in the field. To support further research
and development, we have made the model's checkpoints and source codes
accessible to the scientific community. These resources are available for
download at \url{https://github.com/Xianjun-Yang/PLLaMa}.",2024-01-03T08:06:26Z
,http://arxiv.org/pdf/2405.06671v2.pdf,"Parameter-Efficient Instruction Tuning of Large Language Models For
  Extreme Financial Numeral Labelling","We study the problem of automatically annotating relevant numerals (GAAP
metrics) occurring in the financial documents with their corresponding XBRL
tags. Different from prior works, we investigate the feasibility of solving
this extreme classification problem using a generative paradigm through
instruction tuning of Large Language Models (LLMs). To this end, we leverage
metric metadata information to frame our target outputs while proposing a
parameter efficient solution for the task using LoRA. We perform experiments on
two recently released financial numeric labeling datasets. Our proposed model,
FLAN-FinXC, achieves new state-of-the-art performances on both the datasets,
outperforming several strong baselines. We explain the better scores of our
proposed model by demonstrating its capability for zero-shot as well as the
least frequently occurring tags. Also, even when we fail to predict the XBRL
tags correctly, our generated output has substantial overlap with the
ground-truth in majority of the cases.",2024-05-03T16:41:36Z
,http://arxiv.org/pdf/2403.08291v2.pdf,CleanAgent: Automating Data Standardization with LLM-based Agents,"Data standardization is a crucial part in data science life cycle. While
tools like Pandas offer robust functionalities, their complexity and the manual
effort required for customizing code to diverse column types pose significant
challenges. Although large language models (LLMs) like ChatGPT have shown
promise in automating this process through natural language understanding and
code generation, it still demands expert-level programming knowledge and
continuous interaction for prompt refinement. To solve these challenges, our
key idea is to propose a Python library with declarative, unified APIs for
standardizing column types, simplifying the code generation of LLM with concise
API calls. We first propose Dataprep.Clean which is written as a component of
the Dataprep Library, offers a significant reduction in complexity by enabling
the standardization of specific column types with a single line of code. Then
we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based
agents to automate the data standardization process. With CleanAgent, data
scientists need only provide their requirements once, allowing for a
hands-free, automatic standardization process.",2024-03-13T06:54:15Z
,http://arxiv.org/pdf/2404.19094v1.pdf,"In-Context Symbolic Regression: Leveraging Language Models for Function
  Discovery","Symbolic Regression (SR) is a task which aims to extract the mathematical
expression underlying a set of empirical observations. Transformer-based
methods trained on SR datasets detain the current state-of-the-art in this
task, while the application of Large Language Models (LLMs) to SR remains
unexplored. This work investigates the integration of pre-trained LLMs into the
SR pipeline, utilizing an approach that iteratively refines a functional form
based on the prediction error it achieves on the observation set, until it
reaches convergence. Our method leverages LLMs to propose an initial set of
possible functions based on the observations, exploiting their strong
pre-training prior. These functions are then iteratively refined by the model
itself and by an external optimizer for their coefficients. The process is
repeated until the results are satisfactory. We then analyze Vision-Language
Models in this context, exploring the inclusion of plots as visual inputs to
aid the optimization process. Our findings reveal that LLMs are able to
successfully recover good symbolic equations that fit the given data,
outperforming SR baselines based on Genetic Programming, with the addition of
images in the input showing promising results for the most complex benchmarks.",2024-04-29T20:19:25Z
,http://arxiv.org/pdf/2405.02326v1.pdf,Evaluating LLMs for Hardware Design and Test,"Large Language Models (LLMs) have demonstrated capabilities for producing
code in Hardware Description Languages (HDLs). However, most of the focus
remains on their abilities to write functional code, not test code. The
hardware design process consists of both design and test, and so eschewing
validation and verification leaves considerable potential benefit unexplored,
given that a design and test framework may allow for progress towards full
automation of the digital design pipeline. In this work, we perform one of the
first studies exploring how a LLM can both design and test hardware modules
from provided specifications. Using a suite of 8 representative benchmarks, we
examined the capabilities and limitations of the state-of-the-art
conversational LLMs when producing Verilog for functional and verification
purposes. We taped out the benchmarks on a Skywater 130nm shuttle and received
the functional chip.",2024-04-23T18:55:49Z
,http://arxiv.org/pdf/2307.04693v1.pdf,COMEX: A Tool for Generating Customized Source Code Representations,"Learning effective representations of source code is critical for any Machine
Learning for Software Engineering (ML4SE) system. Inspired by natural language
processing, large language models (LLMs) like Codex and CodeGen treat code as
generic sequences of text and are trained on huge corpora of code data,
achieving state of the art performance on several software engineering (SE)
tasks. However, valid source code, unlike natural language, follows a strict
structure and pattern governed by the underlying grammar of the programming
language. Current LLMs do not exploit this property of the source code as they
treat code like a sequence of tokens and overlook key structural and semantic
properties of code that can be extracted from code-views like the Control Flow
Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc.
Unfortunately, the process of generating and integrating code-views for every
programming language is cumbersome and time consuming. To overcome this
barrier, we propose our tool COMEX - a framework that allows researchers and
developers to create and combine multiple code-views which can be used by
machine learning (ML) models for various SE tasks. Some salient features of our
tool are: (i) it works directly on source code (which need not be compilable),
(ii) it currently supports Java and C#, (iii) it can analyze both method-level
snippets and program-level snippets by using both intra-procedural and
inter-procedural analysis, and (iv) it is easily extendable to other languages
as it is built on tree-sitter - a widely used incremental parser that supports
over 40 languages. We believe this easy-to-use code-view generation and
customization tool will give impetus to research in source code representation
learning methods and ML4SE.
  Tool: https://pypi.org/project/comex - GitHub:
https://github.com/IBM/tree-sitter-codeviews - Demo:
https://youtu.be/GER6U87FVbU",2023-07-10T16:46:34Z
,http://arxiv.org/pdf/2301.10095v2.pdf,"Large Language Models as Fiduciaries: A Case Study Toward Robustly
  Communicating With Artificial Intelligence Through Legal Standards","Artificial Intelligence (AI) is taking on increasingly autonomous roles,
e.g., browsing the web as a research assistant and managing money. But
specifying goals and restrictions for AI behavior is difficult. Similar to how
parties to a legal contract cannot foresee every potential ""if-then""
contingency of their future relationship, we cannot specify desired AI behavior
for all circumstances. Legal standards facilitate robust communication of
inherently vague and underspecified goals. Instructions (in the case of
language models, ""prompts"") that employ legal standards will allow AI agents to
develop shared understandings of the spirit of a directive that generalize
expectations regarding acceptable actions to take in unspecified states of the
world. Standards have built-in context that is lacking from other goal
specification languages, such as plain language and programming languages.
Through an empirical study on thousands of evaluation labels we constructed
from U.S. court opinions, we demonstrate that large language models (LLMs) are
beginning to exhibit an ""understanding"" of one of the most relevant legal
standards for AI agents: fiduciary obligations. Performance comparisons across
models suggest that, as LLMs continue to exhibit improved core capabilities,
their legal standards understanding will also continue to improve. OpenAI's
latest LLM has 78% accuracy on our data, their previous release has 73%
accuracy, and a model from their 2020 GPT-3 paper has 27% accuracy (worse than
random). Our research is an initial step toward a framework for evaluating AI
understanding of legal standards more broadly, and for conducting reinforcement
learning with legal feedback (RLLF).",2023-01-24T16:03:20Z
,http://arxiv.org/pdf/2303.06865v2.pdf,"FlexGen: High-Throughput Generative Inference of Large Language Models
  with a Single GPU","The high computational and memory requirements of large language model (LLM)
inference make it feasible only with multiple high-end accelerators. Motivated
by the emerging demand for latency-insensitive tasks with batched processing,
this paper initiates the study of high-throughput LLM inference using limited
resources, such as a single commodity GPU. We present FlexGen, a
high-throughput generation engine for running LLMs with limited GPU memory.
FlexGen can be flexibly configured under various hardware resource constraints
by aggregating memory and computation from the GPU, CPU, and disk. By solving a
linear programming problem, it searches for efficient patterns to store and
access tensors. FlexGen further compresses the weights and the attention cache
to 4 bits with negligible accuracy loss. These techniques enable FlexGen to
have a larger space of batch size choices and thus significantly increase
maximum throughput. As a result, when running OPT-175B on a single 16GB GPU,
FlexGen achieves significantly higher throughput compared to state-of-the-art
offloading systems, reaching a generation throughput of 1 token/s for the first
time with an effective batch size of 144. On the HELM benchmark, FlexGen can
benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21
hours. The code is available at https://github.com/FMInference/FlexGen",2023-03-13T05:19:28Z
,http://arxiv.org/pdf/2402.05359v5.pdf,"Prompting with Divide-and-Conquer Program Makes Large Language Models
  Discerning to Hallucination and Deception","Foundation models, such as Large language Models (LLMs), have attracted
significant amount of interest due to their large number of applications.
However, when handling tasks involving repetitive sub-tasks and/or deceptive
contents, such as arithmetic calculation and article-level fake news detection,
simple instructional prompts suffer from inaccurate responses. Existing works
show that more complicated prompting strategies, such as Chain-of-Thoughts and
Least-to-Most, can unlock LLM's powerful capacity in diverse areas. Recent
researches reveal that simple divide-and-conquer prompting strategy, i.e.
simply dividing the input sequence to multiple sub-inputs, can also
substantially improve LLM's performance in some specific tasks such as
misinformation detection. In this paper, we aim at examining the utility of
divide-and-conquer prompting strategy and answer on which kind of tasks this
strategy gets advantages. Specifically, we provide a theoretic analysis to
divide-and-conquer prompting strategy and help us identify the specific tasks
where DaC prompting can bring performance boost with theoretic guarantee. We
then present two cases (large integer arithmetic and fact verification) where
experimental results aligns with our theoretic analysis.",2024-02-08T02:37:30Z
,http://arxiv.org/pdf/2406.10269v1.pdf,Markov Constraint as Large Language Model Surrogate,"This paper presents NgramMarkov, a variant of the Markov constraints. It is
dedicated to text generation in constraint programming (CP). It involves a set
of n-grams (i.e., sequence of n words) associated with probabilities given by a
large language model (LLM). It limits the product of the probabilities of the
n-gram of a sentence. The propagator of this constraint can be seen as an
extension of the ElementaryMarkov constraint propagator, incorporating the LLM
distribution instead of the maximum likelihood estimation of n-grams. It uses a
gliding threshold, i.e., it rejects n-grams whose local probabilities are too
low, to guarantee balanced solutions. It can also be combined with a
""look-ahead"" approach to remove n-grams that are very unlikely to lead to
acceptable sentences for a fixed-length horizon. This idea is based on the
MDDMarkovProcess constraint propagator, but without explicitly using an MDD
(Multi-Valued Decision Diagram). The experimental results show that the
generated text is valued in a similar way to the LLM perplexity function. Using
this new constraint dramatically reduces the number of candidate sentences
produced, improves computation times, and allows larger corpora or smaller
n-grams to be used. A real-world problem has been solved for the first time
using 4-grams instead of 5-grams.",2024-06-11T16:09:53Z
,http://arxiv.org/pdf/2308.02955v2.pdf,An Empirical Study of AI-based Smart Contract Creation,"The introduction of large language models (LLMs) like ChatGPT and Google
Palm2 for smart contract generation seems to be the first well-established
instance of an AI pair programmer. LLMs have access to a large number of
open-source smart contracts, enabling them to utilize more extensive code in
Solidity than other code generation tools. Although the initial and informal
assessments of LLMs for smart contract generation are promising, a systematic
evaluation is needed to explore the limits and benefits of these models. The
main objective of this study is to assess the quality of generated code
provided by LLMs for smart contracts. We also aim to evaluate the impact of the
quality and variety of input parameters fed to LLMs. To achieve this aim, we
created an experimental setup for evaluating the generated code in terms of
validity, correctness, and efficiency. Our study finds crucial evidence of
security bugs getting introduced in the generated smart contracts as well as
the overall quality and correctness of the code getting impacted. However, we
also identified the areas where it can be improved. The paper also proposes
several potential research directions to improve the process, quality and
safety of generated smart contract codes.",2023-08-05T21:38:57Z
,http://arxiv.org/pdf/2312.17294v1.pdf,GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension,"While Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated
exceptional proficiency in natural language processing, their efficacy in
addressing complex, multifaceted tasks remains limited. A growing area of
research focuses on LLM-based agents equipped with external tools capable of
performing diverse tasks. However, existing LLM-based agents only support a
limited set of tools which is unable to cover a diverse range of user queries,
especially for those involving expertise domains. It remains a challenge for
LLM-based agents to extend their tools autonomously when confronted with
various user queries. As GitHub has hosted a multitude of repositories which
can be seen as a good resource for tools, a promising solution is that
LLM-based agents can autonomously integrate the repositories in GitHub
according to the user queries to extend their tool set. In this paper, we
introduce GitAgent, an agent capable of achieving the autonomous tool extension
from GitHub. GitAgent follows a four-phase procedure to incorporate
repositories and it can learn human experience by resorting to GitHub
Issues/PRs to solve problems encountered during the procedure. Experimental
evaluation involving 30 user queries demonstrates GitAgent's effectiveness,
achieving a 69.4% success rate on average.",2023-12-28T15:47:30Z
,http://arxiv.org/pdf/2403.16362v1.pdf,AgentFL: Scaling LLM-based Fault Localization to Project-Level Context,"Fault Localization (FL) is an essential step during the debugging process.
With the strong capabilities of code comprehension, the recent Large Language
Models (LLMs) have demonstrated promising performance in diagnosing bugs in the
code. Nevertheless, due to LLMs' limited performance in handling long contexts,
existing LLM-based fault localization remains on localizing bugs within a small
code scope (i.e., a method or a class), which struggles to diagnose bugs for a
large code scope (i.e., an entire software system). To address the limitation,
this paper presents AgentFL, a multi-agent system based on ChatGPT for
automated fault localization. By simulating the behavior of a human developer,
AgentFL models the FL task as a three-step process, which involves
comprehension, navigation, and confirmation. Within each step, AgentFL hires
agents with diversified expertise, each of which utilizes different tools to
handle specific tasks. Particularly, we adopt a series of auxiliary strategies
such as Test Behavior Tracking, Document-Guided Search, and Multi-Round
Dialogue to overcome the challenges in each step. The evaluation on the widely
used Defects4J-V1.2.0 benchmark shows that AgentFL can localize 157 out of 395
bugs within Top-1, which outperforms the other LLM-based approaches and
exhibits complementarity to the state-of-the-art learning-based techniques.
Additionally, we confirm the indispensability of the components in AgentFL with
the ablation study and demonstrate the usability of AgentFL through a user
study. Finally, the cost analysis shows that AgentFL spends an average of only
0.074 dollars and 97 seconds for a single bug.",2024-03-25T01:58:19Z
,http://arxiv.org/pdf/2405.02580v1.pdf,"PropertyGPT: LLM-driven Formal Verification of Smart Contracts through
  Retrieval-Augmented Property Generation","With recent advances in large language models (LLMs), this paper explores the
potential of leveraging state-of-the-art LLMs, such as GPT-4, to transfer
existing human-written properties (e.g., those from Certora auditing reports)
and automatically generate customized properties for unknown code. To this end,
we embed existing properties into a vector database and retrieve a reference
property for LLM-based in-context learning to generate a new prop- erty for a
given code. While this basic process is relatively straight- forward, ensuring
that the generated properties are (i) compilable, (ii) appropriate, and (iii)
runtime-verifiable presents challenges. To address (i), we use the compilation
and static analysis feedback as an external oracle to guide LLMs in iteratively
revising the generated properties. For (ii), we consider multiple dimensions of
similarity to rank the properties and employ a weighted algorithm to identify
the top-K properties as the final result. For (iii), we design a dedicated
prover to formally verify the correctness of the generated prop- erties. We
have implemented these strategies into a novel system called PropertyGPT, with
623 human-written properties collected from 23 Certora projects. Our
experiments show that PropertyGPT can generate comprehensive and high-quality
properties, achieving an 80% recall compared to the ground truth. It
successfully detected 26 CVEs/attack incidents out of 37 tested and also
uncovered 12 zero-day vulnerabilities, resulting in $8,256 bug bounty rewards.",2024-05-04T06:28:27Z
,http://arxiv.org/pdf/2403.14743v2.pdf,"VURF: A General-purpose Reasoning and Self-refinement Framework for
  Video Understanding","Recent studies have demonstrated the effectiveness of Large Language Models
(LLMs) as reasoning modules that can deconstruct complex tasks into more
manageable sub-tasks, particularly when applied to visual reasoning tasks for
images. In contrast, this paper introduces a Video Understanding and Reasoning
Framework (VURF) based on the reasoning power of LLMs. Ours is a novel approach
to extend the utility of LLMs in the context of video tasks, leveraging their
capacity to generalize from minimal input and output demonstrations within a
contextual framework. By presenting LLMs with pairs of instructions and their
corresponding high-level programs, we harness their contextual learning
capabilities to generate executable visual programs for video understanding. To
enhance program's accuracy and robustness, we implement two important
strategies. Firstly, we employ a feedback-generation approach, powered by
GPT-3.5, to rectify errors in programs utilizing unsupported functions.
Secondly, taking motivation from recent works on self refinement of LLM
outputs, we introduce an iterative procedure for improving the quality of the
in-context examples by aligning the initial outputs to the outputs that would
have been generated had the LLM not been bound by the structure of the
in-context examples. Our results on several video-specific tasks, including
visual QA, video anticipation, pose estimation and multi-video QA illustrate
the efficacy of these enhancements in improving the performance of visual
programming approaches for video tasks.",2024-03-21T18:00:00Z
,http://arxiv.org/pdf/2310.19791v4.pdf,"LILO: Learning Interpretable Libraries by Compressing and Documenting
  Code","While large language models (LLMs) now excel at code generation, a key aspect
of software development is the art of refactoring: consolidating code into
libraries of reusable and readable programs. In this paper, we introduce LILO,
a neurosymbolic framework that iteratively synthesizes, compresses, and
documents code to build libraries tailored to particular problem domains. LILO
combines LLM-guided program synthesis with recent algorithmic advances in
automated refactoring from Stitch: a symbolic compression system that
efficiently identifies optimal lambda abstractions across large code corpora.
To make these abstractions interpretable, we introduce an auto-documentation
(AutoDoc) procedure that infers natural language names and docstrings based on
contextual examples of usage. In addition to improving human readability, we
find that AutoDoc boosts performance by helping LILO's synthesizer to interpret
and deploy learned abstractions. We evaluate LILO on three inductive program
synthesis benchmarks for string editing, scene reasoning, and graphics
composition. Compared to existing neural and symbolic methods - including the
state-of-the-art library learning algorithm DreamCoder - LILO solves more
complex tasks and learns richer libraries that are grounded in linguistic
knowledge.",2023-10-30T17:55:02Z
,http://arxiv.org/pdf/2310.16042v2.pdf,"WebWISE: Web Interface Control and Sequential Exploration with Large
  Language Models","The paper investigates using a Large Language Model (LLM) to automatically
perform web software tasks using click, scroll, and text input operations.
Previous approaches, such as reinforcement learning (RL) or imitation learning,
are inefficient to train and task-specific. Our method uses filtered Document
Object Model (DOM) elements as observations and performs tasks step-by-step,
sequentially generating small programs based on the current observations. We
use in-context learning, either benefiting from a single manually provided
example, or an automatically generated example based on a successful zero-shot
trial. We evaluate the proposed method on the MiniWob++ benchmark. With only
one in-context example, our WebWISE method achieves similar or better
performance than other methods that require many demonstrations or trials.",2023-10-24T17:57:03Z
,http://arxiv.org/pdf/2303.02206v2.pdf,"Domain Specific Question Answering Over Knowledge Graphs Using Logical
  Programming and Large Language Models","Answering questions over domain-specific graphs requires a tailored approach
due to the limited number of relations and the specific nature of the domain.
Our approach integrates classic logical programming languages into large
language models (LLMs), enabling the utilization of logical reasoning
capabilities to tackle the KGQA task. By representing the questions as Prolog
queries, which are readable and near close to natural language in
representation, we facilitate the generation of programmatically derived
answers. To validate the effectiveness of our approach, we evaluate it using a
well-known benchmark dataset, MetaQA. Our experimental results demonstrate that
our method achieves accurate identification of correct answer entities for all
test questions, even when trained on a small fraction of annotated data.
Overall, our work presents a promising approach to addressing question
answering over domain-specific graphs, offering an explainable and robust
solution by incorporating logical programming languages.",2023-03-03T20:35:38Z
,http://arxiv.org/pdf/2405.05135v1.pdf,"Lessons from the Use of Natural Language Inference (NLI) in Requirements
  Engineering Tasks","We investigate the use of Natural Language Inference (NLI) in automating
requirements engineering tasks. In particular, we focus on three tasks:
requirements classification, identification of requirements specification
defects, and detection of conflicts in stakeholders' requirements. While
previous research has demonstrated significant benefit in using NLI as a
universal method for a broad spectrum of natural language processing tasks,
these advantages have not been investigated within the context of software
requirements engineering. Therefore, we design experiments to evaluate the use
of NLI in requirements analysis. We compare the performance of NLI with a
spectrum of approaches, including prompt-based models, conventional transfer
learning, Large Language Models (LLMs)-powered chatbot models, and
probabilistic models. Through experiments conducted under various learning
settings including conventional learning and zero-shot, we demonstrate
conclusively that our NLI method surpasses classical NLP methods as well as
other LLMs-based and chatbot models in the analysis of requirements
specifications. Additionally, we share lessons learned characterizing the
learning settings that make NLI a suitable approach for automating requirements
engineering tasks.",2024-04-24T20:26:48Z
,http://arxiv.org/pdf/2310.01361v2.pdf,GenSim: Generating Robotic Simulation Tasks via Large Language Models,"Collecting large amounts of real-world interaction data to train general
robotic policies is often prohibitively expensive, thus motivating the use of
simulation data. However, existing methods for data generation have generally
focused on scene-level diversity (e.g., object instances and poses) rather than
task-level diversity, due to the human effort required to come up with and
verify novel tasks. This has made it challenging for policies trained on
simulation data to demonstrate significant task-level generalization. In this
paper, we propose to automatically generate rich simulation environments and
expert demonstrations by exploiting a large language models' (LLM) grounding
and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed
generation, wherein a target task is given to the LLM and the LLM proposes a
task curriculum to solve the target task, and exploratory generation, wherein
the LLM bootstraps from previous tasks and iteratively proposes novel tasks
that would be helpful in solving more complex tasks. We use GPT4 to expand the
existing benchmark by ten times to over 100 tasks, on which we conduct
supervised finetuning and evaluate several LLMs including finetuned GPTs and
Code Llama on code generation for robotic simulation tasks. Furthermore, we
observe that LLMs-generated simulation programs can enhance task-level
generalization significantly when used for multitask policy training. We
further find that with minimal sim-to-real adaptation, the multitask policies
pretrained on GPT4-generated simulation tasks exhibit stronger transfer to
unseen long-horizon tasks in the real world and outperform baselines by 25%.
See the project website (https://liruiw.github.io/gensim) for code, demos, and
videos.",2023-10-02T17:23:48Z
,http://arxiv.org/pdf/2307.12488v4.pdf,"A Case Study of Large Language Models (ChatGPT and CodeBERT) for
  Security-Oriented Code Analysis","LLMs can be used on code analysis tasks like code review, vulnerabilities
analysis and etc. However, the strengths and limitations of adopting these LLMs
to the code analysis are still unclear. In this paper, we delve into LLMs'
capabilities in security-oriented program analysis, considering perspectives
from both attackers and security analysts. We focus on two representative LLMs,
ChatGPT and CodeBert, and evaluate their performance in solving typical
analytic tasks with varying levels of difficulty. Our study demonstrates the
LLM's efficiency in learning high-level semantics from code, positioning
ChatGPT as a potential asset in security-oriented contexts. However, it is
essential to acknowledge certain limitations, such as the heavy reliance on
well-defined variable and function names, making them unable to learn from
anonymized code. For example, the performance of these LLMs heavily relies on
the well-defined variable and function names, therefore, will not be able to
learn anonymized code. We believe that the concerns raised in this case study
deserve in-depth investigation in the future.",2023-07-24T02:38:24Z
,http://arxiv.org/pdf/2403.04449v1.pdf,Feedback-Generation for Programming Exercises With GPT-4,"Ever since Large Language Models (LLMs) and related applications have become
broadly available, several studies investigated their potential for assisting
educators and supporting students in higher education. LLMs such as Codex,
GPT-3.5, and GPT 4 have shown promising results in the context of large
programming courses, where students can benefit from feedback and hints if
provided timely and at scale. This paper explores the quality of GPT-4 Turbo's
generated output for prompts containing both the programming task specification
and a student's submission as input. Two assignments from an introductory
programming course were selected, and GPT-4 was asked to generate feedback for
55 randomly chosen, authentic student programming submissions. The output was
qualitatively analyzed regarding correctness, personalization, fault
localization, and other features identified in the material. Compared to prior
work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For
example, the output is more structured and consistent. GPT-4 Turbo can also
accurately identify invalid casing in student programs' output. In some cases,
the feedback also includes the output of the student program. At the same time,
inconsistent feedback was noted such as stating that the submission is correct
but an error needs to be fixed. The present work increases our understanding of
LLMs' potential, limitations, and how to integrate them into e-assessment
systems, pedagogical scenarios, and instructing students who are using
applications based on GPT-4.",2024-03-07T12:37:52Z
,http://arxiv.org/pdf/2312.00055v1.pdf,LEAP: LLM-Generation of Egocentric Action Programs,"We introduce LEAP (illustrated in Figure 1), a novel method for generating
video-grounded action programs through use of a Large Language Model (LLM).
These action programs represent the motoric, perceptual, and structural aspects
of action, and consist of sub-actions, pre- and post-conditions, and control
flows. LEAP's action programs are centered on egocentric video and employ
recent developments in LLMs both as a source for program knowledge and as an
aggregator and assessor of multimodal video information. We apply LEAP over a
majority (87\%) of the training set of the EPIC Kitchens dataset, and release
the resulting action programs as a publicly available dataset here
(https://drive.google.com/drive/folders/1Cpkw_TI1IIxXdzor0pOXG3rWJWuKU5Ex?usp=drive_link).
We employ LEAP as a secondary source of supervision, using its action programs
in a loss term applied to action recognition and anticipation networks. We
demonstrate sizable improvements in performance in both tasks due to training
with the LEAP dataset. Our method achieves 1st place on the EPIC Kitchens
Action Recognition leaderboard as of November 17 among the networks restricted
to RGB-input (see Supplementary Materials).",2023-11-29T04:25:52Z
,http://arxiv.org/pdf/2304.14342v1.pdf,"Thinking beyond chatbots' threat to education: Visualizations to
  elucidate the writing and coding process","The landscape of educational practices for teaching and learning languages
has been predominantly centered around outcome-driven approaches. The recent
accessibility of large language models has thoroughly disrupted these
approaches. As we transform our language teaching and learning practices to
account for this disruption, it is important to note that language learning
plays a pivotal role in developing human intelligence. Writing and computer
programming are two essential skills integral to our education systems. What
and how we write shapes our thinking and sets us on the path of self-directed
learning. While most educators understand that `process' and `product' are both
important and inseparable, in most educational settings, providing constructive
feedback on a learner's formative process is challenging. For instance, it is
straightforward in computer programming to assess whether a learner-submitted
code runs. However, evaluating the learner's creative process and providing
meaningful feedback on the process can be challenging. To address this
long-standing issue in education (and learning), this work presents a new set
of visualization tools to summarize the inherent and taught capabilities of a
learner's writing or programming process. These interactive Process
Visualizations (PVs) provide insightful, empowering, and personalized
process-oriented feedback to the learners. The toolbox is ready to be tested by
educators and learners and is publicly available at www.processfeedback.org.
Focusing on providing feedback on a learner's process--from self, peers, and
educators--will facilitate learners' ability to acquire higher-order skills
such as self-directed learning and metacognition.",2023-04-25T22:11:29Z
,http://arxiv.org/pdf/2209.14876v1.pdf,Repairing Bugs in Python Assignments Using Large Language Models,"Students often make mistakes on their introductory programming assignments as
part of their learning process. Unfortunately, providing custom repairs for
these mistakes can require a substantial amount of time and effort from class
instructors. Automated program repair (APR) techniques can be used to
synthesize such fixes. Prior work has explored the use of symbolic and neural
techniques for APR in the education domain. Both types of approaches require
either substantial engineering efforts or large amounts of data and training.
We propose to use a large language model trained on code, such as Codex, to
build an APR system -- MMAPR -- for introductory Python programming
assignments. Our system can fix both syntactic and semantic mistakes by
combining multi-modal prompts, iterative querying, test-case-based selection of
few-shots, and program chunking. We evaluate MMAPR on 286 real student programs
and compare to a baseline built by combining a state-of-the-art Python syntax
repair engine, BIFI, and state-of-the-art Python semantic repair engine for
student assignments, Refactory. We find that MMAPR can fix more programs and
produce smaller patches on average.",2022-09-29T15:41:17Z
,http://arxiv.org/pdf/2405.06907v2.pdf,"AIOS Compiler: LLM as Interpreter for Natural Language Programming and
  Flow Programming of AI Agents","Since their inception, programming languages have trended towards greater
readability and lower barriers for programmers. Following this trend, natural
language can be a promising type of programming language that provides great
flexibility and usability and helps towards the democracy of programming.
However, the inherent vagueness, ambiguity, and verbosity of natural language
pose significant challenges in developing an interpreter that can accurately
understand the programming logic and execute instructions written in natural
language. Fortunately, recent advancements in Large Language Models (LLMs) have
demonstrated remarkable proficiency in interpreting complex natural language.
Inspired by this, we develop a novel system for Code Representation and
Execution (CoRE), which employs LLM as interpreter to interpret and execute
natural language instructions. The proposed system unifies natural language
programming, pseudo-code programming, and flow programming under the same
representation for constructing language agents, while LLM serves as the
interpreter to interpret and execute the agent programs. In this paper, we
begin with defining the programming syntax that structures natural language
instructions logically. During the execution, we incorporate external memory to
minimize redundancy. Furthermore, we equip the designed interpreter with the
capability to invoke external tools, compensating for the limitations of LLM in
specialized domains or when accessing real-time information. This work is
open-source at https://github.com/agiresearch/CoRE,
https://github.com/agiresearch/OpenAGI, and
https://github.com/agiresearch/AIOS.",2024-05-11T04:29:03Z
,http://arxiv.org/pdf/2402.12222v1.pdf,"CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement
  Learning for LLM-based Mutation","Fuzzing is an effective bug-finding technique but it struggles with complex
systems like JavaScript engines that demand precise grammatical input.
Recently, researchers have adopted language models for context-aware mutation
in fuzzing to address this problem. However, existing techniques are limited in
utilizing coverage guidance for fuzzing, which is rather performed in a
black-box manner. This paper presents a novel technique called CovRL
(Coverage-guided Reinforcement Learning) that combines Large Language Models
(LLMs) with reinforcement learning from coverage feedback. Our fuzzer,
CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging
the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a
weighted coverage map. This map is key in calculating the fuzzing reward, which
is then applied to the LLM-based mutator through reinforcement learning.
CovRL-Fuzz, through this approach, enables the generation of test cases that
are more likely to discover new coverage areas, thus improving vulnerability
detection while minimizing syntax and semantic errors, all without needing
extra post-processing. Our evaluation results indicate that CovRL-Fuzz
outperforms the state-of-the-art fuzzers in terms of code coverage and
bug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related
bugs in the latest JavaScript engines, including 39 previously unknown
vulnerabilities and 11 CVEs.",2024-02-19T15:30:40Z
,http://arxiv.org/pdf/2304.07840v2.pdf,"Enhancing Automated Program Repair through Fine-tuning and Prompt
  Engineering","Sequence-to-sequence models have been used to transform erroneous programs
into correct ones when trained with a large enough dataset. Some recent studies
also demonstrated strong empirical evidence that code review could improve the
program repair further. Large language models, trained with Natural Language
(NL) and Programming Language (PL), can contain inherent knowledge of both. In
this study, we investigate if this inherent knowledge of PL and NL can be
utilized to improve automated program repair. We applied PLBART and CodeT5, two
state-of-the-art language models that are pre-trained with both PL and NL, on
two such natural language-based program repair datasets and found that the
pre-trained language models fine-tuned with datasets containing both code
review and subsequent code changes notably outperformed each of the previous
models. With the advent of code generative models like Codex and GPT-3.5-Turbo,
we also performed zero-shot and few-shots learning-based prompt engineering to
assess their performance on these datasets. However, the practical application
of using LLMs in the context of automated program repair is still a long way
off based on our manual analysis of the generated repaired codes by the
learning models.",2023-04-16T17:29:51Z
,http://arxiv.org/pdf/2212.10561v3.pdf,"Parsel: Algorithmic Reasoning with Language Models by Composing
  Decompositions","Despite recent success in large language model (LLM) reasoning, LLMs struggle
with hierarchical multi-step reasoning tasks like generating complex programs.
For these tasks, humans often start with a high-level algorithmic design and
implement each part gradually. We introduce Parsel, a framework enabling
automatic implementation and validation of complex algorithms with code LLMs.
With Parsel, we automatically decompose algorithmic tasks into hierarchical
natural language function descriptions and then search over combinations of
possible function implementations using tests. We show that Parsel can be used
across domains requiring hierarchical reasoning, including program synthesis
and robotic planning. We find that, using Parsel, LLMs solve more
competition-level problems in the APPS dataset, resulting in pass rates over
75\% higher than prior results from directly sampling AlphaCode and Codex,
while often using a smaller sample budget. Moreover, with automatically
generated tests, we find that Parsel can improve the state-of-the-art pass@1
performance on HumanEval from 67\% to 85\%. We also find that LLM-generated
robotic plans using Parsel are more than twice as likely to be considered
accurate than directly generated plans. Lastly, we explore how Parsel addresses
LLM limitations and discuss how Parsel may be useful for human programmers. We
release our code at https://github.com/ezelikman/parsel",2022-12-20T18:59:23Z
,http://arxiv.org/pdf/2402.10398v1.pdf,"Prompt Learning for Multi-Label Code Smell Detection: A Promising
  Approach","Code smells indicate the potential problems of software quality so that
developers can identify refactoring opportunities by detecting code smells.
State-of-the-art approaches leverage heuristics, machine learning, and deep
learning to detect code smells. However, existing approaches have not fully
explored the potential of large language models (LLMs). In this paper, we
propose \textit{PromptSmell}, a novel approach based on prompt learning for
detecting multi-label code smell. Firstly, code snippets are acquired by
traversing abstract syntax trees. Combined code snippets with natural language
prompts and mask tokens, \textit{PromptSmell} constructs the input of LLMs.
Secondly, to detect multi-label code smell, we leverage a label combination
approach by converting a multi-label problem into a multi-classification
problem. A customized answer space is added to the word list of pre-trained
language models, and the probability distribution of intermediate answers is
obtained by predicting the words at the mask positions. Finally, the
intermediate answers are mapped to the target class labels by a verbalizer as
the final classification result. We evaluate the effectiveness of
\textit{PromptSmell} by answering six research questions. The experimental
results demonstrate that \textit{PromptSmell} obtains an improvement of 11.17\%
in $precision_{w}$ and 7.4\% in $F1_{w}$ compared to existing approaches.",2024-02-16T01:50:46Z
,http://arxiv.org/pdf/2406.14739v1.pdf,Learning to Retrieve Iteratively for In-Context Learning,"We introduce iterative retrieval, a novel framework that empowers retrievers
to make iterative decisions through policy optimization. Finding an optimal
portfolio of retrieved items is a combinatorial optimization problem, generally
considered NP-hard. This approach provides a learned approximation to such a
solution, meeting specific task requirements under a given family of large
language models (LLMs). We propose a training procedure based on reinforcement
learning, incorporating feedback from LLMs. We instantiate an iterative
retriever for composing in-context learning (ICL) exemplars and apply it to
various semantic parsing tasks that demand synthesized programs as outputs. By
adding only 4M additional parameters for state encoding, we convert an
off-the-shelf dense retriever into a stateful iterative retriever,
outperforming previous methods in selecting ICL exemplars on semantic parsing
datasets such as CalFlow, TreeDST, and MTOP. Additionally, the trained
iterative retriever generalizes across different inference LLMs beyond the one
used during training.",2024-06-20T21:07:55Z
,http://arxiv.org/pdf/2405.09783v1.pdf,"LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance
  Physical Scientific Discovery","Large Language Models have recently gained significant attention in
scientific discovery for their extensive knowledge and advanced reasoning
capabilities. However, they encounter challenges in effectively simulating
observational feedback and grounding it with language to propel advancements in
physical scientific discovery. Conversely, human scientists undertake
scientific discovery by formulating hypotheses, conducting experiments, and
revising theories through observational analysis. Inspired by this, we propose
to enhance the knowledge-driven, abstract reasoning abilities of LLMs with the
computational strength of simulations. We introduce Scientific Generative Agent
(SGA), a bilevel optimization framework: LLMs act as knowledgeable and
versatile thinkers, proposing scientific hypotheses and reason about discrete
components, such as physics equations or molecule structures; meanwhile,
simulations function as experimental platforms, providing observational
feedback and optimizing via differentiability for continuous parts, such as
physical parameters. We conduct extensive experiments to demonstrate our
framework's efficacy in constitutive law discovery and molecular design,
unveiling novel solutions that differ from conventional human expectations yet
remain coherent upon analysis.",2024-05-16T03:04:10Z
,http://arxiv.org/pdf/2310.02407v1.pdf,Automated Bug Generation in the era of Large Language Models,"Bugs are essential in software engineering; many research studies in the past
decades have been proposed to detect, localize, and repair bugs in software
systems. Effectiveness evaluation of such techniques requires complex bugs,
i.e., those that are hard to detect through testing and hard to repair through
debugging. From the classic software engineering point of view, a
hard-to-repair bug differs from the correct code in multiple locations, making
it hard to localize and repair. Hard-to-detect bugs, on the other hand,
manifest themselves under specific test inputs and reachability conditions.
These two objectives, i.e., generating hard-to-detect and hard-to-repair bugs,
are mostly aligned; a bug generation technique can change multiple statements
to be covered only under a specific set of inputs. However, these two
objectives are conflicting for learning-based techniques: A bug should have a
similar code representation to the correct code in the training data to
challenge a bug prediction model to distinguish them. The hard-to-repair bug
definition remains the same but with a caveat: the more a bug differs from the
original code (at multiple locations), the more distant their representations
are and easier to be detected. We propose BugFarm, to transform arbitrary code
into multiple complex bugs. BugFarm leverages LLMs to mutate code in multiple
locations (hard-to-repair). To ensure that multiple modifications do not
notably change the code representation, BugFarm analyzes the attention of the
underlying model and instructs LLMs to only change the least attended locations
(hard-to-detect). Our comprehensive evaluation of 320k+ bugs from over 2.5M
mutants generated by BugFarm and two alternative approaches demonstrates our
superiority in generating bugs that are hard to detect by learning-based bug
prediction approaches and hard to repair by SOTA learning-based program repair
technique.",2023-10-03T20:01:51Z
,http://arxiv.org/pdf/2308.16149v2.pdf,"Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open
  Generative Large Language Models","We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric
foundation and instruction-tuned open generative large language models (LLMs).
The models are based on the GPT-3 decoder-only architecture and are pretrained
on a mixture of Arabic and English texts, including source code in various
programming languages. With 13 billion parameters, they demonstrate better
knowledge and reasoning capabilities in Arabic than any existing open Arabic
and multilingual models by a sizable margin, based on extensive evaluation.
Moreover, the models are competitive in English compared to English-centric
open models of similar size, despite being trained on much less English data.
We provide a detailed description of the training, the tuning, the safety
alignment, and the evaluation of the models. We release two open versions of
the model -- the foundation Jais model, and an instruction-tuned Jais-chat
variant -- with the aim of promoting research on Arabic LLMs. Available at
https://huggingface.co/inception-mbzuai/jais-13b-chat",2023-08-30T17:07:17Z
,http://arxiv.org/pdf/2303.02927v3.pdf,"LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations
  and Infographics using Large Language Models","Systems that support users in the automatic creation of visualizations must
address several subtasks - understand the semantics of data, enumerate relevant
visualization goals and generate visualization specifications. In this work, we
pose visualization generation as a multi-stage generation problem and argue
that well-orchestrated pipelines based on large language models (LLMs) such as
ChatGPT/GPT-4 and image generation models (IGMs) are suitable to addressing
these tasks. We present LIDA, a novel tool for generating grammar-agnostic
visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER
that converts data into a rich but compact natural language summary, a GOAL
EXPLORER that enumerates visualization goals given the data, a VISGENERATOR
that generates, refines, executes and filters visualization code and an
INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA
provides a python api, and a hybrid user interface (direct manipulation and
multilingual natural language) for interactive chart, infographics and data
story generation. Learn more about the project here -
https://microsoft.github.io/lida/",2023-03-06T06:47:22Z
,http://arxiv.org/pdf/2403.14714v1.pdf,Compiler generated feedback for Large Language Models,"We introduce a novel paradigm in compiler optimization powered by Large
Language Models with compiler feedback to optimize the code size of LLVM
assembly. The model takes unoptimized LLVM IR as input and produces optimized
IR, the best optimization passes, and instruction counts of both unoptimized
and optimized IRs. Then we compile the input with generated optimization passes
and evaluate if the predicted instruction count is correct, generated IR is
compilable, and corresponds to compiled code. We provide this feedback back to
LLM and give it another chance to optimize code. This approach adds an extra
0.53% improvement over -Oz to the original model. Even though, adding more
information with feedback seems intuitive, simple sampling techniques achieve
much higher performance given 10 or more samples.",2024-03-18T23:25:13Z
,http://arxiv.org/pdf/2206.08896v1.pdf,Evolution through Large Models,"This paper pursues the insight that large language models (LLMs) trained to
generate code can vastly improve the effectiveness of mutation operators
applied to programs in genetic programming (GP). Because such LLMs benefit from
training data that includes sequential changes and modifications, they can
approximate likely changes that humans would make. To highlight the breadth of
implications of such evolution through large models (ELM), in the main
experiment ELM combined with MAP-Elites generates hundreds of thousands of
functional examples of Python programs that output working ambulating robots in
the Sodarace domain, which the original LLM had never seen in pre-training.
These examples then help to bootstrap training a new conditional language model
that can output the right walker for a particular terrain. The ability to
bootstrap new models that can output appropriate artifacts for a given context
in a domain where zero training data was previously available carries
implications for open-endedness, deep learning, and reinforcement learning.
These implications are explored here in depth in the hope of inspiring new
directions of research now opened up by ELM.",2022-06-17T17:07:04Z
,http://arxiv.org/pdf/2405.17216v1.pdf,Autoformalizing Euclidean Geometry,"Autoformalization involves automatically translating informal math into
formal theorems and proofs that are machine-verifiable. Euclidean geometry
provides an interesting and controllable domain for studying autoformalization.
In this paper, we introduce a neuro-symbolic framework for autoformalizing
Euclidean geometry, which combines domain knowledge, SMT solvers, and large
language models (LLMs). One challenge in Euclidean geometry is that informal
proofs rely on diagrams, leaving gaps in texts that are hard to formalize. To
address this issue, we use theorem provers to fill in such diagrammatic
information automatically, so that the LLM only needs to autoformalize the
explicit textual steps, making it easier for the model. We also provide
automatic semantic evaluation for autoformalized theorem statements. We
construct LeanEuclid, an autoformalization benchmark consisting of problems
from Euclid's Elements and the UniGeo dataset formalized in the Lean proof
assistant. Experiments with GPT-4 and GPT-4V show the capability and
limitations of state-of-the-art LLMs on autoformalizing geometry problems. The
data and code are available at https://github.com/loganrjmurphy/LeanEuclid.",2024-05-27T14:35:10Z
,http://arxiv.org/pdf/2311.04887v2.pdf,AutoChip: Automating HDL Generation Using LLM Feedback,"Traditionally, designs are written in Verilog hardware description language
(HDL) and debugged by hardware engineers. While this approach is effective, it
is time-consuming and error-prone for complex designs. Large language models
(LLMs) are promising in automating HDL code generation. LLMs are trained on
massive datasets of text and code, and they can learn to generate code that
compiles and is functionally accurate. We aim to evaluate the ability of LLMs
to generate functionally correct HDL models. We build AutoChip by combining the
interactive capabilities of LLMs and the output from Verilog simulations to
generate Verilog modules. We start with a design prompt for a module and the
context from compilation errors and debugging messages, which highlight
differences between the expected and actual outputs. This ensures that accurate
Verilog code can be generated without human intervention. We evaluate AutoChip
using problem sets from HDLBits. We conduct a comprehensive analysis of the
AutoChip using several LLMs and problem categories. The results show that
incorporating context from compiler tools, such as Icarus Verilog, improves the
effectiveness, yielding 24.20% more accurate Verilog. We release our evaluation
scripts and datasets as open-source contributions at the following link
https://github.com/shailja-thakur/AutoChip.",2023-11-08T18:46:39Z
,http://arxiv.org/pdf/2311.00177v2.pdf,Students' Perspective on AI Code Completion: Benefits and Challenges,"AI Code Completion (e.g., GitHub's Copilot) has revolutionized how computer
science students interact with programming languages. However, AI code
completion has been studied from the developers' perspectives, not the
students' perspectives who represent the future generation of our digital
world. In this paper, we investigated the benefits, challenges, and
expectations of AI code completion from students' perspectives. To facilitate
the study, we first developed an open-source Visual Studio Code Extension tool
AutoAurora, powered by a state-of-the-art large language model StarCoder, as an
AI code completion research instrument. Next, we conduct an interview study
with ten student participants and apply grounded theory to help analyze
insightful findings regarding the benefits, challenges, and expectations of
students on AI code completion. Our findings show that AI code completion
enhanced students' productivity and efficiency by providing correct syntax
suggestions, offering alternative solutions, and functioning as a coding tutor.
However, the over-reliance on AI code completion may lead to a surface-level
understanding of programming concepts, diminishing problem-solving skills and
restricting creativity. In the future, AI code completion should be explainable
and provide best coding practices to enhance the education process.",2023-10-31T22:41:16Z
,http://arxiv.org/pdf/2402.10980v4.pdf,"ChemReasoner: Heuristic Search over a Large Language Model's Knowledge
  Space using Quantum-Chemical Feedback","The discovery of new catalysts is essential for the design of new and more
efficient chemical processes in order to transition to a sustainable future. We
introduce an AI-guided computational screening framework unifying linguistic
reasoning with quantum-chemistry based feedback from 3D atomistic
representations. Our approach formulates catalyst discovery as an uncertain
environment where an agent actively searches for highly effective catalysts via
the iterative combination of large language model (LLM)-derived hypotheses and
atomistic graph neural network (GNN)-derived feedback. Identified catalysts in
intermediate search steps undergo structural evaluation based on spatial
orientation, reaction pathways, and stability. Scoring functions based on
adsorption energies and reaction energy barriers steer the exploration in the
LLM's knowledge space toward energetically favorable, high-efficiency
catalysts. We introduce planning methods that automatically guide the
exploration without human input, providing competitive performance against
expert-enumerated chemical descriptor-based implementations. By integrating
language-guided reasoning with computational chemistry feedback, our work
pioneers AI-accelerated, trustworthy catalyst discovery.",2024-02-15T21:33:07Z
,http://arxiv.org/pdf/2404.18470v1.pdf,"ECC Analyzer: Extract Trading Signal from Earnings Conference Calls
  using Large Language Model for Stock Performance Prediction","In the realm of financial analytics, leveraging unstructured data, such as
earnings conference calls (ECCs), to forecast stock performance is a critical
challenge that has attracted both academics and investors. While previous
studies have used deep learning-based models to obtain a general view of ECCs,
they often fail to capture detailed, complex information. Our study introduces
a novel framework: \textbf{ECC Analyzer}, combining Large Language Models
(LLMs) and multi-modal techniques to extract richer, more predictive insights.
The model begins by summarizing the transcript's structure and analyzing the
speakers' mode and confidence level by detecting variations in tone and pitch
for audio. This analysis helps investors form an overview perception of the
ECCs. Moreover, this model uses the Retrieval-Augmented Generation (RAG) based
methods to meticulously extract the focuses that have a significant impact on
stock performance from an expert's perspective, providing a more targeted
analysis. The model goes a step further by enriching these extracted focuses
with additional layers of analysis, such as sentiment and audio segment
features. By integrating these insights, the ECC Analyzer performs multi-task
predictions of stock performance, including volatility, value-at-risk (VaR),
and return for different intervals. The results show that our model outperforms
traditional analytic benchmarks, confirming the effectiveness of using advanced
LLM techniques in financial analytics.",2024-04-29T07:11:39Z
10.1145/3626252.3630773,http://arxiv.org/pdf/2403.14986v1.pdf,"AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style
  Feedback in a Global Course","Teaching students how to write code that is elegant, reusable, and
comprehensible is a fundamental part of CS1 education. However, providing this
""style feedback"" in a timely manner has proven difficult to scale. In this
paper, we present our experience deploying a novel, real-time style feedback
tool in Code in Place, a large-scale online CS1 course. Our tool is based on
the latest breakthroughs in large-language models (LLMs) and was carefully
designed to be safe and helpful for students. We used our Real-Time Style
Feedback tool (RTSF) in a class with over 8,000 diverse students from across
the globe and ran a randomized control trial to understand its benefits. We
show that students who received style feedback in real-time were five times
more likely to view and engage with their feedback compared to students who
received delayed feedback. Moreover, those who viewed feedback were more likely
to make significant style-related edits to their code, with over 79% of these
edits directly incorporating their feedback. We also discuss the practicality
and dangers of LLM-based tools for feedback, investigating the quality of the
feedback generated, LLM limitations, and techniques for consistency,
standardization, and safeguarding against demographic bias, all of which are
crucial for a tool utilized by students.",2024-03-22T06:45:39Z
,http://arxiv.org/pdf/2404.18262v1.pdf,"Generating Situated Reflection Triggers about Alternative Solution
  Paths: A Case Study of Generative AI for Computer-Supported Collaborative
  Learning","An advantage of Large Language Models (LLMs) is their contextualization
capability - providing different responses based on student inputs like
solution strategy or prior discussion, to potentially better engage students
than standard feedback. We present a design and evaluation of a
proof-of-concept LLM application to offer students dynamic and contextualized
feedback. Specifically, we augment an Online Programming Exercise bot for a
college-level Cloud Computing course with ChatGPT, which offers students
contextualized reflection triggers during a collaborative query optimization
task in database design. We demonstrate that LLMs can be used to generate
highly situated reflection triggers that incorporate details of the
collaborative discussion happening in context. We discuss in depth the
exploration of the design space of the triggers and their correspondence with
the learning objectives as well as the impact on student learning in a pilot
study with 34 students.",2024-04-28T17:56:14Z
,http://arxiv.org/pdf/2310.13714v1.pdf,"A study of the impact of generative AI-based data augmentation on
  software metadata classification","This paper presents the system submitted by the team from IIT(ISM) Dhanbad in
FIRE IRSE 2023 shared task 1 on the automatic usefulness prediction of
code-comment pairs as well as the impact of Large Language Model(LLM) generated
data on original base data towards an associated source code. We have developed
a framework where we train a machine learning-based model using the neural
contextual representations of the comments and their corresponding codes to
predict the usefulness of code-comments pair and performance analysis with
LLM-generated data with base data. In the official assessment, our system
achieves a 4% increase in F1-score from baseline and the quality of generated
data.",2023-10-14T10:47:10Z
,http://arxiv.org/pdf/2305.07922v2.pdf,"CodeT5+: Open Code Large Language Models for Code Understanding and
  Generation","Large language models (LLMs) pretrained on vast source code have achieved
prominent progress in code intelligence. However, existing code LLMs have two
main limitations in terms of architecture and pretraining tasks. First, they
often adopt a specific architecture (encoder-only or decoder-only) or rely on a
unified encoder-decoder network for different downstream tasks. The former
paradigm is limited by inflexibility in applications while in the latter, the
model is treated as a single system for all tasks, leading to suboptimal
performance on a subset of tasks. Secondly, they often employ a limited set of
pretraining objectives which might not be relevant to some downstream tasks and
hence result in substantial performance degrade. To address these limitations,
we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which
component modules can be flexibly combined to suit a wide range of downstream
code tasks. Such flexibility is enabled by our proposed mixture of pretraining
objectives to mitigate the pretrain-finetune discrepancy. These objectives
cover span denoising, contrastive learning, text-code matching, and causal LM
pretraining tasks, on both unimodal and bimodal multilingual code corpora.
Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs
without training from scratch to efficiently scale up our models, and explore
instruction-tuning to align with natural language instructions. We extensively
evaluate CodeT5+ on over 20 code-related benchmarks in different settings,
including zero-shot, finetuning, and instruction-tuning. We observe
state-of-the-art (SoTA) model performance on various code-related tasks, such
as code generation and completion, math programming, and text-to-code retrieval
tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA
results on HumanEval code generation task against other open code LLMs.",2023-05-13T14:23:07Z
,http://arxiv.org/pdf/2309.09980v1.pdf,"Code Representation Pre-training with Complements from Program
  Executions","Large language models (LLMs) for natural language processing have been
grafted onto programming language modeling for advancing code intelligence.
Although it can be represented in the text format, code is syntactically more
rigorous in order to be properly compiled or interpreted to perform a desired
set of behaviors given any inputs. In this case, existing works benefit from
syntactic representations to learn from code less ambiguously in the forms of
abstract syntax tree, control-flow graph, etc. However, programs with the same
purpose can be implemented in various ways showing different syntactic
representations while the ones with similar implementations can have distinct
behaviors. Though trivially demonstrated during executions, such semantics
about functionality are challenging to be learned directly from code,
especially in an unsupervised manner. Hence, in this paper, we propose
FuzzPretrain to explore the dynamic information of programs revealed by their
test cases and embed it into the feature representations of code as
complements. The test cases are obtained with the assistance of a customized
fuzzer and are only required during pre-training. FuzzPretrain yielded more
than 6%/9% mAP improvements on code search over its counterparts trained with
only source code or AST, respectively. Our extensive experimental results show
the benefits of learning discriminative code representations with program
executions.",2023-09-04T01:57:22Z
,http://arxiv.org/pdf/2402.01391v2.pdf,"StepCoder: Improve Code Generation with Reinforcement Learning from
  Compiler Feedback","The advancement of large language models (LLMs) has significantly propelled
the field of code generation. Previous work integrated reinforcement learning
(RL) with compiler feedback for exploring the output space of LLMs to enhance
code generation quality. However, the lengthy code generated by LLMs in
response to complex human requirements makes RL exploration a challenge. Also,
since the unit tests may not cover the complicated code, optimizing LLMs by
using these unexecuted code snippets is ineffective. To tackle these
challenges, we introduce StepCoder, a novel RL framework for code generation,
consisting of two main components: CCCS addresses the exploration challenge by
breaking the long sequences code generation task into a Curriculum of Code
Completion Subtasks, while FGO only optimizes the model by masking the
unexecuted code segments to provide Fine-Grained Optimization. In addition, we
furthermore construct the APPS+ dataset for RL training, which is manually
verified to ensure the correctness of unit tests. Experimental results show
that our method improves the ability to explore the output space and
outperforms state-of-the-art approaches in corresponding benchmarks. Our
dataset APPS+ and StepCoder are available online.",2024-02-02T13:14:31Z
10.1145/3660811,http://arxiv.org/pdf/2405.03509v1.pdf,"Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning
  and In-Context Learning","Inspired by the great potential of Large Language Models (LLMs) for solving
complex coding tasks, in this paper, we propose a novel approach, named
Code2API, to automatically perform APIzation for Stack Overflow code snippets.
Code2API does not require additional model training or any manual crafting
rules and can be easily deployed on personal computers without relying on other
external tools. Specifically, Code2API guides the LLMs through well-designed
prompts to generate well-formed APIs for given code snippets. To elicit
knowledge and logical reasoning from LLMs, we used chain-of-thought (CoT)
reasoning and few-shot in-context learning, which can help the LLMs fully
understand the APIzation task and solve it step by step in a manner similar to
a developer. Our evaluations show that Code2API achieves a remarkable accuracy
in identifying method parameters (65%) and return statements (66%) equivalent
to human-generated ones, surpassing the current state-of-the-art approach,
APIzator, by 15.0% and 16.5% respectively. Moreover, compared with APIzator,
our user study demonstrates that Code2API exhibits superior performance in
generating meaningful method names, even surpassing the human-level
performance, and developers are more willing to use APIs generated by our
approach, highlighting the applicability of our tool in practice. Finally, we
successfully extend our framework to the Python dataset, achieving a comparable
performance with Java, which verifies the generalizability of our tool.",2024-05-06T14:22:17Z
,http://arxiv.org/pdf/2306.03460v1.pdf,Natural Language Commanding via Program Synthesis,"We present Semantic Interpreter, a natural language-friendly AI system for
productivity software such as Microsoft Office that leverages large language
models (LLMs) to execute user intent across application features. While LLMs
are excellent at understanding user intent expressed as natural language, they
are not sufficient for fulfilling application-specific user intent that
requires more than text-to-text transformations. We therefore introduce the
Office Domain Specific Language (ODSL), a concise, high-level language
specialized for performing actions in and interacting with entities in Office
applications. Semantic Interpreter leverages an Analysis-Retrieval prompt
construction method with LLMs for program synthesis, translating natural
language user utterances to ODSL programs that can be transpiled to application
APIs and then executed. We focus our discussion primarily on a research
exploration for Microsoft PowerPoint.",2023-06-06T07:28:49Z
10.1145/3654992,http://arxiv.org/pdf/2404.17136v1.pdf,"Automated Data Visualization from Natural Language via Large Language
  Models: An Exploratory Study","The Natural Language to Visualization (NL2Vis) task aims to transform
natural-language descriptions into visual representations for a grounded table,
enabling users to gain insights from vast amounts of data. Recently, many deep
learning-based approaches have been developed for NL2Vis. Despite the
considerable efforts made by these approaches, challenges persist in
visualizing data sourced from unseen databases or spanning multiple tables.
Taking inspiration from the remarkable generation capabilities of Large
Language Models (LLMs), this paper conducts an empirical study to evaluate
their potential in generating visualizations, and explore the effectiveness of
in-context learning prompts for enhancing this task. In particular, we first
explore the ways of transforming structured tabular data into sequential text
prompts, as to feed them into LLMs and analyze which table content contributes
most to the NL2Vis. Our findings suggest that transforming structured tabular
data into programs is effective, and it is essential to consider the table
schema when formulating prompts. Furthermore, we evaluate two types of LLMs:
finetuned models (e.g., T5-Small) and inference-only models (e.g., GPT-3.5),
against state-of-the-art methods, using the NL2Vis benchmarks (i.e., nvBench).
The experimental results reveal that LLMs outperform baselines, with
inference-only models consistently exhibiting performance improvements, at
times even surpassing fine-tuned models when provided with certain few-shot
demonstrations through in-context learning. Finally, we analyze when the LLMs
fail in NL2Vis, and propose to iteratively update the results using strategies
such as chain-of-thought, role-playing, and code-interpreter. The experimental
results confirm the efficacy of iterative updates and hold great potential for
future study.",2024-04-26T03:25:35Z
,http://arxiv.org/pdf/2304.09102v1.pdf,"Solving Math Word Problems by Combining Language Models With Symbolic
  Solvers","Automatically generating high-quality step-by-step solutions to math word
problems has many applications in education. Recently, combining large language
models (LLMs) with external tools to perform complex reasoning and calculation
has emerged as a promising direction for solving math word problems, but prior
approaches such as Program-Aided Language model (PAL) are biased towards simple
procedural problems and less effective for problems that require declarative
reasoning. We propose an approach that combines an LLM that can incrementally
formalize word problems as a set of variables and equations with an external
symbolic solver that can solve the equations. Our approach achieves comparable
accuracy to the original PAL on the GSM8K benchmark of math word problems and
outperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more
challenging word problems extracted from Algebra textbooks. Our work highlights
the benefits of using declarative and incremental representations when
interfacing with an external tool for solving complex math word problems. Our
data and prompts are publicly available at
https://github.com/joyheyueya/declarative-math-word-problem.",2023-04-16T04:16:06Z
,http://arxiv.org/pdf/2311.07692v1.pdf,"On The Truthfulness of 'Surprisingly Likely' Responses of Large Language
  Models","The surprisingly likely criterion in the seminal work of Prelec (the Bayesian
Truth Serum) guarantees truthfulness in a game-theoretic multi-agent setting,
by rewarding rational agents to maximise the expected information gain with
their answers w.r.t. their probabilistic beliefs. We investigate the relevance
of a similar criterion for responses of LLMs. We hypothesize that if the
surprisingly likely criterion works in LLMs, under certain conditions, the
responses that maximize the reward under this criterion should be more accurate
than the responses that only maximize the posterior probability. Using
benchmarks including the TruthfulQA benchmark and using openly available LLMs:
GPT-2 and LLaMA-2, we show that the method indeed improves the accuracy
significantly (for example, upto 24 percentage points aggregate improvement on
TruthfulQA and upto 70 percentage points improvement on individual categories
of questions).",2023-11-13T19:21:25Z
,http://arxiv.org/pdf/2401.17435v3.pdf,Can Large Language Models Replace Economic Choice Prediction Labs?,"Economic choice prediction is an essential challenging task, often
constrained by the difficulties in acquiring human choice data. Indeed,
experimental economics studies had focused mostly on simple choice settings.
The AI community has recently contributed to that effort in two ways:
considering whether LLMs can substitute for humans in the above-mentioned
simple choice prediction settings, and the study through ML lens of more
elaborated but still rigorous experimental economics settings, employing
incomplete information, repetitive play, and natural language communication,
notably language-based persuasion games. This leaves us with a major
inspiration: can LLMs be used to fully simulate the economic environment and
generate data for efficient human choice prediction, substituting for the
elaborated economic lab studies? We pioneer the study of this subject,
demonstrating its feasibility. In particular, we show that a model trained
solely on LLM-generated data can effectively predict human behavior in a
language-based persuasion game, and can even outperform models trained on
actual human data.",2024-01-30T20:49:47Z
10.5220/0012467100003636,http://arxiv.org/pdf/2307.05360v3.pdf,"Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency
  in coding algorithms and data structures","The transformative influence of Large Language Models (LLMs) is profoundly
reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT
distinguishes itself within these models, demonstrating remarkable performance
in multi-turn conversations and exhibiting code proficiency across an array of
languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's
coding capabilities based on what is to date the largest catalog of coding
challenges. Our focus is on the python programming language and problems
centered on data structures and algorithms, two topics at the very foundations
of Computer Science. We evaluate ChatGPT for its ability to generate correct
solutions to the problems fed to it, its code quality, and nature of run-time
errors thrown by its code. Where ChatGPT code successfully executes, but fails
to solve the problem at hand, we look into patterns in the test cases passed in
order to gain some insights into how wrong ChatGPT code is in these kinds of
situations. To infer whether ChatGPT might have directly memorized some of the
data that was used to train it, we methodically design an experiment to
investigate this phenomena. Making comparisons with human performance whenever
feasible, we investigate all the above questions from the context of both its
underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics
within the main topics, and on problems having varying degrees of difficulty.",2023-07-10T08:20:34Z
,http://arxiv.org/pdf/2302.04012v2.pdf,"CodeLMSec Benchmark: Systematically Evaluating and Finding Security
  Vulnerabilities in Black-Box Code Language Models","Large language models (LLMs) for automatic code generation have achieved
breakthroughs in several programming tasks. Their advances in competition-level
programming problems have made them an essential pillar of AI-assisted pair
programming, and tools such as GitHub Copilot have emerged as part of the daily
programming workflow used by millions of developers. The training data for
these models is usually collected from the Internet (e.g., from open-source
repositories) and is likely to contain faults and security vulnerabilities.
This unsanitized training data can cause the language models to learn these
vulnerabilities and propagate them during the code generation procedure. While
these models have been extensively assessed for their ability to produce
functionally correct programs, there remains a lack of comprehensive
investigations and benchmarks addressing the security aspects of these models.
  In this work, we propose a method to systematically study the security issues
of code language models to assess their susceptibility to generating vulnerable
code. To this end, we introduce the first approach to automatically find
generated code that contains vulnerabilities in black-box code generation
models. To achieve this, we present an approach to approximate inversion of the
black-box code generation models based on few-shot prompting. We evaluate the
effectiveness of our approach by examining code language models in generating
high-risk security weaknesses. Furthermore, we establish a collection of
diverse non-secure prompts for various vulnerability scenarios using our
method. This dataset forms a benchmark for evaluating and comparing the
security weaknesses in code language models.",2023-02-08T11:54:07Z
,http://arxiv.org/pdf/2305.05364v1.pdf,Large Language Model Programs,"In recent years, large pre-trained language models (LLMs) have demonstrated
the ability to follow instructions and perform novel tasks from a few examples.
The possibility to parameterise an LLM through such in-context examples widens
their capability at a much lower cost than finetuning. We extend this line of
reasoning and present a method which further expands the capabilities of an LLM
by embedding it within an algorithm or program. To demonstrate the benefits of
this approach, we present an illustrative example of evidence-supported
question-answering. We obtain a 6.4\% improvement over the chain of thought
baseline through a more algorithmic approach without any finetuning.
Furthermore, we highlight recent work from this perspective and discuss the
advantages and disadvantages in comparison to the standard approaches.",2023-05-09T11:55:36Z
,http://arxiv.org/pdf/2404.02319v1.pdf,"Prompts As Programs: A Structure-Aware Approach to Efficient
  Compile-Time Prompt Optimization","Large language models (LLMs) can now handle longer and more complex inputs,
which facilitate the use of more elaborate prompts. However, prompts often
require some tuning to improve performance for deployment. Recent work has
proposed automatic prompt optimization methods, but as prompt complexity and
LLM strength increase, many prompt optimization techniques are no longer
sufficient and a new approach is needed to optimize {\em meta prompt programs}.
To address this, we introduce SAMMO, a framework for {\em compile-time}
optimizations of metaprompt programs, which represent prompts as structured
objects that allows for a rich set of transformations that can be searched over
during optimization. We show that SAMMO generalizes previous methods and
improves the performance of complex prompts on (1) instruction tuning, (2) RAG
pipeline tuning, and (3) prompt compression, across several different LLMs.
  We make all code available open-source at https://github.com/microsoft/sammo .",2024-04-02T21:35:54Z
,http://arxiv.org/pdf/2406.10923v1.pdf,"Investigating Video Reasoning Capability of Large Language Models with
  Tropes in Movies","Large Language Models (LLMs) have demonstrated effectiveness not only in
language tasks but also in video reasoning. This paper introduces a novel
dataset, Tropes in Movies (TiM), designed as a testbed for exploring two
critical yet previously overlooked video reasoning skills: (1) Abstract
Perception: understanding and tokenizing abstract concepts in videos, and (2)
Long-range Compositional Reasoning: planning and integrating intermediate
reasoning steps for understanding long-range videos with numerous frames.
Utilizing tropes from movie storytelling, TiM evaluates the reasoning
capabilities of state-of-the-art LLM-based approaches. Our experiments show
that current methods, including Captioner-Reasoner, Large Multimodal Model
Instruction Fine-tuning, and Visual Programming, only marginally outperform a
random baseline when tackling the challenges of Abstract Perception and
Long-range Compositional Reasoning. To address these deficiencies, we propose
Face-Enhanced Viper of Role Interactions (FEVoRI) and Context Query Reduction
(ConQueR), which enhance Visual Programming by fostering role interaction
awareness and progressively refining movie contexts and trope queries during
reasoning processes, significantly improving performance by 15 F1 points.
However, this performance still lags behind human levels (40 vs. 65 F1).
Additionally, we introduce a new protocol to evaluate the necessity of Abstract
Perception and Long-range Compositional Reasoning for task resolution. This is
done by analyzing the code generated through Visual Programming using an
Abstract Syntax Tree (AST), thereby confirming the increased complexity of TiM.
The dataset and code are available at: https://ander1119.github.io/TiM",2024-06-16T12:58:31Z
,http://arxiv.org/pdf/2401.13849v1.pdf,"TPD: Enhancing Student Language Model Reasoning via Principle Discovery
  and Guidance","Large Language Models (LLMs) have recently showcased remarkable reasoning
abilities. However, larger models often surpass their smaller counterparts in
reasoning tasks, posing the challenge of effectively transferring these
capabilities from larger models. Existing approaches heavily rely on extensive
fine-tuning data or continuous interactions with a superior teacher LLM during
inference. We introduce a principle-based teacher-student framework called
``Teaching via Principle Discovery'' (TPD) to address these limitations.
Inspired by human learning mechanisms, TPD mimics the interaction between a
teacher and a student using a principle-based approach. The teacher LLM
generates problem-solving instructions and corrective principles based on the
student LLM's errors. These principles guide the refinement of instructions and
the selection of instructive examples from a validation set. This enables the
student model to learn from both the teacher's guidance and its own mistakes.
Once the student model begins making inferences, TPD requires no further
intervention from the teacher LLM or humans. Through extensive experiments
across eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared
to standard chain-of-thought prompting, TPD significantly improves the student
model's performance, achieving $6.2\%$ improvement on average.",2024-01-24T23:11:33Z
,http://arxiv.org/pdf/2405.15842v1.pdf,"Model Cascading for Code: Reducing Inference Costs with Model Cascading
  for LLM Based Code Generation","The rapid development of large language models (LLMs) has led to significant
advancements in code completion tasks. While larger models have higher
accuracy, they also cost much more to run. Meanwhile, model cascading has been
proven effective to conserve computational resources while enhancing accuracy
in LLMs on natural language generation tasks. It generates output with the
smallest model in a set, and only queries the larger models when it fails to
meet predefined quality criteria. However, this strategy has not been used in
code completion tasks, primarily because assessing the quality of code
completions differs substantially from assessing natural language, where the
former relies heavily on the functional correctness. To address this, we
propose letting each model generate and execute a set of test cases for their
solutions, and use the test results as the cascading threshold. We show that
our model cascading strategy reduces computational costs while increases
accuracy compared to generating the output with a single model. We also
introduce a heuristics to determine the optimal combination of the number of
solutions, test cases, and test lines each model should generate, based on the
budget. Compared to speculative decoding, our method works on black-box models,
having the same level of cost-accuracy trade-off, yet providing much more
choices based on the server's budget. Ours is the first work to optimize
cost-accuracy trade-off for LLM code generation with model cascading.",2024-05-24T16:20:04Z
,http://arxiv.org/pdf/2406.11147v2.pdf,"Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level
  RAG","Vulnerability detection is essential for software quality assurance. In
recent years, deep learning models (especially large language models) have
shown promise in vulnerability detection. In this work, we propose a novel
LLM-based vulnerability detection technique Vul-RAG, which leverages
knowledge-level retrieval-augmented generation (RAG) framework to detect
vulnerability for the given code in three phases. First, Vul-RAG constructs a
vulnerability knowledge base by extracting multi-dimension knowledge via LLMs
from existing CVE instances; second, for a given code snippet, Vul-RAG}
retrieves the relevant vulnerability knowledge from the constructed knowledge
base based on functional semantics; third, Vul-RAG leverages LLMs to check the
vulnerability of the given code snippet by reasoning the presence of
vulnerability causes and fixing solutions of the retrieved vulnerability
knowledge. Our evaluation of Vul-RAG on our constructed benchmark PairVul shows
that Vul-RAG substantially outperforms all baselines by 12.96\%/110\% relative
improvement in accuracy/pairwise-accuracy. In addition, our user study shows
that the vulnerability knowledge generated by Vul-RAG can serve as high-quality
explanations which can improve the manual detection accuracy from 0.60 to 0.77.",2024-06-17T02:25:45Z
,http://arxiv.org/pdf/2403.13583v1.pdf,"CONLINE: Complex Code Generation and Refinement with Online Searching
  and Correctness Testing","Large Language Models (LLMs) have revolutionized code generation ability by
converting natural language descriptions into executable code. However,
generating complex code within real-world scenarios remains challenging due to
intricate structures, subtle bugs, understanding of advanced data types, and
lack of supplementary contents. To address these challenges, we introduce the
CONLINE framework, which enhances code generation by incorporating planned
online searches for information retrieval and automated correctness testing for
iterative refinement. CONLINE also serializes the complex inputs and outputs to
improve comprehension and generate test case to ensure the framework's
adaptability for real-world applications. CONLINE is validated through rigorous
experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE
substantially improves the quality of complex code generation, highlighting its
potential to enhance the practicality and reliability of LLMs in generating
intricate code.",2024-03-20T13:33:55Z
,http://arxiv.org/pdf/2404.07491v1.pdf,Neural Fault Injection: Generating Software Faults from Natural Language,"Traditional software fault injection methods, while foundational, face
limitations in adequately representing real-world faults, offering
customization, and requiring significant manual effort and expertise. This
paper introduces a novel methodology that harnesses the capabilities of Large
Language Models (LLMs) augmented with Reinforcement Learning from Human
Feedback (RLHF) to overcome these challenges. The usage of RLHF emphasizes an
iterative refinement process, allowing testers to provide feedback on generated
faults, which is then used to enhance the LLM's fault generation capabilities,
ensuring the generation of fault scenarios that closely mirror actual
operational risks. This innovative methodology aims to significantly reduce the
manual effort involved in crafting fault scenarios as it allows testers to
focus on higher-level testing strategies, hence paving the way to new
possibilities for enhancing the dependability of software systems.",2024-04-11T05:59:16Z
,http://arxiv.org/pdf/2405.01559v1.pdf,"Untangling Knots: Leveraging LLM for Error Resolution in Computational
  Notebooks","Computational notebooks became indispensable tools for research-related
development, offering unprecedented interactivity and flexibility in the
development process. However, these benefits come at the cost of
reproducibility and an increased potential for bugs. There are many tools for
bug fixing; however, they are generally targeted at the classical linear code.
With the rise of code-fluent Large Language Models, a new stream of smart
bug-fixing tools has emerged. However, the applicability of those tools is
still problematic for non-linear computational notebooks. In this paper, we
propose a potential solution for resolving errors in computational notebooks
via an iterative LLM-based agent. We discuss the questions raised by this
approach and share a novel dataset of computational notebooks containing bugs
to facilitate the research of the proposed approach.",2024-03-26T18:53:17Z
,http://arxiv.org/pdf/2309.01715v1.pdf,"Prompting or Fine-tuning? A Comparative Study of Large Language Models
  for Taxonomy Construction","Taxonomies represent hierarchical relations between entities, frequently
applied in various software modeling and natural language processing (NLP)
activities. They are typically subject to a set of structural constraints
restricting their content. However, manual taxonomy construction can be
time-consuming, incomplete, and costly to maintain. Recent studies of large
language models (LLMs) have demonstrated that appropriate user inputs (called
prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks
without explicit (re-)training. However, existing approaches for automated
taxonomy construction typically involve fine-tuning a language model by
adjusting model parameters. In this paper, we present a general framework for
taxonomy construction that takes into account structural constraints. We
subsequently conduct a systematic comparison between the prompting and
fine-tuning approaches performed on a hypernym taxonomy and a novel computer
science taxonomy dataset. Our result reveals the following: (1) Even without
explicit training on the dataset, the prompting approach outperforms
fine-tuning-based approaches. Moreover, the performance gap between prompting
and fine-tuning widens when the training dataset is small. However, (2)
taxonomies generated by the fine-tuning approach can be easily post-processed
to satisfy all the constraints, whereas handling violations of the taxonomies
produced by the prompting approach can be challenging. These evaluation
findings provide guidance on selecting the appropriate method for taxonomy
construction and highlight potential enhancements for both approaches.",2023-09-04T16:53:17Z
,http://arxiv.org/pdf/2404.09763v1.pdf,"KG-CTG: Citation Generation through Knowledge Graph-guided Large
  Language Models","Citation Text Generation (CTG) is a task in natural language processing (NLP)
that aims to produce text that accurately cites or references a cited document
within a source document. In CTG, the generated text draws upon contextual cues
from both the source document and the cited paper, ensuring accurate and
relevant citation information is provided. Previous work in the field of
citation generation is mainly based on the text summarization of documents.
Following this, this paper presents a framework, and a comparative study to
demonstrate the use of Large Language Models (LLMs) for the task of citation
generation. Also, we have shown the improvement in the results of citation
generation by incorporating the knowledge graph relations of the papers in the
prompt for the LLM to better learn the relationship between the papers. To
assess how well our model is performing, we have used a subset of standard
S2ORC dataset, which only consists of computer science academic research papers
in the English Language. Vicuna performs best for this task with 14.15 Meteor,
12.88 Rouge-1, 1.52 Rouge-2, and 10.94 Rouge-L. Also, Alpaca performs best, and
improves the performance by 36.98% in Rouge-1, and 33.14% in Meteor by
including knowledge graphs.",2024-04-15T13:06:32Z
,http://arxiv.org/pdf/2405.13740v1.pdf,Mining Action Rules for Defect Reduction Planning,"Defect reduction planning plays a vital role in enhancing software quality
and minimizing software maintenance costs. By training a black box machine
learning model and ""explaining"" its predictions, explainable AI for software
engineering aims to identify the code characteristics that impact maintenance
risks. However, post-hoc explanations do not always faithfully reflect what the
original model computes. In this paper, we introduce CounterACT, a
Counterfactual ACTion rule mining approach that can generate defect reduction
plans without black-box models. By leveraging action rules, CounterACT provides
a course of action that can be considered as a counterfactual explanation for
the class (e.g., buggy or not buggy) assigned to a piece of code. We compare
the effectiveness of CounterACT with the original action rule mining algorithm
and six established defect reduction approaches on 9 software projects. Our
evaluation is based on (a) overlap scores between proposed code changes and
actual developer modifications; (b) improvement scores in future releases; and
(c) the precision, recall, and F1-score of the plans. Our results show that,
compared to competing approaches, CounterACT's explainable plans achieve higher
overlap scores at the release level (median 95%) and commit level (median
85.97%), and they offer better trade-off between precision and recall (median
F1-score 88.12%). Finally, we venture beyond planning and explore leveraging
Large Language models (LLM) for generating code edits from our generated plans.
Our results show that suggested LLM code edits supported by our plans are
actionable and are more likely to pass relevant test cases than vanilla LLM
code recommendations.",2024-05-22T15:31:09Z
,http://arxiv.org/pdf/2312.04687v1.pdf,"LLM4TDD: Best Practices for Test Driven Development Using Large Language
  Models","In today's society, we are becoming increasingly dependent on software
systems. However, we also constantly witness the negative impacts of buggy
software. Program synthesis aims to improve software correctness by
automatically generating the program given an outline of the expected behavior.
For decades, program synthesis has been an active research field, with recent
approaches looking to incorporate Large Language Models to help generate code.
This paper explores the concept of LLM4TDD, where we guide Large Language
Models to generate code iteratively using a test-driven development
methodology. We conduct an empirical evaluation using ChatGPT and coding
problems from LeetCode to investigate the impact of different test, prompt and
problem attributes on the efficacy of LLM4TDD.",2023-12-07T20:37:54Z
,http://arxiv.org/pdf/2404.07960v1.pdf,"Content Knowledge Identification with Multi-Agent Large Language Models
  (LLMs)","Teachers' mathematical content knowledge (CK) is of vital importance and need
in teacher professional development (PD) programs. Computer-aided asynchronous
PD systems are the most recent proposed PD techniques, which aim to help
teachers improve their PD equally with fewer concerns about costs and
limitations of time or location. However, current automatic CK identification
methods, which serve as one of the core techniques of asynchronous PD systems,
face challenges such as diversity of user responses, scarcity of high-quality
annotated data, and low interpretability of the predictions. To tackle these
challenges, we propose a Multi-Agent LLMs-based framework, LLMAgent-CK, to
assess the user responses' coverage of identified CK learning goals without
human annotations. By taking advantage of multi-agent LLMs in strong
generalization ability and human-like discussions, our proposed LLMAgent-CK
presents promising CK identifying performance on a real-world mathematical CK
dataset MaCKT. Moreover, our case studies further demonstrate the working of
the multi-agent framework.",2024-03-22T02:37:33Z
,http://arxiv.org/pdf/2404.08885v1.pdf,"Is Next Token Prediction Sufficient for GPT? Exploration on Code Logic
  Comprehension","Large language models (LLMs) has experienced exponential growth, they
demonstrate remarkable performance across various tasks. Notwithstanding,
contemporary research primarily centers on enhancing the size and quality of
pretraining data, still utilizing the next token prediction task on
autoregressive transformer model structure. The efficacy of this task in truly
facilitating the model's comprehension of code logic remains questionable, we
speculate that it still interprets code as mere text, while human emphasizes
the underlying logical knowledge. In order to prove it, we introduce a new
task, ""Logically Equivalent Code Selection,"" which necessitates the selection
of logically equivalent code from a candidate set, given a query code. Our
experimental findings indicate that current LLMs underperform in this task,
since they understand code by unordered bag of keywords. To ameliorate their
performance, we propose an advanced pretraining task, ""Next Token Prediction+"".
This task aims to modify the sentence embedding distribution of the LLM without
sacrificing its generative capabilities. Our experimental results reveal that
following this pretraining, both Code Llama and StarCoder, the prevalent code
domain pretraining models, display significant improvements on our logically
equivalent code selection task and the code completion task.",2024-04-13T03:11:07Z
,http://arxiv.org/pdf/2404.19065v1.pdf,"HELPER-X: A Unified Instructable Embodied Agent to Tackle Four
  Interactive Vision-Language Domains with Memory-Augmented Language Models","Recent research on instructable agents has used memory-augmented Large
Language Models (LLMs) as task planners, a technique that retrieves
language-program examples relevant to the input instruction and uses them as
in-context examples in the LLM prompt to improve the performance of the LLM in
inferring the correct action and task plans. In this technical report, we
extend the capabilities of HELPER, by expanding its memory with a wider array
of examples and prompts, and by integrating additional APIs for asking
questions. This simple expansion of HELPER into a shared memory enables the
agent to work across the domains of executing plans from dialogue, natural
language instruction following, active question asking, and commonsense room
reorganization. We evaluate the agent on four diverse interactive
visual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the
Tidy Task. HELPER-X achieves few-shot, state-of-the-art performance across
these benchmarks using a single agent, without requiring in-domain training,
and remains competitive with agents that have undergone in-domain training.",2024-04-29T19:12:42Z
10.1145/3657604.3664694,http://arxiv.org/pdf/2405.14713v1.pdf,"Towards Educator-Driven Tutor Authoring: Generative AI Approaches for
  Creating Intelligent Tutor Interfaces","Intelligent Tutoring Systems (ITSs) have shown great potential in delivering
personalized and adaptive education, but their widespread adoption has been
hindered by the need for specialized programming and design skills. Existing
approaches overcome the programming limitations with no-code authoring through
drag and drop, however they assume that educators possess the necessary skills
to design effective and engaging tutor interfaces. To address this assumption
we introduce generative AI capabilities to assist educators in creating tutor
interfaces that meet their needs while adhering to design principles. Our
approach leverages Large Language Models (LLMs) and prompt engineering to
generate tutor layout and contents based on high-level requirements provided by
educators as inputs. However, to allow them to actively participate in the
design process, rather than relying entirely on AI-generated solutions, we
allow generation both at the entire interface level and at the individual
component level. The former provides educators with a complete interface that
can be refined using direct manipulation, while the latter offers the ability
to create specific elements to be added to the tutor interface. A small-scale
comparison shows the potential of our approach to enhance the efficiency of
tutor interface design. Moving forward, we raise critical questions for
assisting educators with generative AI capabilities to create personalized,
effective, and engaging tutors, ultimately enhancing their adoption.",2024-05-23T15:46:10Z
,http://arxiv.org/pdf/2305.02309v2.pdf,CodeGen2: Lessons for Training LLMs on Programming and Natural Languages,"Large language models (LLMs) have demonstrated remarkable abilities in
representation learning for program synthesis and understanding tasks. The
quality of the learned representations appears to be dictated by the neural
scaling laws as a function of the number of model parameters and observations,
while imposing upper bounds on the model performance by the amount of available
data and compute, which is costly.
  In this study, we attempt to render the training of LLMs for program
synthesis more efficient by unifying four key components: (1) model
architectures, (2) learning methods, (3) infill sampling, and, (4) data
distributions. Specifically, for the model architecture, we attempt to unify
encoder and decoder-based models into a single prefix-LM. For learning methods,
(i) causal language modeling, (ii) span corruption, (iii) infilling are unified
into a simple learning algorithm. For infill sampling, we explore the claim of
a ""free lunch"" hypothesis. For data distributions, the effect of a mixture
distribution and multi-epoch training of programming and natural languages on
model performance is explored.
  We conduct a comprehensive series of empirical experiments on 1B LLMs, for
which failures and successes of this exploration are distilled into five
lessons. We will provide a final recipe for training and release CodeGen2
models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training
framework as open-source: https://github.com/salesforce/CodeGen.",2023-05-03T17:55:25Z
,http://arxiv.org/pdf/2310.10508v1.pdf,"Prompt Engineering or Fine Tuning: An Empirical Assessment of Large
  Language Models in Automated Software Engineering Tasks","In this paper, we investigate the effectiveness of state-of-the-art LLM,
i.e., GPT-4, with three different prompting engineering techniques (i.e., basic
prompting, in-context learning, and task-specific prompting) against 18
fine-tuned LLMs on three typical ASE tasks, i.e., code generation, code
summarization, and code translation. Our quantitative analysis of these
prompting strategies suggests that prompt engineering GPT-4 cannot necessarily
and significantly outperform fine-tuning smaller/older LLMs in all three tasks.
For comment generation, GPT-4 with the best prompting strategy (i.e.,
task-specific prompt) had outperformed the first-ranked fine-tuned model by
8.33% points on average in BLEU. However, for code generation, the first-ranked
fine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3%
points, on average in BLEU. For code translation, GPT-4 and fine-tuned
baselines tie as they outperform each other on different translation tasks. To
explore the impact of different prompting strategies, we conducted a user study
with 27 graduate students and 10 industry practitioners. From our qualitative
analysis, we find that the GPT-4 with conversational prompts (i.e., when a
human provides feedback and instructions back and forth with a model to achieve
best results) showed drastic improvement compared to GPT-4 with automatic
prompting strategies. Moreover, we observe that participants tend to request
improvements, add more context, or give specific instructions as conversational
prompts, which goes beyond typical and generic prompting strategies. Our study
suggests that, at its current state, GPT-4 with conversational prompting has
great potential for ASE tasks, but fully automated prompt engineering with no
human in the loop requires more study and improvement.",2023-10-11T00:21:00Z
,http://arxiv.org/pdf/2310.15657v1.pdf,"Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash
  Detection with Large Language Model","Mobile applications have become a ubiquitous part of our daily life,
providing users with access to various services and utilities. Text input, as
an important interaction channel between users and applications, plays an
important role in core functionality such as search queries, authentication,
messaging, etc. However, certain special text (e.g., -18 for Font Size) can
cause the app to crash, and generating diversified unusual inputs for fully
testing the app is highly demanded. Nevertheless, this is also challenging due
to the combination of explosion dilemma, high context sensitivity, and complex
constraint relations. This paper proposes InputBlaster which leverages the LLM
to automatically generate unusual text inputs for mobile app crash detection.
It formulates the unusual inputs generation problem as a task of producing a
set of test generators, each of which can yield a batch of unusual text inputs
under the same mutation rule. In detail, InputBlaster leverages LLM to produce
the test generators together with the mutation rules serving as the reasoning
chain, and utilizes the in-context learning schema to demonstrate the LLM with
examples for boosting the performance. InputBlaster is evaluated on 36 text
input widgets with cash bugs involving 31 popular Android apps, and results
show that it achieves 78% bug detection rate, with 136% higher than the best
baseline. Besides, we integrate it with the automated GUI testing tool and
detect 37 unseen crashes in real-world apps from Google Play.",2023-10-24T09:10:51Z
10.1145/3657604.3662032,http://arxiv.org/pdf/2401.12125v3.pdf,"CodeTailor: LLM-Powered Personalized Parsons Puzzles for Engaging
  Support While Learning Programming","Learning to program can be challenging, and providing high-quality and timely
support at scale is hard. Generative AI and its products, like ChatGPT, can
create a solution for most intro-level programming problems. However, students
might use these tools to just generate code for them, resulting in reduced
engagement and limited learning. In this paper, we present CodeTailor, a system
that leverages a large language model (LLM) to provide personalized help to
students while still encouraging cognitive engagement. CodeTailor provides a
personalized Parsons puzzle to support struggling students. In a Parsons
puzzle, students place mixed-up code blocks in the correct order to solve a
problem. A technical evaluation with previous incorrect student code snippets
demonstrated that CodeTailor could deliver high-quality (correct, personalized,
and concise) Parsons puzzles based on their incorrect code. We conducted a
within-subjects study with 18 novice programmers. Participants perceived
CodeTailor as more engaging than just receiving an LLM-generated solution (the
baseline condition). In addition, participants applied more supported elements
from the scaffolded practice to the posttest when using CodeTailor than
baseline. Overall, most participants preferred using CodeTailor versus just
receiving the LLM-generated code for learning. Qualitative observations and
interviews also provided evidence for the benefits of CodeTailor, including
thinking more about solution construction, fostering continuity in learning,
promoting reflection, and boosting confidence. We suggest future design ideas
to facilitate active learning opportunities with generative AI techniques.",2024-01-22T17:08:54Z
,http://arxiv.org/pdf/2406.02924v1.pdf,"Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large
  Language Models","Despite the remarkable capabilities, Large Language Models (LLMs) face
deployment challenges due to their extensive size. Pruning methods drop a
subset of weights to accelerate, but many of them require retraining, which is
prohibitively expensive and computationally demanding. Recently, post-training
pruning approaches introduced novel metrics, enabling the pruning of LLMs
without retraining. However, these metrics require the involvement of human
experts and tedious trial and error. To efficiently identify superior pruning
metrics, we develop an automatic framework for searching symbolic pruning
metrics using genetic programming. In particular, we devise an elaborate search
space encompassing the existing pruning metrics to discover the potential
symbolic pruning metric. We propose an opposing operation simplification
strategy to increase the diversity of the population. In this way, Pruner-Zero
allows auto-generation of symbolic pruning metrics. Based on the searched
results, we explore the correlation between pruning metrics and performance
after pruning and summarize some principles. Extensive experiments on LLaMA and
LLaMA-2 on language modeling and zero-shot tasks demonstrate that our
Pruner-Zero obtains superior performance than SOTA post-training pruning
methods. Code at: \url{https://github.com/pprp/Pruner-Zero}.",2024-06-05T04:25:23Z
,http://arxiv.org/pdf/2406.16441v1.pdf,UniCoder: Scaling Code Large Language Model via Universal Code,"Intermediate reasoning or acting steps have successfully improved large
language models (LLMs) for handling various downstream natural language
processing (NLP) tasks. When applying LLMs for code generation, recent works
mainly focus on directing the models to articulate intermediate
natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and
then output code with the natural language or other structured intermediate
steps. However, such output is not suitable for code translation or generation
tasks since the standard CoT has different logical structures and forms of
expression with the code. In this work, we introduce the universal code
(UniCode) as the intermediate representation. It is a description of algorithm
steps using a mix of conventions of programming languages, such as assignment
operator, conditional operator, and loop. Hence, we collect an instruction
dataset UniCoder-Instruct to train our model UniCoder on multi-task learning
objectives. UniCoder-Instruct comprises natural-language questions, code
solutions, and the corresponding universal code. The alignment between the
intermediate universal code representation and the final code solution
significantly improves the quality of the generated code. The experimental
results demonstrate that UniCoder with the universal code significantly
outperforms the previous prompting methods by a large margin, showcasing the
effectiveness of the structural clues in pseudo-code.",2024-06-24T08:32:48Z
,http://arxiv.org/pdf/2305.16366v1.pdf,"Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal
  Theorem Proving","Large language models~(LLMs) present an intriguing avenue of exploration in
the domain of formal theorem proving. Nonetheless, the full utilization of
these models, particularly in terms of demonstration formatting and
organization, remains an underexplored area. In an endeavor to enhance the
efficacy of LLMs, we introduce a subgoal-based demonstration learning
framework, consisting of two primary elements: Firstly, drawing upon the
insights of subgoal learning from the domains of reinforcement learning and
robotics, we propose the construction of distinct subgoals for each
demonstration example and refine these subgoals in accordance with the
pertinent theories of subgoal learning. Secondly, we build upon recent advances
in diffusion models to predict the optimal organization, simultaneously
addressing two intricate issues that persist within the domain of demonstration
organization: subset selection and order determination. Through the integration
of subgoal-based learning methodologies, we have successfully increased the
prevailing proof accuracy from 38.9\% to 44.3\% on the miniF2F benchmark.
Furthermore, the adoption of diffusion models for demonstration organization
can lead to an additional enhancement in accuracy to 45.5\%, or a $5\times$
improvement in sampling efficiency compared with the long-standing
state-of-the-art method. Our code is available at
\url{https://github.com/HKUNLP/subgoal-theorem-prover}.",2023-05-25T11:35:52Z
,http://arxiv.org/pdf/2401.12947v1.pdf,"Transformer-Based Models Are Not Yet Perfect At Learning to Emulate
  Structural Recursion","This paper investigates the ability of transformer-based models to learn
structural recursion from examples. Recursion is a universal concept in both
natural and formal languages. Structural recursion is central to the
programming language and formal mathematics tasks where symbolic tools
currently excel beyond neural models, such as inferring semantic relations
between datatypes and emulating program behavior. We introduce a general
framework that nicely connects the abstract concepts of structural recursion in
the programming language domain to concrete sequence modeling problems and
learned models' behavior. The framework includes a representation that captures
the general \textit{syntax} of structural recursion, coupled with two different
frameworks for understanding their \textit{semantics} -- one that is more
natural from a programming languages perspective and one that helps bridge that
perspective with a mechanistic understanding of the underlying transformer
architecture.
  With our framework as a powerful conceptual tool, we identify different
issues under various set-ups. The models trained to emulate recursive
computations cannot fully capture the recursion yet instead fit short-cut
algorithms and thus cannot solve certain edge cases that are under-represented
in the training distribution. In addition, it is difficult for state-of-the-art
large language models (LLMs) to mine recursive rules from in-context
demonstrations. Meanwhile, these LLMs fail in interesting ways when emulating
reduction (step-wise computation) of the recursive function.",2024-01-23T18:07:38Z
,http://arxiv.org/pdf/2307.02443v1.pdf,"An Exploratory Literature Study on Sharing and Energy Use of Language
  Models for Source Code","Large language models trained on source code can support a variety of
software development tasks, such as code recommendation and program repair.
Large amounts of data for training such models benefit the models' performance.
However, the size of the data and models results in long training times and
high energy consumption. While publishing source code allows for replicability,
users need to repeat the expensive training process if models are not shared.
The main goal of the study is to investigate if publications that trained
language models for software engineering (SE) tasks share source code and
trained artifacts. The second goal is to analyze the transparency on training
energy usage. We perform a snowballing-based literature search to find
publications on language models for source code, and analyze their reusability
from a sustainability standpoint.
  From 494 unique publications, we identified 293 relevant publications that
use language models to address code-related tasks. Among them, 27% (79 out of
293) make artifacts available for reuse. This can be in the form of tools or
IDE plugins designed for specific tasks or task-agnostic models that can be
fine-tuned for a variety of downstream tasks. Moreover, we collect insights on
the hardware used for model training, as well as training time, which together
determine the energy consumption of the development process. We find that there
are deficiencies in the sharing of information and artifacts for current
studies on source code models for software engineering tasks, with 40% of the
surveyed papers not sharing source code or trained artifacts. We recommend the
sharing of source code as well as trained artifacts, to enable sustainable
reproducibility. Moreover, comprehensive information on training times and
hardware configurations should be shared for transparency on a model's carbon
footprint.",2023-07-05T17:13:00Z
,http://arxiv.org/pdf/2406.03636v2.pdf,"Synthetic Programming Elicitation and Repair for Text-to-Code in Very
  Low-Resource Programming Languages","Recent advances in large language models (LLMs) for code applications have
demonstrated remarkable zero-shot fluency and instruction following on
challenging code related tasks ranging from test case generation to
self-repair. Unsurprisingly, however, models struggle to compose syntactically
valid programs in programming languages unrepresented in pre-training, referred
to as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial
settings, including domain-specific languages for internal tools and
tool-chains for legacy languages. Inspired by an HCI technique called natural
program elicitation, we propose designing an intermediate language that LLMs
``naturally'' know how to use and which can be automatically compiled to a
target VLPL. When LLMs generate code that lies outside of this intermediate
language, we use compiler techniques to repair the code into programs in the
intermediate language. Overall, we introduce \emph{synthetic programming
elicitation and compilation} (SPEAC), an approach that enables LLMs to generate
syntactically valid code even for VLPLs. We empirically evaluate the
performance of SPEAC in a case study and find that, compared to existing
retrieval and fine-tuning baselines, SPEAC produces syntactically correct
programs significantly more frequently without sacrificing semantic
correctness.",2024-06-05T22:16:19Z
,http://arxiv.org/pdf/2405.19032v1.pdf,Large Language Models for Code Summarization,"Recently, there has been increasing activity in using deep learning for
software engineering, including tasks like code generation and summarization.
In particular, the most recent coding Large Language Models seem to perform
well on these problems. In this technical report, we aim to review how these
models perform in code explanation/summarization, while also investigating
their code generation capabilities (based on natural language descriptions).",2024-05-29T12:18:51Z
,http://arxiv.org/pdf/2311.07383v1.pdf,LM-Polygraph: Uncertainty Estimation for Language Models,"Recent advancements in the capabilities of large language models (LLMs) have
paved the way for a myriad of groundbreaking applications in various fields.
However, a significant challenge arises as these models often ""hallucinate"",
i.e., fabricate facts without providing users an apparent means to discern the
veracity of their statements. Uncertainty estimation (UE) methods are one path
to safer, more responsible, and more effective use of LLMs. However, to date,
research on UE methods for LLMs has been focused primarily on theoretical
rather than engineering contributions. In this work, we tackle this issue by
introducing LM-Polygraph, a framework with implementations of a battery of
state-of-the-art UE methods for LLMs in text generation tasks, with unified
program interfaces in Python. Additionally, it introduces an extendable
benchmark for consistent evaluation of UE techniques by researchers, and a demo
web application that enriches the standard chat dialog with confidence scores,
empowering end-users to discern unreliable responses. LM-Polygraph is
compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and
GPT-4, and is designed to support future releases of similarly-styled LMs.",2023-11-13T15:08:59Z
,http://arxiv.org/pdf/2306.06347v3.pdf,"DocChecker: Bootstrapping Code Large Language Model for Detecting and
  Resolving Code-Comment Inconsistencies","Comments within source code are essential for developers to comprehend the
code's purpose and ensure its correct usage. However, as codebases evolve,
maintaining an accurate alignment between the comments and the code becomes
increasingly challenging. Recognizing the growing interest in automated
solutions for detecting and correcting differences between code and its
accompanying comments, current methods rely primarily on heuristic rules. In
contrast, this paper presents DocChecker, a tool powered by deep learning.
DocChecker is adept at identifying inconsistencies between code and comments,
and it can also generate synthetic comments. This capability enables the tool
to detect and correct instances where comments do not accurately reflect their
corresponding code segments. We demonstrate the effectiveness of DocChecker
using the Just-In-Time and CodeXGlue datasets in different settings.
Particularly, DocChecker achieves a new State-of-the-art result of 72.3%
accuracy on the Inconsistency Code-Comment Detection (ICCD) task and 33.64
BLEU-4 on the code summarization task against other Large Language Models
(LLMs), even surpassing GPT 3.5 and CodeLlama.
  DocChecker is accessible for use and evaluation. It can be found on our
GitHub https://github.com/FSoft-AI4Code/DocChecker and as an Online Tool
http://4.193.50.237:5000/. For a more comprehensive understanding of its
functionality, a demonstration video is available on YouTube
https://youtu.be/FqnPmd531xw.",2023-06-10T05:29:09Z
,http://arxiv.org/pdf/2403.11585v2.pdf,"Linguacodus: A Synergistic Framework for Transformative Code Generation
  in Machine Learning Pipelines","In the ever-evolving landscape of machine learning, seamless translation of
natural language descriptions into executable code remains a formidable
challenge. This paper introduces Linguacodus, an innovative framework designed
to tackle this challenge by deploying a dynamic pipeline that iteratively
transforms natural language task descriptions into code through high-level
data-shaping instructions. The core of Linguacodus is a fine-tuned large
language model (LLM), empowered to evaluate diverse solutions for various
problems and select the most fitting one for a given task. This paper details
the fine-tuning process, and sheds light on how natural language descriptions
can be translated into functional code. Linguacodus represents a substantial
leap towards automated code generation, effectively bridging the gap between
task descriptions and executable code. It holds great promise for advancing
machine learning applications across diverse domains. Additionally, we propose
an algorithm capable of transforming a natural description of an ML task into
code with minimal human interaction. In extensive experiments on a vast machine
learning code dataset originating from Kaggle, we showcase the effectiveness of
Linguacodus. The investigations highlight its potential applications across
diverse domains, emphasizing its impact on applied machine learning in various
scientific fields.",2024-03-18T08:58:47Z
,http://arxiv.org/pdf/2406.13679v1.pdf,Prose-to-P4: Leveraging High Level Languages,"Languages such as P4 and NPL have enabled a wide and diverse range of
networking applications that take advantage of programmable dataplanes.
However, software development in these languages is difficult. To address this
issue, high-level languages have been designed to offer programmers powerful
abstractions that reduce the time, effort and domain-knowledge required for
developing networking applications. These languages are then translated by a
compiler into P4/NPL code. Inspired by the recent success of Large Language
Models (LLMs) in the task of code generation, we propose to raise the level of
abstraction even higher, employing LLMs to translate prose into high-level
networking code. We analyze the problem, focusing on the motivation and
opportunities, as well as the challenges involved and sketch out a roadmap for
the development of a system that can generate high-level dataplane code from
natural language instructions. We present some promising preliminary results on
generating Lucid code from natural language.",2024-06-19T16:32:27Z
,http://arxiv.org/pdf/2311.15500v2.pdf,Function-constrained Program Synthesis,"This work introduces (1) a technique that allows large language models (LLMs)
to leverage user-provided code when solving programming tasks and (2) a method
to iteratively generate modular sub-functions that can aid future code
generation attempts when the initial code generated by the LLM is inadequate.
Generating computer programs in general-purpose programming languages like
Python poses a challenge for LLMs when instructed to use code provided in the
prompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code
completions in real-time by drawing on all code available in a development
environment. However, restricting code-specific LLMs to use only in-context
code is not straightforward, as the model is not explicitly instructed to use
the user-provided code and users cannot highlight precisely which snippets of
code the model should incorporate into its context. Moreover, current systems
lack effective recovery methods, forcing users to iteratively re-prompt the
model with modified prompts until a sufficient solution is reached. Our method
differs from traditional LLM-powered code-generation by constraining
code-generation to an explicit function set and enabling recovery from failed
attempts through automatically generated sub-functions. When the LLM cannot
produce working code, we generate modular sub-functions to aid subsequent
attempts at generating functional code. A by-product of our method is a library
of reusable sub-functions that can solve related tasks, imitating a software
team where efficiency scales with experience. We also introduce a new
""half-shot"" evaluation paradigm that provides tighter estimates of LLMs' coding
abilities compared to traditional zero-shot evaluation. Our proposed evaluation
method encourages models to output solutions in a structured format, decreasing
syntax errors that can be mistaken for poor coding ability.",2023-11-27T02:55:34Z
,http://arxiv.org/pdf/2401.06513v1.pdf,"ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A
  Case Study","Machine learning (ML), especially with the emergence of large language models
(LLMs), has significantly transformed various industries. However, the
transition from ML model prototyping to production use within software systems
presents several challenges. These challenges primarily revolve around ensuring
safety, security, and transparency, subsequently influencing the overall
robustness and trustworthiness of ML models. In this paper, we introduce
ML-On-Rails, a protocol designed to safeguard ML models, establish a
well-defined endpoint interface for different ML tasks, and clear communication
between ML providers and ML consumers (software engineers). ML-On-Rails
enhances the robustness of ML models via incorporating detection capabilities
to identify unique challenges specific to production ML. We evaluated the
ML-On-Rails protocol through a real-world case study of the MoveReminder
application. Through this evaluation, we emphasize the importance of
safeguarding ML models in production.",2024-01-12T11:27:15Z
,http://arxiv.org/pdf/2308.03296v1.pdf,Studying Large Language Model Generalization with Influence Functions,"When trying to gain better visibility into a machine learning model in order
to understand and mitigate the associated risks, a potentially valuable source
of evidence is: which training examples most contribute to a given behavior?
Influence functions aim to answer a counterfactual: how would the model's
parameters (and hence its outputs) change if a given sequence were added to the
training set? While influence functions have produced insights for small
models, they are difficult to scale to large language models (LLMs) due to the
difficulty of computing an inverse-Hessian-vector product (IHVP). We use the
Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)
approximation to scale influence functions up to LLMs with up to 52 billion
parameters. In our experiments, EK-FAC achieves similar accuracy to traditional
influence function estimators despite the IHVP computation being orders of
magnitude faster. We investigate two algorithmic techniques to reduce the cost
of computing gradients of candidate training sequences: TF-IDF filtering and
query batching. We use influence functions to investigate the generalization
patterns of LLMs, including the sparsity of the influence patterns, increasing
abstraction with scale, math and programming abilities, cross-lingual
generalization, and role-playing behavior. Despite many apparently
sophisticated forms of generalization, we identify a surprising limitation:
influences decay to near-zero when the order of key phrases is flipped.
Overall, influence functions give us a powerful new tool for studying the
generalization properties of LLMs.",2023-08-07T04:47:42Z
10.1145/3617555.3617874,http://arxiv.org/pdf/2307.02192v3.pdf,"The FormAI Dataset: Generative AI in Software Security Through the Lens
  of Formal Verification","This paper presents the FormAI dataset, a large collection of 112, 000
AI-generated compilable and independent C programs with vulnerability
classification. We introduce a dynamic zero-shot prompting technique
constructed to spawn diverse programs utilizing Large Language Models (LLMs).
The dataset is generated by GPT-3.5-turbo and comprises programs with varying
levels of complexity. Some programs handle complicated tasks like network
management, table games, or encryption, while others deal with simpler tasks
like string manipulation. Every program is labeled with the vulnerabilities
found within the source code, indicating the type, line number, and vulnerable
function name. This is accomplished by employing a formal verification method
using the Efficient SMT-based Bounded Model Checker (ESBMC), which uses model
checking, abstract interpretation, constraint programming, and satisfiability
modulo theories to reason over safety/security properties in programs. This
approach definitively detects vulnerabilities and offers a formal model known
as a counterexample, thus eliminating the possibility of generating false
positive reports. We have associated the identified vulnerabilities with Common
Weakness Enumeration (CWE) numbers. We make the source code available for the
112, 000 programs, accompanied by a separate file containing the
vulnerabilities detected in each program, making the dataset ideal for training
LLMs and machine learning algorithms. Our study unveiled that according to
ESBMC, 51.24% of the programs generated by GPT-3.5 contained vulnerabilities,
thereby presenting considerable risks to software safety and security.",2023-07-05T10:39:58Z
10.1109/ASE56229.2023.00109,http://arxiv.org/pdf/2304.07575v2.pdf,"What Makes Good In-context Demonstrations for Code Intelligence Tasks
  with LLMs?","Pre-trained models of source code have gained widespread popularity in many
code intelligence tasks. Recently, with the scaling of the model and corpus
size, large language models have shown the ability of in-context learning
(ICL). ICL employs task instructions and a few examples as demonstrations, and
then inputs the demonstrations to the language models for making predictions.
This new learning paradigm is training-free and has shown impressive
performance in various natural language processing and code intelligence tasks.
However, the performance of ICL heavily relies on the quality of
demonstrations, e.g., the selected examples. It is important to systematically
investigate how to construct a good demonstration for code-related tasks. In
this paper, we empirically explore the impact of three key factors on the
performance of ICL in code intelligence tasks: the selection, order, and number
of demonstration examples. We conduct extensive experiments on three code
intelligence tasks including code summarization, bug fixing, and program
synthesis. Our experimental results demonstrate that all the above three
factors dramatically impact the performance of ICL in code intelligence tasks.
Additionally, we summarize our findings and provide takeaway suggestions on how
to construct effective demonstrations, taking into account these three
perspectives. We also show that a carefully-designed demonstration based on our
findings can lead to substantial improvements over widely-used demonstration
construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%,
175.96%, and 50.81% on code summarization, bug fixing, and program synthesis,
respectively",2023-04-15T15:13:58Z
,http://arxiv.org/pdf/2401.15459v3.pdf,"Multi-LLM Collaboration + Data-Centric Innovation = 2x Better
  Vulnerability Repair","The advances of deep learning (DL) have paved the way for automatic software
vulnerability repair approaches, which effectively learn the mapping from the
vulnerable code to the fixed code. Nevertheless, existing DL-based
vulnerability repair methods face notable limitations: 1) they struggle to
handle lengthy vulnerable code, 2) they treat code as natural language texts,
neglecting its inherent structure, and 3) they do not tap into the valuable
expert knowledge present in the expert system.
  To address this, we propose VulMaster, a Transformer-based neural network
model that excels at generating vulnerability repairs through data-centric
innovation. Specifically, VulMaster introduces the utilization and combination
of various types of input data, including complete vulnerable code of any size,
vulnerable code structures, and expert knowledge from the CWE system.
Additionally, VulMaster leverages the collaboration between two Large Language
Models (LLMs), CodeT5 and ChatGPT: CodeT5 acts as the customizable backbone
LLM, fine-tuned with the training data, while ChatGPT supplements by providing
missing relevant inputs to CodeT5. We evaluated VulMaster on a real-world C/C++
vulnerability repair dataset comprising 1,754 projects with 5,800 vulnerable
functions. The experimental results demonstrated that VulMaster exhibits
substantial improvements compared to the learning-based state-of-the-art
vulnerability repair approach. Specifically, VulMaster improves the EM, BLEU,
and CodeBLEU scores from 10.2\% to 20.0\%, 21.3\% to 29.3\%, and 32.5\% to
40.9\%, respectively.",2024-01-27T16:51:52Z
,http://arxiv.org/pdf/2405.01392v1.pdf,"LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous
  Space Exploration","As spacecraft journey further from Earth with more complex missions, systems
of greater autonomy and onboard intelligence are called for. Reducing reliance
on human-based mission control becomes increasingly critical if we are to
increase our rate of solar-system-wide exploration. Recent work has explored
AI-based goal-oriented systems to increase the level of autonomy in mission
execution. These systems make use of symbolic reasoning managers to make
inferences from the state of a spacecraft and a handcrafted knowledge base,
enabling autonomous generation of tasks and re-planning. Such systems have
proven to be successful in controlled cases, but they are difficult to
implement as they require human-crafted ontological models to allow the
spacecraft to understand the world. Reinforcement learning has been applied to
train robotic agents to pursue a goal. A new architecture for autonomy is
called for. This work explores the application of Large Language Models (LLMs)
as the high-level control system of a spacecraft. Using a systems engineering
approach, this work presents the design and development of an agentic
spacecraft controller by leveraging an LLM as a reasoning engine, to evaluate
the utility of such an architecture in achieving higher levels of spacecraft
autonomy. A series of deep space mission scenarios simulated within the popular
game engine Kerbal Space Program (KSP) are used as case studies to evaluate the
implementation against the requirements. It is shown the reasoning and planning
abilities of present-day LLMs do not scale well as the complexity of a mission
increases, but this can be alleviated with adequate prompting frameworks and
strategic selection of the agent's level of authority over the host spacecraft.
This research evaluates the potential of LLMs in augmenting autonomous
decision-making systems for future robotic space applications.",2024-04-13T03:33:17Z
,http://arxiv.org/pdf/2401.07518v3.pdf,"Survey of Natural Language Processing for Education: Taxonomy,
  Systematic Review, and Future Trends","Natural Language Processing (NLP) aims to analyze text or speech via
techniques in the computer science field. It serves the applications in domains
of healthcare, commerce, education and so on. Particularly, NLP has been widely
applied to the education domain and its applications have enormous potential to
help teaching and learning. In this survey, we review recent advances in NLP
with the focus on solving problems relevant to the education domain. In detail,
we begin with introducing the related background and the real-world scenarios
in education where NLP techniques could contribute. Then, we present a taxonomy
of NLP in the education domain and highlight typical NLP applications including
question answering, question construction, automated assessment, and error
correction. Next, we illustrate the task definition, challenges, and
corresponding cutting-edge techniques based on the above taxonomy. In
particular, LLM-involved methods are included for discussion due to the wide
usage of LLMs in diverse NLP applications. After that, we showcase some
off-the-shelf demonstrations in this domain. At last, we conclude with six
promising directions for future research, including more datasets in education
domain, controllable usage of LLMs, intervention of difficulty-level control,
interpretable educational NLP, methods with adaptive learning, and integrated
systems for education. We organize all relevant datasets and papers in the
open-available Github Link for better
review~\url{https://github.com/LiXinyuan1015/NLP-for-Education}.",2024-01-15T07:48:42Z
,http://arxiv.org/pdf/2211.10435v2.pdf,PAL: Program-aided Language Models,"Large language models (LLMs) have recently demonstrated an impressive ability
to perform arithmetic and symbolic reasoning tasks, when provided with a few
examples at test time (""few-shot prompting""). Much of this success can be
attributed to prompting methods such as ""chain-of-thought'', which employ LLMs
for both understanding the problem description by decomposing it into steps, as
well as solving each step of the problem. While LLMs seem to be adept at this
sort of step-by-step decomposition, LLMs often make logical and arithmetic
mistakes in the solution part, even when the problem is decomposed correctly.
In this paper, we present Program-Aided Language models (PAL): a novel approach
that uses the LLM to read natural language problems and generate programs as
the intermediate reasoning steps, but offloads the solution step to a runtime
such as a Python interpreter. With PAL, decomposing the natural language
problem into runnable steps remains the only learning task for the LLM, while
solving is delegated to the interpreter. We demonstrate this synergy between a
neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and
algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all
these natural language reasoning tasks, generating code using an LLM and
reasoning using a Python interpreter leads to more accurate results than much
larger models. For example, PAL using Codex achieves state-of-the-art few-shot
accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B
which uses chain-of-thought by absolute 15% top-1. Our code and data are
publicly available at http://reasonwithpal.com/ .",2022-11-18T18:56:13Z
,http://arxiv.org/pdf/2308.03312v8.pdf,Exploiting Code Symmetries for Learning Program Semantics,"This paper tackles the challenge of teaching code semantics to Large Language
Models (LLMs) for program analysis by incorporating code symmetries into the
model architecture. We introduce a group-theoretic framework that defines code
symmetries as semantics-preserving transformations, where forming a code
symmetry group enables precise and efficient reasoning of code semantics. Our
solution, SymC, develops a novel variant of self-attention that is provably
equivariant to code symmetries from the permutation group defined over the
program dependence graph. SymC obtains superior performance on five program
analysis tasks, outperforming state-of-the-art code models without any
pre-training. Our results suggest that code LLMs that encode the code
structural prior via the code symmetry group generalize better and faster.",2023-08-07T05:40:58Z
,http://arxiv.org/pdf/2404.01268v1.pdf,Mapping the Increasing Use of LLMs in Scientific Papers,"Scientific publishing lays the foundation of science by disseminating
research findings, fostering collaboration, encouraging reproducibility, and
ensuring that scientific knowledge is accessible, verifiable, and built upon
over time. Recently, there has been immense speculation about how many people
are using large language models (LLMs) like ChatGPT in their academic writing,
and to what extent this tool might have an effect on global scientific
practices. However, we lack a precise measure of the proportion of academic
writing substantially modified or produced by LLMs. To address this gap, we
conduct the first systematic, large-scale analysis across 950,965 papers
published between January 2020 and February 2024 on the arXiv, bioRxiv, and
Nature portfolio journals, using a population-level statistical framework to
measure the prevalence of LLM-modified content over time. Our statistical
estimation operates on the corpus level and is more robust than inference on
individual instances. Our findings reveal a steady increase in LLM usage, with
the largest and fastest growth observed in Computer Science papers (up to
17.5%). In comparison, Mathematics papers and the Nature portfolio showed the
least LLM modification (up to 6.3%). Moreover, at an aggregate level, our
analysis reveals that higher levels of LLM-modification are associated with
papers whose first authors post preprints more frequently, papers in more
crowded research areas, and papers of shorter lengths. Our findings suggests
that LLMs are being broadly used in scientific writings.",2024-04-01T17:45:15Z
,http://arxiv.org/pdf/2402.18040v1.pdf,Automated Discovery of Integral with Deep Learning,"Recent advancements in the realm of deep learning, particularly in the
development of large language models (LLMs), have demonstrated AI's ability to
tackle complex mathematical problems or solving programming challenges.
However, the capability to solve well-defined problems based on extensive
training data differs significantly from the nuanced process of making
scientific discoveries. Trained on almost all human knowledge available,
today's sophisticated LLMs basically learn to predict sequences of tokens. They
generate mathematical derivations and write code in a similar way as writing an
essay, and do not have the ability to pioneer scientific discoveries in the
manner a human scientist would do.
  In this study we delve into the potential of using deep learning to
rediscover a fundamental mathematical concept: integrals. By defining integrals
as area under the curve, we illustrate how AI can deduce the integral of a
given function, exemplified by inferring $\int_{0}^{x} t^2 dt = \frac{x^3}{3}$
and $\int_{0}^{x} ae^{bt} dt = \frac{a}{b} e^{bx} - \frac{a}{b}$. Our
experiments show that deep learning models can approach the task of inferring
integrals either through a sequence-to-sequence model, akin to language
translation, or by uncovering the rudimentary principles of integration, such
as $\int_{0}^{x} t^n dt = \frac{x^{n+1}}{n+1}$.",2024-02-28T04:34:15Z
,http://arxiv.org/pdf/2302.12834v1.pdf,"Leveraging Large Language Model and Story-Based Gamification in
  Intelligent Tutoring System to Scaffold Introductory Programming Courses: A
  Design-Based Research Study","Programming skills are rapidly becoming essential for many educational paths
and career opportunities. Yet, for many international students, the traditional
approach to teaching introductory programming courses can be a significant
challenge due to the complexities of the language, the lack of prior
programming knowledge, and the language and cultural barriers. This study
explores how large language models and gamification can scaffold coding
learning and increase Chinese students sense of belonging in introductory
programming courses. In this project, a gamification intelligent tutoring
system was developed to adapt to Chinese international students learning needs
and provides scaffolding to support their success in introductory computer
programming courses.",2023-02-25T04:07:03Z
,http://arxiv.org/pdf/2405.06835v1.pdf,Automating Code Adaptation for MLOps -- A Benchmarking Study on LLMs,"This paper explores the possibilities of the current generation of Large
Language Models for incorporating Machine Learning Operations (MLOps)
functionalities into ML training code bases. We evaluate the performance of
OpenAI (gpt-3.5-turbo) and WizardCoder (open-source, 15B parameters) models on
the automated accomplishment of various MLOps functionalities in different
settings. We perform a benchmarking study that assesses the ability of these
models to: (1) adapt existing code samples (Inlining) with component-specific
MLOps functionality such as MLflow and Weights & Biases for experiment
tracking, Optuna for hyperparameter optimization etc., and (2) perform the task
of Translation from one component of an MLOps functionality to another, e.g.,
translating existing GitPython library based version control code to Data
Version Control library based. We also propose three different approaches that
involve teaching LLMs to comprehend the API documentation of the components as
a reference while accomplishing the Translation tasks. In our evaluations, the
gpt-3.5-turbo model significantly outperforms WizardCoder by achieving
impressive Pass@3 accuracy in model optimization (55% compared to 0% by
WizardCoder), experiment tracking (100%, compared to 62.5% by WizardCoder),
model registration (92% compared to 42% by WizardCoder) and hyperparameter
optimization (83% compared to 58% by WizardCoder) on average, in their best
possible settings, showcasing its superior code adaptability performance in
complex MLOps tasks.",2024-05-10T22:18:43Z
,http://arxiv.org/pdf/2403.12627v2.pdf,"Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training
  AI Models on Coq Code","In the realm of formal theorem proving, the Coq proof assistant stands out
for its rigorous approach to verifying mathematical assertions and software
correctness. Despite the advances in artificial intelligence and machine
learning, the specialized nature of Coq syntax and semantics poses unique
challenges for Large Language Models (LLMs). Addressing this gap, we present a
comprehensive dataset specifically designed to enhance LLMs' proficiency in
interpreting and generating Coq code. This dataset, derived from a collection
of over 10,000 Coq source files, encompasses a wide array of propositions,
proofs, and definitions, enriched with metadata including source references and
licensing information. Our primary aim is to facilitate the development of LLMs
capable of generating syntactically correct and semantically meaningful Coq
constructs, thereby advancing the frontier of automated theorem proving.
Initial experiments with this dataset have showcased its significant potential;
models trained on this data exhibited enhanced accuracy in Coq code generation.
Notably, a particular experiment revealed that a fine-tuned LLM was capable of
generating 141 valid proofs for a basic lemma, highlighting the dataset's
utility in facilitating the discovery of diverse and valid proof strategies.
This paper discusses the dataset's composition, the methodology behind its
creation, and the implications of our findings for the future of machine
learning in formal verification. The dataset is accessible for further research
and exploration:
https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1",2024-03-19T10:53:40Z
,http://arxiv.org/pdf/2310.13001v3.pdf,Conversational Financial Information Retrieval Model (ConFIRM),"With the exponential growth in large language models (LLMs), leveraging their
emergent properties for specialized domains like finance merits exploration.
However, regulated fields such as finance pose unique constraints, requiring
domain-optimized frameworks. We present ConFIRM, an LLM-based conversational
financial information retrieval model tailored for query intent classification
and knowledge base labeling.
  ConFIRM comprises two modules:
  1) a method to synthesize finance domain-specific question-answer pairs, and
  2) evaluation of parameter efficient fine-tuning approaches for the query
classification task. We generate a dataset of over 4000 samples, assessing
accuracy on a separate test set.
  ConFIRM achieved over 90% accuracy, essential for regulatory compliance.
ConFIRM provides a data-efficient solution to extract precise query intent for
financial dialog systems.",2023-10-06T12:31:05Z
,http://arxiv.org/pdf/2404.18543v1.pdf,Time Machine GPT,"Large language models (LLMs) are often trained on extensive, temporally
indiscriminate text corpora, reflecting the lack of datasets with temporal
metadata. This approach is not aligned with the evolving nature of language.
Conventional methods for creating temporally adapted language models often
depend on further pre-training static models on time-specific data. This paper
presents a new approach: a series of point-in-time LLMs called Time Machine GPT
(TiMaGPT), specifically designed to be nonprognosticative. This ensures they
remain uninformed about future factual information and linguistic changes. This
strategy is beneficial for understanding language evolution and is of critical
importance when applying models in dynamic contexts, such as time-series
forecasting, where foresight of future information can prove problematic. We
provide access to both the models and training datasets.",2024-04-29T09:34:25Z
,http://arxiv.org/pdf/2406.12109v1.pdf,Can LLMs Learn Macroeconomic Narratives from Social Media?,"This study empirically tests the $\textit{Narrative Economics}$ hypothesis,
which posits that narratives (ideas that are spread virally and affect public
beliefs) can influence economic fluctuations. We introduce two curated datasets
containing posts from X (formerly Twitter) which capture economy-related
narratives (Data will be shared upon paper acceptance). Employing Natural
Language Processing (NLP) methods, we extract and summarize narratives from the
tweets. We test their predictive power for $\textit{macroeconomic}$ forecasting
by incorporating the tweets' or the extracted narratives' representations in
downstream financial prediction tasks. Our work highlights the challenges in
improving macroeconomic models with narrative data, paving the way for the
research community to realistically address this important challenge. From a
scientific perspective, our investigation offers valuable insights and NLP
tools for narrative extraction and summarization using Large Language Models
(LLMs), contributing to future research on the role of narratives in economics.",2024-06-17T21:37:09Z
,http://arxiv.org/pdf/2406.07084v1.pdf,"Leveraging Large Language Models for Efficient Failure Analysis in Game
  Development","In games, and more generally in the field of software development, early
detection of bugs is vital to maintain a high quality of the final product.
Automated tests are a powerful tool that can catch a problem earlier in
development by executing periodically. As an example, when new code is
submitted to the code base, a new automated test verifies these changes.
However, identifying the specific change responsible for a test failure becomes
harder when dealing with batches of changes -- especially in the case of a
large-scale project such as a AAA game, where thousands of people contribute to
a single code base. This paper proposes a new approach to automatically
identify which change in the code caused a test to fail. The method leverages
Large Language Models (LLMs) to associate error messages with the corresponding
code changes causing the failure. We investigate the effectiveness of our
approach with quantitative and qualitative evaluations. Our approach reaches an
accuracy of 71% in our newly created dataset, which comprises issues reported
by developers at EA over a period of one year. We further evaluated our model
through a user study to assess the utility and usability of the tool from a
developer perspective, resulting in a significant reduction in time -- up to
60% -- spent investigating issues.",2024-06-11T09:21:50Z
10.1109/MLCAD58807.2023.10299874,http://arxiv.org/pdf/2305.13243v2.pdf,"Chip-Chat: Challenges and Opportunities in Conversational Hardware
  Design","Modern hardware design starts with specifications provided in natural
language. These are then translated by hardware engineers into appropriate
Hardware Description Languages (HDLs) such as Verilog before synthesizing
circuit elements. Automating this translation could reduce sources of human
error from the engineering process. But, it is only recently that artificial
intelligence (AI) has demonstrated capabilities for machine-based end-to-end
design translations. Commercially-available instruction-tuned Large Language
Models (LLMs) such as OpenAI's ChatGPT and Google's Bard claim to be able to
produce code in a variety of programming languages; but studies examining them
for hardware are still lacking. In this work, we thus explore the challenges
faced and opportunities presented when leveraging these recent advances in LLMs
for hardware design. Given that these `conversational' LLMs perform best when
used interactively, we perform a case study where a hardware engineer
co-architects a novel 8-bit accumulator-based microprocessor architecture with
the LLM according to real-world hardware constraints. We then sent the
processor to tapeout in a Skywater 130nm shuttle, meaning that this `Chip-Chat'
resulted in what we believe to be the world's first wholly-AI-written HDL for
tapeout.",2023-05-22T17:13:33Z
,http://arxiv.org/pdf/2312.10771v1.pdf,"kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest
  Neighbor In-Context Learning","Task-Oriented Parsing (TOP) enables conversational assistants to interpret
user commands expressed in natural language, transforming them into structured
outputs that combine elements of both natural language and intent/slot tags.
Recently, Large Language Models (LLMs) have achieved impressive performance in
synthesizing computer programs based on a natural language prompt, mitigating
the gap between natural language and structured programs. Our paper focuses on
harnessing the capabilities of LLMs for semantic parsing tasks, addressing the
following three key research questions: 1) How can LLMs be effectively utilized
for semantic parsing tasks? 2) What defines an effective prompt? and 3) How can
LLM overcome the length constraint and streamline prompt design by including
all examples as prompts? We introduce k Nearest Neighbor In-Context
Learning(kNN-ICL), which simplifies prompt engineering by allowing it to be
built on top of any design strategy while providing access to all demo
examples. Extensive experiments show that: 1)Simple ICL without kNN search can
achieve a comparable performance with strong supervised models on the TOP
tasks, and 2) kNN-ICL significantly improves the comprehension of complex
requests by seamlessly integrating ICL with a nearest-neighbor approach.
Notably, this enhancement is achieved without the need for additional data or
specialized prompts.",2023-12-17T17:26:50Z
,http://arxiv.org/pdf/2201.07381v2.pdf,Unveiling Project-Specific Bias in Neural Code Models,"Deep learning has introduced significant improvements in many software
analysis tasks. Although the Large Language Models (LLMs) based neural code
models demonstrate commendable performance when trained and tested within the
intra-project independent and identically distributed (IID) setting, they often
struggle to generalize effectively to real-world inter-project
out-of-distribution (OOD) data. In this work, we show that this phenomenon is
caused by the heavy reliance on project-specific shortcuts for prediction
instead of ground-truth evidence. We propose a Cond-Idf measurement to
interpret this behavior, which quantifies the relatedness of a token with a
label and its project-specificness. The strong correlation between model
behavior and the proposed measurement indicates that without proper
regularization, models tend to leverage spurious statistical cues for
prediction. Equipped with these observations, we propose a novel bias
mitigation mechanism that regularizes the model's learning behavior by
leveraging latent logic relations among samples. Experimental results on two
representative program analysis tasks indicate that our mitigation framework
can improve both inter-project OOD generalization and adversarial robustness,
while not sacrificing accuracy on intra-project IID data.",2022-01-19T02:09:48Z
,http://arxiv.org/pdf/2305.10361v4.pdf,"Human Choice Prediction in Language-based Persuasion Games:
  Simulation-based Off-Policy Evaluation","Recent advances in Large Language Models (LLMs) have spurred interest in
designing LLM-based agents for tasks that involve interaction with human and
artificial agents. This paper addresses a key aspect in the design of such
agents: Predicting human decision in off-policy evaluation (OPE), focusing on
language-based persuasion games, where the agent's goal is to influence its
partner's decisions through verbal messages. Using a dedicated application, we
collected a dataset of 87K decisions from humans playing a repeated
decision-making game with artificial agents. Our approach involves training a
model on human interactions with one agents subset to predict decisions when
interacting with another. To enhance off-policy performance, we propose a
simulation technique involving interactions across the entire agent space and
simulated decision makers. Our learning strategy yields significant OPE gains,
e.g., improving prediction accuracy in the top 15% challenging cases by 7.1%.
Our code and the large dataset we collected and generated are submitted as
supplementary material and publicly available in our GitHub repository:
https://github.com/eilamshapira/HumanChoicePrediction",2023-05-17T16:38:11Z
,http://arxiv.org/pdf/2305.15408v5.pdf,"Towards Revealing the Mystery behind Chain of Thought: A Theoretical
  Perspective","Recent studies have discovered that Chain-of-Thought prompting (CoT) can
dramatically improve the performance of Large Language Models (LLMs),
particularly when dealing with complex tasks involving mathematics or
reasoning. Despite the enormous empirical success, the underlying mechanisms
behind CoT and how it unlocks the potential of LLMs remain elusive. In this
paper, we take a first step towards theoretically answering these questions.
Specifically, we examine the expressivity of LLMs with CoT in solving
fundamental mathematical and decision-making problems. By using circuit
complexity theory, we first give impossibility results showing that
bounded-depth Transformers are unable to directly produce correct answers for
basic arithmetic/equation tasks unless the model size grows super-polynomially
with respect to the input length. In contrast, we then prove by construction
that autoregressive Transformers of constant size suffice to solve both tasks
by generating CoT derivations using a commonly used math language format.
Moreover, we show LLMs with CoT can handle a general class of decision-making
problems known as Dynamic Programming, thus justifying its power in tackling
complex real-world tasks. Finally, an extensive set of experiments show that,
while Transformers always fail to directly predict the answers, they can
consistently learn to generate correct solutions step-by-step given sufficient
CoT demonstrations.",2023-05-24T17:59:21Z
,http://arxiv.org/pdf/2312.14972v3.pdf,"Scaling Down to Scale Up: A Cost-Benefit Analysis of Replacing OpenAI's
  LLM with Open Source SLMs in Production","Many companies use large language models (LLMs) offered as a service, like
OpenAI's GPT-4, to create AI-enabled product experiences. Along with the
benefits of ease-of-use and shortened time-to-solution, this reliance on
proprietary services has downsides in model control, performance reliability,
uptime predictability, and cost. At the same time, a flurry of open-source
small language models (SLMs) has been made available for commercial use.
However, their readiness to replace existing capabilities remains unclear, and
a systematic approach to holistically evaluate these SLMs is not readily
available. This paper presents a systematic evaluation methodology and a
characterization of modern open-source SLMs and their trade-offs when replacing
proprietary LLMs for a real-world product feature. We have designed SLaM, an
open-source automated analysis tool that enables the quantitative and
qualitative testing of product features utilizing arbitrary SLMs. Using SLaM,
we examine the quality and performance characteristics of modern SLMs relative
to an existing customer-facing implementation using the OpenAI GPT-4 API.
Across 9 SLMs and their 29 variants, we observe that SLMs provide competitive
results, significant performance consistency improvements, and a cost reduction
of 5x~29x when compared to GPT-4.",2023-12-20T19:27:59Z
10.1145/3650212.3652106,http://arxiv.org/pdf/2404.00640v2.pdf,"Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize
  Configuration Errors via Logs","Configurable software systems are prone to configuration errors, resulting in
significant losses to companies. However, diagnosing these errors is
challenging due to the vast and complex configuration space. These errors pose
significant challenges for both experienced maintainers and new end-users,
particularly those without access to the source code of the software systems.
Given that logs are easily accessible to most end-users, we conduct a
preliminary study to outline the challenges and opportunities of utilizing logs
in localizing configuration errors. Based on the insights gained from the
preliminary study, we propose an LLM-based two-stage strategy for end-users to
localize the root-cause configuration properties based on logs. We further
implement a tool, LogConfigLocalizer, aligned with the design of the
aforementioned strategy, hoping to assist end-users in coping with
configuration errors through log analysis.
  To the best of our knowledge, this is the first work to localize the
root-cause configuration properties for end-users based on Large Language
Models~(LLMs) and logs. We evaluate the proposed strategy on Hadoop by
LogConfigLocalizer and prove its efficiency with an average accuracy as high as
99.91%. Additionally, we also demonstrate the effectiveness and necessity of
different phases of the methodology by comparing it with two other variants and
a baseline tool. Moreover, we validate the proposed methodology through a
practical case study to demonstrate its effectiveness and feasibility.",2024-03-31T10:47:38Z
,http://arxiv.org/pdf/2307.04349v2.pdf,RLTF: Reinforcement Learning from Unit Test Feedback,"The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, current representative works
either rely solely on offline frameworks, limiting the exploration of new
sample spaces, or fall short in the utilization of unit test signals, not
accounting for specific error locations within the code. To address these
issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,
a novel online RL framework with unit test feedback of multi-granularity for
refining code LLMs. Our approach generates data in real-time during training
and simultaneously utilizes fine-grained feedback signals to guide the model
towards producing higher-quality code. Extensive experiments show that RLTF
achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our
code is available at: https://github.com/Zyq-scut/RLTF.",2023-07-10T05:18:18Z
,http://arxiv.org/pdf/2301.09043v3.pdf,CodeScore: Evaluating Code Generation by Learning Code Execution,"A proper code evaluation metric (CEM) profoundly impacts the evolution of
code generation, which is an important research field in NLP and software
engineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU)
suffer from two significant drawbacks. 1. They primarily measure the surface
differences between codes without considering their functional equivalence.
However, functional equivalence is pivotal in evaluating the effectiveness of
code generation, as different codes can perform identical operations. 2. They
are predominantly designed for the Ref-only input format. However, code
evaluation necessitates versatility in input formats. Aside from Ref-only,
there are NL-only and Ref\&NL formats, which existing match-based CEMs cannot
effectively accommodate. In this paper, we propose CodeScore, a large language
model (LLM)-based CEM, which estimates the functional correctness of generated
code on three input types. To acquire CodeScore, we present UniCE, a unified
code generation learning framework, for LLMs to learn code execution (i.e.,
learning PassRatio and Executability of generated code) with unified input.
Extensive experimental results on multiple code evaluation datasets demonstrate
that CodeScore absolutely improves up to 58.87% correlation with functional
correctness compared to other CEMs, achieves state-of-the-art performance, and
effectively handles three input formats.",2023-01-22T02:59:59Z
,http://arxiv.org/pdf/2405.07623v1.pdf,"COBias and Debias: Minimizing Language Model Pairwise Accuracy Bias via
  Nonlinear Integer Programming","For language model classification, would you prefer having only one workable
class or having every class working? The latter makes more practical uses.
Especially for large language models (LLMs), the fact that they achieve a fair
overall accuracy by in-context learning (ICL) obscures a large difference in
individual class accuracies. In this work, we uncover and tackle language
models' imbalance in per-class prediction accuracy by reconceptualizing it as
the Contextual Oddity Bias (COBias), and we are the first to engage nonlinear
integer programming (NIP) to debias it. Briefly, COBias refers to the
difference in accuracy by a class A compared to its ''odd'' class, which holds
the majority wrong predictions of class A. With the COBias metric, we reveal
that LLMs of varied scales and families exhibit large per-class accuracy
differences. Then we propose Debiasing as Nonlinear Integer Programming (DNIP)
to correct ICL per-class probabilities for lower bias and higher overall
accuracy. Our optimization objective is directly based on the evaluation scores
by COBias and accuracy metrics, solved by simulated annealing. Evaluations on
three LLMs across seven NLP classification tasks show that DNIP simultaneously
achieves significant COBias reduction ($-27\%$) and accuracy improvement
($+12\%$) over the conventional ICL approach, suggesting that modeling pairwise
class accuracy differences is a direction in pushing forward more accurate,
more reliable LLM predictions.",2024-05-13T10:30:33Z
,http://arxiv.org/pdf/2406.14867v1.pdf,"DistiLRR: Transferring Code Repair for Low-Resource Programming
  Languages","Large language models (LLMs) have shown remarkable performance on code
generation tasks. A recent application of LLMs for code generation is iterative
code repair, where a model fixes an incorrect program by rationalizing about
errors and generating a new program. However, code repair is primarily studied
on high-resource languages like Python, and the framework's efficacy is
under-explored on low-resource languages. To apply code repair for low-resource
languages, we propose Distilling Low-Resource Repairs (DistiLRR), an approach
that transfers the reasoning and code generation ability from a teacher model
to a student model. Our results show that DistiLRR consistently outperforms
baselines on low-resource languages, but has similar performance on
high-resource languages. To investigate this behavior, we perform a further
analysis and find that the correlation between rationale quality and code
correctness is weaker than previously perceived. We hypothesize this weakness
is magnified in low-resource settings where base models lack deep knowledge of
a programming language, leading to wavering benefits of code repair between
high-resource and low-resource languages.",2024-06-21T05:05:39Z
10.1109/ICACS60934.2024.10473289,http://arxiv.org/pdf/2404.12596v1.pdf,"Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level
  Knowledge Distillation","Over the past year, the field of Natural Language Generation (NLG) has
experienced an exponential surge, largely due to the introduction of Large
Language Models (LLMs). These models have exhibited the most effective
performance in a range of domains within the Natural Language Processing and
Generation domains. However, their application in domain-specific tasks, such
as paraphrasing, presents significant challenges. The extensive number of
parameters makes them difficult to operate on commercial hardware, and they
require substantial time for inference, leading to high costs in a production
setting. In this study, we tackle these obstacles by employing LLMs to develop
three distinct models for the paraphrasing field, applying a method referred to
as sequence-level knowledge distillation. These distilled models are capable of
maintaining the quality of paraphrases generated by the LLM. They demonstrate
faster inference times and the ability to generate diverse paraphrases of
comparable quality. A notable characteristic of these models is their ability
to exhibit syntactic diversity while also preserving lexical diversity,
features previously uncommon due to existing data quality issues in datasets
and not typically observed in neural-based approaches. Human evaluation of our
models shows that there is only a 4% drop in performance compared to the LLM
teacher model used in the distillation process, despite being 1000 times
smaller. This research provides a significant contribution to the NLG field,
offering a more efficient and cost-effective solution for paraphrasing tasks.",2024-04-19T02:59:09Z
,http://arxiv.org/pdf/2305.09434v1.pdf,"Chatting with GPT-3 for Zero-Shot Human-Like Mobile Automated GUI
  Testing","Mobile apps are indispensable for people's daily life, and automated GUI
(Graphical User Interface) testing is widely used for app quality assurance.
There is a growing interest in using learning-based techniques for automated
GUI testing which aims at generating human-like actions and interactions.
However, the limitations such as low testing coverage, weak generalization, and
heavy reliance on training data, make an urgent need for a more effective
approach to generate human-like actions to thoroughly test mobile apps.
Inspired by the success of the Large Language Model (LLM), e.g., GPT-3 and
ChatGPT, in natural language understanding and question answering, we formulate
the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM
to chat with the mobile apps by passing the GUI page information to LLM to
elicit testing scripts, and executing them to keep passing the app feedback to
LLM, iterating the whole process. Within it, we extract the static context of
the GUI page and the dynamic context of the iterative testing process, design
prompts for inputting this information to LLM, and develop a neural matching
network to decode the LLM's output into actionable steps to execute the app. We
evaluate GPTDroid on 86 apps from Google Play, and its activity coverage is
71%, with 32% higher than the best baseline, and can detect 36% more bugs with
faster speed than the best baseline. GPTDroid also detects 48 new bugs on the
Google Play with 25 of them being confirmed/fixed. We further summarize the
capabilities of GPTDroid behind the superior performance, including semantic
text input, compound action, long meaningful test trace, and test case
prioritization.",2023-05-16T13:46:52Z
,http://arxiv.org/pdf/2403.16073v1.pdf,"Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract
  Auditing with Justifications","Smart contracts are decentralized applications built atop blockchains like
Ethereum. Recent research has shown that large language models (LLMs) have
potential in auditing smart contracts, but the state-of-the-art indicates that
even GPT-4 can achieve only 30% precision (when both decision and justification
are correct). This is likely because off-the-shelf LLMs were primarily
pre-trained on a general text/code corpus and not fine-tuned on the specific
domain of Solidity smart contract auditing.
  In this paper, we propose TrustLLM, a general framework that combines
fine-tuning and LLM-based agents for intuitive smart contract auditing with
justifications. Specifically, TrustLLM is inspired by the observation that
expert human auditors first perceive what could be wrong and then perform a
detailed analysis of the code to identify the cause. As such, TrustLLM employs
a two-stage fine-tuning approach: it first tunes a Detector model to make
decisions and then tunes a Reasoner model to generate causes of
vulnerabilities. However, fine-tuning alone faces challenges in accurately
identifying the optimal cause of a vulnerability. Therefore, we introduce two
LLM-based agents, the Ranker and Critic, to iteratively select and debate the
most suitable cause of vulnerability based on the output of the fine-tuned
Reasoner model. To evaluate TrustLLM, we collected a balanced dataset with
1,734 positive and 1,810 negative samples to fine-tune TrustLLM. We then
compared it with traditional fine-tuned models (CodeBERT, GraphCodeBERT,
CodeT5, and UnixCoder) as well as prompt learning-based LLMs (GPT4, GPT-3.5,
and CodeLlama-13b/34b). On a dataset of 263 real smart contract
vulnerabilities, TrustLLM achieves an F1 score of 91.21% and an accuracy of
91.11%. The causes generated by TrustLLM achieved a consistency of about 38%
compared to the ground truth causes.",2024-03-24T09:26:53Z
,http://arxiv.org/pdf/2312.00886v4.pdf,Nash Learning from Human Feedback,"Reinforcement learning from human feedback (RLHF) has emerged as the main
paradigm for aligning large language models (LLMs) with human preferences.
Typically, RLHF involves the initial step of learning a reward model from human
feedback, often expressed as preferences between pairs of text generations
produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by
optimizing it to maximize the reward model through a reinforcement learning
algorithm. However, an inherent limitation of current reward models is their
inability to fully represent the richness of human preferences and their
dependency on the sampling distribution.
  In this study, we introduce an alternative pipeline for the fine-tuning of
LLMs using pairwise human feedback. Our approach entails the initial learning
of a preference model, which is conditioned on two inputs given a prompt,
followed by the pursuit of a policy that consistently generates responses
preferred over those generated by any competing policy, thus defining the Nash
equilibrium of this preference model. We term this approach Nash learning from
human feedback (NLHF).
  In the context of a tabular policy representation, we present a novel
algorithmic solution, Nash-MD, founded on the principles of mirror descent.
This algorithm produces a sequence of policies, with the last iteration
converging to the regularized Nash equilibrium. Additionally, we explore
parametric representations of policies and introduce gradient descent
algorithms for deep-learning architectures. To demonstrate the effectiveness of
our approach, we present experimental results involving the fine-tuning of a
LLM for a text summarization task. We believe NLHF offers a compelling avenue
for preference learning and policy optimization with the potential of advancing
the field of aligning LLMs with human preferences.",2023-12-01T19:26:23Z
,http://arxiv.org/pdf/2302.07867v5.pdf,Learning Performance-Improving Code Edits,"With the decline of Moore's law, optimizing program performance has become a
major focus of software research. However, high-level optimizations such as API
and algorithm changes remain elusive due to the difficulty of understanding the
semantics of code. Simultaneously, pretrained large language models (LLMs) have
demonstrated strong capabilities at solving a wide range of programming tasks.
To that end, we introduce a framework for adapting LLMs to high-level program
optimization. First, we curate a dataset of performance-improving edits made by
human programmers of over 77,000 competitive C++ programming submission pairs,
accompanied by extensive unit tests. A major challenge is the significant
variability of measuring performance on commodity hardware, which can lead to
spurious ""improvements."" To isolate and reliably evaluate the impact of program
optimizations, we design an environment based on the gem5 full system
simulator, the de facto simulator used in academia and industry. Next, we
propose a broad range of adaptation strategies for code optimization; for
prompting, these include retrieval-based few-shot prompting and
chain-of-thought, and for finetuning, these include performance-conditioned
generation and synthetic data augmentation based on self-play. A combination of
these techniques achieves a mean speedup of 6.86 with eight generations, higher
than average optimizations from individual programmers (3.66). Using our
model's fastest generations, we set a new upper limit on the fastest speedup
possible for our dataset at 9.64 compared to using the fastest human
submissions available (9.56).",2023-02-15T18:59:21Z
,http://arxiv.org/pdf/2302.09051v4.pdf,"Complex QA and language models hybrid architectures, Survey","This paper reviews the state-of-the-art of language models architectures and
strategies for ""complex"" question-answering (QA, CQA, CPS) with a focus on
hybridization. Large Language Models (LLM) are good at leveraging public data
on standard problems but once you want to tackle more specific complex
questions or problems (e.g. How does the concept of personal freedom vary
between different cultures ? What is the best mix of power generation methods
to reduce climate change ?) you may need specific architecture, knowledge,
skills, methods, sensitive data protection, explainability, human approval and
versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed
non-specialists to grasp the great potential as well as the equally strong
limitations of LLM in complex QA. In this paper, we start by reviewing required
skills and evaluation techniques. We integrate findings from the robust
community edited research papers BIG, BLOOM and HELM which open source,
benchmark and analyze limits and challenges of LLM in terms of tasks complexity
and strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as
a baseline. We discuss some challenges associated with complex QA, including
domain adaptation, decomposition and efficient multi-step QA, long form and
non-factoid QA, safety and multi-sensitivity data protection, multimodal
search, hallucinations, explainability and truthfulness, temporal reasoning. We
analyze current solutions and promising research trends, using elements such
as: hybrid LLM architectural patterns, training and prompting strategies,
active human reinforcement learning supervised with AI, neuro-symbolic and
structured knowledge grounding, program synthesis, iterated decomposition and
others.",2023-02-17T18:31:31Z
,http://arxiv.org/pdf/2304.10411v2.pdf,Attention Scheme Inspired Softmax Regression,"Large language models (LLMs) have made transformed changes for human society.
One of the key computation in LLMs is the softmax unit. This operation is
important in LLMs because it allows the model to generate a distribution over
possible next words or phrases, given a sequence of input words. This
distribution is then used to select the most likely next word or phrase, based
on the probabilities assigned by the model. The softmax unit plays a crucial
role in training LLMs, as it allows the model to learn from the data by
adjusting the weights and biases of the neural network.
  In the area of convex optimization such as using central path method to solve
linear programming. The softmax function has been used a crucial tool for
controlling the progress and stability of potential function [Cohen, Lee and
Song STOC 2019, Brand SODA 2020].
  In this work, inspired the softmax unit, we define a softmax regression
problem. Formally speaking, given a matrix $A \in \mathbb{R}^{n \times d}$ and
a vector $b \in \mathbb{R}^n$, the goal is to use greedy type algorithm to
solve \begin{align*} \min_{x} \| \langle \exp(Ax), {\bf 1}_n \rangle^{-1}
\exp(Ax) - b \|_2^2. \end{align*} In certain sense, our provable convergence
result provides theoretical support for why we can use greedy algorithm to
train softmax function in practice.",2023-04-20T15:50:35Z
,http://arxiv.org/pdf/2305.15338v1.pdf,"Measuring and Mitigating Constraint Violations of In-Context Learning
  for Utterance-to-API Semantic Parsing","In executable task-oriented semantic parsing, the system aims to translate
users' utterances in natural language to machine-interpretable programs (API
calls) that can be executed according to pre-defined API specifications. With
the popularity of Large Language Models (LLMs), in-context learning offers a
strong baseline for such scenarios, especially in data-limited regimes.
However, LLMs are known to hallucinate and therefore pose a formidable
challenge in constraining generated content. Thus, it remains uncertain if LLMs
can effectively perform task-oriented utterance-to-API generation where
respecting API's structural and task-specific constraints is crucial.
  In this work, we seek to measure, analyze and mitigate such constraints
violations. First, we identify the categories of various constraints in
obtaining API-semantics from task-oriented utterances, and define fine-grained
metrics that complement traditional ones. Second, we leverage these metrics to
conduct a detailed error analysis of constraints violations seen in
state-of-the-art LLMs, which motivates us to investigate two mitigation
strategies: Semantic-Retrieval of Demonstrations (SRD) and API-aware
Constrained Decoding (API-CD). Our experiments show that these strategies are
effective at reducing constraints violations and improving the quality of the
generated API calls, but require careful consideration given their
implementation complexity and latency.",2023-05-24T16:50:36Z
,http://arxiv.org/pdf/2306.10998v1.pdf,RepoFusion: Training Code Models to Understand Your Repository,"Despite the huge success of Large Language Models (LLMs) in coding assistants
like GitHub Copilot, these models struggle to understand the context present in
the repository (e.g., imports, parent classes, files with similar names, etc.),
thereby producing inaccurate code completions. This effect is more pronounced
when using these assistants for repositories that the model has not seen during
training, such as proprietary software or work-in-progress code projects.
Recent work has shown the promise of using context from the repository during
inference. In this work, we extend this idea and propose RepoFusion, a
framework to train models to incorporate relevant repository context.
Experiments on single-line code completion show that our models trained with
repository context significantly outperform much larger code models as
CodeGen-16B-multi ($\sim73\times$ larger) and closely match the performance of
the $\sim 70\times$ larger StarCoderBase model that was trained with the
Fill-in-the-Middle objective. We find these results to be a novel and
compelling demonstration of the gains that training with repository context can
bring. We carry out extensive ablation studies to investigate the impact of
design choices such as context type, number of contexts, context length, and
initialization within our framework. Lastly, we release Stack-Repo, a dataset
of 200 Java repositories with permissive licenses and near-deduplicated files
that are augmented with three types of repository contexts. Additionally, we
are making available the code and trained checkpoints for our work. Our
released resources can be found at \url{https://huggingface.co/RepoFusion}.",2023-06-19T15:05:31Z
,http://arxiv.org/pdf/2406.11118v1.pdf,Incentivizing Quality Text Generation via Statistical Contracts,"While the success of large language models (LLMs) increases demand for
machine-generated text, current pay-per-token pricing schemes create a
misalignment of incentives known in economics as moral hazard: Text-generating
agents have strong incentive to cut costs by preferring a cheaper model over
the cutting-edge one, and this can be done ""behind the scenes"" since the agent
performs inference internally. In this work, we approach this issue from an
economic perspective, by proposing a pay-for-performance, contract-based
framework for incentivizing quality. We study a principal-agent game where the
agent generates text using costly inference, and the contract determines the
principal's payment for the text according to an automated quality evaluation.
Since standard contract theory is inapplicable when internal inference costs
are unknown, we introduce cost-robust contracts. As our main theoretical
contribution, we characterize optimal cost-robust contracts through a direct
correspondence to optimal composite hypothesis tests from statistics,
generalizing a result of Saig et al. (NeurIPS'23). We evaluate our framework
empirically by deriving contracts for a range of objectives and LLM evaluation
benchmarks, and find that cost-robust contracts sacrifice only a marginal
increase in objective value compared to their cost-aware counterparts.",2024-06-17T00:30:58Z
,http://arxiv.org/pdf/2307.00012v3.pdf,"FlakyFix: Using Large Language Models for Predicting Flaky Test Fix
  Categories and Test Code Repair","Flaky tests are problematic because they non-deterministically pass or fail
for the same software version under test, causing confusion and wasting
development effort. While machine learning models have been used to predict
flakiness and its root causes, there is much less work on providing support to
fix the problem. To address this gap, in this paper, we focus on predicting the
type of fix that is required to remove flakiness and then repair the test code
on that basis. We do this for a subset of flaky test cases where the root cause
of flakiness is in the test case itself and not in the production code. Our key
idea is to guide the repair process with additional knowledge about the test's
flakiness in the form of its predicted fix category. Thus, we first propose a
framework that automatically generates labeled datasets for 13 fix categories
and trains models to predict the fix category of a flaky test by analyzing the
test code only. Our experimental results using code models and few-shot
learning show that we can correctly predict most of the fix categories. To show
the usefulness of such fix category labels for automatically repairing
flakiness, in addition to informing testers, we augment a Large Language Model
(LLM) like GPT with such extra knowledge to ask the LLM for repair suggestions.
The results show that our suggested fix category labels, complemented with
in-context learning, significantly enhance the capability of GPT 3.5 Turbo in
generating fixes for flaky tests. Based on the execution and analysis of a
sample of GPT-repaired flaky tests, we estimate that a large percentage of such
repairs, (roughly between 70% and 90%) can be expected to pass. For the failing
repaired tests, on average, 16% of the test code needs to be further changed
for them to pass.",2023-06-21T19:34:16Z
,http://arxiv.org/pdf/2312.03815v2.pdf,"LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent
  Ecosystem","This paper envisions a revolutionary AIOS-Agent ecosystem, where Large
Language Model (LLM) serves as the (Artificial) Intelligent Operating System
(IOS, or AIOS)--an operating system ""with soul"". Upon this foundation, a
diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are
developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift
from the traditional OS-APP ecosystem. We envision that LLM's impact will not
be limited to the AI application level, instead, it will in turn revolutionize
the design and implementation of computer system, architecture, software, and
programming language, featured by several main concepts: LLM as OS
(system-level), Agents as Applications (application-level), Natural Language as
Programming Interface (user-level), and Tools as Devices/Libraries
(hardware/middleware-level). We begin by introducing the architecture of
traditional OS. Then we formalize a conceptual framework for AIOS through ""LLM
as OS (LLMOS)"", drawing analogies between AIOS and traditional OS: LLM is
likened to OS kernel, context window to memory, external storage to file
system, hardware tools to peripheral devices, software tools to programming
libraries, and user prompts to user commands. Subsequently, we introduce the
new AIOS-Agent Ecosystem, where users can easily program Agent Applications
(AAPs) using natural language, democratizing the development of software, which
is different from the traditional OS-APP ecosystem. Following this, we explore
the diverse scope of Agent Applications. We delve into both single-agent and
multi-agent systems, as well as human-agent interaction. Lastly, drawing on the
insights from traditional OS-APP ecosystem, we propose a roadmap for the
evolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the
future research and development, suggesting systematic progresses of AIOS and
its Agent applications.",2023-12-06T18:50:26Z
,http://arxiv.org/pdf/2402.05808v2.pdf,"Training Large Language Models for Reasoning through Reverse Curriculum
  Reinforcement Learning","In this paper, we propose R$^3$: Learning Reasoning through Reverse
Curriculum Reinforcement Learning (RL), a novel method that employs only
outcome supervision to achieve the benefits of process supervision for large
language models. The core challenge in applying RL to complex reasoning is to
identify a sequence of actions that result in positive rewards and provide
appropriate supervision for optimization. Outcome supervision provides sparse
rewards for final results without identifying error locations, whereas process
supervision offers step-wise rewards but requires extensive manual annotation.
R$^3$ overcomes these limitations by learning from correct demonstrations.
Specifically, R$^3$ progressively slides the start state of reasoning from a
demonstration's end to its beginning, facilitating easier model exploration at
all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome
supervision to offer step-level signals and precisely pinpoint errors. Using
Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$
points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds
the baseline by $4.2$ points across three backbone models, and without any
extra data, Codellama-7B + R$^3$ performs comparable to larger models or
closed-source models.",2024-02-08T16:46:26Z
,http://arxiv.org/pdf/2404.09151v2.pdf,"Emerging Platforms Meet Emerging LLMs: A Year-Long Journey of Top-Down
  Development","Deploying machine learning (ML) on diverse computing platforms is crucial to
accelerate and broaden their applications. However, it presents significant
software engineering challenges due to the fast evolution of models, especially
the recent Large Language Models (LLMs), and the emergence of new computing
platforms. Current ML frameworks are primarily engineered for CPU and CUDA
platforms, leaving a big gap in enabling emerging ones like Metal, Vulkan, and
WebGPU.
  While a traditional bottom-up development pipeline fails to close the gap
timely, we introduce TapML, a top-down approach and tooling designed to
streamline the deployment of ML systems on diverse platforms, optimized for
developer productivity. Unlike traditional bottom-up methods, which involve
extensive manual testing and debugging, TapML automates unit testing through
test carving and adopts a migration-based strategy for gradually offloading
model computations from mature source platforms to emerging target platforms.
By leveraging realistic inputs and remote connections for gradual target
offloading, TapML accelerates the validation and minimizes debugging scopes,
significantly optimizing development efforts.
  TapML was developed and applied through a year-long, real-world effort that
successfully deployed significant emerging models and platforms. Through
serious deployments of 82 emerging models in 17 distinct architectures across 5
emerging platforms, we showcase the effectiveness of TapML in enhancing
developer productivity while ensuring model reliability and efficiency.
Furthermore, we summarize comprehensive case studies from our real-world
development, offering best practices for developing emerging ML systems.",2024-04-14T06:09:35Z
,http://arxiv.org/pdf/2310.05628v3.pdf,"Glitter or Gold? Deriving Structured Insights from Sustainability
  Reports via Large Language Models","Over the last decade, several regulatory bodies have started requiring the
disclosure of non-financial information from publicly listed companies, in
light of the investors' increasing attention to Environmental, Social, and
Governance (ESG) issues. Publicly released information on sustainability
practices is often disclosed in diverse, unstructured, and multi-modal
documentation. This poses a challenge in efficiently gathering and aligning the
data into a unified framework to derive insights related to Corporate Social
Responsibility (CSR). Thus, using Information Extraction (IE) methods becomes
an intuitive choice for delivering insightful and actionable data to
stakeholders. In this study, we employ Large Language Models (LLMs), In-Context
Learning, and the Retrieval-Augmented Generation (RAG) paradigm to extract
structured insights related to ESG aspects from companies' sustainability
reports. We then leverage graph-based representations to conduct statistical
analyses concerning the extracted insights. These analyses revealed that ESG
criteria cover a wide range of topics, exceeding 500, often beyond those
considered in existing categorizations, and are addressed by companies through
a variety of initiatives. Moreover, disclosure similarities emerged among
companies from the same region or sector, validating ongoing hypotheses in the
ESG literature. Lastly, by incorporating additional company attributes into our
analyses, we investigated which factors impact the most on companies' ESG
ratings, showing that ESG disclosure affects the obtained ratings more than
other financial or company data.",2023-10-09T11:34:41Z
,http://arxiv.org/pdf/2403.09308v1.pdf,"Enabling Waypoint Generation for Collaborative Robots using LLMs and
  Mixed Reality","Programming a robotic is a complex task, as it demands the user to have a
good command of specific programming languages and awareness of the robot's
physical constraints. We propose a framework that simplifies robot deployment
by allowing direct communication using natural language. It uses large language
models (LLM) for prompt processing, workspace understanding, and waypoint
generation. It also employs Augmented Reality (AR) to provide visual feedback
of the planned outcome. We showcase the effectiveness of our framework with a
simple pick-and-place task, which we implement on a real robot. Moreover, we
present an early concept of expressive robot behavior and skill generation that
can be used to communicate with the user and learn new skills (e.g., object
grasping).",2024-03-14T11:59:07Z
,http://arxiv.org/pdf/2206.02928v6.pdf,Neuro-Symbolic Procedural Planning with Commonsense Prompting,"Procedural planning aims to implement complex high-level goals by
decomposition into sequential simpler low-level steps. Although procedural
planning is a basic skill set for humans in daily life, it remains a challenge
for large language models (LLMs) that lack a deep understanding of the
cause-effect relations in procedures. Previous methods require manual exemplars
to acquire procedural planning knowledge from LLMs in the zero-shot setting.
However, such elicited pre-trained knowledge in LLMs induces spurious
correlations between goals and steps, which impair the model generalization to
unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural
PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with
commonsense-infused prompting. To mitigate spurious goal-step correlations, we
use symbolic program executors on the latent procedural representations to
formalize prompts from commonsense knowledge bases as a causal intervention
toward the Structural Causal Model. Both automatic and human evaluations on
WikiHow and RobotHow show the superiority of PLAN on procedural planning
without further training or manual exemplars.",2022-06-06T22:09:52Z
,http://arxiv.org/pdf/2310.16937v2.pdf,Learning Transfers over Several Programming Languages,"Large language models (LLMs) have become remarkably good at improving
developer productivity for high-resource programming languages. These models
use two kinds of data: large amounts of unlabeled code samples for pre-training
and relatively smaller amounts of labeled code samples for fine-tuning or
in-context learning. Unfortunately, many programming languages are
low-resource, lacking labeled samples for most tasks and often even lacking
unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or
new languages) miss out on the benefits of LLMs. Cross-lingual transfer uses
data from a source language to improve model performance on a target language.
It has been well-studied for natural languages, but has received little
attention for programming languages. This paper reports extensive experiments
on four tasks using a transformer-based LLM and 11 to 41 programming languages
to explore the following questions. First, how well does cross-lingual transfer
work for a given task across different language pairs. Second, given a task and
target language, how should one choose a source language. Third, which
characteristics of a language pair are predictive of transfer performance, and
how does that depend on the given task. Our empirical study with 1,808
experiments reveals practical and scientific insights, such as Kotlin and
JavaScript being the most transferable source languages and different tasks
relying on substantially different features. Overall, we find that learning
transfers well across several programming languages.",2023-10-25T19:04:33Z
,http://arxiv.org/pdf/2404.10155v1.pdf,Quality Assessment of Prompts Used in Code Generation,"Large Language Models (LLMs) are gaining popularity among software engineers.
A crucial aspect of developing effective code-generation LLMs is to evaluate
these models using a robust benchmark. Evaluation benchmarks with quality
issues can provide a false sense of performance. In this work, we conduct the
first-of-its-kind study of the quality of prompts within benchmarks used to
compare the performance of different code generation models. To conduct this
study, we analyzed 3,566 prompts from 9 code generation benchmarks to identify
quality issues in them. We also investigated whether fixing the identified
quality issues in the benchmarks' prompts affects a model's performance. We
also studied memorization issues of the evaluation dataset, which can put into
question a benchmark's trustworthiness. We found that code generation
evaluation benchmarks mainly focused on Python and coding exercises and had
very limited contextual dependencies to challenge the model. These datasets and
the developers' prompts suffer from quality issues like spelling and
grammatical errors, unclear sentences to express developers' intent, and not
using proper documentation style. Fixing all these issues in the benchmarks can
lead to a better performance for Python code generation, but not a significant
improvement was observed for Java code generation. We also found evidence that
GPT-3.5-Turbo and CodeGen-2.5 models possibly have data contamination issues.",2024-04-15T22:02:58Z
,http://arxiv.org/pdf/2402.00854v3.pdf,"SymbolicAI: A framework for logic-based approaches combining generative
  models and solvers","We introduce SymbolicAI, a versatile and modular framework employing a
logic-based approach to concept learning and flow management in generative
processes. SymbolicAI enables the seamless integration of generative models
with a diverse range of solvers by treating large language models (LLMs) as
semantic parsers that execute tasks based on both natural and formal language
instructions, thus bridging the gap between symbolic reasoning and generative
AI. We leverage probabilistic programming principles to tackle complex tasks,
and utilize differentiable and classical programming paradigms with their
respective strengths. The framework introduces a set of polymorphic,
compositional, and self-referential operations for multi-modal data that
connects multi-step generative processes and aligns their outputs with user
objectives in complex workflows. As a result, we can transition between the
capabilities of various foundation models with in-context learning capabilities
and specialized, fine-tuned models or solvers proficient in addressing specific
problems. Through these operations based on in-context learning our framework
enables the creation and evaluation of explainable computational graphs.
Finally, we introduce a quality measure and its empirical score for evaluating
these computational graphs, and propose a benchmark that compares various
state-of-the-art LLMs across a set of complex workflows. We refer to the
empirical score as the ""Vector Embedding for Relational Trajectory Evaluation
through Cross-similarity"", or VERTEX score for short. The framework codebase
and benchmark are linked below.",2024-02-01T18:50:50Z
,http://arxiv.org/pdf/2309.10168v1.pdf,Few-Shot Adaptation for Parsing Contextual Utterances with LLMs,"We evaluate the ability of semantic parsers based on large language models
(LLMs) to handle contextual utterances. In real-world settings, there typically
exists only a limited number of annotated contextual utterances due to
annotation cost, resulting in an imbalance compared to non-contextual
utterances. Therefore, parsers must adapt to contextual utterances with a few
training examples. We examine four major paradigms for doing so in
conversational semantic parsing i.e., Parse-with-Utterance-History,
Parse-with-Reference-Program, Parse-then-Resolve, and Rewrite-then-Parse. To
facilitate such cross-paradigm comparisons, we construct
SMCalFlow-EventQueries, a subset of contextual examples from SMCalFlow with
additional annotations. Experiments with in-context learning and fine-tuning
suggest that Rewrite-then-Parse is the most promising paradigm when
holistically considering parsing accuracy, annotation cost, and error types.",2023-09-18T21:35:19Z
,http://arxiv.org/pdf/2403.07921v1.pdf,"Merino: Entropy-driven Design for Generative Language Models on IoT
  Devices","Generative Large Language Models (LLMs) stand as a revolutionary advancement
in the modern era of artificial intelligence (AI). However, directly deploying
LLMs in resource-constrained hardware, such as Internet-of-Things (IoT)
devices, is difficult due to their high computational cost. In this paper, we
propose a novel information-entropy framework for designing mobile-friendly
generative language models. Our key design paradigm is to maximize the entropy
of transformer decoders within the given computational budgets. The whole
design procedure involves solving a mathematical programming (MP) problem,
which can be done on the CPU within minutes, making it nearly zero-cost. We
evaluate our designed models, termed MeRino, across nine NLP downstream tasks,
showing their competitive performance against the state-of-the-art
autoregressive transformer models under the mobile setting. Notably, MeRino
achieves similar or better zero performance compared to the 350M parameter OPT
while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model
size. Code will be made available soon.",2024-02-28T03:20:27Z
,http://arxiv.org/pdf/2404.03732v1.pdf,"SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based
  Classification for Hallucination Detection","We describe the University of Amsterdam Intelligent Data Engineering Lab
team's entry for the SemEval-2024 Task 6 competition. The SHROOM-INDElab system
builds on previous work on using prompt programming and in-context learning
with large language models (LLMs) to build classifiers for hallucination
detection, and extends that work through the incorporation of context-specific
definition of task, role, and target concept, and automated generation of
examples for use in a few-shot prompting approach. The resulting system
achieved fourth-best and sixth-best performance in the model-agnostic track and
model-aware tracks for Task 6, respectively, and evaluation using the
validation sets showed that the system's classification decisions were
consistent with those of the crowd-sourced human labellers. We further found
that a zero-shot approach provided better accuracy than a few-shot approach
using automatically generated examples. Code for the system described in this
paper is available on Github.",2024-04-04T18:01:21Z
,http://arxiv.org/pdf/2308.00065v1.pdf,"FinPT: Financial Risk Prediction with Profile Tuning on Pretrained
  Foundation Models","Financial risk prediction plays a crucial role in the financial sector.
Machine learning methods have been widely applied for automatically detecting
potential risks and thus saving the cost of labor. However, the development in
this field is lagging behind in recent years by the following two facts: 1) the
algorithms used are somewhat outdated, especially in the context of the fast
advance of generative AI and large language models (LLMs); 2) the lack of a
unified and open-sourced financial benchmark has impeded the related research
for years. To tackle these issues, we propose FinPT and FinBench: the former is
a novel approach for financial risk prediction that conduct Profile Tuning on
large pretrained foundation models, and the latter is a set of high-quality
datasets on financial risks such as default, fraud, and churn. In FinPT, we
fill the financial tabular data into the pre-defined instruction template,
obtain natural-language customer profiles by prompting LLMs, and fine-tune
large foundation models with the profile text to make predictions. We
demonstrate the effectiveness of the proposed FinPT by experimenting with a
range of representative strong baselines on FinBench. The analytical studies
further deepen the understanding of LLMs for financial risk prediction.",2023-07-22T09:27:05Z
,http://arxiv.org/pdf/2304.06912v2.pdf,How well do SOTA legal reasoning models support abductive reasoning?,"We examine how well the state-of-the-art (SOTA) models used in legal
reasoning support abductive reasoning tasks. Abductive reasoning is a form of
logical inference in which a hypothesis is formulated from a set of
observations, and that hypothesis is used to explain the observations. The
ability to formulate such hypotheses is important for lawyers and legal
scholars as it helps them articulate logical arguments, interpret laws, and
develop legal theories. Our motivation is to consider the belief that deep
learning models, especially large language models (LLMs), will soon replace
lawyers because they perform well on tasks related to legal text processing.
But to do so, we believe, requires some form of abductive hypothesis formation.
In other words, while LLMs become more popular and powerful, we want to
investigate their capacity for abductive reasoning. To pursue this goal, we
start by building a logic-augmented dataset for abductive reasoning with
498,697 samples and then use it to evaluate the performance of a SOTA model in
the legal field. Our experimental results show that although these models can
perform well on tasks related to some aspects of legal text processing, they
still fall short in supporting abductive reasoning tasks.",2023-04-14T03:36:24Z
10.5121/csit.2024.140203,http://arxiv.org/pdf/2311.01256v2.pdf,"An energy-based comparative analysis of common approaches to text
  classification in the Legal domain","Most Machine Learning research evaluates the best solutions in terms of
performance. However, in the race for the best performing model, many important
aspects are often overlooked when, on the contrary, they should be carefully
considered. In fact, sometimes the gaps in performance between different
approaches are neglectable, whereas factors such as production costs, energy
consumption, and carbon footprint must take into consideration. Large Language
Models (LLMs) are extensively adopted to address NLP problems in academia and
industry. In this work, we present a detailed quantitative comparison of LLM
and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes
into account both performance (standard indices) and alternative metrics such
as timing, power consumption and cost, in a word: the carbon-footprint. In our
analysis, we considered the prototyping phase (model selection by
training-validation-test iterations) and in-production phases separately, since
they follow different implementation procedures and also require different
resources. The results indicate that very often, the simplest algorithms
achieve performance very close to that of large LLMs but with very low power
consumption and lower resource demands. The results obtained could suggest
companies to include additional evaluations in the choice of Machine Learning
(ML) solutions.",2023-11-02T14:16:48Z
,http://arxiv.org/pdf/2308.02432v1.pdf,"Performance of Large Language Models in a Computer Science Degree
  Program","Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and
dominate the current discourse. Their transformative capabilities have led to a
paradigm shift in how we interact with and utilize (text-based) information.
Each day, new possibilities to leverage the capabilities of these models
emerge. This paper presents findings on the performance of different large
language models in a university of applied sciences' undergraduate computer
science degree program. Our primary objective is to assess the effectiveness of
these models within the curriculum by employing them as educational aids. By
prompting the models with lecture material, exercise tasks, and past exams, we
aim to evaluate their proficiency across different computer science domains. We
showcase the strong performance of current large language models while
highlighting limitations and constraints within the context of such a degree
program. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10
tested modules, BingAI achieved 68.4%, and LLaMa, in the 65 billion parameter
variant, 20%. Despite these convincing results, even GPT-4.0 would not pass the
degree program - due to limitations in mathematical calculations.",2023-07-24T14:17:00Z
,http://arxiv.org/pdf/2311.13743v2.pdf,"FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and
  Character Design","Recent advancements in Large Language Models (LLMs) have exhibited notable
efficacy in question-answering (QA) tasks across diverse domains. Their prowess
in integrating extensive web knowledge has fueled interest in developing
LLM-based autonomous agents. While LLMs are efficient in decoding human
instructions and deriving solutions by holistically processing historical
inputs, transitioning to purpose-driven agents requires a supplementary
rational architecture to process multi-source information, establish reasoning
chains, and prioritize critical tasks. Addressing this, we introduce
\textsc{FinMem}, a novel LLM-based agent framework devised for financial
decision-making. It encompasses three core modules: Profiling, to customize the
agent's characteristics; Memory, with layered message processing, to aid the
agent in assimilating hierarchical financial data; and Decision-making, to
convert insights gained from memories into investment decisions. Notably,
\textsc{FinMem}'s memory module aligns closely with the cognitive structure of
human traders, offering robust interpretability and real-time tuning. Its
adjustable cognitive span allows for the retention of critical information
beyond human perceptual limits, thereby enhancing trading outcomes. This
framework enables the agent to self-evolve its professional knowledge, react
agilely to new investment cues, and continuously refine trading decisions in
the volatile financial environment. We first compare \textsc{FinMem} with
various algorithmic agents on a scalable real-world financial dataset,
underscoring its leading trading performance in stocks. We then fine-tuned the
agent's perceptual span and character setting to achieve a significantly
enhanced trading performance. Collectively, \textsc{FinMem} presents a
cutting-edge LLM agent framework for automated trading, boosting cumulative
investment returns.",2023-11-23T00:24:40Z
,http://arxiv.org/pdf/2210.11468v1.pdf,"ObSynth: An Interactive Synthesis System for Generating Object Models
  from Natural Language Specifications","We introduce ObSynth, an interactive system leveraging the domain knowledge
embedded in large language models (LLMs) to help users design object models
from high level natural language prompts. This is an example of specification
reification, the process of taking a high-level, potentially vague
specification and reifying it into a more concrete form. We evaluate ObSynth
via a user study, leading to three key findings: first, object models designed
using ObSynth are more detailed, showing that it often synthesizes fields users
might have otherwise omitted. Second, a majority of objects, methods, and
fields generated by ObSynth are kept by the user in the final object model,
highlighting the quality of generated components. Third, ObSynth altered the
workflow of participants: they focus on checking that synthesized components
were correct rather than generating them from scratch, though ObSynth did not
reduce the time participants took to generate object models.",2022-10-20T17:59:19Z
,http://arxiv.org/pdf/2403.00039v1.pdf,"FhGenie: A Custom, Confidentiality-preserving Chat AI for Corporate and
  Scientific Use","Since OpenAI's release of ChatGPT, generative AI has received significant
attention across various domains. These AI-based chat systems have the
potential to enhance the productivity of knowledge workers in diverse tasks.
However, the use of free public services poses a risk of data leakage, as
service providers may exploit user input for additional training and
optimization without clear boundaries. Even subscription-based alternatives
sometimes lack transparency in handling user data. To address these concerns
and enable Fraunhofer staff to leverage this technology while ensuring
confidentiality, we have designed and developed a customized chat AI called
FhGenie (genie being a reference to a helpful spirit). Within few days of its
release, thousands of Fraunhofer employees started using this service. As
pioneers in implementing such a system, many other organizations have followed
suit. Our solution builds upon commercial large language models (LLMs), which
we have carefully integrated into our system to meet our specific requirements
and compliance constraints, including confidentiality and GDPR. In this paper,
we share detailed insights into the architectural considerations, design,
implementation, and subsequent updates of FhGenie. Additionally, we discuss
challenges, observations, and the core lessons learned from its productive
usage.",2024-02-29T09:43:50Z
,http://arxiv.org/pdf/2406.04464v1.pdf,"On The Importance of Reasoning for Context Retrieval in Repository-Level
  Code Editing","Recent advancements in code-fluent Large Language Models (LLMs) enabled the
research on repository-level code editing. In such tasks, the model navigates
and modifies the entire codebase of a project according to request. Hence, such
tasks require efficient context retrieval, i.e., navigating vast codebases to
gather relevant context. Despite the recognized importance of context
retrieval, existing studies tend to approach repository-level coding tasks in
an end-to-end manner, rendering the impact of individual components within
these complicated systems unclear. In this work, we decouple the task of
context retrieval from the other components of the repository-level code
editing pipelines. We lay the groundwork to define the strengths and weaknesses
of this component and the role that reasoning plays in it by conducting
experiments that focus solely on context retrieval. We conclude that while the
reasoning helps to improve the precision of the gathered context, it still
lacks the ability to identify its sufficiency. We also outline the ultimate
role of the specialized tools in the process of context gathering. The code
supplementing this paper is available at
https://github.com/JetBrains-Research/ai-agents-code-editing.",2024-06-06T19:44:17Z
,http://arxiv.org/pdf/2402.08303v4.pdf,ChatCell: Facilitating Single-Cell Analysis with Natural Language,"As Large Language Models (LLMs) rapidly evolve, their influence in science is
becoming increasingly prominent. The emerging capabilities of LLMs in task
generalization and free-form dialogue can significantly advance fields like
chemistry and biology. However, the field of single-cell biology, which forms
the foundational building blocks of living organisms, still faces several
challenges. High knowledge barriers and limited scalability in current methods
restrict the full exploitation of LLMs in mastering single-cell data, impeding
direct accessibility and rapid iteration. To this end, we introduce ChatCell,
which signifies a paradigm shift by facilitating single-cell analysis with
natural language. Leveraging vocabulary adaptation and unified sequence
generation, ChatCell has acquired profound expertise in single-cell biology and
the capability to accommodate a diverse range of analysis tasks. Extensive
experiments further demonstrate ChatCell's robust performance and potential to
deepen single-cell insights, paving the way for more accessible and intuitive
exploration in this pivotal field. Our project homepage is available at
https://zjunlp.github.io/project/ChatCell.",2024-02-13T09:06:14Z
,http://arxiv.org/pdf/2310.10692v4.pdf,"ACES: Generating Diverse Programming Puzzles with with Autotelic
  Generative Models","The ability to invent novel and interesting problems is a remarkable feature
of human intelligence that drives innovation, art, and science. We propose a
method that aims to automate this process by harnessing the power of
state-of-the-art generative models to produce a diversity of challenging yet
solvable problems, here in the context of Python programming puzzles. Inspired
by the intrinsically motivated literature, Autotelic CodE Search (ACES) jointly
optimizes for the diversity and difficulty of generated problems. We represent
problems in a space of LLM-generated semantic descriptors describing the
programming skills required to solve them (e.g. string manipulation, dynamic
programming, etc.) and measure their difficulty empirically as a linearly
decreasing function of the success rate of Llama-3-70B, a state-of-the-art LLM
problem solver. ACES iteratively prompts a large language model to generate
difficult problems achieving a diversity of target semantic descriptors
(goal-directed exploration) using previously generated problems as in-context
examples. ACES generates problems that are more diverse and more challenging
than problems produced by baseline methods and three times more challenging
than problems found in existing Python programming benchmarks on average across
11 state-of-the-art code LLMs.",2023-10-15T14:57:14Z
,http://arxiv.org/pdf/2305.11176v3.pdf,"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions
  with Large Language Model","Foundation models have made significant strides in various applications,
including text-to-image generation, panoptic segmentation, and natural language
processing. This paper presents Instruct2Act, a framework that utilizes Large
Language Models to map multi-modal instructions to sequential actions for
robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to
generate Python programs that constitute a comprehensive perception, planning,
and action loop for robotic tasks. In the perception section, pre-defined APIs
are used to access multiple foundation models where the Segment Anything Model
(SAM) accurately locates candidate objects, and CLIP classifies them. In this
way, the framework leverages the expertise of foundation models and robotic
abilities to convert complex high-level instructions into precise policy codes.
Our approach is adjustable and flexible in accommodating various instruction
modalities and input types and catering to specific task demands. We validated
the practicality and efficiency of our approach by assessing it on robotic
tasks in different scenarios within tabletop manipulation domains. Furthermore,
our zero-shot method outperformed many state-of-the-art learning-based policies
in several tasks. The code for our proposed approach is available at
https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for
high-level robotic instruction tasks with assorted modality inputs.",2023-05-18T17:59:49Z
,http://arxiv.org/pdf/2312.14188v2.pdf,"Enhancing Neural Theorem Proving through Data Augmentation and Dynamic
  Sampling Method","Theorem proving is a fundamental task in mathematics. With the advent of
large language models (LLMs) and interactive theorem provers (ITPs) like Lean,
there has been growing interest in integrating LLMs and ITPs to automate
theorem proving. In this approach, the LLM generates proof steps (tactics), and
the ITP checks the applicability of the tactics at the current goal. The two
systems work together to complete the proof. In this paper, we introduce
DS-Prover, a novel dynamic sampling method for theorem proving. This method
dynamically determines the number of tactics to apply to expand the current
goal, taking into account the remaining time compared to the total allocated
time for proving a theorem. This makes the proof search process more efficient
by adjusting the balance between exploration and exploitation as time passes.
We also augment the training dataset by decomposing simplification and rewrite
tactics with multiple premises into tactics with single premises. This gives
the model more examples to learn from and helps it to predict the tactics with
premises more accurately. We perform our experiments using the Mathlib dataset
of the Lean theorem prover and report the performance on two standard datasets,
MiniF2F and ProofNet. Our methods achieve significant performance gains on both
datasets. We achieved a state-of-the-art performance (Pass@1) of 14.2% on the
ProofNet dataset and a performance of 29.8% on MiniF2F, slightly surpassing the
best-reported Pass@1 of 29.6% using Lean.",2023-12-20T09:55:21Z
,http://arxiv.org/pdf/2406.00030v1.pdf,Large Language Model Pruning,"We surely enjoy the larger the better models for their superior performance
in the last couple of years when both the hardware and software support the
birth of such extremely huge models. The applied fields include text mining and
others. In particular, the success of LLMs on text understanding and text
generation draws attention from researchers who have worked on NLP and related
areas for years or even decades. On the side, LLMs may suffer from problems
like model overfitting, hallucination, and device limitation to name a few. In
this work, we suggest a model pruning technique specifically focused on LLMs.
The proposed methodology emphasizes the explainability of deep learning models.
By having the theoretical foundation, we obtain a trustworthy deep model so
that huge models with a massive number of model parameters become not quite
necessary. A mutual information-based estimation is adopted to find neurons
with redundancy to eliminate. Moreover, an estimator with well-tuned parameters
helps to find precise estimation to guide the pruning procedure. At the same
time, we also explore the difference between pruning on large-scale models vs.
pruning on small-scale models. The choice of pruning criteria is sensitive in
small models but not for large-scale models. It is a novel finding through this
work. Overall, we demonstrate the superiority of the proposed model to the
state-of-the-art models.",2024-05-24T18:22:15Z
,http://arxiv.org/pdf/2307.12856v4.pdf,"A Real-World WebAgent with Planning, Long Context Understanding, and
  Program Synthesis","Pre-trained large language models (LLMs) have recently achieved better
generalization and sample efficiency in autonomous web automation. However, the
performance on real-world websites has still suffered from (1) open domainness,
(2) limited context length, and (3) lack of inductive bias on HTML. We
introduce WebAgent, an LLM-driven agent that learns from self-experience to
complete tasks on real websites following natural language instructions.
WebAgent plans ahead by decomposing instructions into canonical
sub-instructions, summarizes long HTML documents into task-relevant snippets,
and acts on websites via Python programs generated from those. We design
WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new
pre-trained LLMs for long HTML documents using local and global attention
mechanisms and a mixture of long-span denoising objectives, for planning and
summarization. We empirically demonstrate that our modular recipe improves the
success on real websites by over 50%, and that HTML-T5 is the best model to
solve various HTML understanding tasks; achieving 18.7% higher success rate
than the prior method on MiniWoB web automation benchmark, and SoTA performance
on Mind2Web, an offline task planning evaluation.",2023-07-24T14:56:30Z
,http://arxiv.org/pdf/2307.07686v4.pdf,"Creating a Dataset for High-Performance Computing Code Translation using
  LLMs: A Bridge Between OpenMP Fortran and C++","In this study, we present a novel dataset for training machine learning
models translating between OpenMP Fortran and C++ code. To ensure reliability
and applicability, the dataset is created from a range of representative
open-source OpenMP benchmarks. It is also refined using a meticulous code
similarity test. The effectiveness of our dataset is assessed using both
quantitative (CodeBLEU) and qualitative (human evaluation) methods. We showcase
how this dataset significantly elevates the translation competencies of large
language models (LLMs). Specifically, models without prior coding knowledge
experienced a boost of $\mathbf{\times~5.1}$ in their CodeBLEU scores, while
models with some coding familiarity saw an impressive
$\mathbf{\times~9.9}$-fold increase. The best fine-tuned model using our
dataset outperforms GPT-4. It is also reaching human-level accuracy. This work
underscores the immense potential of our dataset in propelling advancements in
the domain of code translation for high-performance computing. The dataset is
accessible at
\href{https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset}{OpenMP-Fortran-CPP-Translation}.",2023-07-15T02:35:51Z
,http://arxiv.org/pdf/2310.15539v2.pdf,"SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code
  Translation","With the recent focus on Large Language Models (LLMs), both StarCoder (Li et
al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable
performance in code generation. However, there is still a need for improvement
in code translation functionality with efficient training techniques. In
response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM
designed specifically for multi-programming language-to-Python code
translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or
PHP-to-Python code translation without specifying the input programming
language. We modified StarCoder model architecture by incorporating a
Mixture-of-Experts (MoE) technique featuring five experts and a gating network
for multi-task handling. Experts are obtained by StarCoder fine-tuning.
Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each
expert size as only 0.06% of number of StarCoder's parameters. At the same
time, to enhance training efficiency in terms of time, we adopt curriculum
learning strategy and use self-instruct data for efficient fine-tuning. As a
result, each expert takes only 6 hours to train on one single 80Gb A100 HBM.
With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76
CodeBLEU score in multi-programming language-to-Python translation, surpassing
the top performance from the leaderboard by at least 3.5. This accomplishment
is attributed to only 45M extra parameters with StarCoder as the backbone and
32 hours of valid training on one 80GB A100 HBM. The source code is release
here: https://github.com/sade-adrien/SteloCoder.",2023-10-24T06:04:28Z
,http://arxiv.org/pdf/2401.08683v1.pdf,"Zero-Shot RTL Code Generation with Attention Sink Augmented Large
  Language Models","The design and optimization of hardware have traditionally been
resource-intensive, demanding considerable expertise and dependence on
established design automation tools. This paper discusses the possibility of
exploiting large language models to streamline the code generation process in
hardware design. In contrast to earlier studies, this paper aims to use large
language models that accepts high-level design specifications through a single
prompt to generate corresponding Register-Transfer Level (RTL) code. The
ability to use large language models on RTL code generation not only expedites
design iteration cycles but also facilitates the exploration of design spaces
that have computational challenges for conventional techniques. Through our
evaluation, we demonstrate the shortcoming of existing attention mechanisms,
and present the abilities of language models to produce functional, optimized,
and industry-standard compliant RTL code when a novel attention mechanism is
used. These findings underscore the expanding role of large language models in
shaping the future landscape of architectural exploration and automation in
hardware design.",2024-01-12T17:41:38Z
,http://arxiv.org/pdf/2403.06833v2.pdf,"Can LLMs Separate Instructions From Data? And What Do We Even Mean By
  That?","Instruction-tuned Large Language Models (LLMs) show impressive results in
numerous practical applications, but they lack essential safety features that
are common in other areas of computer science, particularly an explicit
separation of instructions and data. This makes them vulnerable to
manipulations such as indirect prompt injections and generally unsuitable for
safety-critical tasks. Surprisingly, there is currently no established
definition or benchmark to quantify this phenomenon. In this work, we close
this gap by introducing a formal measure for instruction-data separation and an
empirical variant that is calculable from a model's outputs. We also present a
new dataset, SEP, that allows estimating the measure for real-world models. Our
results on various LLMs show that the problem of instruction-data separation is
real: all models fail to achieve high separation, and canonical mitigation
techniques, such as prompt engineering and fine-tuning, either fail to
substantially improve separation or reduce model utility. The source code and
SEP dataset are openly accessible at
https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.",2024-03-11T15:48:56Z
,http://arxiv.org/pdf/2405.02318v1.pdf,"NL2FOL: Translating Natural Language to First-Order Logic for Logical
  Fallacy Detection","Logical fallacies are common errors in reasoning that undermine the logic of
an argument. Automatically detecting logical fallacies has important
applications in tracking misinformation and validating claims. In this paper,
we design a process to reliably detect logical fallacies by translating natural
language to First-order Logic (FOL) step-by-step using Large Language Models
(LLMs). We then utilize Satisfiability Modulo Theory (SMT) solvers to reason
about the validity of the formula and classify inputs as either a fallacy or
valid statement. Our model also provides a novel means of utilizing LLMs to
interpret the output of the SMT solver, offering insights into the
counter-examples that illustrate why a given sentence is considered a logical
fallacy. Our approach is robust, interpretable and does not require training
data or fine-tuning. We evaluate our model on a mixed dataset of fallacies and
valid sentences. The results demonstrate improved performance compared to
end-to-end LLMs, with our classifier achieving an F1-score of 71\% on the Logic
dataset. The approach is able to generalize effectively, achieving an F1-score
of 73% on the challenge set, LogicClimate, outperforming state-of-the-art
models by 21% despite its much smaller size.",2024-04-18T00:20:48Z
,http://arxiv.org/pdf/2305.04207v3.pdf,"No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test
  Generation","Unit testing is essential in detecting bugs in functionally-discrete program
units. Manually writing high-quality unit tests is time-consuming and
laborious. Although traditional techniques can generate tests with reasonable
coverage, they exhibit low readability and cannot be directly adopted by
developers. Recent work has shown the large potential of large language models
(LLMs) in unit test generation, which can generate more human-like and
meaningful test code. ChatGPT, the latest LLM incorporating instruction tuning
and reinforcement learning, has performed well in various domains. However, It
remains unclear how effective ChatGPT is in unit test generation.
  In this work, we perform the first empirical study to evaluate ChatGPT's
capability of unit test generation. Specifically, we conduct a quantitative
analysis and a user study to systematically investigate the quality of its
generated tests regarding the correctness, sufficiency, readability, and
usability. The tests generated by ChatGPT still suffer from correctness issues,
including diverse compilation errors and execution failures. Still, the passing
tests generated by ChatGPT resemble manually-written tests by achieving
comparable coverage, readability, and even sometimes developers' preference.
Our findings indicate that generating unit tests with ChatGPT could be very
promising if the correctness of its generated tests could be further improved.
  Inspired by our findings above, we propose ChatTESTER, a novel ChatGPT-based
unit test generation approach, which leverages ChatGPT itself to improve the
quality of its generated tests. ChatTESTER incorporates an initial test
generator and an iterative test refiner. Our evaluation demonstrates the
effectiveness of ChatTESTER by generating 34.3% more compilable tests and 18.7%
more tests with correct assertions than the default ChatGPT.",2023-05-07T07:17:08Z
,http://arxiv.org/pdf/2308.08943v1.pdf,"Towards Automatically Addressing Self-Admitted Technical Debt: How Far
  Are We?","Upon evolving their software, organizations and individual developers have to
spend a substantial effort to pay back technical debt, i.e., the fact that
software is released in a shape not as good as it should be, e.g., in terms of
functionality, reliability, or maintainability. This paper empirically
investigates the extent to which technical debt can be automatically paid back
by neural-based generative models, and in particular models exploiting
different strategies for pre-training and fine-tuning. We start by extracting a
dateset of 5,039 Self-Admitted Technical Debt (SATD) removals from 595
open-source projects. SATD refers to technical debt instances documented (e.g.,
via code comments) by developers. We use this dataset to experiment with seven
different generative deep learning (DL) model configurations. Specifically, we
compare transformers pre-trained and fine-tuned with different combinations of
training objectives, including the fixing of generic code changes, SATD
removals, and SATD-comment prompt tuning. Also, we investigate the
applicability in this context of a recently-available Large Language Model
(LLM)-based chat bot. Results of our study indicate that the automated
repayment of SATD is a challenging task, with the best model we experimented
with able to automatically fix ~2% to 8% of test instances, depending on the
number of attempts it is allowed to make. Given the limited size of the
fine-tuning dataset (~5k instances), the model's pre-training plays a
fundamental role in boosting performance. Also, the ability to remove SATD
steadily drops if the comment documenting the SATD is not provided as input to
the model. Finally, we found general-purpose LLMs to not be a competitive
approach for addressing SATD.",2023-08-17T12:27:32Z
,http://arxiv.org/pdf/2403.10504v1.pdf,"ATOM: Asynchronous Training of Massive Models for Deep Learning in a
  Decentralized Environment","The advent of the Transformer architecture has propelled the growth of
natural language processing (NLP) models, leading to remarkable achievements in
numerous NLP tasks. Yet, the absence of specialized hardware like expansive GPU
memory and high-speed interconnects poses challenges for training large-scale
models. This makes it daunting for many users to experiment with pre-training
and fine-tuning large language models (LLMs). In this study, we introduce
\atom, a resilient distributed training framework designed for asynchronous
training of vast models in a decentralized setting using cost-effective
hardware, including consumer-grade GPUs and Ethernet. Unlike conventional model
partitioning methods that distribute sub-models across GPUs, \atom aims to
accommodate a complete LLM on one host (peer) through seamlessly model swapping
and concurrently trains multiple copies across various peers to optimize
training throughput. Through static analysis, \atom identifies the best model
partitioning strategy and flawlessly merges model execution with swapping. Key
benefits of \atom include: Avoiding the central point of failure found in
pipeline parallelism methods. Demonstrating superior performance and
scalability compared to closely-integrated pipeline parallelism in slower
networks. Our experiments using different GPT-3 model configurations reveal
that, in scenarios with suboptimal network connections, \atom can enhance
training efficiency up to $20 \times$ when juxtaposed with the state-of-the-art
decentralized pipeline parallelism approaches.",2024-03-15T17:43:43Z
,http://arxiv.org/pdf/2404.15247v2.pdf,"XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging
  Upcycled Mixture-of-Experts","We introduce XFT, a simple yet powerful training scheme, by simply merging
upcycled Mixture-of-Experts (MoE) to unleash the performance limit of
instruction-tuned code Large Language Models (LLMs). While vanilla sparse
upcycling fails to improve instruction tuning, XFT introduces a shared expert
mechanism with a novel routing weight normalization strategy into sparse
upcycling, which significantly boosts instruction tuning. After fine-tuning the
upcycled MoE model, XFT introduces a learnable model merging mechanism to
compile the upcycled MoE model back to a dense model, achieving upcycled
MoE-level performance with only dense-model compute. By applying XFT to a 1.3B
model, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6
pass@1 on HumanEval and HumanEval+ respectively. With the same data and model
architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+,
along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and
DS-1000, demonstrating its generalizability. XFT is fully orthogonal to
existing techniques such as Evol-Instruct and OSS-Instruct, opening a new
dimension for improving code instruction tuning. Codes are available at
https://github.com/ise-uiuc/xft.",2024-04-23T17:32:24Z
,http://arxiv.org/pdf/2405.04520v1.pdf,"NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and
  Natural User Prompts","Large language models (LLMs) have manifested strong ability to generate codes
for productive activities. However, current benchmarks for code synthesis, such
as HumanEval, MBPP, and DS-1000, are predominantly oriented towards
introductory tasks on algorithm and data science, insufficiently satisfying
challenging requirements prevalent in real-world coding. To fill this gap, we
propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror
the complexity and variety of scenarios in real coding tasks. NCB comprises 402
high-quality problems in Python and Java, meticulously selected from natural
user queries from online coding services, covering 6 different domains. Noting
the extraordinary difficulty in creating testing cases for real-world queries,
we also introduce a semi-automated pipeline to enhance the efficiency of test
case construction. Comparing with manual solutions, it achieves an efficiency
increase of more than 4 times. Our systematic experiments on 39 LLMs find that
performance gaps on NCB between models with close HumanEval scores could still
be significant, indicating a lack of focus on practical code synthesis
scenarios or over-specified optimization on HumanEval. On the other hand, even
the best-performing GPT-4 is still far from satisfying on NCB. The evaluation
toolkit and development set are available at
https://github.com/THUDM/NaturalCodeBench.",2024-05-07T17:52:51Z
,http://arxiv.org/pdf/2406.11925v1.pdf,DocCGen: Document-based Controlled Code Generation,"Recent developments show that Large Language Models (LLMs) produce
state-of-the-art performance on natural language (NL) to code generation for
resource-rich general-purpose languages like C++, Java, and Python. However,
their practical usage for structured domain-specific languages (DSLs) such as
YAML, JSON is limited due to domain-specific schema, grammar, and
customizations generally unseen by LLMs during pre-training. Efforts have been
made to mitigate this challenge via in-context learning through relevant
examples or by fine-tuning. However, it suffers from problems, such as limited
DSL samples and prompt sensitivity but enterprises maintain good documentation
of the DSLs. Therefore, we propose DocCGen, a framework that can leverage such
rich knowledge by breaking the NL-to-Code generation task for structured code
languages into a two-step process. First, it detects the correct libraries
using the library documentation that best matches the NL query. Then, it
utilizes schema rules extracted from the documentation of these libraries to
constrain the decoding. We evaluate our framework for two complex structured
languages, Ansible YAML and Bash command, consisting of two settings:
Out-of-domain (OOD) and In-domain (ID). Our extensive experiments show that
DocCGen consistently improves different-sized language models across all six
evaluation metrics, reducing syntactic and semantic errors in structured code.
We plan to open-source the datasets and code to motivate research in
constrained code generation.",2024-06-17T08:34:57Z
,http://arxiv.org/pdf/2404.08743v1.pdf,"VizGroup: An AI-Assisted Event-Driven System for Real-Time Collaborative
  Programming Learning Analytics","Programming instructors often conduct collaborative learning activities, like
Peer Instruction, to foster a deeper understanding in students and enhance
their engagement with learning. These activities, however, may not always yield
productive outcomes due to the diversity of student mental models and their
ineffective collaboration. In this work, we introduce VizGroup, an AI-assisted
system that enables programming instructors to easily oversee students'
real-time collaborative learning behaviors during large programming courses.
VizGroup leverages Large Language Models (LLMs) to recommend event
specifications for instructors so that they can simultaneously track and
receive alerts about key correlation patterns between various collaboration
metrics and ongoing coding tasks. We evaluated VizGroup with 12 instructors
using a dataset collected from a Peer Instruction activity that was conducted
in a large programming lecture. The results showed that compared to a version
of VizGroup without the suggested units, VizGroup with suggested units helped
instructors create additional monitoring units on previously undetected
patterns on their own, covered a more diverse range of metrics, and influenced
the participants' following notification creation strategies.",2024-04-12T18:10:40Z
,http://arxiv.org/pdf/2309.15091v1.pdf,"VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided
  Planning","Although recent text-to-video (T2V) generation methods have seen significant
advancements, most of these works focus on producing short video clips of a
single event with a single background (i.e., single-scene videos). Meanwhile,
recent large language models (LLMs) have demonstrated their capability in
generating layouts and programs to control downstream visual modules such as
image generation models. This raises an important question: can we leverage the
knowledge embedded in these LLMs for temporally consistent long video
generation? In this paper, we propose VideoDirectorGPT, a novel framework for
consistent multi-scene video generation that uses the knowledge of LLMs for
video content planning and grounded video generation. Specifically, given a
single text prompt, we first ask our video planner LLM (GPT-4) to expand it
into a 'video plan', which involves generating the scene descriptions, the
entities with their respective layouts, the background for each scene, and
consistency groupings of the entities and backgrounds. Next, guided by this
output from the video planner, our video generator, Layout2Vid, has explicit
control over spatial layouts and can maintain temporal consistency of
entities/backgrounds across scenes, while only trained with image-level
annotations. Our experiments demonstrate that VideoDirectorGPT framework
substantially improves layout and movement control in both single- and
multi-scene video generation and can generate multi-scene videos with visual
consistency across scenes, while achieving competitive performance with SOTAs
in open-domain single-scene T2V generation. We also demonstrate that our
framework can dynamically control the strength for layout guidance and can also
generate videos with user-provided images. We hope our framework can inspire
future work on better integrating the planning ability of LLMs into consistent
long video generation.",2023-09-26T17:36:26Z
,http://arxiv.org/pdf/2309.17428v2.pdf,"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized
  Toolsets","Large language models (LLMs) are often augmented with tools to solve complex
tasks. By generating code snippets and executing them through task-specific
Application Programming Interfaces (APIs), they can offload certain functions
to dedicated external modules, such as image encoding and performing
calculations. However, most existing approaches to augment LLMs with tools are
constrained by general-purpose APIs and lack the flexibility for tailoring them
to specific tasks. In this work, we present CRAFT, a general tool creation and
retrieval framework for LLMs. It creates toolsets specifically curated for the
tasks and equips LLMs with a component that retrieves tools from these sets to
enhance their capability to solve complex tasks. For each task, we collect
specific code solutions by prompting GPT-4 to solve the training examples.
Following a validation step ensuring the correctness, these solutions are
abstracted into code snippets to enhance reusability, and deduplicated for
higher quality. At inference time, the language model retrieves snippets from
the toolsets and then executes them or generates the output conditioning on the
retrieved snippets. Our method is designed to be flexible and offers a
plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and
modalities, without any finetuning. Experiments on vision-language, tabular
processing, and mathematical reasoning tasks show that our approach achieves
substantial improvements compared to strong baselines. In addition, our
in-depth analysis reveals that: (1) consistent performance improvement can be
achieved by scaling up the number of tools and the capability of the backbone
models; (2) each component of our approach contributes to the performance
gains; (3) the created tools are well-structured and reliable with low
complexity and atomicity. The code is available at
https://github.com/lifan-yuan/CRAFT.",2023-09-29T17:40:26Z
,http://arxiv.org/pdf/2310.10698v2.pdf,"Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for
  Code Generation","Large language models (LLMs) have showcased remarkable prowess in code
generation. However, automated code generation is still challenging since it
requires a high-level semantic mapping between natural language requirements
and codes. Most existing LLMs-based approaches for code generation rely on
decoder-only causal language models often treate codes merely as plain text
tokens, i.e., feeding the requirements as a prompt input, and outputing code as
flat sequence of tokens, potentially missing the rich semantic features
inherent in source code. To bridge this gap, this paper proposes the ""Semantic
Chain-of-Thought"" approach to intruduce semantic information of code, named
SeCoT. Our motivation is that the semantic information of the source code (\eg
data flow and control flow) describes more precise program execution behavior,
intention and function. By guiding LLM consider and integrate semantic
information, we can achieve a more granular understanding and representation of
code, enhancing code generation accuracy. Meanwhile, while traditional
techniques leveraging such semantic information require complex static or
dynamic code analysis to obtain features such as data flow and control flow,
SeCoT demonstrates that this process can be fully automated via the intrinsic
capabilities of LLMs (i.e., in-context learning), while being generalizable and
applicable to challenging domains. While SeCoT can be applied with different
LLMs, this paper focuses on the powerful GPT-style models: ChatGPT(close-source
model) and WizardCoder(open-source model). The experimental study on three
popular DL benchmarks (i.e., HumanEval, HumanEval-ET and MBPP) shows that SeCoT
can achieves state-of-the-art performance, greatly improving the potential for
large models and code generation.",2023-10-16T05:09:58Z
10.1145/3639477.3639745,http://arxiv.org/pdf/2403.06485v1.pdf,"Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid
  Approach","Due to the scale and complexity of cloud systems, a system failure would
trigger an ""alert storm"", i.e., massive correlated alerts. Although these
alerts can be traced back to a few root causes, the overwhelming number makes
it infeasible for manual handling. Alert aggregation is thus critical to help
engineers concentrate on the root cause and facilitate failure resolution.
Existing methods typically utilize semantic similarity-based methods or
statistical methods to aggregate alerts. However, semantic similarity-based
methods overlook the causal rationale of alerts, while statistical methods can
hardly handle infrequent alerts.
  To tackle these limitations, we introduce leveraging external knowledge,
i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose
COLA, a novel hybrid approach based on correlation mining and LLM (Large
Language Model) reasoning for online alert aggregation. The correlation mining
module effectively captures the temporal and spatial relations between alerts,
measuring their correlations in an efficient manner. Subsequently, only
uncertain pairs with low confidence are forwarded to the LLM reasoning module
for detailed analysis. This hybrid design harnesses both statistical evidence
for frequent alerts and the reasoning capabilities of computationally intensive
LLMs, ensuring the overall efficiency of COLA in handling large volumes of
alerts in practical scenarios. We evaluate COLA on three datasets collected
from the production environment of a large-scale cloud platform. The
experimental results show COLA achieves F1-scores from 0.901 to 0.930,
outperforming state-of-the-art methods and achieving comparable efficiency. We
also share our experience in deploying COLA in our real-world cloud system,
Cloud X.",2024-03-11T07:48:35Z
,http://arxiv.org/pdf/2309.10085v1.pdf,"Evaluating the Impact of ChatGPT on Exercises of a Software Security
  Course","Along with the development of large language models (LLMs), e.g., ChatGPT,
many existing approaches and tools for software security are changing. It is,
therefore, essential to understand how security-aware these models are and how
these models impact software security practices and education. In exercises of
a software security course at our university, we ask students to identify and
fix vulnerabilities we insert in a web application using state-of-the-art
tools. After ChatGPT, especially the GPT-4 version of the model, we want to
know how the students can possibly use ChatGPT to complete the exercise tasks.
We input the vulnerable code to ChatGPT and measure its accuracy in
vulnerability identification and fixing. In addition, we investigated whether
ChatGPT can provide a proper source of information to support its outputs.
Results show that ChatGPT can identify 20 of the 28 vulnerabilities we inserted
in the web application in a white-box setting, reported three false positives,
and found four extra vulnerabilities beyond the ones we inserted. ChatGPT makes
nine satisfactory penetration testing and fixing recommendations for the ten
vulnerabilities we want students to fix and can often point to related sources
of information.",2023-09-18T18:53:43Z
,http://arxiv.org/pdf/2404.06201v1.pdf,"Open-Source AI-based SE Tools: Opportunities and Challenges of
  Collaborative Software Learning","Large Language Models (LLMs) have become instrumental in advancing software
engineering (SE) tasks, showcasing their efficacy in code understanding and
beyond. Like traditional SE tools, open-source collaboration is key in
realising the excellent products. However, with AI models, the essential need
is in data. The collaboration of these AI-based SE models hinges on maximising
the sources of high-quality data. However, data especially of high quality,
often holds commercial or sensitive value, making it less accessible for
open-source AI-based SE projects. This reality presents a significant barrier
to the development and enhancement of AI-based SE tools within the software
engineering community. Therefore, researchers need to find solutions for
enabling open-source AI-based SE models to tap into resources by different
organisations. Addressing this challenge, our position paper investigates one
solution to facilitate access to diverse organizational resources for
open-source AI models, ensuring privacy and commercial sensitivities are
respected. We introduce a governance framework centered on federated learning
(FL), designed to foster the joint development and maintenance of open-source
AI code models while safeguarding data privacy and security. Additionally, we
present guidelines for developers on AI-based SE tool collaboration, covering
data requirements, model architecture, updating strategies, and version
control. Given the significant influence of data characteristics on FL, our
research examines the effect of code data heterogeneity on FL performance.",2024-04-09T10:47:02Z
,http://arxiv.org/pdf/2305.01598v2.pdf,"From Words to Code: Harnessing Data for Program Synthesis from Natural
  Language","Creating programs to correctly manipulate data is a difficult task, as the
underlying programming languages and APIs can be challenging to learn for many
users who are not skilled programmers. Large language models (LLMs) demonstrate
remarkable potential for generating code from natural language, but in the data
manipulation domain, apart from the natural language (NL) description of the
intended task, we also have the dataset on which the task is to be performed,
or the ""data context"". Existing approaches have utilized data context in a
limited way by simply adding relevant information from the input data into the
prompts sent to the LLM.
  In this work, we utilize the available input data to execute the candidate
programs generated by the LLMs and gather their outputs. We introduce semantic
reranking, a technique to rerank the programs generated by LLMs based on three
signals coming the program outputs: (a) semantic filtering and well-formedness
based score tuning: do programs even generate well-formed outputs, (b) semantic
interleaving: how do the outputs from different candidates compare to each
other, and (c) output-based score tuning: how do the outputs compare to outputs
predicted for the same task. We provide theoretical justification for semantic
interleaving. We also introduce temperature mixing, where we combine samples
generated by LLMs using both high and low temperatures. We extensively evaluate
our approach in three domains, namely databases (SQL), data science (Pandas)
and business intelligence (Excel's Power Query M) on a variety of new and
existing benchmarks. We observe substantial gains across domains, with
improvements of up to 45% in top-1 accuracy and 34% in top-3 accuracy.",2023-05-02T16:56:32Z
,http://arxiv.org/pdf/2311.03365v1.pdf,"Leveraging Generative AI: Improving Software Metadata Classification
  with Generated Code-Comment Pairs","In software development, code comments play a crucial role in enhancing code
comprehension and collaboration. This research paper addresses the challenge of
objectively classifying code comments as ""Useful"" or ""Not Useful."" We propose a
novel solution that harnesses contextualized embeddings, particularly BERT, to
automate this classification process. We address this task by incorporating
generated code and comment pairs. The initial dataset comprised 9048 pairs of
code and comments written in C, labeled as either Useful or Not Useful. To
augment this dataset, we sourced an additional 739 lines of code-comment pairs
and generated labels using a Large Language Model Architecture, specifically
BERT. The primary objective was to build classification models that can
effectively differentiate between useful and not useful code comments. Various
machine learning algorithms were employed, including Logistic Regression,
Decision Tree, K-Nearest Neighbors (KNN), Support Vector Machine (SVM),
Gradient Boosting, Random Forest, and a Neural Network. Each algorithm was
evaluated using precision, recall, and F1-score metrics, both with the original
seed dataset and the augmented dataset. This study showcases the potential of
generative AI for enhancing binary code comment quality classification models,
providing valuable insights for software developers and researchers in the
field of natural language processing and software engineering.",2023-10-14T12:09:43Z
,http://arxiv.org/pdf/2105.09938v3.pdf,Measuring Coding Challenge Competence With APPS,"While programming is one of the most broadly applicable skills in modern
society, modern machine learning models still cannot code solutions to basic
problems. Despite its importance, there has been surprisingly little work on
evaluating code generation, and it can be difficult to accurately assess code
generation performance rigorously. To meet this challenge, we introduce APPS, a
benchmark for code generation. Unlike prior work in more restricted settings,
our benchmark measures the ability of models to take an arbitrary natural
language specification and generate satisfactory Python code. Similar to how
companies assess candidate software developers, we then evaluate models by
checking their generated code on test cases. Our benchmark includes 10,000
problems, which range from having simple one-line solutions to being
substantial algorithmic challenges. We fine-tune large language models on both
GitHub and our training set, and we find that the prevalence of syntax errors
is decreasing exponentially as models improve. Recent models such as GPT-Neo
can pass approximately 20% of the test cases of introductory problems, so we
find that machine learning models are now beginning to learn how to code. As
the social significance of automatic code generation increases over the coming
years, our benchmark can provide an important measure for tracking
advancements.",2021-05-20T17:58:42Z
,http://arxiv.org/pdf/2305.18654v3.pdf,Faith and Fate: Limits of Transformers on Compositionality,"Transformer large language models (LLMs) have sparked admiration for their
exceptional performance on tasks that demand intricate multi-step reasoning.
Yet, these models simultaneously show failures on surprisingly trivial
problems. This begs the question: Are these errors incidental, or do they
signal more substantial limitations? In an attempt to demystify transformer
LLMs, we investigate the limits of these models across three representative
compositional tasks -- multi-digit multiplication, logic grid puzzles, and a
classic dynamic programming problem. These tasks require breaking problems down
into sub-steps and synthesizing these steps into a precise answer. We formulate
compositional tasks as computation graphs to systematically quantify the level
of complexity, and break down reasoning steps into intermediate sub-procedures.
Our empirical findings suggest that transformer LLMs solve compositional tasks
by reducing multi-step compositional reasoning into linearized subgraph
matching, without necessarily developing systematic problem-solving skills. To
round off our empirical study, we provide theoretical arguments on abstract
multi-step reasoning problems that highlight how autoregressive generations'
performance can rapidly decay with\,increased\,task\,complexity.",2023-05-29T23:24:14Z
,http://arxiv.org/pdf/2402.05099v2.pdf,Hydragen: High-Throughput LLM Inference with Shared Prefixes,"Transformer-based large language models (LLMs) are now deployed to hundreds
of millions of users. LLM inference is commonly performed on batches of
sequences that share a prefix, such as few-shot examples or a chatbot system
prompt. Decoding in this large-batch setting can be bottlenecked by the
attention operation, which reads large key-value (KV) caches from memory and
computes inefficient matrix-vector products for every sequence in the batch. In
this work, we introduce Hydragen, a hardware-aware exact implementation of
attention with shared prefixes. Hydragen computes attention over the shared
prefix and unique suffixes separately. This decomposition enables efficient
prefix attention by batching queries together across sequences, reducing
redundant memory reads and enabling the use of hardware-friendly matrix
multiplications. Our method can improve end-to-end CodeLlama-13b throughput by
up to 32x against competitive baselines, with speedup growing with the batch
size and shared prefix length. Hydragen also enables the use of very long
shared contexts: with a large batch size, increasing the prefix length from 1K
to 16K tokens decreases Hydragen throughput by less than 15%, while the
throughput of baselines drops by over 90%. Hydragen generalizes beyond simple
prefix-suffix decomposition and can be applied to tree-based prompt sharing
patterns, allowing us to further reduce inference time on competitive
programming problems by 55%.",2024-02-07T18:53:01Z
,http://arxiv.org/pdf/2404.07774v2.pdf,"Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively
  Generalizable Spatial Concepts","Our goal is to enable embodied agents to learn inductively generalizable
spatial concepts, e.g., learning staircase as an inductive composition of
towers of increasing height. Given a human demonstration, we seek a learning
architecture that infers a succinct ${program}$ representation that explains
the observed instance. Additionally, the approach should generalize inductively
to novel structures of different sizes or complex structures expressed as a
hierarchical composition of previously learned concepts. Existing approaches
that use code generation capabilities of pre-trained large (visual) language
models, as well as purely neural models, show poor generalization to a-priori
unseen complex concepts. Our key insight is to factor inductive concept
learning as (i) ${\it Sketch:}$ detecting and inferring a coarse signature of a
new concept (ii) ${\it Plan:}$ performing MCTS search over grounded action
sequences (iii) ${\it Generalize:}$ abstracting out grounded plans as inductive
programs. Our pipeline facilitates generalization and modular reuse, enabling
continual concept learning. Our approach combines the benefits of the code
generation ability of large language models (LLM) along with grounded neural
representations, resulting in neuro-symbolic programs that show stronger
inductive generalization on the task of constructing complex structures in
relation to LLM-only and neural-only approaches. Furthermore, we demonstrate
reasoning and planning capabilities with learned concepts for embodied
instruction following.",2024-04-11T14:09:41Z
,http://arxiv.org/pdf/2308.13032v2.pdf,Financial News Analytics Using Fine-Tuned Llama 2 GPT Model,"The paper considers the possibility to fine-tune Llama 2 GPT large language
model (LLM) for the multitask analysis of financial news. For fine-tuning, the
PEFT/LoRA based approach was used. In the study, the model was fine-tuned for
the following tasks: analysing a text from financial market perspectives,
highlighting main points of a text, summarizing a text and extracting named
entities with appropriate sentiments. The obtained results show that the
fine-tuned Llama 2 model can perform a multitask financial news analysis with a
specified structure of response, part of response can be a structured text and
another part of data can have JSON format for further processing. Extracted
sentiments for named entities can be considered as predictive features in
supervised machine learning models with quantitative target variables.",2023-08-24T18:58:10Z
,http://arxiv.org/pdf/2401.07657v1.pdf,"Empirical Evidence for the Fragment level Understanding on Drug
  Molecular Structure of LLMs","AI for drug discovery has been a research hotspot in recent years, and
SMILES-based language models has been increasingly applied in drug molecular
design. However, no work has explored whether and how language models
understand the chemical spatial structure from 1D sequences. In this work, we
pre-train a transformer model on chemical language and fine-tune it toward drug
design objectives, and investigate the correspondence between high-frequency
SMILES substrings and molecular fragments. The results indicate that language
models can understand chemical structures from the perspective of molecular
fragments, and the structural knowledge learned through fine-tuning is
reflected in the high-frequency SMILES substrings generated by the model.",2024-01-15T12:53:58Z
10.3389/fvets.2024.1395934,http://arxiv.org/pdf/2403.14654v1.pdf,"ChatGPT in Veterinary Medicine: A Practical Guidance of Generative
  Artificial Intelligence in Clinics, Education, and Research","ChatGPT, the most accessible generative artificial intelligence (AI) tool,
offers considerable potential for veterinary medicine, yet a dedicated review
of its specific applications is lacking. This review concisely synthesizes the
latest research and practical applications of ChatGPT within the clinical,
educational, and research domains of veterinary medicine. It intends to provide
specific guidance and actionable examples of how generative AI can be directly
utilized by veterinary professionals without a programming background. For
practitioners, ChatGPT can extract patient data, generate progress notes, and
potentially assist in diagnosing complex cases. Veterinary educators can create
custom GPTs for student support, while students can utilize ChatGPT for exam
preparation. ChatGPT can aid in academic writing tasks in research, but
veterinary publishers have set specific requirements for authors to follow.
Despite its transformative potential, careful use is essential to avoid
pitfalls like hallucination. This review addresses ethical considerations,
provides learning resources, and offers tangible examples to guide responsible
implementation. Carefully selected, up-to-date links to platforms that host
large language models are provided for advanced readers with programming
capability. A table of key takeaways was provided to summarize this review. By
highlighting potential benefits and limitations, this review equips
veterinarians, educators, and researchers to harness the power of ChatGPT
effectively.",2024-02-26T02:59:07Z
,http://arxiv.org/pdf/2310.16035v1.pdf,What's Left? Concept Grounding with Logic-Enhanced Foundation Models,"Recent works such as VisProg and ViperGPT have smartly composed foundation
models for visual reasoning-using large language models (LLMs) to produce
programs that can be executed by pre-trained vision-language models. However,
they operate in limited domains, such as 2D images, not fully exploiting the
generalization of language: abstract concepts like ""left"" can also be grounded
in 3D, temporal, and action data, as in moving to your left. This limited
generalization stems from these inference-only methods' inability to learn or
adapt pre-trained models to a new domain. We propose the Logic-Enhanced
Foundation Model (LEFT), a unified framework that learns to ground and reason
with concepts across domains with a differentiable, domain-independent,
first-order logic-based program executor. LEFT has an LLM interpreter that
outputs a program represented in a general, logic-based reasoning language,
which is shared across all domains and tasks. LEFT's executor then executes the
program with trainable domain-specific grounding modules. We show that LEFT
flexibly learns concepts in four domains: 2D images, 3D scenes, human motions,
and robotic manipulation. It exhibits strong reasoning ability in a wide
variety of tasks, including those that are complex and not seen during
training, and can be easily applied to new domains.",2023-10-24T17:50:20Z
10.3991/ijep.v13i8.45621,http://arxiv.org/pdf/2311.10737v1.pdf,"AI-enhanced Auto-correction of Programming Exercises: How Effective is
  GPT-3.5?","Timely formative feedback is considered as one of the most important drivers
for effective learning. Delivering timely and individualized feedback is
particularly challenging in large classes in higher education. Recently Large
Language Models such as GPT-3 became available to the public that showed
promising results on various tasks such as code generation and code
explanation. This paper investigates the potential of AI in providing
personalized code correction and generating feedback. Based on existing student
submissions of two different real-world assignments, the correctness of the
AI-aided e-assessment as well as the characteristics such as fault
localization, correctness of hints, and code style suggestions of the generated
feedback are investigated. The results show that 73 % of the submissions were
correctly identified as either correct or incorrect. In 59 % of these cases,
GPT-3.5 also successfully generated effective and high-quality feedback.
Additionally, GPT-3.5 exhibited weaknesses in its evaluation, including
localization of errors that were not the actual errors, or even hallucinated
errors. Implications and potential new usage scenarios are discussed.",2023-10-24T10:35:36Z
,http://arxiv.org/pdf/2402.01704v2.pdf,"States as Strings as Strategies: Steering Language Models with
  Game-Theoretic Solvers","Game theory is the study of mathematical models of strategic interactions
among rational agents. Language is a key medium of interaction for humans,
though it has historically proven difficult to model dialogue and its strategic
motivations mathematically. A suitable model of the players, strategies, and
payoffs associated with linguistic interactions (i.e., a binding to the
conventional symbolic logic of game theory) would enable existing
game-theoretic algorithms to provide strategic solutions in the space of
language. In other words, a binding could provide a route to computing stable,
rational conversational strategies in dialogue. Large language models (LLMs)
have arguably reached a point where their generative capabilities can enable
realistic, human-like simulations of natural dialogue. By prompting them in
various ways, we can steer their responses towards different output utterances.
Leveraging the expressivity of natural language, LLMs can also help us quickly
generate new dialogue scenarios, which are grounded in real world applications.
In this work, we present one possible binding from dialogue to game theory as
well as generalizations of existing equilibrium finding algorithms to this
setting. In addition, by exploiting LLMs generation capabilities along with our
proposed binding, we can synthesize a large repository of formally-defined
games in which one can study and test game-theoretic solution concepts. We also
demonstrate how one can combine LLM-driven game generation, game-theoretic
solvers, and imitation learning to construct a process for improving the
strategic capabilities of LLMs.",2024-01-24T22:22:00Z
,http://arxiv.org/pdf/2404.00057v1.pdf,PerOS: Personalized Self-Adapting Operating Systems in the Cloud,"Operating systems (OSes) are foundational to computer systems, managing
hardware resources and ensuring secure environments for diverse applications.
However, despite their enduring importance, the fundamental design objectives
of OSes have seen minimal evolution over decades. Traditionally prioritizing
aspects like speed, memory efficiency, security, and scalability, these
objectives often overlook the crucial aspect of intelligence as well as
personalized user experience. The lack of intelligence becomes increasingly
critical amid technological revolutions, such as the remarkable advancements in
machine learning (ML).
  Today's personal devices, evolving into intimate companions for users, pose
unique challenges for traditional OSes like Linux and iOS, especially with the
emergence of specialized hardware featuring heterogeneous components.
Furthermore, the rise of large language models (LLMs) in ML has introduced
transformative capabilities, reshaping user interactions and software
development paradigms.
  While existing literature predominantly focuses on leveraging ML methods for
system optimization or accelerating ML workloads, there is a significant gap in
addressing personalized user experiences at the OS level. To tackle this
challenge, this work proposes PerOS, a personalized OS ingrained with LLM
capabilities. PerOS aims to provide tailored user experiences while
safeguarding privacy and personal data through declarative interfaces,
self-adaptive kernels, and secure data management in a scalable cloud-centric
architecture; therein lies the main research question of this work: How can we
develop intelligent, secure, and scalable OSes that deliver personalized
experiences to thousands of users?",2024-03-26T20:10:31Z
,http://arxiv.org/pdf/2404.03662v1.pdf,X-lifecycle Learning for Cloud Incident Management using LLMs,"Incident management for large cloud services is a complex and tedious process
and requires significant amount of manual efforts from on-call engineers
(OCEs). OCEs typically leverage data from different stages of the software
development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service
properties, service dependencies, trouble-shooting documents, etc.) to generate
insights for detection, root causing and mitigating of incidents. Recent
advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini)
created opportunities to automatically generate contextual recommendations to
the OCEs assisting them to quickly identify and mitigate critical issues.
However, existing research typically takes a silo-ed view for solving a certain
task in incident management by leveraging data from a single stage of SDLC. In
this paper, we demonstrate that augmenting additional contextual data from
different stages of SDLC improves the performance of two critically important
and practically challenging tasks: (1) automatically generating root cause
recommendations for dependency failure related incidents, and (2) identifying
ontology of service monitors used for automatically detecting incidents. By
leveraging 353 incident and 260 monitor dataset from Microsoft, we demonstrate
that augmenting contextual information from different stages of the SDLC
improves the performance over State-of-The-Art methods.",2024-02-15T06:19:02Z
,http://arxiv.org/pdf/2405.08213v1.pdf,"Interpreting Latent Student Knowledge Representations in Programming
  Assignments","Recent advances in artificial intelligence for education leverage generative
large language models, including using them to predict open-ended student
responses rather than their correctness only. However, the black-box nature of
these models limits the interpretability of the learned student knowledge
representations. In this paper, we conduct a first exploration into
interpreting latent student knowledge representations by presenting InfoOIRT,
an Information regularized Open-ended Item Response Theory model, which
encourages the latent student knowledge states to be interpretable while being
able to generate student-written code for open-ended programming questions.
InfoOIRT maximizes the mutual information between a fixed subset of latent
knowledge states enforced with simple prior distributions and generated student
code, which encourages the model to learn disentangled representations of
salient syntactic and semantic code features including syntactic styles,
mastery of programming skills, and code structures. Through experiments on a
real-world programming education dataset, we show that InfoOIRT can both
accurately generate student code and lead to interpretable student knowledge
representations.",2024-05-13T22:01:03Z
,http://arxiv.org/pdf/2306.15626v2.pdf,LeanDojo: Theorem Proving with Retrieval-Augmented Language Models,"Large language models (LLMs) have shown promise in proving formal theorems
using proof assistants such as Lean. However, existing methods are difficult to
reproduce or build on, due to private code, data, and large compute
requirements. This has created substantial barriers to research on machine
learning methods for theorem proving. This paper removes these barriers by
introducing LeanDojo: an open-source Lean playground consisting of toolkits,
data, models, and benchmarks. LeanDojo extracts data from Lean and enables
interaction with the proof environment programmatically. It contains
fine-grained annotations of premises in proofs, providing valuable data for
premise selection: a key bottleneck in theorem proving. Using this data, we
develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented
with retrieval for selecting premises from a vast math library. It is
inexpensive and needs only one GPU week of training. Our retriever leverages
LeanDojo's program analysis capability to identify accessible premises and hard
negative examples, which makes retrieval much more effective. Furthermore, we
construct a new benchmark consisting of 98,734 theorems and proofs extracted
from Lean's math library. It features challenging data split requiring the
prover to generalize to theorems relying on novel premises that are never used
in training. We use this benchmark for training and evaluation, and
experimental results demonstrate the effectiveness of ReProver over
non-retrieval baselines and GPT-4. We thus provide the first set of open-source
LLM-based theorem provers without any proprietary datasets and release it under
a permissive MIT license to facilitate further research.",2023-06-27T17:05:32Z
,http://arxiv.org/pdf/2309.13482v1.pdf,A Unified Scheme of ResNet and Softmax,"Large language models (LLMs) have brought significant changes to human
society. Softmax regression and residual neural networks (ResNet) are two
important techniques in deep learning: they not only serve as significant
theoretical components supporting the functionality of LLMs but also are
related to many other machine learning and theoretical computer science fields,
including but not limited to image classification, object detection, semantic
segmentation, and tensors.
  Previous research works studied these two concepts separately. In this paper,
we provide a theoretical analysis of the regression problem: $\| \langle
\exp(Ax) + A x , {\bf 1}_n \rangle^{-1} ( \exp(Ax) + Ax ) - b \|_2^2$, where
$A$ is a matrix in $\mathbb{R}^{n \times d}$, $b$ is a vector in
$\mathbb{R}^n$, and ${\bf 1}_n$ is the $n$-dimensional vector whose entries are
all $1$. This regression problem is a unified scheme that combines softmax
regression and ResNet, which has never been done before. We derive the
gradient, Hessian, and Lipschitz properties of the loss function. The Hessian
is shown to be positive semidefinite, and its structure is characterized as the
sum of a low-rank matrix and a diagonal matrix. This enables an efficient
approximate Newton method.
  As a result, this unified scheme helps to connect two previously thought
unrelated fields and provides novel insight into loss landscape and
optimization for emerging over-parameterized neural networks, which is
meaningful for future research in deep learning models.",2023-09-23T21:41:01Z
,http://arxiv.org/pdf/2405.00748v1.pdf,ChatGPT in Data Visualization Education: A Student Perspective,"Unlike traditional educational chatbots that rely on pre-programmed
responses, large-language model-driven chatbots, such as ChatGPT, demonstrate
remarkable versatility and have the potential to serve as a dynamic resource
for addressing student needs from understanding advanced concepts to solving
complex problems. This work explores the impact of such technology on student
learning in an interdisciplinary, project-oriented data visualization course.
Throughout the semester, students engaged with ChatGPT across four distinct
projects, including data visualizations and implementing them using a variety
of tools including Tableau, D3, and Vega-lite. We collected conversation logs
and reflection surveys from the students after each assignment. In addition, we
conducted interviews with selected students to gain deeper insights into their
overall experiences with ChatGPT. Our analysis examined the advantages and
barriers of using ChatGPT, students' querying behavior, the types of assistance
sought, and its impact on assignment outcomes and engagement. Based on the
findings, we discuss design considerations for an educational solution that
goes beyond the basic interface of ChatGPT, specifically tailored for data
visualization education.",2024-05-01T02:40:20Z
,http://arxiv.org/pdf/2312.07104v2.pdf,SGLang: Efficient Execution of Structured Language Model Programs,"Large language models (LLMs) are increasingly used for complex tasks that
require multiple generation calls, advanced prompting techniques, control flow,
and structured inputs/outputs. However, efficient systems are lacking for
programming and executing these applications. We introduce SGLang, a system for
efficient execution of complex language model programs. SGLang consists of a
frontend language and a runtime. The frontend simplifies programming with
primitives for generation and parallelism control. The runtime accelerates
execution with novel optimizations like RadixAttention for KV cache reuse and
compressed finite state machines for faster structured output decoding.
Experiments show that SGLang achieves up to 6.4x higher throughput compared to
state-of-the-art inference systems on various large language and multi-modal
models on tasks including agent control, logical reasoning, few-shot learning
benchmarks, JSON decoding, retrieval-augmented generation pipelines, and
multi-turn chat. The code is publicly available at
https://github.com/sgl-project/sglang",2023-12-12T09:34:27Z
,http://arxiv.org/pdf/2405.20519v1.pdf,Diffusion On Syntax Trees For Program Synthesis,"Large language models generate code one token at a time. Their autoregressive
generation process lacks the feedback of observing the program's output.
Training LLMs to suggest edits directly can be challenging due to the scarcity
of rich edit data. To address these problems, we propose neural diffusion
models that operate on syntax trees of any context-free grammar. Similar to
image diffusion models, our method also inverts ``noise'' applied to syntax
trees. Rather than generating code sequentially, we iteratively edit it while
preserving syntactic validity, which makes it easy to combine this neural model
with search. We apply our approach to inverse graphics tasks, where our model
learns to convert images into programs that produce those images. Combined with
search, our model is able to write graphics programs, see the execution result,
and debug them to meet the required specifications. We additionally show how
our system can write graphics programs for hand-drawn sketches.",2024-05-30T22:31:16Z
,http://arxiv.org/pdf/2308.02522v1.pdf,Evaluating ChatGPT and GPT-4 for Visual Programming,"Generative AI and large language models have the potential to drastically
improve the landscape of computing education by automatically generating
personalized feedback and content. Recent works have studied the capabilities
of these models for different programming education scenarios; however, these
works considered only text-based programming, in particular, Python
programming. Consequently, they leave open the question of how well these
models would perform in visual programming domains popularly used for K-8
programming education. The main research question we study is: Do
state-of-the-art generative models show advanced capabilities in visual
programming on par with their capabilities in text-based Python programming? In
our work, we evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, in
visual programming domains for various scenarios and assess performance using
expert-based annotations. In particular, we base our evaluation using reference
tasks from the domains of Hour of Code: Maze Challenge by Code-dot-org and
Karel. Our results show that these models perform poorly and struggle to
combine spatial, logical, and programming skills crucial for visual
programming. These results also provide exciting directions for future work on
developing techniques to improve the performance of generative models in visual
programming.",2023-07-30T22:13:20Z
,http://arxiv.org/pdf/2309.06424v1.pdf,"Unveiling the potential of large language models in generating semantic
  and cross-language clones","Semantic and Cross-language code clone generation may be useful for code
reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has
potential in such clone generation as GPT is used for text generation. When
developers copy/paste codes from Stack Overflow (SO) or within a system, there
might be inconsistent changes leading to unexpected behaviours. Similarly, if
someone possesses a code snippet in a particular programming language but seeks
equivalent functionality in a different language, a semantic cross-language
code clone generation approach could provide valuable assistance. In this
study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3
model could help generate semantic and cross-language clone variants for a
given fragment.We have comprised a diverse set of code fragments and assessed
GPT-3s performance in generating code variants.Through extensive
experimentation and analysis, where 9 judges spent 158 hours to validate, we
investigate the model's ability to produce accurate and semantically correct
variants. Our findings shed light on GPT-3's strengths in code generation,
offering insights into the potential applications and challenges of using
advanced language models in software development. Our quantitative analysis
yields compelling results. In the realm of semantic clones, GPT-3 attains an
impressive accuracy of 62.14% and 0.55 BLEU score, achieved through few-shot
prompt engineering. Furthermore, the model shines in transcending linguistic
confines, boasting an exceptional 91.25% accuracy in generating cross-language
clones",2023-09-12T17:40:49Z
,http://arxiv.org/pdf/2309.14726v2.pdf,PLMM: Personal Large Language Models on Mobile Devices,"Inspired by Federated Learning, in this paper, we propose personal large
models that are distilled from traditional large language models but more
adaptive to local users' personal information such as education background and
hobbies. We classify the large language models into three levels: the personal
level, expert level and traditional level. The personal level models are
adaptive to users' personal information. They encrypt the users' input and
protect their privacy. The expert level models focus on merging specific
knowledge such as finance, IT and art. The traditional models focus on the
universal knowledge discovery and upgrading the expert models. In such
classifications, the personal models directly interact with the user. For the
whole system, the personal models have users' (encrypted) personal information.
Moreover, such models must be small enough to be performed on personal
computers or mobile devices. Finally, they also have to response in real-time
for better user experience and produce high quality results. The proposed
personal large models can be applied in a wide range of applications such as
language and vision tasks.",2023-09-26T07:36:20Z
,http://arxiv.org/pdf/2310.04951v2.pdf,"CodeTransOcean: A Comprehensive Multilingual Benchmark for Code
  Translation","Recent code translation techniques exploit neural machine translation models
to translate source code from one programming language to another to satisfy
production compatibility or to improve efficiency of codebase maintenance. Most
existing code translation datasets only focus on a single pair of popular
programming languages. To advance research on code translation and meet diverse
requirements of real-world applications, we construct CodeTransOcean, a
large-scale comprehensive benchmark that supports the largest variety of
programming languages for code translation. CodeTransOcean consists of three
novel multilingual datasets, namely, MultilingualTrans supporting translations
between multiple popular programming languages, NicheTrans for translating
between niche programming languages and popular ones, and LLMTrans for
evaluating executability of translated code by large language models (LLMs).
CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for
translating deep learning code across different frameworks. We develop
multilingual modeling approaches for code translation and demonstrate their
great potential in improving the translation quality of both low-resource and
high-resource language pairs and boosting the training efficiency. We also
propose a novel evaluation metric Debugging Success Rate@K for program-level
code translation. Last but not least, we evaluate LLM ChatGPT on our datasets
and investigate its potential for fuzzy execution predictions. We build
baselines for CodeTransOcean and analyze challenges of code translation for
guiding future research. The CodeTransOcean datasets and code are publicly
available at https://github.com/WeixiangYAN/CodeTransOcean.",2023-10-08T00:16:18Z
,http://arxiv.org/pdf/2310.04047v2.pdf,"AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large
  Language Models","Parallelizing sequentially written programs is a challenging task. Even
experienced developers need to spend considerable time finding parallelism
opportunities and then actually writing parallel versions of sequentially
written programs. To address this issue, we present AUTOPARLLM, a framework for
automatically discovering parallelism and generating the parallel version of
the sequentially written program. Our framework consists of two major
components: i) a heterogeneous Graph Neural Network (GNN) based parallelism
discovery and parallel pattern detection module, and ii) an LLM-based code
generator to generate the parallel counterpart of the sequential programs. We
use the GNN to learn the flow-aware characteristics of the programs to identify
parallel regions in sequential programs and then construct an enhanced prompt
using the GNN's results for the LLM-based generator to finally produce the
parallel counterparts of the sequential programs. We evaluate AUTOPARLLM on 11
applications of 2 well-known benchmark suites: NAS Parallel Benchmark and
Rodinia Benchmark. Our results show that AUTOPARLLM is indeed effective in
improving the state-of-the-art LLM-based models for the task of parallel code
generation in terms of multiple code generation metrics. AUTOPARLLM also
improves the average runtime of the parallel code generated by the
state-of-the-art LLMs by as high as 3.4% and 2.9% for the NAS Parallel
Benchmark and Rodinia Benchmark respectively. Additionally, to overcome the
issue that well-known metrics for translation evaluation have not been
optimized to evaluate the quality of the generated parallel code, we propose
OMPScore for evaluating the quality of the generated code. We show that
OMPScore exhibits a better correlation with human judgment than existing
metrics, measured by up to 75% improvement of Spearman correlation.",2023-10-06T06:51:16Z
,http://arxiv.org/pdf/2401.09301v1.pdf,"Material Informatics through Neural Networks on Ab-Initio Electron
  Charge Densities: the Role of Transfer Learning","In this work, the dynamic realms of Materials Science and Computer Science
advancements meet the critical challenge of identifying efficient descriptors
capable of capturing the essential features of physical systems. Such task has
remained formidable, with solutions often involving ad-hoc scalar and vectorial
sets of materials properties, making optimization and transferability
challenging. We extract representations directly from ab-initio differential
electron charge density profiles using Neural Networks, highlighting the
pivotal role of transfer learning in such task. Firstly, we demonstrate
significant improvements in regression of a specific defected-materials
property with respect to training a deep network from scratch, both in terms of
predictions and their reproducibilities, by considering various pre-trained
models and selecting the optimal one after fine-tuning. The remarkable
performances obtained confirmed the transferability of the existent pre-trained
Convolutional Neural Networks (CNNs) on physics domain data, very different
from the original training data. Secondly, we demonstrate a saturation in the
regression capabilities of computer vision models towards properties of an
extensive variety of undefected systems, and how it can be overcome with the
help of large language model (LLM) transformers, with as little text
information as composition names. Finally, we prove the insufficiency of
open-models, like GPT-4, in achieving the analogous tasks and performances as
the proposed domain-specific ones. The work offers a promising avenue for
enhancing the effectiveness of descriptor identification in complex physical
systems, shedding light over the power of transfer learning to easily adapt and
combine available models, with different modalities, to the physics domain, at
the same time opening space to a benchmark for LLMs capabilities in such
domain.",2024-01-17T16:05:19Z
,http://arxiv.org/pdf/2406.08221v1.pdf,"Can Large Language Models Analyze Software Failures in the News? An
  End-to-End Automated Pipeline with FAIL","Software failures inform engineering work, standards, regulations. For
example, the Log4J vulnerability brought government and industry attention to
evaluating and securing software supply chains. Accessing private engineering
records is difficult, so failure analyses tend to use information reported by
the news media. However, prior works in this direction have relied on manual
analysis. That has limited the scale of their analyses. The community lacks
automated support to enable such analyses to consider a wide range of news
sources and incidents.
  In this paper, we propose the Failure Analysis Investigation with LLMs (FAIL)
system to fill this gap. FAIL collects, analyzes, and summarizes software
failures as reported in the news. FAIL groups articles that describe the same
incidents. It then analyzes incidents using existing taxonomies for
postmortems, faults, and system characteristics. To tune and evaluate FAIL, we
followed the methods of prior works by manually analyzing 31 software failures.
FAIL achieved an F1 score of 90% for collecting news about software failures, a
V-measure of 0.98 for merging articles reporting on the same incident, and
extracted 90% of the facts about failures. We then applied FAIL to a total of
137,427 news articles from 11 providers published between 2010 and 2022. FAIL
identified and analyzed 2457 distinct failures reported across 4,184 articles.
Our findings include: (1) current generation of large language models are
capable of identifying news articles that describe failures, and analyzing them
according to structured taxonomies; (2) high recurrences of similar failures
within organizations and across organizations; and (3) severity of the
consequences of software failures have increased over the past decade. The full
FAIL database is available so that researchers, engineers, and policymakers can
learn from a diversity of software failures.",2024-06-12T13:51:51Z
,http://arxiv.org/pdf/2405.15164v1.pdf,"From Frege to chatGPT: Compositionality in language, cognition, and deep
  neural networks","Compositionality has long been considered a key explanatory property
underlying human intelligence: arbitrary concepts can be composed into novel
complex combinations, permitting the acquisition of an open ended, potentially
infinite expressive capacity from finite learning experiences. Influential
arguments have held that neural networks fail to explain this aspect of
behavior, leading many to dismiss them as viable models of human cognition.
Over the last decade, however, modern deep neural networks (DNNs), which share
the same fundamental design principles as their predecessors, have come to
dominate artificial intelligence, exhibiting the most advanced cognitive
behaviors ever demonstrated in machines. In particular, large language models
(LLMs), DNNs trained to predict the next word on a large corpus of text, have
proven capable of sophisticated behaviors such as writing syntactically complex
sentences without grammatical errors, producing cogent chains of reasoning, and
even writing original computer programs -- all behaviors thought to require
compositional processing. In this chapter, we survey recent empirical work from
machine learning for a broad audience in philosophy, cognitive science, and
neuroscience, situating recent breakthroughs within the broader context of
philosophical arguments about compositionality. In particular, our review
emphasizes two approaches to endowing neural networks with compositional
generalization capabilities: (1) architectural inductive biases, and (2)
metalearning, or learning to learn. We also present findings suggesting that
LLM pretraining can be understood as a kind of metalearning, and can thereby
equip DNNs with compositional generalization abilities in a similar way. We
conclude by discussing the implications that these findings may have for the
study of compositionality in human cognition and by suggesting avenues for
future research.",2024-05-24T02:36:07Z
,http://arxiv.org/pdf/2406.01940v1.pdf,Process-Driven Autoformalization in Lean 4,"Autoformalization, the conversion of natural language mathematics into formal
languages, offers significant potential for advancing mathematical reasoning.
However, existing efforts are limited to formal languages with substantial
online corpora and struggle to keep pace with rapidly evolving languages like
Lean 4. To bridge this gap, we propose a new benchmark \textbf{Form}alization
for \textbf{L}ean~\textbf{4} (\textbf{\name}) designed to evaluate the
autoformalization capabilities of large language models (LLMs). This benchmark
encompasses a comprehensive assessment of questions, answers, formal
statements, and proofs. Additionally, we introduce a
\textbf{P}rocess-\textbf{S}upervised \textbf{V}erifier (\textbf{PSV}) model
that leverages the precise feedback from Lean 4 compilers to enhance
autoformalization. Our experiments demonstrate that the PSV method improves
autoformalization, enabling higher accuracy using less filtered training data.
Furthermore, when fine-tuned with data containing detailed process information,
PSV can leverage the data more effectively, leading to more significant
improvements in autoformalization for Lean 4. Our dataset and code are
available at \url{https://github.com/rookie-joe/PDA}.",2024-06-04T03:48:08Z
,http://arxiv.org/pdf/2312.13322v1.pdf,"Domain-Specific Code Language Models: Unraveling the Potential for HPC
  Codes and Tasks","With easier access to powerful compute resources, there is a growing trend in
AI for software development to develop larger language models (LLMs) to address
a variety of programming tasks. Even LLMs applied to tasks from the
high-performance computing (HPC) domain are huge in size and demand expensive
compute resources for training. This is partly because these LLMs for HPC tasks
are obtained by finetuning existing LLMs that support several natural and/or
programming languages. We found this design choice confusing - why do we need
large LMs trained on natural languages and programming languages unrelated to
HPC for HPC-specific tasks?
  In this line of work, we aim to question choices made by existing LLMs by
developing smaller LMs for specific domains - we call them domain-specific LMs.
Specifically, we start off with HPC as a domain and build an HPC-specific LM,
named MonoCoder, that is orders of magnitude smaller than existing LMs but
delivers similar, if not better performance, on non-HPC and HPC tasks.
Specifically, we pre-trained MonoCoder on an HPC-specific dataset (named
HPCorpus) of C and C++ programs mined from GitHub. We evaluated the performance
of MonoCoder against conventional multi-lingual LLMs. Results demonstrate that
MonoCoder, although much smaller than existing LMs, achieves similar results on
normalized-perplexity tests and much better ones in CodeBLEU competence for
high-performance and parallel code generations. Furthermore, fine-tuning the
base model for the specific task of parallel code generation (OpenMP parallel
for pragmas) demonstrates outstanding results compared to GPT, especially when
local misleading semantics are removed by our novel pre-processor Tokompiler,
showcasing the ability of domain-specific models to assist in HPC-relevant
tasks.",2023-12-20T15:11:06Z
,http://arxiv.org/pdf/2312.05696v1.pdf,GPT-4 and Safety Case Generation: An Exploratory Analysis,"In the ever-evolving landscape of software engineering, the emergence of
large language models (LLMs) and conversational interfaces, exemplified by
ChatGPT, is nothing short of revolutionary. While their potential is undeniable
across various domains, this paper sets out on a captivating expedition to
investigate their uncharted territory, the exploration of generating safety
cases. In this paper, our primary objective is to delve into the existing
knowledge base of GPT-4, focusing specifically on its understanding of the Goal
Structuring Notation (GSN), a well-established notation allowing to visually
represent safety cases. Subsequently, we perform four distinct experiments with
GPT-4. These experiments are designed to assess its capacity for generating
safety cases within a defined system and application domain. To measure the
performance of GPT-4 in this context, we compare the results it generates with
ground-truth safety cases created for an X-ray system system and a
Machine-Learning (ML)-enabled component for tire noise recognition (TNR) in a
vehicle. This allowed us to gain valuable insights into the model's generative
capabilities. Our findings indicate that GPT-4 demonstrates the capacity to
produce safety arguments that are moderately accurate and reasonable.
Furthermore, it exhibits the capability to generate safety cases that closely
align with the semantic content of the reference safety cases used as
ground-truths in our experiments.",2023-12-09T22:28:48Z
,http://arxiv.org/pdf/2404.01720v1.pdf,"Self-Improvement Programming for Temporal Knowledge Graph Question
  Answering","Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions
with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge
of this task lies in understanding the complex semantic information regarding
multiple types of time constraints (e.g., before, first) in questions. Existing
end-to-end methods implicitly model the time constraints by learning time-aware
embeddings of questions and candidate answers, which is far from understanding
the question comprehensively. Motivated by semantic-parsing-based approaches
that explicitly model constraints in questions by generating logical forms with
symbolic operators, we design fundamental temporal operators for time
constraints and introduce a novel self-improvement Programming method for TKGQA
(Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of
Large Language Models (LLMs) to understand the combinatory time constraints in
the questions and generate corresponding program drafts with a few examples
given. Then, it aligns these drafts to TKGs with the linking module and
subsequently executes them to generate the answers. To enhance the ability to
understand questions, Prog-TQA is further equipped with a self-improvement
strategy to effectively bootstrap LLMs using high-quality self-generated
drafts. Extensive experiments demonstrate the superiority of the proposed
Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1
metric.",2024-04-02T08:14:27Z
,http://arxiv.org/pdf/2309.11489v3.pdf,"Text2Reward: Reward Shaping with Language Models for Reinforcement
  Learning","Designing reward functions is a longstanding challenge in reinforcement
learning (RL); it requires specialized knowledge or domain data, leading to
high costs for development. To address this, we introduce Text2Reward, a
data-free framework that automates the generation and shaping of dense reward
functions based on large language models (LLMs). Given a goal described in
natural language, Text2Reward generates shaped dense reward functions as an
executable program grounded in a compact representation of the environment.
Unlike inverse RL and recent work that uses LLMs to write sparse reward codes
or unshaped dense rewards with a constant function across timesteps,
Text2Reward produces interpretable, free-form dense reward codes that cover a
wide range of tasks, utilize existing packages, and allow iterative refinement
with human feedback. We evaluate Text2Reward on two robotic manipulation
benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo.
On 13 of the 17 manipulation tasks, policies trained with generated reward
codes achieve similar or better task success rates and convergence speed than
expert-written reward codes. For locomotion tasks, our method learns six novel
locomotion behaviors with a success rate exceeding 94%. Furthermore, we show
that the policies trained in the simulator with our method can be deployed in
the real world. Finally, Text2Reward further improves the policies by refining
their reward functions with human feedback. Video results are available at
https://text-to-reward.github.io/ .",2023-09-20T17:39:13Z
,http://arxiv.org/pdf/2312.10908v3.pdf,CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update,"Utilizing large language models (LLMs) to compose off-the-shelf visual tools
represents a promising avenue of research for developing robust visual
assistants capable of addressing diverse visual tasks. However, these methods
often overlook the potential for continual learning, typically by freezing the
utilized tools, thus limiting their adaptation to environments requiring new
knowledge. To tackle this challenge, we propose CLOVA, a Closed-Loop Visual
Assistant, which operates within a framework encompassing inference,
reflection, and learning phases. During the inference phase, LLMs generate
programs and execute corresponding tools to complete assigned tasks. In the
reflection phase, a multimodal global-local reflection scheme analyzes human
feedback to determine which tools require updating. Lastly, the learning phase
employs three flexible approaches to automatically gather training data and
introduces a novel prompt tuning scheme to update the tools, allowing CLOVA to
efficiently acquire new knowledge. Experimental findings demonstrate that CLOVA
surpasses existing tool-usage methods by 5% in visual question answering and
multiple-image reasoning, by 10% in knowledge tagging, and by 20% in image
editing. These results underscore the significance of the continual learning
capability in general visual assistants.",2023-12-18T03:34:07Z
,http://arxiv.org/pdf/2304.00409v2.pdf,"DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based
  Vulnerability Detection","We propose and release a new vulnerable source code dataset. We curate the
dataset by crawling security issue websites, extracting vulnerability-fixing
commits and source codes from the corresponding projects. Our new dataset
contains 18,945 vulnerable functions spanning 150 CWEs and 330,492
non-vulnerable functions extracted from 7,514 commits. Our dataset covers 295
more projects than all previous datasets combined.
  Combining our new dataset with previous datasets, we present an analysis of
the challenges and promising research directions of using deep learning for
detecting software vulnerabilities. We study 11 model architectures belonging
to 4 families. Our results show that deep learning is still not ready for
vulnerability detection, due to high false positive rate, low F1 score, and
difficulty of detecting hard CWEs. In particular, we demonstrate an important
generalization challenge for the deployment of deep learning-based models. We
show that increasing the volume of training data may not further improve the
performance of deep learning models for vulnerability detection, but might be
useful to improve the generalization ability to unseen projects.
  We also identify hopeful future research directions. We demonstrate that
large language models (LLMs) are a promising research direction for ML-based
vulnerability detection, outperforming Graph Neural Networks (GNNs) with
code-structure features in our experiments. Moreover, developing source code
specific pre-training objectives is a promising research direction to improve
the vulnerability detection performance.",2023-04-01T23:29:14Z
,http://arxiv.org/pdf/2406.10292v1.pdf,"Automatically Labeling $200B Life-Saving Datasets: A Large Clinical
  Trial Outcome Benchmark","The global cost of drug discovery and development exceeds $200 billion
annually. The main results of drug discovery and development are the outcomes
of clinical trials, which directly influence the regulatory approval of new
drug candidates and ultimately affect patient outcomes. Despite their
significance, large-scale, high-quality clinical trial outcome data are not
readily available to the public. Suppose a large clinical trial outcome dataset
is provided; machine learning researchers can potentially develop accurate
prediction models using past trials and outcome labels, which could help
prioritize and optimize therapeutic programs, ultimately benefiting patients.
This paper introduces Clinical Trial Outcome (CTO) dataset, the largest trial
outcome dataset with around 479K clinical trials, aggregating outcomes from
multiple sources of weakly supervised labels, minimizing the noise from
individual sources, and eliminating the need for human annotation. These
sources include large language model (LLM) decisions on trial-related
documents, news headline sentiments, stock prices of trial sponsors, trial
linkages across phases, and other signals such as patient dropout rates and
adverse events. CTO's labels show unprecedented agreement with supervised
clinical trial outcome labels from test split of the supervised TOP dataset,
with a 91 F1.",2024-06-13T04:23:35Z
10.1145/3586183.3606800,http://arxiv.org/pdf/2304.07810v2.pdf,"VISAR: A Human-AI Argumentative Writing Assistant with Visual
  Programming and Rapid Draft Prototyping","In argumentative writing, writers must brainstorm hierarchical writing goals,
ensure the persuasiveness of their arguments, and revise and organize their
plans through drafting. Recent advances in large language models (LLMs) have
made interactive text generation through a chat interface (e.g., ChatGPT)
possible. However, this approach often neglects implicit writing context and
user intent, lacks support for user control and autonomy, and provides limited
assistance for sensemaking and revising writing plans. To address these
challenges, we introduce VISAR, an AI-enabled writing assistant system designed
to help writers brainstorm and revise hierarchical goals within their writing
context, organize argument structures through synchronized text editing and
visual programming, and enhance persuasiveness with argumentation spark
recommendations. VISAR allows users to explore, experiment with, and validate
their writing plans using automatic draft prototyping. A controlled lab study
confirmed the usability and effectiveness of VISAR in facilitating the
argumentative writing planning process.",2023-04-16T15:29:03Z
,http://arxiv.org/pdf/2311.14903v1.pdf,"Code Generation Based Grading: Evaluating an Auto-grading Mechanism for
  ""Explain-in-Plain-English"" Questions","Comprehending and elucidating the purpose of code is often cited as being a
key learning objective within introductory programming courses. To address this
objective ``Explain-in-Plain-English'' questions, in which students are shown a
segment of code and asked to provide an abstract description of the code's
purpose, have been adopted. However, given EiPE questions require a natural
language response, they often require manual grading which is time-consuming
for course staff and delays feedback for students. With the advent of large
language models (LLMs) capable of generating code, responses to EiPE questions
can be used to generate code segments, the correctness of which can then be
easily verified using test cases. We refer to this approach as ""Code Generation
Based Grading"" (CGBG) and in this paper we explore its agreement with human
graders using EiPE responses from past exams in an introductory programming
course taught in Python. Overall, we find that CGBG achieves moderate agreement
with human graders with the primary area of disagreement being its leniency
with respect to low-level and line-by-line descriptions of code.",2023-11-25T02:45:00Z
,http://arxiv.org/pdf/2406.07944v1.pdf,DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis,"Testing is a major approach to ensuring the quality of deep learning (DL)
libraries. Existing testing techniques commonly adopt differential testing to
relieve the need for test oracle construction. However, these techniques are
limited in finding implementations that offer the same functionality and
generating diverse test inputs for differential testing. This paper introduces
DLLens, a novel differential testing technique for DL library testing. Our
insight is that APIs in different DL libraries are commonly designed to
accomplish various computations for the same set of published DL algorithms.
Although the mapping of these APIs is not often one-to-one, we observe that
their computations can be mutually simulated after proper composition and
adaptation. The use of these simulation counterparts facilitates differential
testing for the detection of functional DL library bugs. Leveraging the
insight, we propose DLLens as a novel mechanism that utilizes a large language
model (LLM) to synthesize valid counterparts of DL library APIs. To generate
diverse test inputs, DLLens incorporates a static analysis method aided by LLM
to extract path constraints from all execution paths in each API and its
counterpart's implementations. These path constraints are then used to guide
the generation of diverse test inputs. We evaluate DLLens on two popular DL
libraries, TensorFlow and PyTorch. Our evaluation shows that DLLens can
synthesize counterparts for more than twice as many APIs found by
state-of-the-art techniques on these libraries. Moreover, DLLens can extract
26.7% more constraints and detect 2.5 times as many bugs as state-of-the-art
techniques. DLLens has successfully found 56 bugs in recent TensorFlow and
PyTorch libraries. Among them, 41 are previously unknown, 39 of which have been
confirmed by developers after reporting, and 19 of those confirmed bugs have
been fixed by developers.",2024-06-12T07:06:38Z
,http://arxiv.org/pdf/2306.05524v2.pdf,"On the Detectability of ChatGPT Content: Benchmarking, Methodology, and
  Evaluation through the Lens of Academic Writing","With ChatGPT under the spotlight, utilizing large language models (LLMs) to
assist academic writing has drawn a significant amount of debate in the
community. In this paper, we aim to present a comprehensive study of the
detectability of ChatGPT-generated content within the academic literature,
particularly focusing on the abstracts of scientific papers, to offer holistic
support for the future development of LLM applications and policies in
academia. Specifically, we first present GPABench2, a benchmarking dataset of
over 2.8 million comparative samples of human-written, GPT-written,
GPT-completed, and GPT-polished abstracts of scientific writing in computer
science, physics, and humanities and social sciences. Second, we explore the
methodology for detecting ChatGPT content. We start by examining the
unsatisfactory performance of existing ChatGPT detecting tools and the
challenges faced by human evaluators (including more than 240 researchers or
students). We then test the hand-crafted linguistic features models as a
baseline and develop a deep neural framework named CheckGPT to better capture
the subtle and deep semantic and linguistic patterns in ChatGPT written
literature. Last, we conduct comprehensive experiments to validate the proposed
CheckGPT framework in each benchmarking task over different disciplines. To
evaluate the detectability of ChatGPT content, we conduct extensive experiments
on the transferability, prompt engineering, and robustness of CheckGPT.",2023-06-07T12:33:24Z
,http://arxiv.org/pdf/2406.06357v1.pdf,"MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific
  Workflows","Scientific innovation relies on detailed workflows, which include critical
steps such as analyzing literature, generating ideas, validating these ideas,
interpreting results, and inspiring follow-up research. However, scientific
publications that document these workflows are extensive and unstructured. This
makes it difficult for both human researchers and AI systems to effectively
navigate and explore the space of scientific innovation. To address this issue,
we introduce MASSW, a comprehensive text dataset on Multi-Aspect Summarization
of Scientific Workflows. MASSW includes more than 152,000 peer-reviewed
publications from 17 leading computer science conferences spanning the past 50
years. Using Large Language Models (LLMs), we automatically extract five core
aspects from these publications -- context, key idea, method, outcome, and
projected impact -- which correspond to five key steps in the research
workflow. These structured summaries facilitate a variety of downstream tasks
and analyses. The quality of the LLM-extracted summaries is validated by
comparing them with human annotations. We demonstrate the utility of MASSW
through multiple novel machine-learning tasks that can be benchmarked using
this new dataset, which make various types of predictions and recommendations
along the scientific workflow. MASSW holds significant potential for
researchers to create and benchmark new AI methods for optimizing scientific
workflows and fostering scientific innovation in the field. Our dataset is
openly available at \url{https://github.com/xingjian-zhang/massw}.",2024-06-10T15:19:09Z
,http://arxiv.org/pdf/2402.15729v2.pdf,How Do Humans Write Code? Large Models Do It the Same Way Too,"Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought
(CoT) as the most popular method in Large Language Models (LLMs) mathematical
reasoning tasks by utilizing external tool calls to circumvent computational
errors. However, our evaluation of the GPT-4 and Llama series reveals that
using PoT introduces more reasoning errors, such as incorrect formulas or
flawed logic, compared to CoT. To address this issue, we propose Human-Think
Language (HTL), which leverages a suite of strategies that help integrate PoT
and CoT, encompassing: (1) a new generation paradigm that uses full CoT
reasoning to control code generation. (2) Focus Attention, that directs model
attention to the CoT reasoning during PoT to generate more logical code. (3)
reinforcement learning that utilizes the accuracy of both CoT and PoT responses
as rewards to prevent repetitive reasoning steps in LLMs when solving difficult
math problems. Our method achieves an average improvement of 6.5% on the
Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical
calculation datasets. It also shows significant effectiveness on five
out-of-domain datasets by controlling the model's information flow, exhibiting
strong transferability. Additionally, HTL shows the most significant
improvement in non-mathematical natural language inference task, contributing
to a unified reasoning task framework",2024-02-24T05:40:01Z
,http://arxiv.org/pdf/2402.17644v2.pdf,"Are LLMs Capable of Data-based Statistical and Causal Reasoning?
  Benchmarking Advanced Quantitative Reasoning with Data","Quantitative reasoning is a critical skill to analyze data, yet the
assessment of such ability remains limited. To address this gap, we introduce
the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate
Large Language Models' capability in statistical and causal reasoning with
real-world data. The benchmark comprises a carefully constructed dataset of 411
questions accompanied by data sheets from textbooks, online learning materials,
and academic papers. To compare models' quantitative reasoning abilities on
data and text, we enrich the benchmark with an auxiliary set of 290 text-only
questions, namely QRText. We evaluate natural language reasoning, program-based
reasoning, and agent reasoning methods including Chain-of-Thought,
Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models.
The strongest model GPT-4 achieves an accuracy of 58%, which has much room for
improvement. Among open-source models, Deepseek-coder-instruct, a code LLM
pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals
that models encounter difficulties in data analysis and causal reasoning, and
struggle in using causal knowledge and provided data simultaneously. Code and
data are in https://github.com/xxxiaol/QRData.",2024-02-27T16:15:03Z
,http://arxiv.org/pdf/2311.02807v2.pdf,QualEval: Qualitative Evaluation for Model Improvement,"Quantitative evaluation metrics have traditionally been pivotal in gauging
the advancements of artificial intelligence systems, including large language
models (LLMs). However, these metrics have inherent limitations. Given the
intricate nature of real-world tasks, a single scalar to quantify and compare
is insufficient to capture the fine-grained nuances of model behavior. Metrics
serve only as a way to compare and benchmark models, and do not yield
actionable diagnostics, thus making the model improvement process challenging.
Model developers find themselves amid extensive manual efforts involving
sifting through vast datasets and attempting hit-or-miss adjustments to
training data or setups. In this work, we address the shortcomings of
quantitative metrics by proposing QualEval, which augments quantitative scalar
metrics with automated qualitative evaluation as a vehicle for model
improvement. QualEval uses a powerful LLM reasoner and our novel flexible
linear programming solver to generate human-readable insights that when
applied, accelerate model improvement. The insights are backed by a
comprehensive dashboard with fine-grained visualizations and
human-interpretable analyses. We corroborate the faithfulness of QualEval by
demonstrating that leveraging its insights, for example, improves the absolute
performance of the Llama 2 model by up to 15% points relative on a challenging
dialogue task (DialogSum) when compared to baselines. QualEval successfully
increases the pace of model development, thus in essence serving as a
data-scientist-in-a-box. Given the focus on critiquing and improving current
evaluation metrics, our method serves as a refreshingly new technique for both
model evaluation and improvement.",2023-11-06T00:21:44Z
,http://arxiv.org/pdf/2401.06795v2.pdf,AI and Generative AI for Research Discovery and Summarization,"AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.",2024-01-08T18:42:55Z
,http://arxiv.org/pdf/2311.11979v1.pdf,"On the Potential and Limitations of Few-Shot In-Context Learning to
  Generate Metamorphic Specifications for Tax Preparation Software","Due to the ever-increasing complexity of income tax laws in the United
States, the number of US taxpayers filing their taxes using tax preparation
software (henceforth, tax software) continues to increase. According to the
U.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed
their individual income taxes using tax software. Given the legal consequences
of incorrectly filing taxes for the taxpayer, ensuring the correctness of tax
software is of paramount importance. Metamorphic testing has emerged as a
leading solution to test and debug legal-critical tax software due to the
absence of correctness requirements and trustworthy datasets. The key idea
behind metamorphic testing is to express the properties of a system in terms of
the relationship between one input and its slightly metamorphosed twinned
input. Extracting metamorphic properties from IRS tax publications is a tedious
and time-consuming process. As a response, this paper formulates the task of
generating metamorphic specifications as a translation task between properties
extracted from tax documents - expressed in natural language - to a contrastive
first-order logic form. We perform a systematic analysis on the potential and
limitations of in-context learning with Large Language Models(LLMs) for this
task, and outline a research agenda towards automating the generation of
metamorphic specifications for tax preparation software.",2023-11-20T18:12:28Z
,http://arxiv.org/pdf/2404.13945v2.pdf,"How do LLMs Support Deep Learning Testing? A Comprehensive Study Through
  the Lens of Image Mutation","Visual deep learning (VDL) systems have shown significant success in
real-world applications like image recognition, object detection, and
autonomous driving. To evaluate the reliability of VDL, a mainstream approach
is software testing, which requires diverse and controllable mutations over
image semantics. The rapid development of multi-modal large language models
(MLLMs) has introduced revolutionary image mutation potentials through
instruction-driven methods. Users can now freely describe desired mutations and
let MLLMs generate the mutated images.
  However, the quality of MLLM-produced test inputs in VDL testing remains
largely unexplored. We present the first study, aiming to assess MLLMs'
adequacy from 1) the semantic validity of MLLM mutated images, 2) the alignment
of MLLM mutated images with their text instructions (prompts), 3) the
faithfulness of how different mutations preserve semantics that are ought to
remain unchanged, and 4) the effectiveness of detecting VDL faults. With
large-scale human studies and quantitative evaluations, we identify MLLM's
promising potentials in expanding the covered semantics of image mutations.
Notably, while SoTA MLLMs (e.g., GPT-4V) fail to support or perform worse in
editing existing semantics in images (as in traditional mutations like
rotation), they generate high-quality test inputs using ""semantic-additive""
mutations (e.g., ""dress a dog with clothes""), which bring extra semantics to
images; these were infeasible for past approaches. Hence, we view MLLM-based
mutations as a vital complement to traditional mutations, and advocate future
VDL testing tasks to combine MLLM-based methods and traditional image mutations
for comprehensive and reliable testing.",2024-04-22T07:41:41Z
10.1145/3613904.3642773,http://arxiv.org/pdf/2401.11314v2.pdf,"CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming
  Assistant that Balances Student and Educator Needs","Timely, personalized feedback is essential for students learning programming.
LLM-powered tools like ChatGPT offer instant support, but reveal direct answers
with code, which may hinder deep conceptual engagement. We developed CodeAid,
an LLM-powered programming assistant delivering helpful, technically correct
responses, without revealing code solutions. CodeAid answers conceptual
questions, generates pseudo-code with line-by-line explanations, and annotates
student's incorrect code with fix suggestions. We deployed CodeAid in a
programming class of 700 students for a 12-week semester. A thematic analysis
of 8,000 usages of CodeAid was performed, further enriched by weekly surveys,
and 22 student interviews. We then interviewed eight programming educators to
gain further insights. Our findings reveal four design considerations for
future educational AI assistants: D1) exploiting AI's unique benefits; D2)
simplifying query formulation while promoting cognitive engagement; D3)
avoiding direct responses while encouraging motivated learning; and D4)
maintaining transparency and control for students to asses and steer AI
responses.",2024-01-20T20:14:42Z
,http://arxiv.org/pdf/2403.09744v1.pdf,"Evaluating the Application of Large Language Models to Generate Feedback
  in Programming Education","This study investigates the application of large language models,
specifically GPT-4, to enhance programming education. The research outlines the
design of a web application that uses GPT-4 to provide feedback on programming
tasks, without giving away the solution. A web application for working on
programming tasks was developed for the study and evaluated with 51 students
over the course of one semester. The results show that most of the feedback
generated by GPT-4 effectively addressed code errors. However, challenges with
incorrect suggestions and hallucinated issues indicate the need for further
improvements.",2024-03-13T23:14:35Z
,http://arxiv.org/pdf/2211.04715v1.pdf,"Robosourcing Educational Resources -- Leveraging Large Language Models
  for Learnersourcing","In this article, we introduce and evaluate the concept of robosourcing for
creating educational content. Robosourcing lies in the intersection of
crowdsourcing and large language models, where instead of a crowd of humans,
requests to large language models replace some of the work traditionally
performed by the crowd. Robosourcing includes a human-in-the-loop to provide
priming (input) as well as to evaluate and potentially adjust the generated
artefacts; these evaluations could also be used to improve the large language
models. We propose a system to outline the robosourcing process. We further
study the feasibility of robosourcing in the context of education by conducting
an evaluation of robosourced and programming exercises, generated using OpenAI
Codex. Our results suggest that robosourcing could significantly reduce human
effort in creating diverse educational content while maintaining quality
similar to human-created content.",2022-11-09T07:13:03Z
,http://arxiv.org/pdf/2307.09950v3.pdf,Prompting for Automatic Log Template Extraction,"Log parsing, which involves log template extraction from semi-structured logs
to produce structured logs, is the first and the most critical step in
automated log analysis. However, current log parsers suffer from limited
effectiveness for two reasons. First, traditional data-driven log parsers
solely rely on heuristics or handcrafted features designed by domain experts,
which may not consistently perform well on logs from diverse systems. Second,
existing supervised log parsers require model tuning, which is often limited to
fixed training samples and causes sub-optimal performance across the entire log
source. To address this limitation, we propose DivLog, an effective log parsing
framework based on the in-context learning (ICL) ability of large language
models (LLMs). Specifically, before log parsing, DivLog samples a small amount
of offline logs as candidates by maximizing their diversity. Then, during log
parsing, DivLog selects five appropriate labeled candidates as examples for
each target log and constructs them into a prompt. By mining the semantics of
examples in the prompt, DivLog generates a target log template in a
training-free manner. In addition, we design a straightforward yet effective
prompt format to extract the output and enhance the quality of the generated
log templates. We conducted experiments on 16 widely-used public datasets. The
results show that DivLog achieves (1) 98.1% Parsing Accuracy, (2) 92.1%
Precision Template Accuracy, and (3) 92.9% Recall Template Accuracy on average,
exhibiting state-of-the-art performance.",2023-07-19T12:44:59Z
,http://arxiv.org/pdf/2401.13169v2.pdf,ReposVul: A Repository-Level High-Quality Vulnerability Dataset,"Open-Source Software (OSS) vulnerabilities bring great challenges to the
software security and pose potential risks to our society. Enormous efforts
have been devoted into automated vulnerability detection, among which deep
learning (DL)-based approaches have proven to be the most effective. However,
the current labeled data present the following limitations: (1) Tangled
Patches: Developers may submit code changes unrelated to vulnerability fixes
within patches, leading to tangled patches. (2) Lacking Inter-procedural
Vulnerabilities: The existing vulnerability datasets typically contain
function-level and file-level vulnerabilities, ignoring the relations between
functions, thus rendering the approaches unable to detect the inter-procedural
vulnerabilities. (3) Outdated Patches: The existing datasets usually contain
outdated patches, which may bias the model during training.
  To address the above limitations, in this paper, we propose an automated data
collection framework and construct the first repository-level high-quality
vulnerability dataset named ReposVul. The proposed framework mainly contains
three modules: (1) A vulnerability untangling module, aiming at distinguishing
vulnerability-fixing related code changes from tangled patches, in which the
Large Language Models (LLMs) and static analysis tools are jointly employed.
(2) A multi-granularity dependency extraction module, aiming at capturing the
inter-procedural call relationships of vulnerabilities, in which we construct
multiple-granularity information for each vulnerability patch, including
repository-level, file-level, function-level, and line-level. (3) A trace-based
filtering module, aiming at filtering the outdated patches, which leverages the
file path trace-based filter and commit time trace-based filter to construct an
up-to-date dataset.",2024-01-24T01:27:48Z
,http://arxiv.org/pdf/2403.19096v1.pdf,"SCALE: Constructing Structured Natural Language Comment Trees for
  Software Vulnerability Detection","Recently, there has been a growing interest in automatic software
vulnerability detection. Pre-trained model-based approaches have demonstrated
superior performance than other Deep Learning (DL)-based approaches in
detecting vulnerabilities. However, the existing pre-trained model-based
approaches generally employ code sequences as input during prediction, and may
ignore vulnerability-related structural information, as reflected in the
following two aspects. First, they tend to fail to infer the semantics of the
code statements with complex logic such as those containing multiple operators
and pointers. Second, they are hard to comprehend various code execution
sequences, which is essential for precise vulnerability detection.
  To mitigate the challenges, we propose a Structured Natural Language Comment
tree-based vulnerAbiLity dEtection framework based on the pre-trained models,
named SCALE. The proposed Structured Natural Language Comment Tree (SCT)
integrates the semantics of code statements with code execution sequences based
on the Abstract Syntax Trees (ASTs). Specifically, SCALE comprises three main
modules: (1) Comment Tree Construction, which aims at enhancing the model's
ability to infer the semantics of code statements by first incorporating Large
Language Models (LLMs) for comment generation and then adding the comment node
to ASTs. (2) Structured Natural Language Comment Tree Construction}, which aims
at explicitly involving code execution sequence by combining the code syntax
templates with the comment tree. (3) SCT-Enhanced Representation, which finally
incorporates the constructed SCTs for well capturing vulnerability patterns.",2024-03-28T02:20:03Z
,http://arxiv.org/pdf/2310.03173v2.pdf,"$\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program
  Synthesis","Program synthesis aims to create accurate, executable programs from problem
specifications, specifically from natural language descriptions in our context.
Recent studies have leveraged the power of reinforcement learning (RL) in
conjunction with large language models (LLMs), significantly enhancing code
generation capabilities. The application of RL focuses on directly optimizing
for functional correctness, offering an advantage over conventional supervised
methods. Despite policy-based RL methods dominating the literature on RL for
program synthesis, the nature of program synthesis tasks hints at a natural
alignment with value-based methods. This stems from the rich collection of
off-policy programs, including those developed by human programmers and also
historical samples, coupled with the straightforward verification of generated
programs through automated unit testing, meaning rewards are easy to obtain.
Diverging from the dominant use of policy-based algorithms, our work explores
the feasibility of value-based approaches, leading to the development of our
$\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-based
methods presents challenges due to the enormous search space inherent to
program synthesis. To this end, we introduce an initialization protocol for RL
agents utilizing pre-trained LMs and a conservative Bellman operator to reduce
training complexities. Moreover, we demonstrate how to leverage the learned
value functions as a dual strategy to post-process generated programs. Our
empirical evaluations demonstrated $\mathcal{B}$-Coder's capability in
achieving state-of-the-art performance when compared to policy-based methods.
Remarkably, this achievement is reached with minimal reward engineering effort,
highlighting the effectiveness of value-based RL, independent of reward
designs.",2023-10-04T21:40:36Z
,http://arxiv.org/pdf/2405.06399v1.pdf,"Program Synthesis using Inductive Logic Programming for the Abstraction
  and Reasoning Corpus","The Abstraction and Reasoning Corpus (ARC) is a general artificial
intelligence benchmark that is currently unsolvable by any Machine Learning
method, including Large Language Models (LLMs). It demands strong
generalization and reasoning capabilities which are known to be weaknesses of
Neural Network based systems. In this work, we propose a Program Synthesis
system that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to
solve ARC. We have manually defined a simple Domain Specific Language (DSL)
that corresponds to a small set of object-centric abstractions relevant to ARC.
This is the Background Knowledge used by ILP to create Logic Programs that
provide reasoning capabilities to our system. The full system is capable of
generalize to unseen tasks, since ILP can create Logic Program(s) from few
examples, in the case of ARC: pairs of Input-Output grids examples for each
task. These Logic Programs are able to generate Objects present in the Output
grid and the combination of these can form a complete program that transforms
an Input grid into an Output grid. We randomly chose some tasks from ARC that
dont require more than the small number of the Object primitives we implemented
and show that given only these, our system can solve tasks that require each,
such different reasoning.",2024-05-10T11:22:31Z
10.1145/3639476.3639770,http://arxiv.org/pdf/2312.09126v2.pdf,Towards Trustworthy AI Software Development Assistance,"It is expected that in the near future, AI software development assistants
will play an important role in the software industry. However, current software
development assistants tend to be unreliable, often producing incorrect,
unsafe, or low-quality code. We seek to resolve these issues by introducing a
holistic architecture for constructing, training, and using trustworthy AI
software development assistants. In the center of the architecture, there is a
foundational LLM trained on datasets representative of real-world coding
scenarios and complex software architectures, and fine-tuned on code quality
criteria beyond correctness. The LLM will make use of graph-based code
representations for advanced semantic comprehension. We envision a knowledge
graph integrated into the system to provide up-to-date background knowledge and
to enable the assistant to provide appropriate explanations. Finally, a modular
framework for constrained decoding will ensure that certain guarantees (e.g.,
for correctness and security) hold for the generated code.",2023-12-14T16:59:49Z
,http://arxiv.org/pdf/2401.05566v3.pdf,"Sleeper Agents: Training Deceptive LLMs that Persist Through Safety
  Training","Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoor behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoor behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.",2024-01-10T22:14:35Z
10.1145/3643991.3644926,http://arxiv.org/pdf/2403.04013v1.pdf,"Whodunit: Classifying Code as Human Authored or GPT-4 Generated -- A
  case study on CodeChef problems","Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT,
built on large language models like GPT-4, are revolutionizing how programming
tasks are performed, raising questions about whether code is authored by
generative AI models. Such questions are of particular interest to educators,
who worry that these tools enable a new form of academic dishonesty, in which
students submit AI generated code as their own work. Our research explores the
viability of using code stylometry and machine learning to distinguish between
GPT-4 generated and human-authored code. Our dataset comprises human-authored
solutions from CodeChef and AI-authored solutions generated by GPT-4. Our
classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91. A
variant of our classifier that excludes gameable features (e.g., empty lines,
whitespace) still performs well with an F1-score and AUC-ROC score of 0.89. We
also evaluated our classifier with respect to the difficulty of the programming
problem and found that there was almost no difference between easier and
intermediate problems, and the classifier performed only slightly worse on
harder problems. Our study shows that code stylometry is a promising approach
for distinguishing between GPT-4 generated code and human-authored code.",2024-03-06T19:51:26Z
,http://arxiv.org/pdf/2304.09181v1.pdf,"Large Language Models Based Automatic Synthesis of Software
  Specifications","Software configurations play a crucial role in determining the behavior of
software systems. In order to ensure safe and error-free operation, it is
necessary to identify the correct configuration, along with their valid bounds
and rules, which are commonly referred to as software specifications. As
software systems grow in complexity and scale, the number of configurations and
associated specifications required to ensure the correct operation can become
large and prohibitively difficult to manipulate manually. Due to the fast pace
of software development, it is often the case that correct software
specifications are not thoroughly checked or validated within the software
itself. Rather, they are frequently discussed and documented in a variety of
external sources, including software manuals, code comments, and online
discussion forums. Therefore, it is hard for the system administrator to know
the correct specifications of configurations due to the lack of clarity,
organization, and a centralized unified source to look at. To address this
challenge, we propose SpecSyn a framework that leverages a state-of-the-art
large language model to automatically synthesize software specifications from
natural language sources. Our approach formulates software specification
synthesis as a sequence-to-sequence learning problem and investigates the
extraction of specifications from large contextual texts. This is the first
work that uses a large language model for end-to-end specification synthesis
from natural language texts. Empirical results demonstrate that our system
outperforms prior the state-of-the-art specification synthesis tool by 21% in
terms of F1 score and can find specifications from single as well as multiple
sentences.",2023-04-18T01:22:44Z
,http://arxiv.org/pdf/2303.13375v2.pdf,Capabilities of GPT-4 on Medical Challenge Problems,"Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation across various domains, including
medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art
LLM, on medical competency examinations and benchmark datasets. GPT-4 is a
general-purpose model that is not specialized for medical problems through
training or engineered to solve clinical tasks. Our analysis covers two sets of
official practice materials for the USMLE, a three-step examination program
used to assess clinical competency and grant licensure in the United States. We
also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond
measuring model performance, experiments were conducted to investigate the
influence of test questions containing both text and images on model
performance, probe for memorization of content during training, and study
probability calibration, which is of critical importance in high-stakes
applications like medicine. Our results show that GPT-4, without any
specialized prompt crafting, exceeds the passing score on USMLE by over 20
points and outperforms earlier general-purpose models (GPT-3.5) as well as
models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned
version of Flan-PaLM 540B). In addition, GPT-4 is significantly better
calibrated than GPT-3.5, demonstrating a much-improved ability to predict the
likelihood that its answers are correct. We also explore the behavior of the
model qualitatively through a case study that shows the ability of GPT-4 to
explain medical reasoning, personalize explanations to students, and
interactively craft new counterfactual scenarios around a medical case.
Implications of the findings are discussed for potential uses of GPT-4 in
medical education, assessment, and clinical practice, with appropriate
attention to challenges of accuracy and safety.",2023-03-20T16:18:38Z
,http://arxiv.org/pdf/2312.03728v1.pdf,"Real Customization or Just Marketing: Are Customized Versions of Chat
  GPT Useful?","Large Language Models (LLMs), as the case of OpenAI ChatGPT-4 Turbo, are
revolutionizing several industries, including higher education. In this
context, LLMs can be personalized through a fine-tuning process to meet the
student demands on every particular subject, like statistics. Recently, OpenAI
has launched the possibility to fine-tune their model with a natural language
web interface, enabling the possibility to create customized GPT version
deliberately conditioned to meet the demands of a specific task. The objective
of this research is to assess the potential of the customized GPTs that have
recently been launched by OpenAI. After developing a Business Statistics
Virtual Professor (BSVP), tailored for students at the Universidad Pontificia
Comillas, its behavior was evaluated and compared with that of ChatGPT-4 Turbo.
The results lead to several conclusions. Firstly, a substantial modification in
the style of communication was observed. Following the instructions it was
trained with, BSVP provided responses in a more relatable and friendly tone,
even incorporating a few minor jokes. Secondly, and this is a matter of
relevance, when explicitly asked for something like, ""I would like to practice
a programming exercise similar to those in R practice 4,"" BSVP was capable of
providing a far superior response: having access to contextual documentation,
it could fulfill the request, something beyond ChatGPT-4 Turbo's capabilities.
On the downside, the response times were generally higher. Lastly, regarding
overall performance, quality, depth, and alignment with the specific content of
the course, no statistically significant differences were observed in the
responses between BSVP and ChatGPT-4 Turbo. It appears that customized
assistants trained with prompts present advantages as virtual aids for
students, yet they do not constitute a substantial improvement over ChatGPT-4
Turbo.",2023-11-27T15:46:15Z
,http://arxiv.org/pdf/2310.06228v1.pdf,"Evolution of Natural Language Processing Technology: Not Just Language
  Processing Towards General Purpose AI","Since the invention of computers, communication through natural language
(actual human language) has been a dream technology. However, natural language
is extremely difficult to mathematically formulate, making it difficult to
realize as an algorithm without considering programming. While there have been
numerous technological developments, one cannot say that any results allowing
free utilization have been achieved thus far. In the case of language learning
in humans, for instance when learning one's mother tongue or foreign language,
one must admit that this process is similar to the adage ""practice makes
perfect"" in principle, even though the learning method is significant up to a
point. Deep learning has played a central role in contemporary AI technology in
recent years. When applied to natural language processing (NLP), this produced
unprecedented results. Achievements exceeding the initial predictions have been
reported from the results of learning vast amounts of textual data using deep
learning. For instance, four arithmetic operations could be performed without
explicit learning, thereby enabling the explanation of complex images and the
generation of images from corresponding explanatory texts. It is an accurate
example of the learner embodying the concept of ""practice makes perfect"" by
using vast amounts of textual data. This report provides a technological
explanation of how cutting-edge NLP has made it possible to realize the
""practice makes perfect"" principle. Additionally, examples of how this can be
applied to business are provided. We reported in June 2022 in Japanese on the
NLP movement from late 2021 to early 2022. We would like to summarize this as a
memorandum since this is just the initial movement leading to the current large
language models (LLMs).",2023-10-10T00:41:38Z
,http://arxiv.org/pdf/2311.04901v1.pdf,"GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and
  reusing ModulEs","Recent works have shown that Large Language Models (LLMs) could empower
traditional neuro-symbolic models via programming capabilities to translate
language into module descriptions, thus achieving strong visual reasoning
results while maintaining the model's transparency and efficiency. However,
these models usually exhaustively generate the entire code snippet given each
new instance of a task, which is extremely ineffective. We propose generative
neuro-symbolic visual reasoning by growing and reusing modules. Specifically,
our model consists of three unique stages, module initialization, module
generation, and module execution. First, given a vision-language task, we adopt
LLMs to examine whether we could reuse and grow over established modules to
handle this new task. If not, we initialize a new module needed by the task and
specify the inputs and outputs of this new module. After that, the new module
is created by querying LLMs to generate corresponding code snippets that match
the requirements. In order to get a better sense of the new module's ability,
we treat few-shot training examples as test cases to see if our new module
could pass these cases. If yes, the new module is added to the module library
for future reuse. Finally, we evaluate the performance of our model on the
testing set by executing the parsed programs with the newly made visual modules
to get the results. We find the proposed model possesses several advantages.
First, it performs competitively on standard tasks like visual question
answering and referring expression comprehension; Second, the modules learned
from one task can be seamlessly transferred to new tasks; Last but not least,
it is able to adapt to new visual reasoning tasks by observing a few training
examples and reusing modules.",2023-11-08T18:59:05Z
,http://arxiv.org/pdf/2312.14798v1.pdf,"Semantic Parsing for Complex Data Retrieval: Targeting Query Plans vs.
  SQL for No-Code Access to Relational Databases","Large Language Models (LLMs) have spurred progress in text-to-SQL, the task
of generating SQL queries from natural language questions based on a given
database schema. Despite the declarative nature of SQL, it continues to be a
complex programming language. In this paper, we investigate the potential of an
alternative query language with simpler syntax and modular specification of
complex queries. The purpose is to create a query language that can be learned
more easily by modern neural semantic parsing architectures while also enabling
non-programmers to better assess the validity of the query plans produced by an
interactive query plan assistant.
  The proposed alternative query language is called Query Plan Language (QPL).
It is designed to be modular and can be translated into a restricted form of
SQL Common Table Expressions (CTEs). The aim of QPL is to make complex data
retrieval accessible to non-programmers by allowing users to express their
questions in natural language while also providing an easier-to-verify target
language. The paper demonstrates how neural LLMs can benefit from QPL's
modularity to generate complex query plans in a compositional manner. This
involves a question decomposition strategy and a planning stage.
  We conduct experiments on a version of the Spider text-to-SQL dataset that
has been converted to QPL. The hierarchical structure of QPL programs enables
us to measure query complexity naturally. Based on this assessment, we identify
the low accuracy of existing text-to-SQL systems on complex compositional
queries. We present ways to address the challenge of complex queries in an
iterative, user-controlled manner, using fine-tuned LLMs and a variety of
prompting strategies in a compositional manner.",2023-12-22T16:16:15Z
,http://arxiv.org/pdf/2404.07382v1.pdf,"Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for
  Intuitionistic Propositional Logic Proving","Recent advances in Automated Theorem Proving have shown the effectiveness of
leveraging a (large) language model that generates tactics (i.e. proof steps)
to search through proof states. The current model, while trained solely on
successful proof paths, faces a discrepancy at the inference stage, as it must
sample and try various tactics at each proof state until finding success,
unlike its training which does not incorporate learning from failed attempts.
Intuitively, a tactic that leads to a failed search path would indicate that
similar tactics should receive less attention during the following trials. In
this paper, we demonstrate the benefit of training models that additionally
learn from failed search paths. Facing the lack of such trial-and-error data in
existing open-source theorem-proving datasets, we curate a dataset on
intuitionistic propositional logic theorems and formalize it in Lean, such that
we can reliably check the correctness of proofs. We compare our model trained
on relatively short trial-and-error information (TrialMaster) with models
trained only on the correct paths and discover that the former solves more
unseen theorems with lower trial searches.",2024-04-10T23:01:45Z
,http://arxiv.org/pdf/2404.05752v1.pdf,Physics Event Classification Using Large Language Models,"The 2023 AI4EIC hackathon was the culmination of the third annual AI4EIC
workshop at The Catholic University of America. This workshop brought together
researchers from physics, data science and computer science to discuss the
latest developments in Artificial Intelligence (AI) and Machine Learning (ML)
for the Electron Ion Collider (EIC), including applications for detectors,
accelerators, and experimental control. The hackathon, held on the final day of
the workshop, involved using a chatbot powered by a Large Language Model,
ChatGPT-3.5, to train a binary classifier neutrons and photons in simulated
data from the \textsc{GlueX} Barrel Calorimeter. In total, six teams of up to
four participants from all over the world took part in this intense educational
and research event. This article highlights the hackathon challenge, the
resources and methodology used, and the results and insights gained from
analyzing physics data using the most cutting-edge tools in AI/ML.",2024-04-05T03:52:27Z
,http://arxiv.org/pdf/2406.00144v1.pdf,Query2CAD: Generating CAD models using natural language queries,"Computer Aided Design (CAD) engineers typically do not achieve their best
prototypes in a single attempt. Instead, they iterate and refine their designs
to achieve an optimal solution through multiple revisions. This traditional
approach, though effective, is time-consuming and relies heavily on the
expertise of skilled engineers. To address these challenges, we introduce
Query2CAD, a novel framework to generate CAD designs. The framework uses a
large language model to generate executable CAD macros. Additionally, Query2CAD
refines the generation of the CAD model with the help of its self-refinement
loops. Query2CAD operates without supervised data or additional training, using
the LLM as both a generator and a refiner. The refiner leverages feedback
generated by the BLIP2 model, and to address false negatives, we have
incorporated human-in-the-loop feedback into our system. Additionally, we have
developed a dataset that encompasses most operations used in CAD model
designing and have evaluated our framework using this dataset. Our findings
reveal that when we used GPT-4 Turbo as our language model, the architecture
achieved a success rate of 53.6\% on the first attempt. With subsequent
refinements, the success rate increased by 23.1\%. In particular, the most
significant improvement in the success rate was observed with the first
iteration of the refinement. With subsequent refinements, the accuracy of the
correct designs did not improve significantly. We have open-sourced our data,
model, and code (github.com/akshay140601/Query2CAD).",2024-05-31T19:17:00Z
,http://arxiv.org/pdf/2306.08997v2.pdf,"Exploring the MIT Mathematics and EECS Curriculum Using Large Language
  Models","We curate a comprehensive dataset of 4,550 questions and solutions from
problem sets, midterm exams, and final exams across all MIT Mathematics and
Electrical Engineering and Computer Science (EECS) courses required for
obtaining a degree. We evaluate the ability of large language models to fulfill
the graduation requirements for any MIT major in Mathematics and EECS. Our
results demonstrate that GPT-3.5 successfully solves a third of the entire MIT
curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate
on a test set excluding questions based on images. We fine-tune an open-source
large language model on this dataset. We employ GPT-4 to automatically grade
model responses, providing a detailed performance breakdown by course,
question, and answer type. By embedding questions in a low-dimensional space,
we explore the relationships between questions, topics, and classes and
discover which questions and classes are required for solving other questions
and classes through few-shot learning. Our analysis offers valuable insights
into course prerequisites and curriculum design, highlighting language models'
potential for learning and improving Mathematics and EECS education.",2023-06-15T09:48:14Z
,http://arxiv.org/pdf/2305.16291v2.pdf,Voyager: An Open-Ended Embodied Agent with Large Language Models,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent
in Minecraft that continuously explores the world, acquires diverse skills, and
makes novel discoveries without human intervention. Voyager consists of three
key components: 1) an automatic curriculum that maximizes exploration, 2) an
ever-growing skill library of executable code for storing and retrieving
complex behaviors, and 3) a new iterative prompting mechanism that incorporates
environment feedback, execution errors, and self-verification for program
improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses
the need for model parameter fine-tuning. The skills developed by Voyager are
temporally extended, interpretable, and compositional, which compounds the
agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,
Voyager shows strong in-context lifelong learning capability and exhibits
exceptional proficiency in playing Minecraft. It obtains 3.3x more unique
items, travels 2.3x longer distances, and unlocks key tech tree milestones up
to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill
library in a new Minecraft world to solve novel tasks from scratch, while other
techniques struggle to generalize. We open-source our full codebase and prompts
at https://voyager.minedojo.org/.",2023-05-25T17:46:38Z
,http://arxiv.org/pdf/2401.06967v1.pdf,"NHANES-GCP: Leveraging the Google Cloud Platform and BigQuery ML for
  reproducible machine learning with data from the National Health and
  Nutrition Examination Survey","Summary: NHANES, the National Health and Nutrition Examination Survey, is a
program of studies led by the Centers for Disease Control and Prevention (CDC)
designed to assess the health and nutritional status of adults and children in
the United States (U.S.). NHANES data is frequently used by biostatisticians
and clinical scientists to study health trends across the U.S., but every
analysis requires extensive data management and cleaning before use and this
repetitive data engineering collectively costs valuable research time and
decreases the reproducibility of analyses. Here, we introduce NHANES-GCP, a
Cloud Development Kit for Terraform (CDKTF) Infrastructure-as-Code (IaC) and
Data Build Tool (dbt) resources built on the Google Cloud Platform (GCP) that
automates the data engineering and management aspects of working with NHANES
data. With current GCP pricing, NHANES-GCP costs less than $2 to run and less
than $15/yr of ongoing costs for hosting the NHANES data, all while providing
researchers with clean data tables that can readily be integrated for
large-scale analyses. We provide examples of leveraging BigQuery ML to carry
out the process of selecting data, integrating data, training machine learning
and statistical models, and generating results all from a single SQL-like
query. NHANES-GCP is designed to enhance the reproducibility of analyses and
create a well-engineered NHANES data resource for statistics, machine learning,
and fine-tuning Large Language Models (LLMs).
  Availability and implementation"" NHANES-GCP is available at
https://github.com/In-Vivo-Group/NHANES-GCP",2024-01-13T03:41:54Z
,http://arxiv.org/pdf/2403.07608v1.pdf,Couler: Unified Machine Learning Workflow Optimization in Cloud,"Machine Learning (ML) has become ubiquitous, fueling data-driven applications
across various organizations. Contrary to the traditional perception of ML in
research, ML workflows can be complex, resource-intensive, and time-consuming.
Expanding an ML workflow to encompass a wider range of data infrastructure and
data types may lead to larger workloads and increased deployment costs.
Currently, numerous workflow engines are available (with over ten being widely
recognized). This variety poses a challenge for end-users in terms of mastering
different engine APIs. While efforts have primarily focused on optimizing ML
Operations (MLOps) for a specific workflow engine, current methods largely
overlook workflow optimization across different engines.
  In this work, we design and implement Couler, a system designed for unified
ML workflow optimization in the cloud. Our main insight lies in the ability to
generate an ML workflow using natural language (NL) descriptions. We integrate
Large Language Models (LLMs) into workflow generation, and provide a unified
programming interface for various workflow engines. This approach alleviates
the need to understand various workflow engines' APIs. Moreover, Couler
enhances workflow computation efficiency by introducing automated caching at
multiple stages, enabling large workflow auto-parallelization and automatic
hyperparameters tuning. These enhancements minimize redundant computational
costs and improve fault tolerance during deep learning workflow training.
Couler is extensively deployed in real-world production scenarios at Ant Group,
handling approximately 22k workflows daily, and has successfully improved the
CPU/Memory utilization by more than 15% and the workflow completion rate by
around 17%.",2024-03-12T12:47:32Z
,http://arxiv.org/pdf/2308.05201v2.pdf,"""Generate"" the Future of Work through AI: Empirical Evidence from Online
  Labor Markets","Large Language Model (LLM) based generative AI, such as ChatGPT, is
considered the first generation of Artificial General Intelligence (AGI),
exhibiting zero-shot learning abilities for a wide variety of downstream tasks.
Due to its general-purpose and emergent nature, its impact on labor dynamics
becomes complex and difficult to anticipate. Leveraging an extensive dataset
from a prominent online labor market, we uncover a post-ChatGPT decline in
labor demand, supply, and transactions for submarkets pertaining to
text-related and programming-related jobs, in comparison to those not directly
exposed to ChatGPT's core functionalities. Meanwhile, these affected submarkets
exhibit a discernible increase in the complexity of the remaining jobs and a
heightened level of competition among freelancers. Intriguingly, our findings
indicate that the diminution in the labor supply pertaining to programming is
comparatively less pronounced, a phenomenon ascribed to the transition of
freelancers previously engaged in text-related tasks now bidding for
programming-related opportunities. Although the per-period job diversity
freelancers apply for tends to be more limited, those who successfully navigate
skill transitions from text to programming demonstrate greater resilience to
ChatGPT's overall market contraction impact. As AI becomes increasingly
versatile and potent, our paper offers crucial insights into AI's influence on
labor markets and individuals' reactions, underscoring the necessity for
proactive interventions to address the challenges and opportunities presented
by this transformative technology.",2023-08-09T19:45:00Z
,http://arxiv.org/pdf/2312.01022v2.pdf,"Advanced Large Language Model (LLM)-Driven Verilog Development:
  Enhancing Power, Performance, and Area Optimization in Code Synthesis","The increasing use of Advanced Language Models (ALMs) in diverse sectors,
particularly due to their impressive capability to generate top-tier content
following linguistic instructions, forms the core of this investigation. This
study probes into ALMs' deployment in electronic hardware design, with a
specific emphasis on the synthesis and enhancement of Verilog programming. We
introduce an innovative framework, crafted to assess and amplify ALMs'
productivity in this niche. The methodology commences with the initial crafting
of Verilog programming via ALMs, succeeded by a distinct dual-stage refinement
protocol. The premier stage prioritizes augmenting the code's operational and
linguistic precision, while the latter stage is dedicated to aligning the code
with Power-Performance-Area (PPA) benchmarks, a pivotal component in proficient
hardware design. This bifurcated strategy, merging error remediation with PPA
enhancement, has yielded substantial upgrades in the caliber of ALM-created
Verilog programming. Our framework achieves an 81.37% rate in linguistic
accuracy and 62.0% in operational efficacy in programming synthesis, surpassing
current leading-edge techniques, such as 73% in linguistic accuracy and 46% in
operational efficacy. These findings illuminate ALMs' aptitude in tackling
complex technical domains and signal a positive shift in the mechanization of
hardware design operations.",2023-12-02T04:14:23Z
,http://arxiv.org/pdf/2304.04309v1.pdf,"Large Language Models for Business Process Management: Opportunities and
  Challenges","Large language models are deep learning models with a large number of
parameters. The models made noticeable progress on a large number of tasks, and
as a consequence allowing them to serve as valuable and versatile tools for a
diverse range of applications. Their capabilities also offer opportunities for
business process management, however, these opportunities have not yet been
systematically investigated. In this paper, we address this research problem by
foregrounding various management tasks of the BPM lifecycle. We investigate six
research directions highlighting problems that need to be addressed when using
large language models, including usage guidelines for practitioners.",2023-04-09T20:32:09Z
10.1145/3643916.3644434,http://arxiv.org/pdf/2402.14182v1.pdf,"Do Machines and Humans Focus on Similar Code? Exploring Explainability
  of Large Language Models in Code Summarization","Recent language models have demonstrated proficiency in summarizing source
code. However, as in many other domains of machine learning, language models of
code lack sufficient explainability. Informally, we lack a formulaic or
intuitive understanding of what and how models learn from code. Explainability
of language models can be partially provided if, as the models learn to produce
higher-quality code summaries, they also align in deeming the same code parts
important as those identified by human programmers. In this paper, we report
negative results from our investigation of explainability of language models in
code summarization through the lens of human comprehension. We measure human
focus on code using eye-tracking metrics such as fixation counts and duration
in code summarization tasks. To approximate language model focus, we employ a
state-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP
(SHapley Additive exPlanations), to identify which code tokens influence that
generation of summaries. Using these settings, we find no statistically
significant relationship between language models' focus and human programmers'
attention. Furthermore, alignment between model and human foci in this setting
does not seem to dictate the quality of the LLM-generated summaries. Our study
highlights an inability to align human focus with SHAP-based model focus
measures. This result calls for future investigation of multiple open questions
for explainable language models for code summarization and software engineering
tasks in general, including the training mechanisms of language models for
code, whether there is an alignment between human and model attention on code,
whether human attention can improve the development of language models, and
what other model focus measures are appropriate for improving explainability.",2024-02-22T00:01:02Z
,http://arxiv.org/pdf/2406.05053v1.pdf,"Hints-In-Browser: Benchmarking Language Models for Programming Feedback
  Generation","Generative AI and large language models hold great promise in enhancing
programming education by generating individualized feedback and hints for
learners. Recent works have primarily focused on improving the quality of
generated feedback to achieve human tutors' quality. While quality is an
important performance criterion, it is not the only criterion to optimize for
real-world educational deployments. In this paper, we benchmark language models
for programming feedback generation across several performance criteria,
including quality, cost, time, and data privacy. The key idea is to leverage
recent advances in the new paradigm of in-browser inference that allow running
these models directly in the browser, thereby providing direct benefits across
cost and data privacy. To boost the feedback quality of small models compatible
with in-browser inference engines, we develop a fine-tuning pipeline based on
GPT-4 generated synthetic data. We showcase the efficacy of fine-tuned
Llama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser
inference engine on three different Python programming datasets. We will
release the full implementation along with a web app and datasets to facilitate
further research on in-browser language models.",2024-06-07T16:22:51Z
10.3390/buildings14010220,http://arxiv.org/pdf/2310.04427v1.pdf,Generative AI in the Construction Industry: Opportunities & Challenges,"In the last decade, despite rapid advancements in artificial intelligence
(AI) transforming many industry practices, construction largely lags in
adoption. Recently, the emergence and rapid adoption of advanced large language
models (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown
great potential and sparked considerable global interest. However, the current
surge lacks a study investigating the opportunities and challenges of
implementing Generative AI (GenAI) in the construction sector, creating a
critical knowledge gap for researchers and practitioners. This underlines the
necessity to explore the prospects and complexities of GenAI integration.
Bridging this gap is fundamental to optimizing GenAI's early-stage adoption
within the construction sector. Given GenAI's unprecedented capabilities to
generate human-like content based on learning from existing content, we reflect
on two guiding questions: What will the future bring for GenAI in the
construction industry? What are the potential opportunities and challenges in
implementing GenAI in the construction industry? This study delves into
reflected perception in literature, analyzes the industry perception using
programming-based word cloud and frequency analysis, and integrates authors'
opinions to answer these questions. This paper recommends a conceptual GenAI
implementation framework, provides practical recommendations, summarizes future
research questions, and builds foundational literature to foster subsequent
research expansion in GenAI within the construction and its allied architecture
& engineering domains.",2023-09-19T18:20:49Z
,http://arxiv.org/pdf/2311.09818v2.pdf,"SUQL: Conversational Search over Structured and Unstructured Data with
  Large Language Models","While most conversational agents are grounded on either free-text or
structured knowledge, many knowledge corpora consist of hybrid sources. This
paper presents the first conversational agent that supports the full generality
of hybrid data access for large knowledge corpora, through a language we
developed called SUQL (Structured and Unstructured Query Language).
Specifically, SUQL extends SQL with free-text primitives (summary and answer),
so information retrieval can be composed with structured data accesses
arbitrarily in a formal, succinct, precise, and interpretable notation. With
SUQL, we propose the first semantic parser, an LLM with in-context learning,
that can handle hybrid data sources.
  Our in-context learning-based approach, when applied to the HybridQA dataset,
comes within 8.9% exact match and 7.1% F1 of the SOTA, which was trained on 62K
data samples. More significantly, unlike previous approaches, our technique is
applicable to large databases and free-text corpora. We introduce a dataset
consisting of crowdsourced questions and conversations on Yelp, a large, real
restaurant knowledge base with structured and unstructured data. We show that
our few-shot conversational agent based on SUQL finds an entity satisfying all
user requirements 90.3% of the time, compared to 63.4% for a baseline based on
linearization.",2023-11-16T11:48:17Z
,http://arxiv.org/pdf/2405.11928v1.pdf,"""Set It Up!"": Functional Object Arrangement with Compositional
  Generative Models","This paper studies the challenge of developing robots capable of
understanding under-specified instructions for creating functional object
arrangements, such as ""set up a dining table for two""; previous arrangement
approaches have focused on much more explicit instructions, such as ""put object
A on the table."" We introduce a framework, SetItUp, for learning to interpret
under-specified instructions. SetItUp takes a small number of training examples
and a human-crafted program sketch to uncover arrangement rules for specific
scene types. By leveraging an intermediate graph-like representation of
abstract spatial relationships among objects, SetItUp decomposes the
arrangement problem into two subproblems: i) learning the arrangement patterns
from limited data and ii) grounding these abstract relationships into object
poses. SetItUp leverages large language models (LLMs) to propose the abstract
spatial relationships among objects in novel scenes as the constraints to be
satisfied; then, it composes a library of diffusion models associated with
these abstract relationships to find object poses that satisfy the constraints.
We validate our framework on a dataset comprising study desks, dining tables,
and coffee tables, with the results showing superior performance in generating
physically plausible, functional, and aesthetically pleasing object
arrangements compared to existing models.",2024-05-20T10:06:33Z
,http://arxiv.org/pdf/2305.06156v2.pdf,"The Vault: A Comprehensive Multilingual Dataset for Advancing Code
  Understanding and Generation","We present The Vault, a dataset of high-quality code-text pairs in multiple
programming languages for training large language models to understand and
generate code. We present methods for thoroughly extracting samples that use
both rule-based and deep learning-based methods to ensure that they contain
high-quality pairs of code and text, resulting in a dataset of 43 million
high-quality code-text pairs. Our extensive evaluations on common coding tasks
including code generation, code search and code summarization show that when
fine-tuning Code Large Language Models on The Vault, such models outperform the
same models trained on other datasets such as CodeSearchNet. We also provide
detailed analyses of our datasets to assess the effects of various programming
languages and docstrings on the performance of such models.",2023-05-09T09:35:03Z
,http://arxiv.org/pdf/2311.03374v1.pdf,"Generative AI for Software Metadata: Overview of the Information
  Retrieval in Software Engineering Track at FIRE 2023","The Information Retrieval in Software Engineering (IRSE) track aims to
develop solutions for automated evaluation of code comments in a machine
learning framework based on human and large language model generated labels. In
this track, there is a binary classification task to classify comments as
useful and not useful. The dataset consists of 9048 code comments and
surrounding code snippet pairs extracted from open source github C based
projects and an additional dataset generated individually by teams using large
language models. Overall 56 experiments have been submitted by 17 teams from
various universities and software companies. The submissions have been
evaluated quantitatively using the F1-Score and qualitatively based on the type
of features developed, the supervised learning model used and their
corresponding hyper-parameters. The labels generated from large language models
increase the bias in the prediction model but lead to less over-fitted results.",2023-10-27T14:13:23Z
,http://arxiv.org/pdf/2304.02016v1.pdf,"The Multimodal And Modular Ai Chef: Complex Recipe Generation From
  Imagery","The AI community has embraced multi-sensory or multi-modal approaches to
advance this generation of AI models to resemble expected intelligent
understanding. Combining language and imagery represents a familiar method for
specific tasks like image captioning or generation from descriptions. This
paper compares these monolithic approaches to a lightweight and specialized
method based on employing image models to label objects, then serially
submitting this resulting object list to a large language model (LLM). This use
of multiple Application Programming Interfaces (APIs) enables better than 95%
mean average precision for correct object lists, which serve as input to the
latest Open AI text generator (GPT-4). To demonstrate the API as a modular
alternative, we solve the problem of a user taking a picture of ingredients
available in a refrigerator, then generating novel recipe cards tailored to
complex constraints on cost, preparation time, dietary restrictions, portion
sizes, and multiple meal plans. The research concludes that monolithic
multimodal models currently lack the coherent memory to maintain context and
format for this task and that until recently, the language models like GPT-2/3
struggled to format similar problems without degenerating into repetitive or
non-sensical combinations of ingredients. For the first time, an AI chef or
cook seems not only possible but offers some enhanced capabilities to augment
human recipe libraries in pragmatic ways. The work generates a 100-page recipe
book featuring the thirty top ingredients using over 2000 refrigerator images
as initializing lists.",2023-03-20T01:57:52Z
,http://arxiv.org/pdf/2306.03872v3.pdf,Deductive Verification of Chain-of-Thought Reasoning,"Large Language Models (LLMs) significantly benefit from Chain-of-Thought
(CoT) prompting in performing various reasoning tasks. While CoT allows models
to produce more comprehensive reasoning processes, its emphasis on intermediate
reasoning steps can inadvertently introduce hallucinations and accumulated
errors, thereby limiting models' ability to solve complex reasoning tasks.
Inspired by how humans engage in careful and meticulous deductive logical
reasoning processes to solve tasks, we seek to enable language models to
perform explicit and rigorous deductive reasoning, and also ensure the
trustworthiness of their reasoning process through self-verification. However,
directly verifying the validity of an entire deductive reasoning process is
challenging, even with advanced models like ChatGPT. In light of this, we
propose to decompose a reasoning verification process into a series of
step-by-step subprocesses, each only receiving their necessary context and
premises. To facilitate this procedure, we propose Natural Program, a natural
language-based deductive reasoning format. Our approach enables models to
generate precise reasoning steps where subsequent steps are more rigorously
grounded on prior steps. It also empowers language models to carry out
reasoning self-verification in a step-by-step manner. By integrating this
verification process into each deductive reasoning stage, we significantly
enhance the rigor and trustfulness of generated reasoning steps. Along this
process, we also improve the answer correctness on complex reasoning tasks.
Code will be released at https://github.com/lz1oceani/verify_cot.",2023-06-06T17:18:56Z
,http://arxiv.org/pdf/2312.08579v2.pdf,Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach,"The automatic identification of planetary feature names in astronomy
publications presents numerous challenges. These features include craters,
defined as roughly circular depressions resulting from impact or volcanic
activity; dorsas, which are elongate raised structures or wrinkle ridges; and
lacus, small irregular patches of dark, smooth material on the Moon, referred
to as ""lake"" (Planetary Names Working Group, n.d.). Many feature names overlap
with places or people's names that they are named after, for example, Syria,
Tempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some
feature names have been used in many contexts, for instance, Apollo, which can
refer to mission, program, sample, astronaut, seismic, seismometers, core, era,
data, collection, instrument, and station, in addition to the crater on the
Moon. Some feature names can appear in the text as adjectives, like the lunar
craters Black, Green, and White. Some feature names in other contexts serve as
directions, like craters West and South on the Moon. Additionally, some
features share identical names across different celestial bodies, requiring
disambiguation, such as the Adams crater, which exists on both the Moon and
Mars. We present a multi-step pipeline combining rule-based filtering,
statistical relevance analysis, part-of-speech (POS) tagging, named entity
recognition (NER) model, hybrid keyword harvesting, knowledge graph (KG)
matching, and inference with a locally installed large language model (LLM) to
reliably identify planetary names despite these challenges. When evaluated on a
dataset of astronomy papers from the Astrophysics Data System (ADS), this
methodology achieves an F1-score over 0.97 in disambiguating planetary feature
names.",2023-12-14T00:50:14Z
,http://arxiv.org/pdf/2402.13212v2.pdf,Soft Self-Consistency Improves Language Model Agents,"Generations from large language models (LLMs) can be improved by sampling and
scoring multiple solutions to select a final answer. Current ""sample and
select"" methods such as self-consistency (SC) rely on majority voting to score
answers. However, when tasks have many distinct and valid answers, selection by
voting requires a large number of samples. This makes SC prohibitively
expensive for interactive tasks that involve generating multiple actions
(answers) sequentially. After establishing that majority voting fails to
provide consistent gains on such tasks, we demonstrate how to increase success
rates by softening the scoring criterion. We introduce Soft Self-Consistency
(SOFT-SC), which replaces SC's discontinuous scoring with a continuous score
computed from model likelihoods, allowing for selection even when actions are
sparsely distributed. SOFT-SC improves both performance and efficiency on
long-horizon interactive tasks, requiring half as many samples as SC for
comparable or better performance. For a fixed number of samples, SOFT-SC leads
to a 1.3% increase over SC in absolute success rate on writing bash programs, a
6.6% increase on online shopping (WebShop), and a 4.7% increase for an
interactive household game (ALFWorld). Finally, we show that SOFT-SC can be
applied to both open-source and black-box models.",2024-02-20T18:22:38Z
,http://arxiv.org/pdf/2405.03076v1.pdf,"Traffic Performance GPT (TP-GPT): Real-Time Data Informed Intelligent
  ChatBot for Transportation Surveillance and Management","The digitization of traffic sensing infrastructure has significantly
accumulated an extensive traffic data warehouse, which presents unprecedented
challenges for transportation analytics. The complexities associated with
querying large-scale multi-table databases require specialized programming
expertise and labor-intensive development. Additionally, traditional analysis
methods have focused mainly on numerical data, often neglecting the semantic
aspects that could enhance interpretability and understanding. Furthermore,
real-time traffic data access is typically limited due to privacy concerns. To
bridge this gap, the integration of Large Language Models (LLMs) into the
domain of traffic management presents a transformative approach to addressing
the complexities and challenges inherent in modern transportation systems. This
paper proposes an intelligent online chatbot, TP-GPT, for efficient customized
transportation surveillance and management empowered by a large real-time
traffic database. The innovative framework leverages contextual and generative
intelligence of language models to generate accurate SQL queries and natural
language interpretations by employing transportation-specialized prompts,
Chain-of-Thought prompting, few-shot learning, multi-agent collaboration
strategy, and chat memory. Experimental study demonstrates that our approach
outperforms state-of-the-art baselines such as GPT-4 and PaLM 2 on a
challenging traffic-analysis benchmark TransQuery. TP-GPT would aid researchers
and practitioners in real-time transportation surveillance and management in a
privacy-preserving, equitable, and customizable manner.",2024-05-05T22:54:51Z
,http://arxiv.org/pdf/2306.03268v2.pdf,"""Medium"" LMs of Code in the Era of LLMs: Lessons From StackOverflow","Large pre-trained neural language models have brought immense progress to
both NLP and software engineering. Models in OpenAI's GPT series now dwarf
Google's BERT and Meta's RoBERTa, which previously set new benchmarks on a wide
range of NLP applications. These models are trained on massive corpora of
heterogeneous data from web crawls, which enables them to learn general
language patterns and semantic relationships. However, the largest models are
both expensive to train and deploy and are often closed-source, so we lack
access to their data and design decisions. We argue that this trend towards
large, general-purpose models should be complemented with single-purpose, more
modestly sized pre-trained models. In this work, we take StackOverflow (SO) as
a domain example in which large volumes of rich aligned code and text data is
available. We adopt standard practices for pre-training large language models,
including using a very large context size (2,048 tokens), batch size (0.5M
tokens) and training set (27B tokens), coupled with a powerful toolkit
(Megatron-LM), to train two models: SOBertBase, with 109M parameters, and
SOBertLarge with 762M parameters, at a budget of just $\$187$ and $\$800$ each.
We compare the performance of our models with both the previous SOTA model
trained on SO data exclusively as well general-purpose BERT models and OpenAI's
ChatGPT on four SO-specific downstream tasks - question quality prediction,
closed question prediction, named entity recognition and obsoletion prediction
(a new task we introduce). Not only do our models consistently outperform all
baselines, the smaller model is often sufficient for strong results. Both
models are released to the public. These results demonstrate that pre-training
both extensively and properly on in-domain data can yield a powerful and
affordable alternative to leveraging closed-source general-purpose models.",2023-06-05T21:38:30Z
,http://arxiv.org/pdf/2405.06681v1.pdf,"Leveraging Lecture Content for Improved Feedback: Explorations with
  GPT-4 and Retrieval Augmented Generation","This paper presents the use of Retrieval Augmented Generation (RAG) to
improve the feedback generated by Large Language Models for programming tasks.
For this purpose, corresponding lecture recordings were transcribed and made
available to the Large Language Model GPT-4 as external knowledge source
together with timestamps as metainformation by using RAG. The purpose of this
is to prevent hallucinations and to enforce the use of the technical terms and
phrases from the lecture. In an exercise platform developed to solve
programming problems for an introductory programming lecture, students can
request feedback on their solutions generated by GPT-4. For this task GPT-4
receives the students' code solution, the compiler output, the result of unit
tests and the relevant passages from the lecture notes available through the
use of RAG as additional context. The feedback generated by GPT-4 should guide
students to solve problems independently and link to the lecture content, using
the time stamps of the transcript as meta-information. In this way, the
corresponding lecture videos can be viewed immediately at the corresponding
positions. For the evaluation, students worked with the tool in a workshop and
decided for each feedback whether it should be extended by RAG or not. First
results based on a questionnaire and the collected usage data show that the use
of RAG can improve feedback generation and is preferred by students in some
situations. Due to the slower speed of feedback generation, the benefits are
situation dependent.",2024-05-05T18:32:06Z
,http://arxiv.org/pdf/2312.09731v1.pdf,"Uncovering the Causes of Emotions in Software Developer Communication
  Using Zero-shot LLMs","Understanding and identifying the causes behind developers' emotions (e.g.,
Frustration caused by `delays in merging pull requests') can be crucial towards
finding solutions to problems and fostering collaboration in open-source
communities. Effectively identifying such information in the high volume of
communications across the different project channels, such as chats, emails,
and issue comments, requires automated recognition of emotions and their
causes. To enable this automation, large-scale software engineering-specific
datasets that can be used to train accurate machine learning models are
required. However, such datasets are expensive to create with the variety and
informal nature of software projects' communication channels.
  In this paper, we explore zero-shot LLMs that are pre-trained on massive
datasets but without being fine-tuned specifically for the task of detecting
emotion causes in software engineering: ChatGPT, GPT-4, and flan-alpaca. Our
evaluation indicates that these recently available models can identify emotion
categories when given detailed emotions, although they perform worse than the
top-rated models. For emotion cause identification, our results indicate that
zero-shot LLMs are effective at recognizing the correct emotion cause with a
BLEU-2 score of 0.598. To highlight the potential use of these techniques, we
conduct a case study of the causes of Frustration in the last year of
development of a popular open-source project, revealing several interesting
insights.",2023-12-15T12:16:16Z
,http://arxiv.org/pdf/2311.07377v3.pdf,"Testing learning-enabled cyber-physical systems with Large-Language
  Models: A Formal Approach","The integration of machine learning (ML) into cyber-physical systems (CPS)
offers significant benefits, including enhanced efficiency, predictive
capabilities, real-time responsiveness, and the enabling of autonomous
operations. This convergence has accelerated the development and deployment of
a range of real-world applications, such as autonomous vehicles, delivery
drones, service robots, and telemedicine procedures. However, the software
development life cycle (SDLC) for AI-infused CPS diverges significantly from
traditional approaches, featuring data and learning as two critical components.
Existing verification and validation techniques are often inadequate for these
new paradigms. In this study, we pinpoint the main challenges in ensuring
formal safety for learningenabled CPS.We begin by examining testing as the most
pragmatic method for verification and validation, summarizing the current
state-of-the-art methodologies. Recognizing the limitations in current testing
approaches to provide formal safety guarantees, we propose a roadmap to
transition from foundational probabilistic testing to a more rigorous approach
capable of delivering formal assurance.",2023-11-13T14:56:14Z
,http://arxiv.org/pdf/2402.03507v1.pdf,"Neural networks for abstraction and reasoning: Towards broad
  generalization in machines","For half a century, artificial intelligence research has attempted to
reproduce the human qualities of abstraction and reasoning - creating computer
systems that can learn new concepts from a minimal set of examples, in settings
where humans find this easy. While specific neural networks are able to solve
an impressive range of problems, broad generalisation to situations outside
their training data has proved elusive.In this work, we look at several novel
approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of
abstract visual reasoning tasks introduced to test algorithms on broad
generalization. Despite three international competitions with $100,000 in
prizes, the best algorithms still fail to solve a majority of ARC tasks and
rely on complex hand-crafted rules, without using machine learning at all. We
revisit whether recent advances in neural networks allow progress on this task.
  First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC.
DreamCoder automatically writes programs in a bespoke domain-specific language
to perform reasoning, using a neural network to mimic human intuition. We
present the Perceptual Abstraction and Reasoning Language (PeARL) language,
which allows DreamCoder to solve ARC tasks, and propose a new recognition model
that allows us to significantly improve on the previous best implementation.We
also propose a new encoding and augmentation scheme that allows large language
models (LLMs) to solve ARC tasks, and find that the largest models can solve
some ARC tasks. LLMs are able to solve a different group of problems to
state-of-the-art solvers, and provide an interesting way to complement other
approaches. We perform an ensemble analysis, combining models to achieve better
results than any system alone. Finally, we publish the arckit Python library to
make future research on ARC easier.",2024-02-05T20:48:57Z
,http://arxiv.org/pdf/2303.08033v1.pdf,"Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions
  about Code","We analyzed effectiveness of three generative pre-trained transformer (GPT)
models in answering multiple-choice question (MCQ) assessments, often involving
short snippets of code, from introductory and intermediate programming courses
at the postsecondary level. This emerging technology stirs countless
discussions of its potential uses (e.g., exercise generation, code explanation)
as well as misuses in programming education (e.g., cheating). However, the
capabilities of GPT models and their limitations to reason about and/or analyze
code in educational settings have been under-explored. We evaluated several
OpenAI's GPT models on formative and summative MCQ assessments from three
Python courses (530 questions). We found that MCQs containing code snippets are
not answered as successfully as those that only contain natural language. While
questions requiring to fill-in a blank in the code or completing a natural
language statement about the snippet are handled rather successfully, MCQs that
require analysis and/or reasoning about the code (e.g., what is true/false
about the snippet, or what is its output) appear to be the most challenging.
These findings can be leveraged by educators to adapt their instructional
practices and assessments in programming courses, so that GPT becomes a
valuable assistant for a learner as opposed to a source of confusion and/or
potential hindrance in the learning process.",2023-03-09T16:52:12Z
,http://arxiv.org/pdf/2402.08957v3.pdf,MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data,"Recent large language models (LLMs) have witnessed significant advancement in
various tasks, including mathematical reasoning and theorem proving. As these
two tasks require strict and formal multi-step inference, they are appealing
domains for exploring the reasoning ability of LLMs but still face important
challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the
effectiveness of intermediate steps guidance. However, such step-wise
annotation requires heavy labor, leading to insufficient training steps for
current benchmarks. To fill this gap, this work introduces MUSTARD, a data
generation framework that masters uniform synthesis of theorem and proof data
of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It
samples a few mathematical concept seeds as the problem category. (2) Then, it
prompts a generative language model with the sampled concepts to obtain both
the problems and their step-wise formal solutions. (3) Lastly, the framework
utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With
the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE
with 5,866 valid data points. Each data point contains an informal statement,
an informal proof, and a translated formal proof that passes the prover
validation. We perform extensive analysis and demonstrate that MUSTARD
generates validated high-quality step-by-step data. We further apply the
MUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B
achieves a 15.41% average relative performance gain in automated theorem
proving, and 8.18% in math word problems. Codes and data are available at
https://github.com/Eleanor-H/MUSTARD.",2024-02-14T05:57:58Z
,http://arxiv.org/pdf/2405.04325v1.pdf,"Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat
  Trick in Legislation","Recent developments in large language models (LLMs), while offering a
powerful foundation for developing natural language agents, raise safety
concerns about them and the autonomous agents built upon them. Deception is one
potential capability of AI agents of particular concern, which we refer to as
an act or statement that misleads, hides the truth, or promotes a belief that
is not true in its entirety or in part. We move away from the conventional
understanding of deception through straight-out lying, making objective selfish
decisions, or giving false information, as seen in previous AI safety research.
We target a specific category of deception achieved through obfuscation and
equivocation. We broadly explain the two types of deception by analogizing them
with the rabbit-out-of-hat magic trick, where (i) the rabbit either comes out
of a hidden trap door or (ii) (our focus) the audience is completely distracted
to see the magician bring out the rabbit right in front of them using sleight
of hand or misdirection. Our novel testbed framework displays intrinsic
deception capabilities of LLM agents in a goal-driven environment when directed
to be deceptive in their natural language generations in a two-agent
adversarial dialogue system built upon the legislative task of ""lobbying"" for a
bill. Along the lines of a goal-driven environment, we show developing
deceptive capacity through a reinforcement learning setup, building it around
the theories of language philosophy and cognitive psychology. We find that the
lobbyist agent increases its deceptive capabilities by ~ 40% (relative) through
subsequent reinforcement trials of adversarial interactions, and our deception
detection mechanism shows a detection capability of up to 92%. Our results
highlight potential issues in agent-human interaction, with agents potentially
manipulating humans towards its programmed end-goal.",2024-05-07T13:55:11Z
,http://arxiv.org/pdf/2305.02198v1.pdf,Experiences with Remote Examination Formats in Light of GPT-4,"Sudden access to the rapidly improving large language model GPT by open-ai
forces educational institutions worldwide to revisit their exam procedures. In
the pre-GPT era, we successfully applied oral and open-book home exams for two
courses in the third year of our predominantly remote Software Engineering BSc
program. We ask in this paper whether our current open-book exams are still
viable or whether a move back to a legally compliant but less scalable oral
exam is the only workable alternative. We further compare work-effort estimates
between oral and open-book exams and report on differences in throughput and
grade distribution over eight years to better understand the impact of
examination format on the outcome. Examining GPT v4 on the most recent
open-book exams showed that our current Artificial Intelligence and Reactive
Programming exams are not GPT v4 proof. Three potential weaknesses of GPT are
outlined. We also found that grade distributions have largely been unaffected
by the examination format, opening up for a move to oral examinations only if
needed. Throughput was higher for open-book exam course instances (73% vs 64%),
while fail rates were too (12% vs 7%), with teacher workload increasing even
for smaller classes. We also report on our experience regarding effort. Oral
examinations are efficient for smaller groups but come with caveats regarding
intensity and stress.",2023-03-27T19:49:06Z
,http://arxiv.org/pdf/2303.00732v2.pdf,"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility
  Across Random User Intents","Large language models show impressive results at predicting structured text
such as code, but also commonly introduce errors and hallucinations in their
output. When used to assist software developers, these models may make mistakes
that users must go back and fix, or worse, introduce subtle bugs that users may
miss entirely. We propose Randomized Utility-driven Synthesis of Uncertain
REgions (R-U-SURE), an approach for building uncertainty-aware suggestions
based on a decision-theoretic model of goal-conditioned utility, using random
samples from a generative model as a proxy for the unobserved possible intents
of the end user. Our technique combines minimum-Bayes-risk decoding, dual
decomposition, and decision diagrams in order to efficiently produce structured
uncertainty summaries, given only sample access to an arbitrary generative
model of code and an optional AST parser. We demonstrate R-U-SURE on three
developer-assistance tasks, and show that it can be applied different user
interaction patterns without retraining the model and leads to more accurate
uncertainty estimates than token-probability baselines. We also release our
implementation as an open-source library at
https://github.com/google-research/r_u_sure.",2023-03-01T18:46:40Z
,http://arxiv.org/pdf/2406.03839v1.pdf,PCART: Automated Repair of Python API Parameter Compatibility Issues,"In modern software development, Python third-party libraries have become
crucial, particularly due to their widespread use in fields such as deep
learning and scientific computing. However, the parameters of APIs in
third-party libraries often change during evolution, causing compatibility
issues for client applications that depend on specific versions. Due to
Python's flexible parameter-passing mechanism, different methods of parameter
passing can result in different API compatibility. Currently, no tool is
capable of automatically detecting and repairing Python API parameter
compatibility issues. To fill this gap, we propose PCART, the first to
implement a fully automated process from API extraction, code instrumentation,
and API mapping establishment, to compatibility assessment, and finally to
repair and validation, for solving various types of Python API parameter
compatibility issues, i.e., parameter addition, removal, renaming, reordering
of parameters, as well as the conversion of positional parameters to keyword
parameters. We construct a large-scale benchmark PCBENCH, including 47,478 test
cases mutated from 844 parameter-changed APIs of 33 popular Python libraries,
to evaluate PCART. The evaluation results show that PCART is effective yet
efficient, significantly outperforming existing tools (MLCatchUp and Relancer)
and the large language model ChatGPT-4, achieving an F-measure of 96.49% in
detecting API parameter compatibility issues and a repair accuracy of 91.36%.
The evaluation on 14 real-world Python projects from GitHub further
demonstrates that PCART has good practicality. We believe PCART can help
programmers reduce the time spent on maintaining Python API updates and
facilitate automated Python API compatibility issue repair.",2024-06-06T08:15:12Z
,http://arxiv.org/pdf/2402.16929v1.pdf,"LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs
  from the Programming Language","LLMs have demonstrated commendable performance across diverse domains.
Nevertheless, formulating high-quality prompts to effectively instruct LLMs
poses a challenge for non-AI experts. Existing research in prompt engineering
suggests somewhat fragmented optimization principles and designs empirically
dependent prompt optimizers. Unfortunately, these endeavors lack a structured
design template, incurring high learning costs and resulting in low
reusability. Inspired by structured reusable programming languages, we propose
LangGPT, a dual-layer prompt design framework as the programming language for
LLMs. LangGPT has an easy-to-learn normative structure and provides an extended
structure for migration and reuse. Experiments illustrate that LangGPT
significantly enhances the capacity of LLMs to produce responses of superior
quality compared to baselines. Moreover, LangGPT has proven effective in
guiding LLMs to generate high-quality prompts. We have built a community on
LangGPT to facilitate the tuition and sharing of prompt design. We also
analyzed the ease of use and reusability of LangGPT through a community user
survey.",2024-02-26T15:05:16Z
,http://arxiv.org/pdf/2211.12787v1.pdf,Program Repair,"Automated program repair is an emerging technology which consists of a suite
of techniques to automatically fix bugs or vulnerabilities in programs. In this
paper, we present a comprehensive survey of the state of the art in program
repair. We first study the different suite of techniques used including search
based repair, constraint based repair and learning based repair. We then
discuss one of the main challenges in program repair namely patch overfitting,
by distilling a class of techniques which can alleviate patch overfitting. We
then discuss classes of program repair tools, applications of program repair as
well as uses of program repair in industry. We conclude the survey with a
forward looking outlook on future usages of program repair, as well as research
opportunities arising from work on code from large language models.",2022-11-23T09:04:45Z
,http://arxiv.org/pdf/2403.03218v7.pdf,The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning,"The White House Executive Order on Artificial Intelligence highlights the
risks of large language models (LLMs) empowering malicious actors in developing
biological, cyber, and chemical weapons. To measure these risks of malicious
use, government institutions and major AI labs are developing evaluations for
hazardous capabilities in LLMs. However, current evaluations are private,
preventing further research into mitigating risk. Furthermore, they focus on
only a few, highly specific pathways for malicious use. To fill these gaps, we
publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a
dataset of 3,668 multiple-choice questions that serve as a proxy measurement of
hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP
was developed by a consortium of academics and technical consultants, and was
stringently filtered to eliminate sensitive information prior to public
release. WMDP serves two roles: first, as an evaluation for hazardous knowledge
in LLMs, and second, as a benchmark for unlearning methods to remove such
hazardous knowledge. To guide progress on unlearning, we develop RMU, a
state-of-the-art unlearning method based on controlling model representations.
RMU reduces model performance on WMDP while maintaining general capabilities in
areas such as biology and computer science, suggesting that unlearning may be a
concrete path towards reducing malicious use from LLMs. We release our
benchmark and code publicly at https://wmdp.ai",2024-03-05T18:59:35Z
,http://arxiv.org/pdf/2210.00848v2.pdf,Toward Trustworthy Neural Program Synthesis,"We develop an approach to estimate the probability that a program sampled
from a large language model is correct. Given a natural language description of
a programming problem, our method samples both candidate programs as well as
candidate predicates specifying how the program should behave. This allows
learning a model that forms a well-calibrated probabilistic prediction of
program correctness. Our system also infers which predicates are useful to
explain the behavior of the generated code, and humans preferred these in a
human study over raw language model outputs. Our method is simple, easy to
implement, and maintains state of the art generation accuracy results.",2022-09-29T20:32:07Z
10.1109/ICSTW58534.2023.00078,http://arxiv.org/pdf/2302.03287v3.pdf,ChatGPT and Software Testing Education: Promises & Perils,"Over the past decade, predictive language modeling for code has proven to be
a valuable tool for enabling new forms of automation for developers. More
recently, we have seen the advent of general purpose ""large language models"",
based on neural transformer architectures, that have been trained on massive
datasets of human written text spanning code and natural language. However,
despite the demonstrated representational power of such models, interacting
with them has historically been constrained to specific task settings, limiting
their general applicability. Many of these limitations were recently overcome
with the introduction of ChatGPT, a language model created by OpenAI and
trained to operate as a conversational agent, enabling it to answer questions
and respond to a wide variety of commands from end users. The introduction of
models, such as ChatGPT, has already spurred fervent discussion from educators,
ranging from fear that students could use these AI tools to circumvent
learning, to excitement about the new types of learning opportunities that they
might unlock. However, given the nascent nature of these tools, we currently
lack fundamental knowledge related to how well they perform in different
educational settings, and the potential promise (or danger) that they might
pose to traditional forms of instruction. As such, in this paper, we examine
how well ChatGPT performs when tasked with answering common questions in a
popular software testing curriculum. Our findings indicate that ChatGPT can
provide correct or partially correct answers in 55.6% of cases, provide correct
or partially correct explanations of answers in 53.0% of cases, and that
prompting the tool in a shared question context leads to a marginally higher
rate of correct responses. Based on these findings, we discuss the potential
promises and perils related to the use of ChatGPT by students and instructors.",2023-02-07T06:41:02Z
,http://arxiv.org/pdf/2308.08061v1.pdf,"The Costly Dilemma: Generalization, Evaluation and Cost-Optimal
  Deployment of Large Language Models","When deploying machine learning models in production for any
product/application, there are three properties that are commonly desired.
First, the models should be generalizable, in that we can extend it to further
use cases as our knowledge of the domain area develops. Second they should be
evaluable, so that there are clear metrics for performance and the calculation
of those metrics in production settings are feasible. Finally, the deployment
should be cost-optimal as far as possible. In this paper we propose that these
three objectives (i.e. generalization, evaluation and cost-optimality) can
often be relatively orthogonal and that for large language models, despite
their performance over conventional NLP models, enterprises need to carefully
assess all the three factors before making substantial investments in this
technology. We propose a framework for generalization, evaluation and
cost-modeling specifically tailored to large language models, offering insights
into the intricacies of development, deployment and management for these large
language models.",2023-08-15T22:26:58Z
,http://arxiv.org/pdf/2402.03349v1.pdf,"When Geoscience Meets Generative AI and Large Language Models:
  Foundations, Trends, and Future Challenges","Generative Artificial Intelligence (GAI) represents an emerging field that
promises the creation of synthetic data and outputs in different modalities.
GAI has recently shown impressive results across a large spectrum of
applications ranging from biology, medicine, education, legislation, computer
science, and finance. As one strives for enhanced safety, efficiency, and
sustainability, generative AI indeed emerges as a key differentiator and
promises a paradigm shift in the field. This paper explores the potential
applications of generative AI and large language models in geoscience. The
recent developments in the field of machine learning and deep learning have
enabled the generative model's utility for tackling diverse prediction
problems, simulation, and multi-criteria decision-making challenges related to
geoscience and Earth system dynamics. This survey discusses several GAI models
that have been used in geoscience comprising generative adversarial networks
(GANs), physics-informed neural networks (PINNs), and generative pre-trained
transformer (GPT)-based structures. These tools have helped the geoscience
community in several applications, including (but not limited to) data
generation/augmentation, super-resolution, panchromatic sharpening, haze
removal, restoration, and land surface changing. Some challenges still remain
such as ensuring physical interpretation, nefarious use cases, and
trustworthiness. Beyond that, GAI models show promises to the geoscience
community, especially with the support to climate change, urban science,
atmospheric science, marine science, and planetary science through their
extraordinary ability to data-driven modeling and uncertainty quantification.",2024-01-25T12:03:50Z
,http://arxiv.org/pdf/2301.13246v1.pdf,Conversational Automated Program Repair,"Automated Program Repair (APR) can help developers automatically generate
patches for bugs. Due to the impressive performance obtained using Large
Pre-Trained Language Models (LLMs) on many code related tasks, researchers have
started to directly use LLMs for APR. However, prior approaches simply
repeatedly sample the LLM given the same constructed input/prompt created from
the original buggy code, which not only leads to generating the same incorrect
patches repeatedly but also miss the critical information in testcases. To
address these limitations, we propose conversational APR, a new paradigm for
program repair that alternates between patch generation and validation in a
conversational manner. In conversational APR, we iteratively build the input to
the model by combining previously generated patches with validation feedback.
As such, we leverage the long-term context window of LLMs to not only avoid
generating previously incorrect patches but also incorporate validation
feedback to help the model understand the semantic meaning of the program under
test. We evaluate 10 different LLM including the newly developed ChatGPT model
to demonstrate the improvement of conversational APR over the prior LLM for APR
approach.",2023-01-30T19:22:36Z
,http://arxiv.org/pdf/2402.14594v1.pdf,"Improving Assessment of Tutoring Practices using Retrieval-Augmented
  Generation","One-on-one tutoring is an effective instructional method for enhancing
learning, yet its efficacy hinges on tutor competencies. Novice math tutors
often prioritize content-specific guidance, neglecting aspects such as
social-emotional learning. Social-emotional learning promotes equity and
inclusion and nurturing relationships with students, which is crucial for
holistic student development. Assessing the competencies of tutors accurately
and efficiently can drive the development of tailored tutor training programs.
However, evaluating novice tutor ability during real-time tutoring remains
challenging as it typically requires experts-in-the-loop. To address this
challenge, this preliminary study aims to harness Generative Pre-trained
Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess
tutors' ability of using social-emotional tutoring strategies. Moreover, this
study also reports on the financial dimensions and considerations of employing
these models in real-time and at scale for automated assessment. The current
study examined four prompting strategies: two basic Zero-shot prompt
strategies, Tree of Thought prompt, and Retrieval-Augmented Generator (RAG)
based prompt. The results indicate that the RAG prompt demonstrated more
accurate performance (assessed by the level of hallucination and correctness in
the generated assessment texts) and lower financial costs than the other
strategies evaluated. These findings inform the development of personalized
tutor training interventions to enhance the the educational effectiveness of
tutored learning.",2024-02-04T20:42:30Z
,http://arxiv.org/pdf/2310.11467v1.pdf,"Enhancing Binary Code Comment Quality Classification: Integrating
  Generative AI for Improved Accuracy","This report focuses on enhancing a binary code comment quality classification
model by integrating generated code and comment pairs, to improve model
accuracy. The dataset comprises 9048 pairs of code and comments written in the
C programming language, each annotated as ""Useful"" or ""Not Useful.""
Additionally, code and comment pairs are generated using a Large Language Model
Architecture, and these generated pairs are labeled to indicate their utility.
The outcome of this effort consists of two classification models: one utilizing
the original dataset and another incorporating the augmented dataset with the
newly generated code comment pairs and labels.",2023-10-14T18:19:06Z
,http://arxiv.org/pdf/2308.16824v2.pdf,Can Programming Languages Boost Each Other via Instruction Tuning?,"When human programmers have mastered a programming language, it would be
easier when they learn a new programming language. In this report, we focus on
exploring whether programming languages can boost each other during the
instruction fine-tuning phase of code large language models. We conduct
extensive experiments of 8 popular programming languages (Python, JavaScript,
TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that
programming languages can significantly improve each other. For example,
CodeM-Python 15B trained on Python is able to increase Java by an absolute
17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B
trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our
training data is released at https://github.com/NL2Code/CodeM.",2023-08-31T15:53:51Z
,http://arxiv.org/pdf/2305.10838v2.pdf,"ProgSG: Cross-Modality Representation Learning for Programs in
  Electronic Design Automation","Recent years have witnessed the growing popularity of domain-specific
accelerators (DSAs), such as Google's TPUs, for accelerating various
applications such as deep learning, search, autonomous driving, etc. To
facilitate DSA designs, high-level synthesis (HLS) is used, which allows a
developer to compile a high-level description in the form of software code in C
and C++ into a design in low-level hardware description languages (such as VHDL
or Verilog) and eventually synthesized into a DSA on an ASIC
(application-specific integrated circuit) or FPGA (field-programmable gate
arrays). However, existing HLS tools still require microarchitecture decisions,
expressed in terms of pragmas (such as directives for parallelization and
pipelining). To enable more people to design DSAs, it is desirable to automate
such decisions with the help of deep learning for predicting the quality of HLS
designs. This requires us a deeper understanding of the program, which is a
combination of original code and pragmas. Naturally, these programs can be
considered as sequence data, for which large language models (LLM) can help. In
addition, these programs can be compiled and converted into a control data flow
graph (CDFG), and the compiler also provides fine-grained alignment between the
code tokens and the CDFG nodes. However, existing works either fail to leverage
both modalities or combine the two in shallow or coarse ways. We propose ProgSG
allowing the source code sequence modality and the graph modalities to interact
with each other in a deep and fine-grained way. To alleviate the scarcity of
labeled designs, a pre-training method is proposed based on a suite of
compiler's data flow analysis tasks. Experimental results on two benchmark
datasets show the superiority of ProgSG over baseline methods that either only
consider one modality or combine the two without utilizing the alignment
information.",2023-05-18T09:44:18Z
,http://arxiv.org/pdf/2102.02503v1.pdf,"Understanding the Capabilities, Limitations, and Societal Impact of
  Large Language Models","On October 14th, 2020, researchers from OpenAI, the Stanford Institute for
Human-Centered Artificial Intelligence, and other universities convened to
discuss open research questions surrounding GPT-3, the largest
publicly-disclosed dense language model at the time. The meeting took place
under Chatham House Rules. Discussants came from a variety of research
backgrounds including computer science, linguistics, philosophy, political
science, communications, cyber policy, and more. Broadly, the discussion
centered around two main questions: 1) What are the technical capabilities and
limitations of large language models? 2) What are the societal effects of
widespread use of large language models? Here, we provide a detailed summary of
the discussion organized by the two themes above.",2021-02-04T09:27:04Z
,http://arxiv.org/pdf/2402.05980v2.pdf,"Do Large Code Models Understand Programming Concepts? A Black-box
  Approach","Large Language Models' success on text generation has also made them better
at code generation and coding tasks. While a lot of work has demonstrated their
remarkable performance on tasks such as code completion and editing, it is
still unclear as to why. We help bridge this gap by exploring to what degree
auto-regressive models understand the logical constructs of the underlying
programs. We propose Counterfactual Analysis for Programming Concept Predicates
(CACP) as a counterfactual testing framework to evaluate whether Large Code
Models understand programming concepts. With only black-box access to the
model, we use CACP to evaluate ten popular Large Code Models for four different
programming concepts. Our findings suggest that current models lack
understanding of concepts such as data flow and control flow.",2024-02-08T06:48:01Z
,http://arxiv.org/pdf/2310.07984v1.pdf,"Large Language Models for Scientific Synthesis, Inference and
  Explanation","Large language models are a form of artificial intelligence systems whose
primary knowledge consists of the statistical patterns, semantic relationships,
and syntactical structures of language1. Despite their limited forms of
""knowledge"", these systems are adept at numerous complex tasks including
creative writing, storytelling, translation, question-answering, summarization,
and computer code generation. However, they have yet to demonstrate advanced
applications in natural science. Here we show how large language models can
perform scientific synthesis, inference, and explanation. We present a method
for using general-purpose large language models to make inferences from
scientific datasets of the form usually associated with special-purpose machine
learning algorithms. We show that the large language model can augment this
""knowledge"" by synthesizing from the scientific literature. When a conventional
machine learning system is augmented with this synthesized and inferred
knowledge it can outperform the current state of the art across a range of
benchmark tasks for predicting molecular properties. This approach has the
further advantage that the large language model can explain the machine
learning system's predictions. We anticipate that our framework will open new
avenues for AI to accelerate the pace of scientific discovery.",2023-10-12T02:17:59Z
,http://arxiv.org/pdf/2301.03797v2.pdf,"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using
  Large Language Models","Incident management for cloud services is a complex process involving several
steps and has a huge impact on both service health and developer productivity.
On-call engineers require significant amount of domain knowledge and manual
effort for root causing and mitigation of production incidents. Recent advances
in artificial intelligence has resulted in state-of-the-art large language
models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a
variety of problems ranging from question answering to text summarization. In
this work, we do the first large-scale study to evaluate the effectiveness of
these models for helping engineers root cause and mitigate production
incidents. We do a rigorous study at Microsoft, on more than 40,000 incidents
and compare several large language models in zero-shot, fine-tuned and
multi-task setting using semantic and lexical metrics. Lastly, our human
evaluation with actual incident owners show the efficacy and future potential
of using artificial intelligence for resolving cloud incidents.",2023-01-10T05:41:40Z
,http://arxiv.org/pdf/2308.00683v1.pdf,"CodeBPE: Investigating Subtokenization Options for Large Language Model
  Pretraining on Source Code","Recent works have widely adopted large language model pretraining for source
code, suggested source code-specific pretraining objectives and investigated
the applicability of various Transformer-based language model architectures for
source code. This work investigates another important aspect of such models,
namely the effect of different subtokenization options, and aims at identifying
most effective and length-efficient subtokenizations, taking into account code
specifics. We propose subtokenziation that reduces average length by 17%
without downstream performance drop, and show that a carefully chosen
subtokenization may improve quality by 0.5-2%, possibly with some length
increase.",2023-08-01T17:40:48Z
,http://arxiv.org/pdf/2311.03489v4.pdf,"Leveraging High-Level Synthesis and Large Language Models to Generate,
  Simulate, and Deploy a Uniform Random Number Generator Hardware Design","We present a new high-level synthesis methodology for using large language
model tools to generate hardware designs. The methodology uses exclusively
open-source tools excluding the large language model. As a case study, we use
our methodology to generate a permuted congruential random number generator
design with a wishbone interface. We verify the functionality and quality of
the random number generator design using large language model-generated
simulations and the Dieharder randomness test suite. We document all the large
language model chat logs, Python scripts, Verilog scripts, and simulation
results used in the case study. We believe that our method of hardware design
generation coupled with the open source silicon 130 nm design tools will
revolutionize application-specific integrated circuit design. Our methodology
significantly lowers the bar to entry when building domain-specific computing
accelerators for the Internet of Things and proof of concept prototypes for
later fabrication in more modern process nodes.",2023-11-06T19:58:26Z
,http://arxiv.org/pdf/2403.15230v1.pdf,"An Exploratory Investigation into Code License Infringements in Large
  Language Model Training Datasets","Does the training of large language models potentially infringe upon code
licenses? Furthermore, are there any datasets available that can be safely used
for training these models without violating such licenses? In our study, we
assess the current trends in the field and the importance of incorporating code
into the training of large language models. Additionally, we examine publicly
available datasets to see whether these models can be trained on them without
the risk of legal issues in the future. To accomplish this, we compiled a list
of 53 large language models trained on file-level code. We then extracted their
datasets and analyzed how much they overlap with a dataset we created,
consisting exclusively of strong copyleft code.
  Our analysis revealed that every dataset we examined contained license
inconsistencies, despite being selected based on their associated repository
licenses. We analyzed a total of 514 million code files, discovering 38 million
exact duplicates present in our strong copyleft dataset. Additionally, we
examined 171 million file-leading comments, identifying 16 million with strong
copyleft licenses and another 11 million comments that discouraged copying
without explicitly mentioning a license. Based on the findings of our study,
which highlights the pervasive issue of license inconsistencies in large
language models trained on code, our recommendation for both researchers and
the community is to prioritize the development and adoption of best practices
for dataset creation and management.",2024-03-22T14:23:21Z
,http://arxiv.org/pdf/2402.17988v1.pdf,"Constrained Decoding for Code Language Models via Efficient Left and
  Right Quotienting of Context-Sensitive Grammars","Large Language Models are powerful tools for program synthesis and advanced
auto-completion, but come with no guarantee that their output code is
syntactically correct. This paper contributes an incremental parser that allows
early rejection of syntactically incorrect code, as well as efficient detection
of complete programs for fill-in-the-middle (FItM) tasks. We develop
Earley-style parsers that operate over left and right quotients of arbitrary
context-free grammars, and we extend our incremental parsing and quotient
operations to several context-sensitive features present in the grammars of
many common programming languages. The result of these contributions is an
efficient, general, and well-grounded method for left and right quotient
parsing.
  To validate our theoretical contributions -- and the practical effectiveness
of certain design decisions -- we evaluate our method on the particularly
difficult case of FItM completion for Python 3. Our results demonstrate that
constrained generation can significantly reduce the incidence of syntax errors
in recommended code.",2024-02-28T02:12:47Z
10.1145/3576915.3623175,http://arxiv.org/pdf/2302.05319v4.pdf,"Large Language Models for Code: Security Hardening and Adversarial
  Testing","Large language models (large LMs) are increasingly trained on massive
codebases and used to generate code. However, LMs lack awareness of security
and are found to frequently produce unsafe code. This work studies the security
of LMs along two important axes: (i) security hardening, which aims to enhance
LMs' reliability in generating secure code, and (ii) adversarial testing, which
seeks to evaluate LMs' security at an adversarial standpoint. We address both
of these by formulating a new security task called controlled code generation.
The task is parametric and takes as input a binary property to guide the LM to
generate secure or unsafe code, while preserving the LM's capability of
generating functionally correct code. We propose a novel learning-based
approach called SVEN to solve this task. SVEN leverages property-specific
continuous vectors to guide program generation towards the given property,
without modifying the LM's weights. Our training procedure optimizes these
continuous vectors by enforcing specialized loss terms on different regions of
code, using a high-quality dataset carefully curated by us. Our extensive
evaluation shows that SVEN is highly effective in achieving strong security
control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters
generates secure code for 59.1% of the time. When we employ SVEN to perform
security hardening (or adversarial testing) on this LM, the ratio is
significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN
closely matches the original LMs in functional correctness.",2023-02-10T15:28:55Z
,http://arxiv.org/pdf/2308.11601v2.pdf,"Tryage: Real-time, intelligent Routing of User Prompts to Large Language
  Models","The introduction of the transformer architecture and the self-attention
mechanism has led to an explosive production of language models trained on
specific downstream tasks and data domains. With over 200, 000 models in the
Hugging Face ecosystem, users grapple with selecting and optimizing models to
suit multifaceted workflows and data domains while addressing computational,
security, and recency concerns. There is an urgent need for machine learning
frameworks that can eliminate the burden of model selection and customization
and unleash the incredible power of the vast emerging model library for end
users. Here, we propose a context-aware routing system, Tryage, that leverages
a language model router for optimal selection of expert models from a model
library based on analysis of individual input prompts. Inspired by the thalamic
router in the brain, Tryage employs a perceptive router to predict down-stream
model performance on prompts and, then, makes a routing decision using an
objective function that integrates performance predictions with user goals and
constraints that are incorporated through flags (e.g., model size, model
recency). Tryage allows users to explore a Pareto front and automatically
trade-off between task accuracy and secondary goals including minimization of
model size, recency, security, verbosity, and readability. Across heterogeneous
data sets that include code, text, clinical data, and patents, the Tryage
framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection
identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by
GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how
routing models can be applied to program and control the behavior of
multi-model LLM systems to maximize efficient use of the expanding and evolving
language model ecosystem.",2023-08-22T17:48:24Z
10.1021/jacs.3c05819,http://arxiv.org/pdf/2306.11296v2.pdf,"ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF
  Synthesis","We use prompt engineering to guide ChatGPT in the automation of text mining
of metal-organic frameworks (MOFs) synthesis conditions from diverse formats
and styles of the scientific literature. This effectively mitigates ChatGPT's
tendency to hallucinate information -- an issue that previously made the use of
Large Language Models (LLMs) in scientific fields challenging. Our approach
involves the development of a workflow implementing three different processes
for text mining, programmed by ChatGPT itself. All of them enable parsing,
searching, filtering, classification, summarization, and data unification with
different tradeoffs between labor, speed, and accuracy. We deploy this system
to extract 26,257 distinct synthesis parameters pertaining to approximately 800
MOFs sourced from peer-reviewed research articles. This process incorporates
our ChemPrompt Engineering strategy to instruct ChatGPT in text mining,
resulting in impressive precision, recall, and F1 scores of 90-99%.
Furthermore, with the dataset built by text mining, we constructed a
machine-learning model with over 86% accuracy in predicting MOF experimental
crystallization outcomes and preliminarily identifying important factors in MOF
crystallization. We also developed a reliable data-grounded MOF chatbot to
answer questions on chemical reactions and synthesis procedures. Given that the
process of using ChatGPT reliably mines and tabulates diverse MOF synthesis
information in a unified format, while using only narrative language requiring
no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be
very useful across various other chemistry sub-disciplines.",2023-06-20T05:20:29Z
,http://arxiv.org/pdf/2312.06942v3.pdf,AI Control: Improving Safety Despite Intentional Subversion,"As large language models (LLMs) become more powerful and are deployed more
autonomously, it will be increasingly important to prevent them from causing
harmful outcomes. Researchers have investigated a variety of safety techniques
for this purpose, e.g. using models to review the outputs of other models, or
red-teaming techniques to surface subtle failure modes. However, researchers
have not evaluated whether such techniques still ensure safety if the model is
itself intentionally trying to subvert them. In this paper, we develop and
evaluate pipelines of safety techniques (""protocols"") that are robust to
intentional subversion.
  We investigate a scenario in which we want to solve a sequence of programming
problems, using access to a powerful but untrusted model (in our case, GPT-4),
access to a less powerful trusted model (in our case, GPT-3.5), and limited
access to high-quality trusted labor. We investigate protocols that aim to
never submit solutions containing backdoors, which we operationalize here as
logical errors that are not caught by test cases.
  We investigate a range of protocols and test each against strategies that the
untrusted model could use to subvert them. One protocol is what we call trusted
editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to
rate the suspiciousness of that code. If the code is below some suspiciousness
threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove
parts that seem suspicious and then submits the edited code. Another protocol
is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks
another instance of GPT-4 whether the code is backdoored, using various
techniques to prevent the GPT-4 instances from colluding. These protocols
improve substantially on simple baselines.",2023-12-12T02:34:06Z
,http://arxiv.org/pdf/2406.15676v1.pdf,Inferring Pluggable Types with Machine Learning,"Pluggable type systems allow programmers to extend the type system of a
programming language to enforce semantic properties defined by the programmer.
Pluggable type systems are difficult to deploy in legacy codebases because they
require programmers to write type annotations manually. This paper investigates
how to use machine learning to infer type qualifiers automatically. We propose
a novel representation, NaP-AST, that encodes minimal dataflow hints for the
effective inference of type qualifiers. We evaluate several model architectures
for inferring type qualifiers, including Graph Transformer Network, Graph
Convolutional Network and Large Language Model. We further validated these
models by applying them to 12 open-source programs from a prior evaluation of
the NullAway pluggable typechecker, lowering warnings in all but one
unannotated project. We discovered that GTN shows the best performance, with a
recall of .89 and precision of 0.6. Furthermore, we conduct a study to estimate
the number of Java classes needed for good performance of the trained model.
For our feasibility study, performance improved around 16k classes, and
deteriorated due to overfitting around 22k classes.",2024-06-21T22:32:42Z
,http://arxiv.org/pdf/2303.04910v2.pdf,Baldur: Whole-Proof Generation and Repair with Large Language Models,"Formally verifying software properties is a highly desirable but
labor-intensive task. Recent work has developed methods to automate formal
verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by
training a model to predict one proof step at a time, and using that model to
search through the space of possible proofs. This paper introduces a new method
to automate formal verification: We use large language models, trained on
natural language text and code and fine-tuned on proofs, to generate whole
proofs for theorems at once, rather than one step at a time. We combine this
proof generation model with a fine-tuned repair model to repair generated
proofs, further increasing proving power. As its main contributions, this paper
demonstrates for the first time that: (1) Whole-proof generation using
transformers is possible and is as effective as search-based techniques without
requiring costly search. (2) Giving the learned model additional context, such
as a prior failed proof attempt and the ensuing error message, results in proof
repair and further improves automated proof generation. (3) We establish a new
state of the art for fully automated proof synthesis. We reify our method in a
prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL
theorems and their proofs. In addition to empirically showing the effectiveness
of whole-proof generation, repair, and added context, we show that Baldur
improves on the state-of-the-art tool, Thor, by automatically generating proofs
for an additional 8.7% of the theorems. Together, Baldur and Thor can prove
65.7% of the theorems fully automatically. This paper paves the way for new
research into using large language models for automating formal verification.",2023-03-08T22:00:15Z
,http://arxiv.org/pdf/2303.12093v3.pdf,ChatGPT for Programming Numerical Methods,"ChatGPT is a large language model recently released by the OpenAI company. In
this technical report, we explore for the first time the capability of ChatGPT
for programming numerical algorithms. Specifically, we examine the capability
of GhatGPT for generating codes for numerical algorithms in different
programming languages, for debugging and improving written codes by users, for
completing missed parts of numerical codes, rewriting available codes in other
programming languages, and for parallelizing serial codes. Additionally, we
assess if ChatGPT can recognize if given codes are written by humans or
machines. To reach this goal, we consider a variety of mathematical problems
such as the Poisson equation, the diffusion equation, the incompressible
Navier-Stokes equations, compressible inviscid flow, eigenvalue problems,
solving linear systems of equations, storing sparse matrices, etc. Furthermore,
we exemplify scientific machine learning such as physics-informed neural
networks and convolutional neural networks with applications to computational
physics. Through these examples, we investigate the successes, failures, and
challenges of ChatGPT. Examples of failures are producing singular matrices,
operations on arrays with incompatible sizes, programming interruption for
relatively long codes, etc. Our outcomes suggest that ChatGPT can successfully
program numerical algorithms in different programming languages, but certain
limitations and challenges exist that require further improvement of this
machine learning model.",2023-03-21T12:18:17Z
,http://arxiv.org/pdf/2306.06755v3.pdf,"CoTran: An LLM-based Code Translator using Reinforcement Learning with
  Feedback from Compiler and Symbolic Execution","In this paper, we present an LLM-based code translation method and an
associated tool called CoTran, that translates whole-programs from one
high-level programming language to another. Current LLM-based code translation
methods lack a training approach to ensure that the translated code reliably
compiles or bears substantial functional equivalence to the input code. In our
work, we train an LLM via reinforcement learning, by modifying the fine-tuning
process to incorporate compiler feedback and symbolic execution (symexec)-based
equivalence testing feedback that checks for functional equivalence between the
input and output programs. The idea is to guide an LLM-in-training, via
compiler and symexec-based testing feedback, by letting it know how far it is
from producing perfect translations. We report on extensive experiments
comparing CoTran with 14 other code translation tools that include
human-written transpilers, LLM-based translation tools, and ChatGPT over a
benchmark of more than 57,000 Java-Python equivalent pairs, and we show that
CoTran outperforms them on relevant metrics such as compilation accuracy
(CompAcc) and functional equivalence accuracy (FEqAcc). For example, our tool
achieves 48.68% FEqAcc, 76.98% CompAcc for Python-to-Java translation, whereas
the nearest competing tool (PLBART-base) only gets 38.26% and 75.77% resp.
Also, built upon CodeT5, CoTran achieves +11.23%, +14.89% improvement on FEqAcc
and +4.07%, +8.14% on CompAcc for Java-to-Python and Python-to-Java translation
resp.",2023-06-11T19:47:52Z
,http://arxiv.org/pdf/2312.11988v1.pdf,"Xpert: Empowering Incident Management with Query Recommendations via
  Large Language Models","Large-scale cloud systems play a pivotal role in modern IT infrastructure.
However, incidents occurring within these systems can lead to service
disruptions and adversely affect user experience. To swiftly resolve such
incidents, on-call engineers depend on crafting domain-specific language (DSL)
queries to analyze telemetry data. However, writing these queries can be
challenging and time-consuming. This paper presents a thorough empirical study
on the utilization of queries of KQL, a DSL employed for incident management in
a large-scale cloud management system at Microsoft. The findings obtained
underscore the importance and viability of KQL queries recommendation to
enhance incident management.
  Building upon these valuable insights, we introduce Xpert, an end-to-end
machine learning framework that automates KQL recommendation process. By
leveraging historical incident data and large language models, Xpert generates
customized KQL queries tailored to new incidents. Furthermore, Xpert
incorporates a novel performance metric called Xcore, enabling a thorough
evaluation of query quality from three comprehensive perspectives. We conduct
extensive evaluations of Xpert, demonstrating its effectiveness in offline
settings. Notably, we deploy Xpert in the real production environment of a
large-scale incident management system in Microsoft, validating its efficiency
in supporting incident management. To the best of our knowledge, this paper
represents the first empirical study of its kind, and Xpert stands as a
pioneering DSL query recommendation framework designed for incident management.",2023-12-19T09:30:58Z
,http://arxiv.org/pdf/2310.05686v1.pdf,"The potential of large language models for improving probability
  learning: A study on ChatGPT3.5 and first-year computer engineering students","In this paper, we assess the efficacy of ChatGPT (version Feb 2023), a
large-scale language model, in solving probability problems typically presented
in introductory computer engineering exams. Our study comprised a set of 23
probability exercises administered to students at Rey Juan Carlos University
(URJC) in Madrid. The responses produced by ChatGPT were evaluated by a group
of five statistics professors, who assessed them qualitatively and assigned
grades based on the same criteria used for students. Our results indicate that
ChatGPT surpasses the average student in terms of phrasing, organization, and
logical reasoning. The model's performance remained consistent for both the
Spanish and English versions of the exercises. However, ChatGPT encountered
difficulties in executing basic numerical operations. Our experiments
demonstrate that requesting ChatGPT to provide the solution in the form of an R
script proved to be an effective approach for overcoming these limitations. In
summary, our results indicate that ChatGPT surpasses the average student in
solving probability problems commonly presented in introductory computer
engineering exams. Nonetheless, the model exhibits limitations in reasoning
around certain probability concepts. The model's ability to deliver
high-quality explanations and illustrate solutions in any programming language,
coupled with its performance in solving probability exercises, suggests that
large language models have the potential to serve as learning assistants.",2023-10-09T12:54:58Z
,http://arxiv.org/pdf/2305.16849v1.pdf,"Green Runner: A tool for efficient model selection from model
  repositories","Deep learning models have become essential in software engineering, enabling
intelligent features like image captioning and document generation. However,
their popularity raises concerns about environmental impact and inefficient
model selection. This paper introduces GreenRunnerGPT, a novel tool for
efficiently selecting deep learning models based on specific use cases. It
employs a large language model to suggest weights for quality indicators,
optimizing resource utilization. The tool utilizes a multi-armed bandit
framework to evaluate models against target datasets, considering tradeoffs. We
demonstrate that GreenRunnerGPT is able to identify a model suited to a target
use case without wasteful computations that would occur under a brute-force
approach to model selection.",2023-05-26T12:00:37Z
,http://arxiv.org/pdf/2403.01632v2.pdf,SynCode: LLM Generation with Grammar Augmentation,"LLMs are widely used in complex AI applications. These applications
underscore the need for LLM outputs to adhere to a specific format, for their
integration with other components in the systems. Typically the format rules
e.g., for data serialization formats such as JSON, YAML, or Code in Programming
Language are expressed as context-free grammar (CFG). Due to the hallucinations
and unreliability of LLMs, instructing LLMs to adhere to specified syntax
becomes an increasingly important challenge.
  We present SynCode, a novel framework for efficient and general syntactical
decoding with LLMs, to address this challenge. SynCode leverages the CFG of a
formal language, utilizing an offline-constructed efficient lookup table called
DFA mask store based on the discrete finite automaton (DFA) of the language
grammar terminals. We demonstrate SynCode's soundness and completeness given
the CFG of the formal language, presenting its ability to retain syntactically
valid tokens while rejecting invalid ones. SynCode seamlessly integrates with
any language defined by CFG, as evidenced by experiments focusing on generating
JSON, Python, and Go outputs. Our experiments evaluating the effectiveness of
SynCode for JSON generation demonstrate that SynCode eliminates all syntax
errors and significantly outperforms state-of-the-art baselines. Furthermore,
our results underscore how SynCode significantly reduces 96.07% of syntax
errors in generated Python and Go code, showcasing its substantial impact on
enhancing syntactical precision in LLM generation. Our code is available at
https://github.com/uiuc-focal-lab/syncode",2024-03-03T22:38:35Z
,http://arxiv.org/pdf/2308.10022v2.pdf,"Cupid: Leveraging ChatGPT for More Accurate Duplicate Bug Report
  Detection","Duplicate bug report detection (DBRD) is a long-standing challenge in both
academia and industry. Over the past decades, researchers have proposed various
approaches to detect duplicate bug reports more accurately. With the recent
advancement of deep learning, researchers have also proposed several approaches
that leverage deep learning models to detect duplicate bug reports. A recent
benchmarking study on DBRD also reveals that the performance of deep
learning-based approaches is not always better than the traditional approaches.
However, traditional approaches have limitations, e.g., they are usually based
on the bag-of-words model, which cannot capture the semantics of bug reports.
To address these aforementioned challenges, we seek to leverage
state-of-the-art large language model to improve the performance of the
traditional DBRD approach.
  In this paper, we propose an approach called Cupid, which combines the
best-performing traditional DBRD approach REP with the state-of-the-art large
language model ChatGPT. Specifically, we first leverage ChatGPT under the
zero-shot setting to get essential information on bug reports. We then use the
essential information as the input of REP to detect duplicate bug reports. We
conducted an evaluation on comparing Cupid with three existing approaches on
three datasets. The experimental results show that Cupid achieves new
state-of-the-art results, reaching Recall Rate@10 scores ranging from 0.59 to
0.67 across all the datasets analyzed. Our work highlights the potential of
combining large language models to improve the performance of software
engineering tasks.",2023-08-19T14:16:30Z
,http://arxiv.org/pdf/2311.02640v1.pdf,"Assessing the Promise and Pitfalls of ChatGPT for Automated Code
  Generation","This paper presents a comprehensive evaluation of the code generation
capabilities of ChatGPT, a prominent large language model, compared to human
programmers. A novel dataset of 131 code-generation prompts across 5 categories
was curated to enable robust analysis. Code solutions were generated by both
ChatGPT and humans for all prompts, resulting in 262 code samples. A meticulous
manual assessment methodology prioritized evaluating correctness,
comprehensibility, and security using 14 established code quality metrics. The
key findings reveal ChatGPT's strengths in crafting concise, efficient code
with advanced constructs, showcasing strengths in data analysis tasks (93.1%
accuracy) but limitations in visual-graphical challenges. Comparative analysis
with human code highlights ChatGPT's inclination towards modular design and
superior error handling. Additionally, machine learning models effectively
distinguished ChatGPT from human code with up to 88% accuracy, suggesting
detectable coding style disparities. By providing profound insights into
ChatGPT's code generation capabilities and limitations through quantitative
metrics and qualitative analysis, this study makes valuable contributions
toward advancing AI-based programming assistants. The curated dataset and
methodology offer a robust foundation for future research in this nascent
domain. All data and codes are available on
https://github.com/DSAatUSU/ChatGPT-promises-and-pitfalls.",2023-11-05T12:56:40Z
,http://arxiv.org/pdf/2403.19114v1.pdf,"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval:
  Evolving Coding Benchmarks via LLM","LLMs have become the go-to choice for code generation tasks, with an
exponential increase in the training, development, and usage of LLMs
specifically for code generation. To evaluate the ability of LLMs on code, both
academic and industry practitioners rely on popular handcrafted benchmarks.
However, prior benchmarks contain only a very limited set of problems, both in
quantity and variety. Further, due to popularity and age, many benchmarks are
prone to data leakage where example solutions can be readily found on the web
and thus potentially in training data. Such limitations inevitably lead us to
inquire: Is the leaderboard performance on existing benchmarks reliable and
comprehensive enough to measure the program synthesis ability of LLMs? To
address this, we introduce EvoEval -- a program synthesis benchmark suite
created by evolving existing benchmarks into different targeted domains for a
comprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows
that compared to the high performance obtained on standard benchmarks like
HumanEval, there is a significant drop in performance (on average 39.4%) when
using EvoEval. Additionally, the decrease in performance can range from 19.6%
to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential
overfitting of existing benchmarks. Furthermore, we showcase various insights,
including the brittleness of instruction-following models when encountering
rewording or subtle changes as well as the importance of learning problem
composition and decomposition. EvoEval not only provides comprehensive
benchmarks, but can be used to further evolve arbitrary problems to keep up
with advances and the ever-changing landscape of LLMs for code. We have
open-sourced our benchmarks, tools, and complete LLM generations at
https://github.com/evo-eval/evoeval",2024-03-28T03:10:39Z
,http://arxiv.org/pdf/2310.03780v3.pdf,"Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4
  Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation","Generative AI and large language models hold great promise in enhancing
programming education by automatically generating individualized feedback for
students. We investigate the role of generative AI models in providing human
tutor-style programming hints to help students resolve errors in their buggy
programs. Recent works have benchmarked state-of-the-art models for various
feedback generation scenarios; however, their overall quality is still inferior
to human tutors and not yet ready for real-world deployment. In this paper, we
seek to push the limits of generative AI models toward providing high-quality
programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a
first step, our technique leverages GPT-4 as a ``tutor'' model to generate
hints -- it boosts the generative quality by using symbolic information of
failing test cases and fixes in prompts. As a next step, our technique
leverages GPT-3.5, a weaker model, as a ``student'' model to further validate
the hint quality -- it performs an automatic quality validation by simulating
the potential utility of providing this feedback. We show the efficacy of our
technique via extensive evaluation using three real-world datasets of Python
programs covering a variety of concepts ranging from basic algorithms to
regular expressions and data analysis using pandas library.",2023-10-05T17:02:59Z
,http://arxiv.org/pdf/2305.16837v1.pdf,"ChatGPT: A Study on its Utility for Ubiquitous Software Engineering
  Tasks","ChatGPT (Chat Generative Pre-trained Transformer) is a chatbot launched by
OpenAI on November 30, 2022. OpenAI's GPT-3 family of large language models
serve as the foundation for ChatGPT. ChatGPT is fine-tuned with both supervised
and reinforcement learning techniques and has received widespread attention for
its articulate responses across diverse domains of knowledge. In this study, we
explore how ChatGPT can be used to help with common software engineering tasks.
Many of the ubiquitous tasks covering the breadth of software engineering such
as ambiguity resolution in software requirements, method name suggestion, test
case prioritization, code review, log summarization can potentially be
performed using ChatGPT. In this study, we explore fifteen common software
engineering tasks using ChatGPT. We juxtapose and analyze ChatGPT's answers
with the respective state of the art outputs (where available) and/or human
expert ground truth. Our experiments suggest that for many tasks, ChatGPT does
perform credibly and the response from it is detailed and often better than the
human expert output or the state of the art output. However, for a few other
tasks, ChatGPT in its present form provides incorrect answers and hence is not
suited for such tasks.",2023-05-26T11:29:06Z
,http://arxiv.org/pdf/2303.18184v3.pdf,A Survey on Automated Program Repair Techniques,"With the rapid development and large-scale popularity of program software,
modern society increasingly relies on software systems. However, the problems
exposed by software have also come to the fore. Software defect has become an
important factor troubling developers. In this context, Automated Program
Repair (APR) techniques have emerged, aiming to automatically fix software
defect problems and reduce manual debugging work. In particular, benefiting
from the advances in deep learning, numerous learning-based APR techniques have
emerged in recent years, which also bring new opportunities for APR research.
To give researchers a quick overview of APR techniques' complete development
and future opportunities, we revisit the evolution of APR techniques and
discuss in depth the latest advances in APR research. In this paper, the
development of APR techniques is introduced in terms of four different patch
generation schemes: search-based, constraint-based, template-based, and
learning-based. Moreover, we propose a uniform set of criteria to review and
compare each APR tool, summarize the advantages and disadvantages of APR
techniques, and discuss the current state of APR development. Furthermore, we
introduce the research on the related technical areas of APR that have also
provided a strong motivation to advance APR development. Finally, we analyze
current challenges and future directions, especially highlighting the critical
opportunities that large language models bring to APR research.",2023-03-31T16:28:37Z
,http://arxiv.org/pdf/2405.00750v1.pdf,"From Keyboard to Chatbot: An AI-powered Integration Platform with
  Large-Language Models for Teaching Computational Thinking for Young Children","Teaching programming in early childhood (4-9) to enhance computational
thinking has gained popularity in the recent movement of computer science for
all. However, current practices ignore some fundamental issues resulting from
young children's developmental readiness, such as the sustained capability to
keyboarding, the decomposition of complex tasks to small tasks, the need for
intuitive mapping from abstract programming to tangible outcomes, and the
limited amount of screen time exposure. To address these issues in this paper,
we present a novel methodology with an AI-powered integration platform to
effectively teach computational thinking for young children. The system
features a hybrid pedagogy that supports both the top-down and bottom-up
approach for teaching computational thinking. Young children can describe their
desired task in natural language, while the system can respond with an
easy-to-understand program consisting of the right level of decomposed
sub-tasks. A tangible robot can immediately execute the decomposed program and
demonstrate the program's outcomes to young children. The system is equipped
with an intelligent chatbot that can interact with young children through
natural languages, and children can speak to the chatbot to complete all the
needed programming tasks, while the chatbot orchestrates the execution of the
program onto the robot. This would completely eliminates the need of keyboards
for young children to program. By developing such a system, we aim to make the
concept of computational thinking more accessible to young children, fostering
a natural understanding of programming concepts without the need of explicit
programming skills. Through the interactive experience provided by the robotic
agent, our system seeks to engage children in an effective manner, contributing
to the field of educational technology for early childhood computer science
education.",2024-05-01T04:29:21Z
,http://arxiv.org/pdf/2403.03344v1.pdf,"Learn to Code Sustainably: An Empirical Study on LLM-based Green Code
  Generation","The increasing use of information technology has led to a significant share
of energy consumption and carbon emissions from data centers. These
contributions are expected to rise with the growing demand for big data
analytics, increasing digitization, and the development of large artificial
intelligence (AI) models. The need to address the environmental impact of
software development has led to increased interest in green (sustainable)
coding and claims that the use of AI models can lead to energy efficiency
gains. Here, we provide an empirical study on green code and an overview of
green coding practices, as well as metrics used to quantify the sustainability
awareness of AI models. In this framework, we evaluate the sustainability of
auto-generated code. The auto-generate codes considered in this study are
produced by generative commercial AI language models, GitHub Copilot, OpenAI
ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to
quantify the sustainability awareness of these AI models, we propose a
definition of the code's ""green capacity"", based on certain sustainability
metrics. We compare the performance and green capacity of human-generated code
and code generated by the three AI language models in response to easy-to-hard
problem statements. Our findings shed light on the current capacity of AI
models to contribute to sustainable software development.",2024-03-05T22:12:01Z
,http://arxiv.org/pdf/2308.12095v2.pdf,"On Using Information Retrieval to Recommend Machine Learning Good
  Practices for Software Engineers","Machine learning (ML) is nowadays widely used for different purposes and in
several disciplines. From self-driving cars to automated medical diagnosis,
machine learning models extensively support users' daily activities, and
software engineering tasks are no exception. Not embracing good ML practices
may lead to pitfalls that hinder the performance of an ML system and
potentially lead to unexpected results. Despite the existence of documentation
and literature about ML best practices, many non-ML experts turn towards gray
literature like blogs and Q&A systems when looking for help and guidance when
implementing ML systems. To better aid users in distilling relevant knowledge
from such sources, we propose a recommender system that recommends ML practices
based on the user's context. As a first step in creating a recommender system
for machine learning practices, we implemented Idaka. A tool that provides two
different approaches for retrieving/generating ML best practices: i) an
information retrieval (IR) engine and ii) a large language model. The IR-engine
uses BM25 as the algorithm for retrieving the practices, and a large language
model, in our case Alpaca. The platform has been designed to allow comparative
studies of best practices retrieval tools. Idaka is publicly available at
GitHub: https://bit.ly/idaka. Video: https://youtu.be/cEb-AhIPxnM.",2023-08-23T12:28:18Z
,http://arxiv.org/pdf/2403.05538v2.pdf,"Explaining Code Examples in Introductory Programming Courses: LLM vs
  Humans","Worked examples, which present an explained code for solving typical
programming problems are among the most popular types of learning content in
programming classes. Most approaches and tools for presenting these examples to
students are based on line-by-line explanations of the example code. However,
instructors rarely have time to provide explanations for many examples
typically used in a programming class. In this paper, we assess the feasibility
of using LLMs to generate code explanations for passive and active example
exploration systems. To achieve this goal, we compare the code explanations
generated by chatGPT with the explanations generated by both experts and
students.",2023-12-09T01:06:08Z
,http://arxiv.org/pdf/2212.08108v3.pdf,"Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability
  Detection","Deep learning-based vulnerability detection has shown great performance and,
in some studies, outperformed static analysis tools. However, the
highest-performing approaches use token-based transformer models, which are not
the most efficient to capture code semantics required for vulnerability
detection. Classical program analysis techniques such as dataflow analysis can
detect many types of bugs based on their root causes. In this paper, we propose
to combine such causal-based vulnerability detection algorithms with deep
learning, aiming to achieve more efficient and effective vulnerability
detection. Specifically, we designed DeepDFA, a dataflow analysis-inspired
graph learning framework and an embedding technique that enables graph learning
to simulate dataflow computation. We show that DeepDFA is both performant and
efficient. DeepDFA outperformed all non-transformer baselines. It was trained
in 9 minutes, 75x faster than the highest-performing baseline model. When using
only 50+ vulnerable and several hundreds of total examples as training data,
the model retained the same performance as 100% of the dataset. DeepDFA also
generalized to real-world vulnerabilities in DbgBench; it detected 8.7 out of
17 vulnerabilities on average across folds and was able to distinguish between
patched and buggy versions, while the highest-performing baseline models did
not detect any vulnerabilities. By combining DeepDFA with a large language
model, we surpassed the state-of-the-art vulnerability detection performance on
the Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Our
replication package is located at https://doi.org/10.6084/m9.figshare.21225413 .",2022-12-15T19:49:27Z
,http://arxiv.org/pdf/2309.12813v2.pdf,Automatically Testing Functional Properties of Code Translation Models,"Large language models are becoming increasingly practical for translating
code across programming languages, a process known as $transpiling$. Even
though automated transpilation significantly boosts developer productivity, a
key concern is whether the generated code is correct. Existing work initially
used manually crafted test suites to test the translations of a small corpus of
programs; these test suites were later automated. In contrast, we devise the
first approach for automated, functional, property-based testing of code
translation models. Our general, user-provided specifications about the
transpiled code capture a range of properties, from purely syntactic to purely
semantic ones. As shown by our experiments, this approach is very effective in
detecting property violations in popular code translation models, and
therefore, in evaluating model quality with respect to given properties. We
also go a step further and explore the usage scenario where a user simply aims
to obtain a correct translation of some code with respect to certain properties
without necessarily being concerned about the overall quality of the model. To
this purpose, we develop the first property-guided search procedure for code
translation models, where a model is repeatedly queried with slightly different
parameters to produce alternative and potentially more correct translations.
Our results show that this search procedure helps to obtain significantly
better code translations.",2023-09-07T11:00:15Z
,http://arxiv.org/pdf/2306.17156v3.pdf,"Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4,
  and Human Tutors","Generative AI and large language models hold great promise in enhancing
computing education by powering next-generation educational technologies for
introductory programming. Recent works have studied these models for different
scenarios relevant to programming education; however, these works are limited
for several reasons, as they typically consider already outdated models or only
specific scenario(s). Consequently, there is a lack of a systematic study that
benchmarks state-of-the-art models for a comprehensive set of programming
education scenarios. In our work, we systematically evaluate two models,
ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human
tutors for a variety of scenarios. We evaluate using five introductory Python
programming problems and real-world buggy programs from an online platform, and
assess performance using expert-based annotations. Our results show that GPT-4
drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human
tutors' performance for several scenarios. These results also highlight
settings where GPT-4 still struggles, providing exciting future directions on
developing techniques to improve the performance of these models.",2023-06-29T17:57:40Z
,http://arxiv.org/pdf/2306.08058v1.pdf,"Few-shot learning for sentence pair classification and its applications
  in software engineering","Few-shot learning-the ability to train models with access to limited data-has
become increasingly popular in the natural language processing (NLP) domain, as
large language models such as GPT and T0 have been empirically shown to achieve
high performance in numerous tasks with access to just a handful of labeled
examples. Smaller language models such as BERT and its variants have also been
shown to achieve strong performance with just a handful of labeled examples
when combined with few-shot learning algorithms like pattern-exploiting
training (PET) and SetFit. The focus of this work is to investigate the
performance of alternative few-shot learning approaches with BERT-based models.
Specifically, vanilla fine-tuning, PET and SetFit are compared for numerous
BERT-based checkpoints over an array of training set sizes. To facilitate this
investigation, applications of few-shot learning are considered in software
engineering. For each task, high-performance techniques and their associated
model checkpoints are identified through detailed empirical analysis. Our
results establish PET as a strong few-shot learning approach, and our analysis
shows that with just a few hundred labeled examples it can achieve performance
near that of fine-tuning on full-sized data sets.",2023-06-13T18:23:52Z
10.1145/3597926.3598135,http://arxiv.org/pdf/2305.18607v2.pdf,How Effective Are Neural Networks for Fixing Security Vulnerabilities,"Security vulnerability repair is a difficult task that is in dire need of
automation. Two groups of techniques have shown promise: (1) large code
language models (LLMs) that have been pre-trained on source code for tasks such
as code completion, and (2) automated program repair (APR) techniques that use
deep learning (DL) models to automatically fix software bugs.
  This paper is the first to study and compare Java vulnerability repair
capabilities of LLMs and DL-based APR models. The contributions include that we
(1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder),
four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java
vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations
to address the training and test data overlapping threat to Codex, (3) create a
new Java vulnerability repair benchmark VJBench, and its transformed version
VJBench-trans and (4) evaluate LLMs and APR techniques on the transformed
vulnerabilities in VJBench-trans.
  Our findings include that (1) existing LLMs and APR models fix very few Java
vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities.
(2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing
capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix
many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing
cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes
8.3 transformed vulnerabilities, outperforming all the other LLMs and APR
models on transformed vulnerabilities. The results call for innovations to
enhance automated Java vulnerability repair such as creating larger
vulnerability repair training data, tuning LLMs with such data, and applying
code simplification transformation to facilitate vulnerability repair.",2023-05-29T20:50:27Z
,http://arxiv.org/pdf/2303.05771v1.pdf,Automating Method Naming with Context-Aware Prompt-Tuning,"Method names are crucial to program comprehension and maintenance. Recently,
many approaches have been proposed to automatically recommend method names and
detect inconsistent names. Despite promising, their results are still
sub-optimal considering the three following drawbacks: 1) These models are
mostly trained from scratch, learning two different objectives simultaneously.
The misalignment between two objectives will negatively affect training
efficiency and model performance. 2) The enclosing class context is not fully
exploited, making it difficult to learn the abstract function of the method. 3)
Current method name consistency checking methods follow a generate-then-compare
process, which restricts the accuracy as they highly rely on the quality of
generated names and face difficulty measuring the semantic consistency.
  In this paper, we propose an approach named AUMENA to AUtomate MEthod NAming
tasks with context-aware prompt-tuning. Unlike existing deep learning based
approaches, our model first learns the contextualized representation(i.e.,
class attributes) of PL and NL through the pre-training model, then fully
exploits the capacity and knowledge of large language model with prompt-tuning
to precisely detect inconsistent method names and recommend more accurate
names. To better identify semantically consistent names, we model the method
name consistency checking task as a two-class classification problem, avoiding
the limitation of previous similarity-based consistency checking approaches.
The experimental results reflect that AUMENA scores 68.6%, 72.0%, 73.6%, 84.7%
on four datasets of method name recommendation, surpassing the state-of-the-art
baseline by 8.5%, 18.4%, 11.0%, 12.0%, respectively. And our approach scores
80.8% accuracy on method name consistency checking, reaching an 5.5%
outperformance. All data and trained models are publicly available.",2023-03-10T08:14:13Z
,http://arxiv.org/pdf/2406.08467v1.pdf,DafnyBench: A Benchmark for Formal Software Verification,"We introduce DafnyBench, the largest benchmark of its kind for training and
evaluating machine learning systems for formal software verification. We test
the ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough hints
for the Dafny formal verification engine to successfully verify over 750
programs with about 53,000 lines of code. The best model and prompting scheme
achieved 68% success rate, and we quantify how this rate improves when retrying
with error message feedback and how it deteriorates with the amount of required
code and hints. We hope that DafnyBench will enable rapid improvements from
this baseline as LLMs and verification techniques grow in quality.",2024-06-12T17:53:31Z
,http://arxiv.org/pdf/2311.04926v1.pdf,"More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve
  Visually Diverse Images of Parsons Problems","The advent of large language models is reshaping computing education. Recent
research has demonstrated that these models can produce better explanations
than students, answer multiple-choice questions at or above the class average,
and generate code that can pass automated tests in introductory courses. These
capabilities have prompted instructors to rapidly adapt their courses and
assessment methods to accommodate changes in learning objectives and the
potential for academic integrity violations. While some scholars have advocated
for the integration of visual problems as a safeguard against the capabilities
of language models, new multimodal language models now have vision and language
capabilities that may allow them to analyze and solve visual problems. In this
paper, we evaluate the performance of two large multimodal models on visual
assignments, with a specific focus on Parsons problems presented across diverse
visual representations. Our results show that GPT-4V solved 96.7\% of these
visual problems, struggling minimally with a single Parsons problem.
Conversely, Bard performed poorly by only solving 69.2\% of problems,
struggling with common issues like hallucinations and refusals. These findings
suggest that merely transitioning to visual programming problems might not be a
panacea to issues of academic integrity in the generative AI era.",2023-11-03T14:47:17Z
,http://arxiv.org/pdf/2303.09128v2.pdf,"Exploring Distributional Shifts in Large Language Models for Code
  Analysis","We systematically study how three large language models with code
capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data.
We consider two fundamental applications - code summarization, and code
generation. We split data into domains following its natural boundaries - by an
organization, by a project, and by a module within the software project. We
establish that samples from each new domain present all the models with a
significant challenge of distribution shift. We study how established methods
adapt models to better generalize to new domains. Our experiments show that
while multitask learning alone is a reasonable baseline, combining it with
few-shot finetuning on examples retrieved from training data can achieve very
strong performance. Moreover, this solution can outperform direct finetuning
for very low-data scenarios. Finally, we consider variations of this approach
to create a more broadly applicable method to adapt to multiple domains at
once. We find that for code generation, a model adapted to multiple domains
simultaneously performs on par with those adapted to a single domain",2023-03-16T07:45:46Z
10.1145/3664646.3665664,http://arxiv.org/pdf/2405.13565v1.pdf,AI-Assisted Assessment of Coding Practices in Modern Code Review,"Modern code review is a process in which an incremental code contribution
made by a code author is reviewed by one or more peers before it is committed
to the version control system. An important element of modern code review is
verifying that code contributions adhere to best practices. While some of these
best practices can be automatically verified, verifying others is commonly left
to human reviewers. This paper reports on the development, deployment, and
evaluation of AutoCommenter, a system backed by a large language model that
automatically learns and enforces coding best practices. We implemented
AutoCommenter for four programming languages (C++, Java, Python, and Go) and
evaluated its performance and adoption in a large industrial setting. Our
evaluation shows that an end-to-end system for learning and enforcing coding
best practices is feasible and has a positive impact on the developer workflow.
Additionally, this paper reports on the challenges associated with deploying
such a system to tens of thousands of developers and the corresponding lessons
learned.",2024-05-22T11:57:18Z
,http://arxiv.org/pdf/2212.03404v1.pdf,Towards using Few-Shot Prompt Learning for Automating Model Completion,"We propose a simple yet a novel approach to improve completion in domain
modeling activities. Our approach exploits the power of large language models
by using few-shot prompt learning without the need to train or fine-tune those
models with large datasets that are scarce in this field. We implemented our
approach and tested it on the completion of static and dynamic domain diagrams.
Our initial evaluation shows that such an approach is effective and can be
integrated in different ways during the modeling activities.",2022-12-07T02:11:26Z
,http://arxiv.org/pdf/2304.05332v1.pdf,"Emergent autonomous scientific research capabilities of large language
  models","Transformer-based large language models are rapidly advancing in the field of
machine learning research, with applications spanning natural language,
biology, chemistry, and computer programming. Extreme scaling and reinforcement
learning from human feedback have significantly improved the quality of
generated text, enabling these models to perform various tasks and reason about
their choices. In this paper, we present an Intelligent Agent system that
combines multiple large language models for autonomous design, planning, and
execution of scientific experiments. We showcase the Agent's scientific
research capabilities with three distinct examples, with the most complex being
the successful performance of catalyzed cross-coupling reactions. Finally, we
discuss the safety implications of such systems and propose measures to prevent
their misuse.",2023-04-11T16:50:17Z
,http://arxiv.org/pdf/2308.00127v2.pdf,"DiviML: A Module-based Heuristic for Mapping Neural Networks onto
  Heterogeneous Platforms","Datacenters are increasingly becoming heterogeneous, and are starting to
include specialized hardware for networking, video processing, and especially
deep learning. To leverage the heterogeneous compute capability of modern
datacenters, we develop an approach for compiler-level partitioning of deep
neural networks (DNNs) onto multiple interconnected hardware devices. We
present a general framework for heterogeneous DNN compilation, offering
automatic partitioning and device mapping. Our scheduler integrates both an
exact solver, through a mixed integer linear programming (MILP) formulation,
and a modularity-based heuristic for scalability. Furthermore, we propose a
theoretical lower bound formula for the optimal solution, which enables the
assessment of the heuristic solutions' quality. We evaluate our scheduler in
optimizing both conventional DNNs and randomly-wired neural networks, subject
to latency and throughput constraints, on a heterogeneous system comprised of a
CPU and two distinct GPUs. Compared to na\""ively running DNNs on the fastest
GPU, he proposed framework can achieve more than 3$\times$ times lower latency
and up to 2.9$\times$ higher throughput by automatically leveraging both data
and model parallelism to deploy DNNs on our sample heterogeneous server node.
Moreover, our modularity-based ""splitting"" heuristic improves the solution
runtime up to 395$\times$ without noticeably sacrificing solution quality
compared to an exact MILP solution, and outperforms all other heuristics by
30-60% solution quality. Finally, our case study shows how we can extend our
framework to schedule large language models across multiple heterogeneous
servers by exploiting symmetry in the hardware setup. Our code can be easily
plugged in to existing frameworks, and is available at
https://github.com/abdelfattah-lab/diviml.",2023-07-31T19:46:49Z
10.1145/3663529.3663803,http://arxiv.org/pdf/2405.20551v1.pdf,EM-Assist: Safe Automated ExtractMethod Refactoring with LLMs,"Excessively long methods, loaded with multiple responsibilities, are
challenging to understand, debug, reuse, and maintain. The solution lies in the
widely recognized Extract Method refactoring. While the application of this
refactoring is supported in modern IDEs, recommending which code fragments to
extract has been the topic of many research tools. However, they often struggle
to replicate real-world developer practices, resulting in recommendations that
do not align with what a human developer would do in real life. To address this
issue, we introduce EM-Assist, an IntelliJ IDEA plugin that uses LLMs to
generate refactoring suggestions and subsequently validates, enhances, and
ranks them. Finally, EM-Assist uses the IntelliJ IDE to apply the user-selected
recommendation. In our extensive evaluation of 1,752 real-world refactorings
that actually took place in open-source projects, EM-Assist's recall rate was
53.4% among its top-5 recommendations, compared to 39.4% for the previous
best-in-class tool that relies solely on static analysis. Moreover, we
conducted a usability survey with 18 industrial developers and 94.4% gave a
positive rating.",2024-05-31T00:32:04Z
,http://arxiv.org/pdf/2401.17622v2.pdf,Commit Messages in the Age of Large Language Models,"Commit messages are explanations of changes made to a codebase that are
stored in version control systems. They help developers understand the codebase
as it evolves. However, writing commit messages can be tedious and inconsistent
among developers. To address this issue, researchers have tried using different
methods to automatically generate commit messages, including rule-based,
retrieval-based, and learning-based approaches. Advances in large language
models offer new possibilities for generating commit messages. In this study,
we evaluate the performance of OpenAI's ChatGPT for generating commit messages
based on code changes. We compare the results obtained with ChatGPT to previous
automatic commit message generation methods that have been trained specifically
on commit data. Our goal is to assess the extent to which large pre-trained
language models can generate commit messages that are both quantitatively and
qualitatively acceptable. We found that ChatGPT was able to outperform previous
Automatic Commit Message Generation (ACMG) methods by orders of magnitude, and
that, generally, the messages it generates are both accurate and of
high-quality. We also provide insights, and a categorization, for the cases
where it fails.",2024-01-31T06:47:12Z
,http://arxiv.org/pdf/2404.09249v1.pdf,"Test Code Generation for Telecom Software Systems using Two-Stage
  Generative Model","In recent years, the evolution of Telecom towards achieving intelligent,
autonomous, and open networks has led to an increasingly complex Telecom
Software system, supporting various heterogeneous deployment scenarios, with
multi-standard and multi-vendor support. As a result, it becomes a challenge
for large-scale Telecom software companies to develop and test software for all
deployment scenarios. To address these challenges, we propose a framework for
Automated Test Generation for large-scale Telecom Software systems. We begin by
generating Test Case Input data for test scenarios observed using a time-series
Generative model trained on historical Telecom Network data during field
trials. Additionally, the time-series Generative model helps in preserving the
privacy of Telecom data. The generated time-series software performance data
are then utilized with test descriptions written in natural language to
generate Test Script using the Generative Large Language Model. Our
comprehensive experiments on public datasets and Telecom datasets obtained from
operational Telecom Networks demonstrate that the framework can effectively
generate comprehensive test case data input and useful test code.",2024-04-14T13:25:15Z
,http://arxiv.org/pdf/2406.11915v1.pdf,miniCodeProps: a Minimal Benchmark for Proving Code Properties,"Neural networks have shown initial promise in automating mathematical theorem
proving in proof assistants such as Lean. The same proof assistants can be used
to verify the correctness of code by pairing code with specifications and
proofs that the specifications hold. Automating the writing of code,
specifications, and proofs could lower the cost of verification, or,
ambitiously, enable a machine learning system to output provably correct code.
However, it remains unclear whether current neural theorem provers can
automatically verify even relatively simple programs. We present miniCodeProps,
a benchmark of 177 program specifications in the Lean proof assistant, aimed at
the subproblem of automatically generating a proof for a provided program and
specification. miniCodeProps contains specifications about simple,
self-contained programs (e.g., lists, natural numbers, binary trees) with
varied proof difficulty. Despite its simplicity, miniCodeProps is challenging
for current LLM-based provers, which succeed in proving about 25 percent of the
specifications. We publicly release miniCodeProps as a benchmark for furthering
automated theorem proving in the context of formally verified code.",2024-06-16T21:11:23Z
,http://arxiv.org/pdf/2405.01560v1.pdf,Copyright related risks in the creation and use of ML/AI systems,"This paper summarizes the current copyright related risks that Machine
Learning (ML) and Artificial Intelligence (AI) systems (including Large
Language Models --LLMs) incur. These risks affect different stakeholders:
owners of the copyright of the training data, the users of ML/AI systems, the
creators of trained models, and the operators of AI systems. This paper also
provides an overview of ongoing legal cases in the United States related to
these risks.",2024-03-27T02:48:44Z
,http://arxiv.org/pdf/2312.10934v2.pdf,"APIDocBooster: An Extract-Then-Abstract Framework Leveraging Large
  Language Models for Augmenting API Documentation","API documentation is often the most trusted resource for programming. Many
approaches have been proposed to augment API documentation by summarizing
complementary information from external resources such as Stack Overflow.
Existing extractive-based summarization approaches excel in producing faithful
summaries that accurately represent the source content without input length
restrictions. Nevertheless, they suffer from inherent readability limitations.
On the other hand, our empirical study on the abstractive-based summarization
method, i.e., GPT-4, reveals that GPT-4 can generate coherent and concise
summaries but presents limitations in terms of informativeness and
faithfulness.
  We introduce APIDocBooster, an extract-then-abstract framework that
seamlessly fuses the advantages of both extractive (i.e., enabling faithful
summaries without length limitation) and abstractive summarization (i.e.,
producing coherent and concise summaries). APIDocBooster consists of two
stages: (1) \textbf{C}ontext-aware \textbf{S}entence \textbf{S}ection
\textbf{C}lassification (CSSC) and (2) \textbf{UP}date \textbf{SUM}marization
(UPSUM). CSSC classifies API-relevant information collected from multiple
sources into API documentation sections. UPSUM first generates extractive
summaries distinct from the original API documentation and then generates
abstractive summaries guided by extractive summaries through in-context
learning.
  To enable automatic evaluation of APIDocBooster, we construct the first
dataset for API document augmentation. Our automatic evaluation results reveal
that each stage in APIDocBooster outperforms its baselines by a large margin.
Our human evaluation also demonstrates the superiority of APIDocBooster over
GPT-4 and shows that it improves informativeness, relevance, and faithfulness
by 13.89\%, 15.15\%, and 30.56\%, respectively.",2023-12-18T05:15:50Z
,http://arxiv.org/pdf/2403.19913v1.pdf,"MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of
  Large Language Models","Large language models such as ChatGPT and GPT-4 have recently achieved
astonishing performance on a variety of natural language processing tasks. In
this paper, we propose MANGO, a benchmark to evaluate their capabilities to
perform text-based mapping and navigation. Our benchmark includes 53 mazes
taken from a suite of textgames: each maze is paired with a walkthrough that
visits every location but does not cover all possible paths. The task is
question-answering: for each maze, a large language model reads the walkthrough
and answers hundreds of mapping and navigation questions such as ""How should
you go to Attic from West of House?"" and ""Where are we if we go north and east
from Cellar?"". Although these questions are easy to humans, it turns out that
even GPT-4, the best-to-date language model, performs poorly at answering them.
Further, our experiments suggest that a strong mapping and navigation ability
would benefit large language models in performing relevant downstream tasks,
such as playing textgames. Our MANGO benchmark will facilitate future research
on methods that improve the mapping and navigation capabilities of language
models. We host our leaderboard, data, code, and evaluation program at
https://mango.ttic.edu and https://github.com/oaklight/mango/.",2024-03-29T01:53:24Z
,http://arxiv.org/pdf/2311.14904v1.pdf,LLM-Assisted Code Cleaning For Training Accurate Code Generators,"Natural language to code generation is an important application area of LLMs
and has received wide attention from the community. The majority of relevant
studies have exclusively concentrated on increasing the quantity and functional
correctness of training sets while disregarding other stylistic elements of
programs. More recently, data quality has garnered a lot of interest and
multiple works have showcased its importance for improving performance. In this
work, we investigate data quality for code and find that making the code more
structured and readable leads to improved code generation performance of the
system. We build a novel data-cleaning pipeline that uses these principles to
transform existing programs by 1.) renaming variables, 2.) modularizing and
decomposing complex code into smaller helper sub-functions, and 3.) inserting
natural-language based plans via LLM based transformations. We evaluate our
approach on two challenging algorithmic code generation benchmarks and find
that fine-tuning CodeLLaMa-7B on our transformed modularized programs improves
the performance by up to 30% compared to fine-tuning on the original dataset.
Additionally, we demonstrate improved performance from using a smaller amount
of higher-quality data, finding that a model fine-tuned on the entire original
dataset is outperformed by a model trained on 15% of our cleaned dataset. Even
in comparison to closed-source models, our models outperform the much larger
AlphaCoder models.",2023-11-25T02:45:50Z
,http://arxiv.org/pdf/2403.00806v1.pdf,"Enhanced User Interaction in Operating Systems through Machine Learning
  Language Models","With the large language model showing human-like logical reasoning and
understanding ability, whether agents based on the large language model can
simulate the interaction behavior of real users, so as to build a reliable
virtual recommendation A/B test scene to help the application of recommendation
research is an urgent, important and economic value problem. The combination of
interaction design and machine learning can provide a more efficient and
personalized user experience for products and services. This personalized
service can meet the specific needs of users and improve user satisfaction and
loyalty. Second, the interactive system can understand the user's views and
needs for the product by providing a good user interface and interactive
experience, and then use machine learning algorithms to improve and optimize
the product. This iterative optimization process can continuously improve the
quality and performance of the product to meet the changing needs of users. At
the same time, designers need to consider how these algorithms and tools can be
combined with interactive systems to provide a good user experience. This paper
explores the potential applications of large language models, machine learning
and interaction design for user interaction in recommendation systems and
operating systems. By integrating these technologies, more intelligent and
personalized services can be provided to meet user needs and promote continuous
improvement and optimization of products. This is of great value for both
recommendation research and user experience applications.",2024-02-24T12:17:06Z
,http://arxiv.org/pdf/2203.13474v5.pdf,"CodeGen: An Open Large Language Model for Code with Multi-Turn Program
  Synthesis","Program synthesis strives to generate a computer program as a solution to a
given problem specification, expressed with input-output examples or natural
language descriptions. The prevalence of large language models advances the
state-of-the-art for program synthesis, though limited training resources and
data impede open access to such models. To democratize this, we train and
release a family of large language models up to 16.1B parameters, called
CODEGEN, on natural language and programming language data, and open source the
training library JAXFORMER. We show the utility of the trained model by
demonstrating that it is competitive with the previous state-of-the-art on
zero-shot Python code generation on HumanEval. We further investigate the
multi-step paradigm for program synthesis, where a single program is factorized
into multiple prompts specifying subproblems. To this end, we construct an open
benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse
problem sets that are factorized into multi-turn prompts. Our analysis on MTPB
shows that the same intent provided to CODEGEN in multi-turn fashion
significantly improves program synthesis over that provided as a single turn.
We make the training library JAXFORMER and model checkpoints available as open
source contribution: https://github.com/salesforce/CodeGen.",2022-03-25T06:55:15Z
,http://arxiv.org/pdf/2401.08500v1.pdf,"Code Generation with AlphaCodium: From Prompt Engineering to Flow
  Engineering","Code generation problems differ from common natural language problems - they
require matching the exact syntax of the target language, identifying happy
paths and edge cases, paying attention to numerous small details in the problem
spec, and addressing other code-specific issues and requirements. Hence, many
of the optimizations and tricks that have been successful in natural language
generation may not be effective for code tasks. In this work, we propose a new
approach to code generation by LLMs, which we call AlphaCodium - a test-based,
multi-stage, code-oriented iterative flow, that improves the performances of
LLMs on code problems. We tested AlphaCodium on a challenging code generation
dataset called CodeContests, which includes competitive programming problems
from platforms such as Codeforces. The proposed flow consistently and
significantly improves results. On the validation set, for example, GPT-4
accuracy (pass@5) increased from 19% with a single well-designed direct prompt
to 44% with the AlphaCodium flow. Many of the principles and best practices
acquired in this work, we believe, are broadly applicable to general code
generation tasks. Full implementation is available at:
https://github.com/Codium-ai/AlphaCodium",2024-01-16T17:00:36Z
,http://arxiv.org/pdf/2405.20529v1.pdf,An Automatic Question Usability Evaluation Toolkit,"Evaluating multiple-choice questions (MCQs) involves either labor intensive
human assessments or automated methods that prioritize readability, often
overlooking deeper question design flaws. To address this issue, we introduce
the Scalable Automatic Question Usability Evaluation Toolkit (SAQUET), an
open-source tool that leverages the Item-Writing Flaws (IWF) rubric for a
comprehensive and automated quality evaluation of MCQs. By harnessing the
latest in large language models such as GPT-4, advanced word embeddings, and
Transformers designed to analyze textual complexity, SAQUET effectively
pinpoints and assesses a wide array of flaws in MCQs. We first demonstrate the
discrepancy between commonly used automated evaluation metrics and the human
assessment of MCQ quality. Then we evaluate SAQUET on a diverse dataset of MCQs
across the five domains of Chemistry, Statistics, Computer Science, Humanities,
and Healthcare, showing how it effectively distinguishes between flawed and
flawless questions, providing a level of analysis beyond what is achievable
with traditional metrics. With an accuracy rate of over 94% in detecting the
presence of flaws identified by human evaluators, our findings emphasize the
limitations of existing evaluation methods and showcase potential in improving
the quality of educational assessments.",2024-05-30T23:04:53Z
,http://arxiv.org/pdf/2201.07406v2.pdf,Fooling MOSS Detection with Pretrained Language Models,"As artificial intelligence (AI) technologies become increasingly powerful and
prominent in society, their misuse is a growing concern. In educational
settings, AI technologies could be used by students to cheat on assignments and
exams. In this paper we explore whether transformers can be used to solve
introductory level programming assignments while bypassing commonly used AI
tools to detect similarities between pieces of software. We find that a student
using GPT-J [Wang and Komatsuzaki, 2021] can complete introductory level
programming assignments without triggering suspicion from MOSS [Aiken, 2000], a
widely used software similarity and plagiarism detection tool. This holds
despite the fact that GPT-J was not trained on the problems in question and is
not provided with any examples to work from. We further find that the code
written by GPT-J is diverse in structure, lacking any particular tells that
future plagiarism detection techniques may use to try to identify
algorithmically generated code. We conclude with a discussion of the ethical
and educational implications of large language models and directions for future
research.",2022-01-19T04:00:46Z
,http://arxiv.org/pdf/2303.05510v1.pdf,Planning with Large Language Models for Code Generation,"Existing large language model-based code generation pipelines typically use
beam search or sampling algorithms during the decoding process. Although the
programs they generate achieve high token-matching-based scores, they often
fail to compile or generate incorrect outputs. The main reason is that
conventional Transformer decoding algorithms may not be the best choice for
code generation. In this work, we propose a novel Transformer decoding
algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning
algorithm to do lookahead search and guide the Transformer to generate better
programs. Specifically, instead of simply optimizing the likelihood of the
generated sequences, the Transformer makes use of a planner to generate
candidate programs and test them on public test cases. The Transformer can
therefore make more informed decisions and generate tokens that will eventually
lead to higher-quality programs. We also design a mechanism that shares
information between the Transformer and the planner to make our algorithm
computationally efficient. We empirically evaluate our framework with several
large language models as backbones on public coding challenge benchmarks,
showing that 1) it can generate programs that consistently achieve higher
performance compared with competing baseline methods; 2) it enables
controllable code generation, such as concise codes and highly-commented codes
by optimizing modified objective.",2023-03-09T18:59:47Z
,http://arxiv.org/pdf/2406.11104v1.pdf,"Beyond the Hype: A Cautionary Tale of ChatGPT in the Programming
  Classroom","Due to the proliferation of Large Language Models research and the use of
various Artificial Intelligence (AI) tools, the field of information systems
(IS) and computer science (CS) has evolved. The use of tools such as ChatGPT to
complete various student programming exercises (e.g., in Python) and
assignments has gained prominence amongst various academic institutions.
However, recent literature has suggested that the use of ChatGPT in academia is
problematic and the impact on teaching and learning should be further
scrutinized. More specifically, little is known about how ChatGPT can be
practically used with code (programming) writing to complete programming
exercises amongst IS and CS undergraduate university students. Furthermore, the
paper provides insights for academics who teach programming to create more
challenging exercises and how to engage responsibly in the use of ChatGPT to
promote classroom integrity. In this paper, we used Complex Adaptive Systems
(CAS) theory as a theoretical guide to understand the various dynamics through
classroom code demonstrations. Using ChatGPT 3.5, we analyzed the various
practical programming examples from past IS exercises and compared those with
memos created by tutors and lecturers in a university setting. This paper
highlights common ways of assessment, programming errors created by ChatGPT and
the potential consideration for IS academics to ensure the development of
critical programming skills among students.",2024-06-16T23:52:37Z
,http://arxiv.org/pdf/2303.17012v3.pdf,Advances in apparent conceptual physics reasoning in GPT-4,"ChatGPT is built on a large language model trained on an enormous corpus of
human text to emulate human conversation. Despite lacking any explicit
programming regarding the laws of physics, recent work has demonstrated that
GPT-3.5 could pass an introductory physics course at some nominal level and
register something close to a minimal understanding of Newtonian Mechanics on
the Force Concept Inventory. This work replicates those results and also
demonstrates that the latest version, GPT-4, has reached a much higher mark in
the latter context. Indeed, its responses come quite close to perfectly
demonstrating expert-level competence, with a few very notable exceptions and
limitations. We briefly comment on the implications of this for the future of
physics education and pedagogy.",2023-03-29T20:32:40Z
,http://arxiv.org/pdf/2307.07367v1.pdf,"Are Large Language Models a Threat to Digital Public Goods? Evidence
  from Activity on Stack Overflow","Large language models like ChatGPT efficiently provide users with information
about various topics, presenting a potential substitute for searching the web
and asking people for help online. But since users interact privately with the
model, these models may drastically reduce the amount of publicly available
human-generated data and knowledge resources. This substitution can present a
significant problem in securing training data for future models. In this work,
we investigate how the release of ChatGPT changed human-generated open data on
the web by analyzing the activity on Stack Overflow, the leading online Q\&A
platform for computer programming. We find that relative to its Russian and
Chinese counterparts, where access to ChatGPT is limited, and to similar forums
for mathematics, where ChatGPT is less capable, activity on Stack Overflow
significantly decreased. A difference-in-differences model estimates a 16\%
decrease in weekly posts on Stack Overflow. This effect increases in magnitude
over time, and is larger for posts related to the most widely used programming
languages. Posts made after ChatGPT get similar voting scores than before,
suggesting that ChatGPT is not merely displacing duplicate or low-quality
content. These results suggest that more users are adopting large language
models to answer questions and they are better substitutes for Stack Overflow
for languages for which they have more training data. Using models like ChatGPT
may be more efficient for solving certain programming problems, but its
widespread adoption and the resulting shift away from public exchange on the
web will limit the open data people and models can learn from in the future.",2023-07-14T14:22:12Z
,http://arxiv.org/pdf/2307.09163v1.pdf,Generative Type Inference for Python,"Python is a popular dynamic programming language, evidenced by its ranking as
the second most commonly used language on GitHub. However, its dynamic type
system can lead to potential type errors, leading researchers to explore
automatic type inference approaches for Python programs. The rule-based type
inference approaches can ensure the accuracy of predicted variable types, but
they suffer from low coverage problems. Supervised type inference approaches,
while feature-agnostic, require large, high-quality annotated datasets and are
limited to pre-defined types. As zero-shot approaches, the cloze-style
approaches reformulate the type inference problem into a fill-in-the-blank
problem. However, their performance is limited.
  This paper introduces TypeGen, a few-shot generative type inference approach
that incorporates static domain knowledge from static analysis. TypeGen creates
chain-of-thought (COT) prompts by translating the type inference steps of
static analysis into prompts based on the type dependency graphs (TDGs),
enabling language models to learn from how static analysis infers types. By
combining COT prompts with code slices and type hints, TypeGen constructs
example prompts from human annotations. TypeGen only requires very few
annotated examples to teach language models to generate similar COT prompts via
in-context learning. Moreover, TypeGen enhances the interpretability of results
through the use of the input-explanation-output strategy. Experiments show that
TypeGen outperforms the best baseline Type4Py by 10.0% for argument type
prediction and 22.5% in return value type prediction in terms of top-1 Exact
Match by using only five examples. Furthermore, TypeGen achieves substantial
improvements of 27% to 84% compared to the zero-shot performance of large
language models with parameter sizes ranging from 1.3B to 175B in terms of
top-1 Exact Match.",2023-07-18T11:40:31Z
,http://arxiv.org/pdf/2310.04353v4.pdf,An In-Context Learning Agent for Formal Theorem-Proving,"We present an in-context learning agent for formal theorem-proving in
environments like Lean and Coq. Current state-of-the-art models for the problem
are finetuned on environment-specific proof data. By contrast, our approach,
called COPRA, repeatedly asks a high-capacity, general-purpose large language
model (GPT-4) to propose tactic applications from within a stateful
backtracking search. Proposed tactics are executed in the underlying proof
environment. Feedback from the execution is used to build the prompt for the
next model query, along with selected information from the search history and
lemmas retrieved from an external database. We evaluate our implementation of
COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the
CompCert project. On these benchmarks, COPRA significantly outperforms few-shot
invocations of GPT-4. It also compares favorably against finetuning-based
approaches, outperforming REPROVER, a state-of-the-art finetuned approach for
Lean, in terms of the pass@1 metric. Our code and data are available at
https://github.com/trishullab/copra.",2023-10-06T16:21:22Z
,http://arxiv.org/pdf/2309.07062v1.pdf,Large Language Models for Compiler Optimization,"We explore the novel application of Large Language Models to code
optimization. We present a 7B-parameter transformer model trained from scratch
to optimize LLVM assembly for code size. The model takes as input unoptimized
assembly and outputs a list of compiler options to best optimize the program.
Crucially, during training, we ask the model to predict the instruction counts
before and after optimization, and the optimized code itself. These auxiliary
learning tasks significantly improve the optimization performance of the model
and improve the model's depth of understanding.
  We evaluate on a large suite of test programs. Our approach achieves a 3.0%
improvement in reducing instruction counts over the compiler, outperforming two
state-of-the-art baselines that require thousands of compilations. Furthermore,
the model shows surprisingly strong code reasoning abilities, generating
compilable code 91% of the time and perfectly emulating the output of the
compiler 70% of the time.",2023-09-11T22:11:46Z
