{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e13d1249-c211-483d-87a4-7d49311efeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching papers: 230paper [00:06, 35.75paper/s]\n",
      "Processing papers: 100%|██████████| 230/230 [03:24<00:00,  1.12paper/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction complete. 193 papers extracted. Check the Scopus.csv file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Scopus API key\n",
    "API_KEY = 'a17167505f5d6799ad4cf9c9f28de7f1'\n",
    "\n",
    "# Search query\n",
    "query = 'TITLE-ABS-KEY((\"software engineering\" OR \"programming\" OR \"software development\" OR \"computer science\" OR \"computer engineering\") AND (\"education\" OR \"teaching\") AND (\"LLM\" OR \"large language model\"))'\n",
    "\n",
    "# Base URL for Scopus API\n",
    "base_url = \"https://api.elsevier.com/content/search/scopus\"\n",
    "\n",
    "# Headers for the API request\n",
    "headers = {\n",
    "    'X-ELS-APIKey': API_KEY,\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# Parameters for the API request\n",
    "params = {\n",
    "    'query': query,\n",
    "    'view': 'STANDARD',\n",
    "    'start': 0,\n",
    "    'count': 25  # Initial count to handle pagination\n",
    "}\n",
    "\n",
    "# Function to get metadata\n",
    "def get_metadata(base_url, params, headers):\n",
    "    all_data = []\n",
    "    pbar = tqdm(desc=\"Fetching papers\", unit=\"paper\")\n",
    "    while True:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        entries = data['search-results']['entry']\n",
    "        all_data.extend(entries)\n",
    "        pbar.update(len(entries))\n",
    "        # Check if there's a next page link\n",
    "        if any(link['@ref'] == 'next' for link in data['search-results']['link']):\n",
    "            params['start'] += params['count']\n",
    "        else:\n",
    "            break\n",
    "    pbar.close()\n",
    "    return all_data\n",
    "\n",
    "# Function to parse the metadata\n",
    "def parse_metadata(entries):\n",
    "    papers_metadata = []\n",
    "    for entry in tqdm(entries, desc=\"Processing papers\", unit=\"paper\"):\n",
    "        eid = entry['eid']\n",
    "        doi = entry.get('prism:doi', 'N/A')\n",
    "        if doi == 'N/A':\n",
    "            continue  # Skip entries without a DOI\n",
    "        title = entry.get('dc:title')\n",
    "        url = next((link['@href'] for link in entry.get('link', []) if link['@ref'] == 'scopus'), 'N/A')\n",
    "        abstract, bibtex, year = get_abstract_bibtex(eid, url, title)\n",
    "        metadata = {\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'doi': doi,\n",
    "            'abstract': abstract,\n",
    "            'year': year,\n",
    "            'num_pages': calculate_num_pages(entry.get('prism:pageRange', 'N/A')),\n",
    "            'paper_type': entry.get('subtypeDescription', 'N/A'),\n",
    "            'bibtex': bibtex\n",
    "        }\n",
    "        papers_metadata.append(metadata)\n",
    "    return papers_metadata\n",
    "\n",
    "# Function to get abstract, bibtex, and year using BeautifulSoup and regex\n",
    "def get_abstract_bibtex(eid, url, title):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    content = response.text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Extract abstract using regex\n",
    "    abstract = 'N/A'\n",
    "    abstract_match = re.search(r'abstractSection.*?>(.*?)</section>', content, re.DOTALL)\n",
    "    if abstract_match:\n",
    "        abstract = re.sub('<.*?>', '', abstract_match.group(1)).strip()\n",
    "    \n",
    "    # Extract authors\n",
    "    authors = []\n",
    "    authors_match = soup.find_all('a', {'class': 'authorName'})\n",
    "    if authors_match:\n",
    "        authors = [author.get_text(strip=True) for author in authors_match]\n",
    "    author_list = ', '.join(authors) if authors else 'N/A'\n",
    "    \n",
    "    # Extract year using BeautifulSoup\n",
    "    year = 'N/A'\n",
    "    journal_info = soup.find('span', {'id': 'journalInfo'})\n",
    "    if journal_info:\n",
    "        year_match = re.search(r'\\b\\d{4}\\b', journal_info.get_text())\n",
    "        if year_match:\n",
    "            year = year_match.group(0)\n",
    "    \n",
    "    # Construct bibtex\n",
    "    bibtex = f\"@article{{{eid.replace(':', '_')},\\n\" \\\n",
    "             f\"  title={{{title}}},\\n\" \\\n",
    "             f\"  author={{{author_list}}},\\n\" \\\n",
    "             f\"  journal={{N/A}},\\n\" \\\n",
    "             f\"  year={{{year}}},\\n\" \\\n",
    "             f\"  volume={{N/A}},\\n\" \\\n",
    "             f\"  pages={{N/A}},\\n\" \\\n",
    "             f\"  doi={{N/A}}\\n\" \\\n",
    "             f\"}}\"\n",
    "    return abstract, bibtex, year\n",
    "\n",
    "# Function to calculate number of pages from page range\n",
    "def calculate_num_pages(page_range):\n",
    "    if page_range and '-' in page_range:\n",
    "        start, end = page_range.split('-')\n",
    "        return int(end) - int(start) + 1\n",
    "    return 4  # Default to 4 pages if page range is not available\n",
    "\n",
    "# Get metadata\n",
    "entries = get_metadata(base_url, params, headers)\n",
    "papers_metadata = parse_metadata(entries)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(papers_metadata)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('Scopus.csv', index=False)\n",
    "\n",
    "print(f\"Metadata extraction complete. {len(papers_metadata)} papers extracted. Check the Scopus.csv file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d9ec8-ff6b-4b86-8e98-041e4d6615ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
